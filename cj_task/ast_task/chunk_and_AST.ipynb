{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 Cangjie files in ../../dataset/cangjie.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  95%|█████████▌| 190/200 [05:40<00:42,  4.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: <!DOCTYPE html>\n",
      "<!--[if lt IE 7]> <html class=\"no-js ie6 oldie\" lang=\"en-US\"> <![endif]-->\n",
      "<!--[if IE 7]>    <html class=\"no-js ie7 oldie\" lang=\"en-US\"> <![endif]-->\n",
      "<!--[if IE 8]>    <html class=\"no-js ie8 oldie\" lang=\"en-US\"> <![endif]-->\n",
      "<!--[if gt IE 8]><!--> <html class=\"no-js\" lang=\"en-US\"> <!--<![endif]-->\n",
      "<head>\n",
      "\n",
      "\n",
      "<title>xiaoai.plus | 524: A timeout occurred</title>\n",
      "<meta charset=\"UTF-8\" />\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "<meta name=\"robots\" content=\"noindex, nofollow\" />\n",
      "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
      "<link rel=\"stylesheet\" id=\"cf_styles-css\" href=\"/cdn-cgi/styles/main.css\" />\n",
      "\n",
      "\n",
      "</head>\n",
      "<body>\n",
      "<div id=\"cf-wrapper\">\n",
      "    <div id=\"cf-error-details\" class=\"p-0\">\n",
      "        <header class=\"mx-auto pt-10 lg:pt-6 lg:px-8 w-240 lg:w-full mb-8\">\n",
      "            <h1 class=\"inline-block sm:block sm:mb-2 font-light text-60 lg:text-4xl text-black-dark leading-tight mr-2\">\n",
      "              <span class=\"inline-block\">A timeout occurred</span>\n",
      "              <span class=\"code-label\">Error code 524</span>\n",
      "            </h1>\n",
      "            <div>\n",
      "               Visit <a href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_524&utm_campaign=xiaoai.plus\" target=\"_blank\" rel=\"noopener noreferrer\">cloudflare.com</a> for more information.\n",
      "            </div>\n",
      "            <div class=\"mt-3\">2025-01-19 06:49:00 UTC</div>\n",
      "        </header>\n",
      "        <div class=\"my-8 bg-gradient-gray\">\n",
      "            <div class=\"w-240 lg:w-full mx-auto\">\n",
      "                <div class=\"clearfix md:px-8\">\n",
      "                  \n",
      "<div id=\"cf-browser-status\" class=\" relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\">\n",
      "  <div class=\"relative mb-10 md:m-0\">\n",
      "    \n",
      "    <span class=\"cf-icon-browser block md:hidden h-20 bg-center bg-no-repeat\"></span>\n",
      "    <span class=\"cf-icon-ok w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"></span>\n",
      "    \n",
      "  </div>\n",
      "  <span class=\"md:block w-full truncate\">You</span>\n",
      "  <h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\">\n",
      "    \n",
      "    Browser\n",
      "    \n",
      "  </h3>\n",
      "  <span class=\"leading-1.3 text-2xl text-green-success\">Working</span>\n",
      "</div>\n",
      "\n",
      "<div id=\"cf-cloudflare-status\" class=\" relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\">\n",
      "  <div class=\"relative mb-10 md:m-0\">\n",
      "    <a href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_524&utm_campaign=xiaoai.plus\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
      "    <span class=\"cf-icon-cloud block md:hidden h-20 bg-center bg-no-repeat\"></span>\n",
      "    <span class=\"cf-icon-ok w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"></span>\n",
      "    </a>\n",
      "  </div>\n",
      "  <span class=\"md:block w-full truncate\">London</span>\n",
      "  <h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\">\n",
      "    <a href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_524&utm_campaign=xiaoai.plus\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
      "    Cloudflare\n",
      "    </a>\n",
      "  </h3>\n",
      "  <span class=\"leading-1.3 text-2xl text-green-success\">Working</span>\n",
      "</div>\n",
      "\n",
      "<div id=\"cf-host-status\" class=\"cf-error-source relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\">\n",
      "  <div class=\"relative mb-10 md:m-0\">\n",
      "    \n",
      "    <span class=\"cf-icon-server block md:hidden h-20 bg-center bg-no-repeat\"></span>\n",
      "    <span class=\"cf-icon-error w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"></span>\n",
      "    \n",
      "  </div>\n",
      "  <span class=\"md:block w-full truncate\">xiaoai.plus</span>\n",
      "  <h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\">\n",
      "    \n",
      "    Host\n",
      "    \n",
      "  </h3>\n",
      "  <span class=\"leading-1.3 text-2xl text-red-error\">Error</span>\n",
      "</div>\n",
      "\n",
      "                </div>\n",
      "            </div>\n",
      "        </div>\n",
      "\n",
      "        <div class=\"w-240 lg:w-full mx-auto mb-8 lg:px-8\">\n",
      "            <div class=\"clearfix\">\n",
      "                <div class=\"w-1/2 md:w-full float-left pr-6 md:pb-10 md:pr-0 leading-relaxed\">\n",
      "                    <h2 class=\"text-3xl font-normal leading-1.3 mb-4\">What happened?</h2>\n",
      "                    <p>The origin web server timed out responding to this request.</p>\n",
      "                </div>\n",
      "                <div class=\"w-1/2 md:w-full float-left leading-relaxed\">\n",
      "                    <h2 class=\"text-3xl font-normal leading-1.3 mb-4\">What can I do?</h2>\n",
      "                          <h3 class=\"text-15 font-semibold mb-2\">If you're a visitor of this website:</h3>\n",
      "      <p class=\"mb-6\">Please try again in a few minutes.</p>\n",
      "\n",
      "      <h3 class=\"text-15 font-semibold mb-2\">If you're the owner of this website:</h3>\n",
      "      <p><span>The connection to the origin web server was made, but the origin web server timed out before responding. The likely cause is an overloaded background task, database or application, stressing the resources on your web server. To resolve, please work with your hosting provider or web development team to free up resources for your database or overloaded application.</span> <a rel=\"noopener noreferrer\" href=\"https://support.cloudflare.com/hc/en-us/articles/200171926-Error-524\">Additional troubleshooting information here.</a></p>\n",
      "                </div>\n",
      "            </div>\n",
      "        </div>\n",
      "\n",
      "        <div class=\"cf-error-footer cf-wrapper w-240 lg:w-full py-10 sm:py-4 sm:px-8 mx-auto text-center sm:text-left border-solid border-0 border-t border-gray-300\">\n",
      "  <p class=\"text-13\">\n",
      "    <span class=\"cf-footer-item sm:block sm:mb-1\">Cloudflare Ray ID: <strong class=\"font-semibold\">9044df6edf7b9489</strong></span>\n",
      "    <span class=\"cf-footer-separator sm:hidden\">&bull;</span>\n",
      "    <span id=\"cf-footer-item-ip\" class=\"cf-footer-item hidden sm:block sm:mb-1\">\n",
      "      Your IP:\n",
      "      <button type=\"button\" id=\"cf-footer-ip-reveal\" class=\"cf-footer-ip-reveal-btn\">Click to reveal</button>\n",
      "      <span class=\"hidden\" id=\"cf-footer-ip\">124.127.236.111</span>\n",
      "      <span class=\"cf-footer-separator sm:hidden\">&bull;</span>\n",
      "    </span>\n",
      "    <span class=\"cf-footer-item sm:block sm:mb-1\"><span>Performance &amp; security by</span> <a rel=\"noopener noreferrer\" href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_524&utm_campaign=xiaoai.plus\" id=\"brand_link\" target=\"_blank\">Cloudflare</a></span>\n",
      "    \n",
      "  </p>\n",
      "  <script>(function(){function d(){var b=a.getElementById(\"cf-footer-item-ip\"),c=a.getElementById(\"cf-footer-ip-reveal\");b&&\"classList\"in b&&(b.classList.remove(\"hidden\"),c.addEventListener(\"click\",function(){c.classList.add(\"hidden\");a.getElementById(\"cf-footer-ip\").classList.remove(\"hidden\")}))}var a=document;document.addEventListener&&a.addEventListener(\"DOMContentLoaded\",d)})();</script>\n",
      "</div><!-- /.error-footer -->\n",
      "\n",
      "\n",
      "    </div>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  96%|█████████▌| 192/200 [07:49<01:33, 11.67s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-threaded script for Python code analysis:\n",
    "\n",
    "1) LLM-based AST generation (via CFG + partial block approach):\n",
    "   - parse_cfg_structure() => get line ranges of classes/functions\n",
    "   - build_ast_from_cfg() => recursively exclude child function/class lines from the parent block,\n",
    "     parse only the remaining lines, then insert function/class placeholders back in the correct position.\n",
    "   - Each node has global-level start_token/end_token, thanks to a single global tokenize_code_with_lines().\n",
    "\n",
    "2) Tree-sitter-based static AST:\n",
    "   - generate_tree_sitter_ast() => returns {type, label, children}.\n",
    "\n",
    "3) Compare snippet-level labels (optional).\n",
    "\n",
    "4) Save both ASTs as JSON.\n",
    "\n",
    "5) Multi-file parallel processing with ThreadPoolExecutor.\n",
    "\n",
    "See the \"llm_build_ast_from_tokens\" function's prompt – we keep it intact as requested.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import re\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "###############################################################################\n",
    "#                           LLM interface (stub)                              #\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Replace 'get_llm_answers' with your actual LLM API call or function.\n",
    "Here we just provide a stub or minimal placeholder.\n",
    "\"\"\"\n",
    "try:\n",
    "    from llm import get_llm_answers\n",
    "except ImportError:\n",
    "    # If no llm.py found, define a placeholder\n",
    "    def get_llm_answers(prompt, model_name=\"\", require_json=False, temperature=0):\n",
    "        # Return a minimal JSON for demonstration\n",
    "        # In reality, you'd implement the actual LLM call (OpenAI, local model, etc.)\n",
    "        return \"{}\"\n",
    "\n",
    "###############################################################################\n",
    "#                   1) 全局分词，含行号 -> global_tokens                       #\n",
    "###############################################################################\n",
    "def tokenize_code_with_lines(code: str) -> List[Tuple[int, int, str, int]]:\n",
    "    \"\"\"\n",
    "    使用正则分词，直接在原始代码上通过 finditer() 获取匹配位置，\n",
    "    并保留换行符，让 LLM 能识别多行结构。\n",
    "    返回形式: [(start_offset, end_offset, token_text, line_number), ...]\n",
    "    \"\"\"\n",
    "    token_pattern = (\n",
    "        r'[A-Za-z_]\\w*|'  # 标识符\n",
    "        r'[0-9]+|'        # 数字\n",
    "        r'\"[^\"]*\"|'       # 双引号字符串\n",
    "        r\"'[^']*'|\"       # 单引号字符串\n",
    "        r'\\\\[ntr]|'       # 转义符 \\n \\t \\r\n",
    "        r'//.*|'          # 单行注释 (如 C++/Java/JS 风格)\n",
    "        r'/\\*.*?\\*/|'     # 多行注释 (如 C 风格)\n",
    "        r'\\n|\\r|\\t|'      # 换行/回车/制表符\n",
    "        r'\\S'             # 其他符号(如 +, -, {, }, 以及任何其它非空白字符)\n",
    "    )\n",
    "\n",
    "    tokens_with_offset = []\n",
    "    lines = code.splitlines(keepends=True)\n",
    "    current_line = 1\n",
    "    current_pos = 0\n",
    "    \n",
    "    for match in re.finditer(token_pattern, code, re.MULTILINE | re.DOTALL):\n",
    "        tk = match.group(0)\n",
    "        start_offset, end_offset = match.span()\n",
    "        \n",
    "        # 计算当前token所在行号\n",
    "        while current_line <= len(lines) and current_pos + len(lines[current_line-1]) <= start_offset:\n",
    "            current_pos += len(lines[current_line-1])\n",
    "            current_line += 1\n",
    "            \n",
    "        tokens_with_offset.append((start_offset, end_offset, tk, current_line))\n",
    "\n",
    "    return tokens_with_offset\n",
    "\n",
    "###############################################################################\n",
    "#                      2) 获取 CFG (class/function) 行范围                    #\n",
    "###############################################################################\n",
    "def get_structure_prompt(code_text: str) -> str:\n",
    "    \"\"\"\n",
    "    构造提示给 LLM，让其解析出脚本中的类/函数行范围。\n",
    "    \"\"\"\n",
    "    lines = code_text.splitlines()\n",
    "    lines_json = [{\"line\": i+1, \"code\": line} for i, line in enumerate(lines)]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are given Python code lines (with line numbers). Identify all top-level and nested classes/functions,\n",
    "and return a JSON with this structure:\n",
    "\n",
    "{{\n",
    "  \"name\": \"example_script\",\n",
    "  \"type\": \"CFG\",\n",
    "  \"start_line\": 1,\n",
    "  \"end_line\": {len(lines)},\n",
    "  \"functions\": [\n",
    "    {{\n",
    "      \"name\": \"function_name\",\n",
    "      \"type\": \"function\",\n",
    "      \"start_line\": 10,\n",
    "      \"end_line\": 20,\n",
    "      \"functions\": [],\n",
    "      \"classes\": []\n",
    "    }}\n",
    "  ],\n",
    "  \"classes\": [\n",
    "    {{\n",
    "      \"name\": \"class_name\",\n",
    "      \"type\": \"class\",\n",
    "      \"start_line\": 30,\n",
    "      \"end_line\": 40,\n",
    "      \"functions\": [...],\n",
    "      \"classes\": [...]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Do not omit or rename fields.\n",
    "Here is the code lines:\n",
    "{json.dumps(lines_json, indent=2)}\n",
    "\n",
    "Return valid JSON only.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def parse_cfg_structure(code_text: str) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    调用LLM，获取CFG JSON结构\n",
    "    \"\"\"\n",
    "    prompt = get_structure_prompt(code_text)\n",
    "    try:\n",
    "        raw = get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True, temperature=0)\n",
    "        return json.loads(raw)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] parse_cfg_structure: {e}\")\n",
    "        lines = code_text.split('\\n')\n",
    "        return {\n",
    "            \"type\": \"CFG\",\n",
    "            \"name\": \"fallback\",\n",
    "            \"start_line\": 1,\n",
    "            \"end_line\": len(lines),\n",
    "            \"functions\": [],\n",
    "            \"classes\": []\n",
    "        }\n",
    "\n",
    "###############################################################################\n",
    "#        3) 在 block 内排除子函数/类行, 用 LLM 局部解析 => remap 索引          #\n",
    "###############################################################################\n",
    "def filter_tokens_for_block(\n",
    "    global_tokens: List[Tuple[int,int,str,int]],\n",
    "    block_start_line: int,\n",
    "    block_end_line: int,\n",
    "    excluded_line_ranges: List[Tuple[int,int]]\n",
    ") -> Tuple[List[Tuple[int,int,str,int]], List[int]]:\n",
    "    \"\"\"\n",
    "    在 [block_start_line..block_end_line] 内，排除excluded_line_ranges，\n",
    "    返回局部token列表 local_tokens 及其到全局的 mapping。\n",
    "    \"\"\"\n",
    "    def in_excluded(line_no: int) -> bool:\n",
    "        for (ex_st, ex_end) in excluded_line_ranges:\n",
    "            if ex_st <= line_no <= ex_end:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    filtered = []\n",
    "    mapping = []\n",
    "    for global_idx, (so, eo, tk, ln) in enumerate(global_tokens):\n",
    "        if ln < block_start_line or ln > block_end_line:\n",
    "            continue\n",
    "        if in_excluded(ln):\n",
    "            continue\n",
    "        filtered.append((so, eo, tk, ln))\n",
    "        mapping.append(global_idx)\n",
    "    return filtered, mapping\n",
    "\n",
    "def remap_ast_local_to_global(ast_node: Dict[str,Any], mapping: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    递归把局部下标 (start_token/end_token) => 全局下标\n",
    "    mapping[ local_idx ] = global_idx\n",
    "    \"\"\"\n",
    "    st_local = ast_node.get(\"start_token\", -1)\n",
    "    et_local = ast_node.get(\"end_token\", -1)\n",
    "\n",
    "    if 0 <= st_local < len(mapping):\n",
    "        ast_node[\"start_token\"] = mapping[st_local]\n",
    "    else:\n",
    "        ast_node[\"start_token\"] = -1\n",
    "\n",
    "    if 0 <= et_local < len(mapping):\n",
    "        ast_node[\"end_token\"] = mapping[et_local]\n",
    "    else:\n",
    "        ast_node[\"end_token\"] = -1\n",
    "\n",
    "    for c in ast_node.get(\"children\", []):\n",
    "        remap_ast_local_to_global(c, mapping)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#   4) 保持原样: llm_build_ast_from_tokens() (prompt 不变, 勿改)              #\n",
    "###############################################################################\n",
    "def llm_build_ast_from_tokens(tokens_with_offset: List[Tuple[int, int, str]], top_level=True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    给定 tokens列表 => 调用 LLM => 生成 JSON AST.\n",
    "    - top_level: 是否最外层(只有最外层允许 'module'), 否则用 'block' 等\n",
    "    \"\"\"\n",
    "    indexed_tokens = [(i, t[2]) for i, t in enumerate(tokens_with_offset)]\n",
    "    token_info = \"\\n\".join(f\"{i}: {text}\" for (i, text) in indexed_tokens)\n",
    "\n",
    "    # 构造 prompt\n",
    "    allowed_types = [\n",
    "        \"ABSTRACT\", \"AS\", \"BOOLEAN\", \"BREAK\", \"CASE\", \"CATCH\", \"CHAR\", \"CLASS\", \"CONST\", \"CONTINUE\",\n",
    "        \"DO\", \"ELSE\", \"ENUM\", \"ERROR\", \"EXTEND\", \"FALSE\", \"FINALLY\", \"FOR\", \"FOREIGN\", \"FROM\", \n",
    "        \"FUNC\", \"IF\", \"IMPORT\", \"IN\", \"INIT\", \"INOUT\", \"INTERFACE\", \"INTNATIVE\", \"IS\", \"LET\",\n",
    "        \"MACRO\", \"MAIN\", \"MATCH\", \"MUT\", \"OPEN\", \"OPERATOR\", \"OVERRIDE\", \"PACKAGE\", \"PRIVATE\",\n",
    "        \"PROP\", \"PROTECTED\", \"PUBLIC\", \"QUOTE\", \"REDEF\", \"RETURN\", \"SEALED\", \"SPAWN\", \"STATIC\",\n",
    "        \"STRUCT\", \"SUPER\", \"SYNCHRONIZED\", \"THIS\", \"THROW\", \"TRUE\", \"TRY\", \"TYPE\", \"UINTNATIVE\",\n",
    "        \"UNIT\", \"UNSAFE\", \"VAR\", \"WHERE\", \"WHILE\", \"argumentList\", \"arrayLiteral\", \"arrowType\",\n",
    "        \"assignmentExpression\", \"binaryExpression\", \"block\", \"body\", \"booleanLiteral\",\n",
    "        \"breakExpression\", \"builtinFunction\", \"callExpression\", \"caseBody\", \"charLangTypes\",\n",
    "        \"characterLiteral\", \"classDefinition\", \"classType\", \"collectionLiteral\", \"comment\",\n",
    "        \"constantPattern\", \"continueExpression\", \"dollarIdentifier\", \"element\", \"elements\",\n",
    "        \"enumBody\", \"enumDefinition\", \"enumPattern\", \"enumPatternParameters\", \"escapeSeq\",\n",
    "        \"exceptionTypePattern\", \"extendBody\", \"extendDefinition\", \"extendMemberDeclaration\",\n",
    "        \"fieldExpression\", \"floatLiteral\", \"forInExpression\", \"foreignDeclaration\",\n",
    "        \"foreignMemberDeclaration\", \"functionDefinition\", \"functionParameters\", \"genericConstraints\",\n",
    "        \"genericsType\", \"identifier\", \"ifExpression\", \"ifLetExpression\", \"importAll\", \"importContent\",\n",
    "        \"importList\", \"importSpecified\", \"incDecExpression\", \"initBody\", \"initialize\",\n",
    "        \"integerLiteral\", \"interfaceBody\", \"interfaceDefinition\", \"interfaceMemberDeclaration\",\n",
    "        \"lambdaExpression\", \"lambdaParameter\", \"lambdaParameters\", \"lineStringContent\",\n",
    "        \"lineStringExpression\", \"lineStringLiteral\", \"macroAttrExpr\", \"macroDecl\", \"macroDefinition\",\n",
    "        \"macroExpression\", \"macroInputExprWithParens\", \"macroInputExprWithoutParens\",\n",
    "        \"macroWithAttrParam\", \"macroWithoutAttrParam\", \"mainDefinition\", \"matchCase\",\n",
    "        \"matchExpression\", \"memberDeclaration\", \"modifiers\", \"multiLineStringContent\",\n",
    "        \"multiLineStringExpression\", \"multiLineStringLiteral\", \"multilineRawStringLiteral\",\n",
    "        \"operatorFunctionDefinition\", \"overloadedOperators\", \"packageHeader\", \"packageNameIdentifier\",\n",
    "        \"parameter\", \"parameterList\", \"parenthesizedExpression\", \"patternGuard\", \"prefixType\",\n",
    "        \"primaryInit\", \"propertyBody\", \"propertyDefinition\", \"propertyMemberDeclaration\",\n",
    "        \"quoteClose\", \"quoteExpression\", \"quoteOpen\", \"quoteParameters\", \"quoteParametersToken\",\n",
    "        \"quoteToken\", \"rangeExpression\", \"resourceSpecification\", \"resourceSpecifications\",\n",
    "        \"returnExpression\", \"sourceFile\", \"spawnExpression\", \"stringLiteral\", \"structDefinition\",\n",
    "        \"subscriptExpression\", \"superInterfaces\", \"synchronizedExpression\", \"throwExpression\",\n",
    "        \"tripleQuoteClose\", \"tripleQuoteOpen\", \"tryExpression\", \"tupleLiteral\", \"tuplePattern\",\n",
    "        \"tupleType\", \"typeAlias\", \"typeArguments\", \"typeExpression\", \"typeIdentifier\",\n",
    "        \"typeParameters\", \"typePattern\", \"unaryExpression\", \"unitLiteral\", \"unsafeExpression\",\n",
    "        \"upperBounds\", \"userType\", \"varBindingPattern\", \"variableDeclaration\", \"variableModifiers\",\n",
    "        \"whileExpression\", \"wildcardPattern\"\n",
    "    ]\n",
    "    allowed_types_str = \", \".join(allowed_types)\n",
    "\n",
    "    top_level_instruction = \"Exactly one 'module' node can appear at the root. Use 'block' if nested.\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        \"Below is a list of tokens (index -> token_string) for a code snippet:\\n\"\n",
    "        f\"{token_info}\\n\\n\"\n",
    "        \"Create a JSON-based AST with these fields:\\n\"\n",
    "        f\"- 'type': must be in {{{allowed_types_str}}}\\n\"\n",
    "        \"- 'start_token', 'end_token'\\n\"\n",
    "        \"- 'children' (array)\\n\\n\"\n",
    "        \"Leaf nodes => start_token == end_token.\\n\"\n",
    "        \"No overlapping sibling token ranges.\\n\"\n",
    "        \"Return valid JSON only.\\n\"\n",
    "    )\n",
    "    if top_level:\n",
    "        prompt += \"\\nAt the root, use 'module'. Do not nest multiple 'module'.\\n\" + top_level_instruction\n",
    "    else:\n",
    "        prompt += \"\\nInside blocks, do not produce 'module'. Use 'block' or suitable type.\\n\" + top_level_instruction\n",
    "\n",
    "    try:\n",
    "        llm_output = get_llm_answers(\n",
    "            prompt,\n",
    "            model_name=\"gpt-4o\",\n",
    "            require_json=True,\n",
    "            temperature=0\n",
    "        )\n",
    "        ast_dict = json.loads(llm_output)\n",
    "        return ast_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] llm_build_ast_from_tokens: {e}\")\n",
    "        return {\n",
    "            \"type\": \"ErrorNode\",\n",
    "            \"start_token\": -1,\n",
    "            \"end_token\": -1,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "def llm_parse_block_ast(local_tokens: List[Tuple[int,int,str,int]], top_level=True) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    使用 llm_build_ast_from_tokens 来解析局部token，得到局部AST(下标0..N-1)。\n",
    "    \"\"\"\n",
    "    # tokens_with_offset 只需要 (start_offset, end_offset, text)，忽略行号\n",
    "    tokens_for_llm = [(so, eo, tk) for (so, eo, tk, ln) in local_tokens]\n",
    "    return llm_build_ast_from_tokens(tokens_for_llm, top_level=top_level)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#    5) 递归构建AST: 处理本块普通语句 + 子函数/类 => 插占位 => merge & sort      #\n",
    "###############################################################################\n",
    "def find_first_token_index(global_tokens: List[Tuple[int,int,str,int]], line_start: int) -> int:\n",
    "    \"\"\"\n",
    "    找到在全局tokens里，行号 >= line_start 的第一个token的索引\n",
    "    \"\"\"\n",
    "    for i,(s_off,e_off,tk,ln) in enumerate(global_tokens):\n",
    "        if ln >= line_start:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def find_last_token_index(global_tokens: List[Tuple[int,int,str,int]], line_end: int) -> int:\n",
    "    \"\"\"\n",
    "    找到在全局tokens里，行号 <= line_end 的最后一个token的索引\n",
    "    \"\"\"\n",
    "    idx = -1\n",
    "    for i,(s_off,e_off,tk,ln) in enumerate(global_tokens):\n",
    "        if ln <= line_end:\n",
    "            idx = i\n",
    "        else:\n",
    "            break\n",
    "    return idx\n",
    "\n",
    "def build_ast_from_cfg(\n",
    "    cfg_node: Dict[str,Any],\n",
    "    global_tokens: List[Tuple[int,int,str,int]],\n",
    "    top_level=True\n",
    ") -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    递归构建 AST:\n",
    "      1) 在 [start_line..end_line] 排除 functions/classes 行\n",
    "      2) 解析剩余 => block_ast_local\n",
    "      3) remap => 全局索引\n",
    "      4) 对每个子函数/类递归 build => 占位节点 => 插入\n",
    "      5) 按 start_token 排序 => 返回\n",
    "    \"\"\"\n",
    "    st_line = cfg_node.get(\"start_line\", 1)\n",
    "    ed_line = cfg_node.get(\"end_line\", 1)\n",
    "\n",
    "    # 收集排除行\n",
    "    excluded_line_ranges = []\n",
    "    for fn in cfg_node.get(\"functions\", []):\n",
    "        excluded_line_ranges.append((fn[\"start_line\"], fn[\"end_line\"]))\n",
    "    for cl in cfg_node.get(\"classes\", []):\n",
    "        excluded_line_ranges.append((cl[\"start_line\"], cl[\"end_line\"]))\n",
    "\n",
    "    # 1) filter\n",
    "    local_tokens, mapping = filter_tokens_for_block(global_tokens, st_line, ed_line, excluded_line_ranges)\n",
    "    # 2) LLM解析 => block_ast_local\n",
    "    block_ast_local = llm_parse_block_ast(local_tokens, top_level=top_level)\n",
    "    # 3) remap\n",
    "    remap_ast_local_to_global(block_ast_local, mapping)\n",
    "\n",
    "    if \"children\" not in block_ast_local:\n",
    "        block_ast_local[\"children\"] = []\n",
    "\n",
    "    # 4) 处理子函数/类 => 占位\n",
    "    placeholders = []\n",
    "    for fn_cfg in cfg_node.get(\"functions\", []):\n",
    "        fn_ast = build_ast_from_cfg(fn_cfg, global_tokens, top_level=False)\n",
    "        placeholders.append({\n",
    "            \"type\": \"function_placeholder\",\n",
    "            \"name\": fn_cfg[\"name\"],\n",
    "            \"start_line\": fn_cfg[\"start_line\"],\n",
    "            \"end_line\": fn_cfg[\"end_line\"],\n",
    "            \"start_token\": find_first_token_index(global_tokens, fn_cfg[\"start_line\"]),\n",
    "            \"end_token\": find_last_token_index(global_tokens, fn_cfg[\"end_line\"]),\n",
    "            \"children\": [fn_ast]\n",
    "        })\n",
    "    for cl_cfg in cfg_node.get(\"classes\", []):\n",
    "        cl_ast = build_ast_from_cfg(cl_cfg, global_tokens, top_level=False)\n",
    "        placeholders.append({\n",
    "            \"type\": \"class_placeholder\",\n",
    "            \"name\": cl_cfg[\"name\"],\n",
    "            \"start_line\": cl_cfg[\"start_line\"],\n",
    "            \"end_line\": cl_cfg[\"end_line\"],\n",
    "            \"start_token\": find_first_token_index(global_tokens, cl_cfg[\"start_line\"]),\n",
    "            \"end_token\": find_last_token_index(global_tokens, cl_cfg[\"end_line\"]),\n",
    "            \"children\": [cl_ast]\n",
    "        })\n",
    "\n",
    "    merged_children = block_ast_local[\"children\"] + placeholders\n",
    "    merged_children.sort(key=lambda n: n.get(\"start_token\", -1))\n",
    "    block_ast_local[\"children\"] = merged_children\n",
    "\n",
    "    if top_level:\n",
    "        block_ast_local[\"type\"] = \"module\"\n",
    "\n",
    "    return block_ast_local\n",
    "\n",
    "def generate_llm_ast_via_cfg(code: str) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    最终对外函数: \n",
    "    1) 全局分词 => global_tokens\n",
    "    2) parse_cfg_structure => cfg\n",
    "    3) build_ast_from_cfg => AST(全局下标)\n",
    "    \"\"\"\n",
    "    global_tokens = tokenize_code_with_lines(code)\n",
    "    cfg_root = parse_cfg_structure(code)\n",
    "    llm_ast = build_ast_from_cfg(cfg_root, global_tokens, top_level=True)\n",
    "    return llm_ast\n",
    "\n",
    "###############################################################################\n",
    "#     8) 单文件处理: 生成LLM AST, Tree-sitter AST, 存JSON,可选对比             #\n",
    "###############################################################################\n",
    "def process_llm_ast(code: str, file_path: str) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    生成 LLM AST, 并保存到 JSON\n",
    "    \"\"\"\n",
    "    llm_dir = \"llm_ast/chunk_block\"\n",
    "    os.makedirs(llm_dir, exist_ok=True)\n",
    "    llm_path = os.path.join(llm_dir, os.path.basename(file_path) + \".json\")\n",
    "\n",
    "    if os.path.exists(llm_path):\n",
    "        with open(llm_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            llm_ast = json.load(f)\n",
    "        # print(f\"[LLM AST cached] => {llm_path}\")\n",
    "        return llm_ast\n",
    "\n",
    "    llm_ast = generate_llm_ast_via_cfg(code)\n",
    "    with open(llm_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(llm_ast, fout, indent=4, ensure_ascii=False)\n",
    "    # print(f\"[LLM AST] => {llm_path}\")\n",
    "    return llm_ast\n",
    "\n",
    "def process_single_file(file_path: str):\n",
    "    \"\"\"\n",
    "    1) read code\n",
    "    2) LLM AST\n",
    "    3) Tree-sitter AST\n",
    "    4) optionally compare\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            code = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error reading {file_path}]: {e}\")\n",
    "        return\n",
    "\n",
    "    process_llm_ast(code, file_path)\n",
    "###############################################################################\n",
    "#                            9) main() 并行处理                               #\n",
    "###############################################################################\n",
    "def main():\n",
    "    source_dir = \"../../dataset/cangjie\"  # 修改为你的实际源文件目录\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"[Error] Directory {source_dir} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # 收集所有 .cj 文件\n",
    "    files = [f for f in os.listdir(source_dir) if f.endswith(\".cj\")][:200]\n",
    "    print(f\"Found {len(files)} Cangjie files in {source_dir}.\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = []\n",
    "        pbar = tqdm(total=len(files), desc=\"Processing files\")\n",
    "        for fname in files:\n",
    "            full_path = os.path.join(source_dir, fname)\n",
    "            future = executor.submit(process_single_file, full_path)\n",
    "            future.add_done_callback(lambda _: pbar.update(1))\n",
    "            futures.append(future)\n",
    "        concurrent.futures.wait(futures)\n",
    "        pbar.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processed] => llm_ast/chunk_block_processed/247.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/37.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/46.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/34.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/97.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/238.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/9.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/105.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/32.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/60.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/101.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/180.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/157.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/19.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/20.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/272.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/282.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/140.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/271.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/77.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/141.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/176.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/121.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/244.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/5.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/24.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/33.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/201.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/147.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/86.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/188.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/219.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/18.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/89.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/305.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/277.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/109.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/237.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/195.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/218.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/56.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/199.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/209.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/253.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/165.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/213.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/207.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/115.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/104.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/212.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/133.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/279.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/194.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/134.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/285.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/185.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/143.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/189.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/187.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/245.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/289.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/268.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/73.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/16.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/280.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/270.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/138.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/205.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/71.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/136.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/216.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/255.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/118.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/173.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/273.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/27.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/326.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/319.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/256.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/210.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/302.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/67.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/318.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/75.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/300.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/39.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/14.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/107.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/156.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/145.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/65.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/192.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/22.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/72.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/283.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/100.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/119.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/239.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/311.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/112.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/191.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/50.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/214.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/162.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/320.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/13.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/114.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/144.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/224.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/91.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/111.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/265.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/206.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/63.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/146.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/178.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/11.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/306.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/166.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/113.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/183.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/129.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/31.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/243.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/287.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/127.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/116.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/132.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/182.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/263.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/15.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/117.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/232.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/83.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/1.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/108.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/317.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/49.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/231.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/228.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/236.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/120.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/267.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/281.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/149.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/6.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/95.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/23.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/7.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/276.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/246.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/221.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/98.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/137.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/316.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/90.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/70.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/197.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/30.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/148.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/126.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/93.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/47.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/227.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/312.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/122.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/322.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/184.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/284.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/124.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/164.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/79.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/233.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/96.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/69.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/200.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/68.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/175.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/222.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/241.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/181.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/190.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/142.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/299.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/193.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/35.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/52.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/160.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/171.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/106.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/87.cj.json\n",
      "[Processed] => llm_ast/chunk_block_processed/55.cj.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#                   1) 全局分词，含行号 -> global_tokens                       #\n",
    "###############################################################################\n",
    "def tokenize_code_with_lines(code: str) -> List[Tuple[int, int, str, int]]:\n",
    "    \"\"\"\n",
    "    使用正则分词，直接在原始代码上通过 finditer() 获取匹配位置，\n",
    "    并保留换行符，让 LLM 能识别多行结构。\n",
    "    返回形式: [(start_offset, end_offset, token_text, line_number), ...]\n",
    "    \"\"\"\n",
    "    token_pattern = (\n",
    "        r'[A-Za-z_]\\w*|'  # 标识符\n",
    "        r'[0-9]+|'        # 数字\n",
    "        r'\"[^\"]*\"|'       # 双引号字符串\n",
    "        r\"'[^']*'|\"       # 单引号字符串\n",
    "        r'\\\\[ntr]|'       # 转义符 \\n \\t \\r\n",
    "        r'//.*|'          # 单行注释 (如 C++/Java/JS 风格)\n",
    "        r'/\\*.*?\\*/|'     # 多行注释 (如 C 风格)\n",
    "        r'\\n|\\r|\\t|'      # 换行/回车/制表符\n",
    "        r'\\S'             # 其他符号(如 +, -, {, }, 以及任何其它非空白字符)\n",
    "    )\n",
    "\n",
    "    tokens_with_offset = []\n",
    "    lines = code.splitlines(keepends=True)\n",
    "    current_line = 1\n",
    "    current_pos = 0\n",
    "    \n",
    "    for match in re.finditer(token_pattern, code, re.MULTILINE | re.DOTALL):\n",
    "        tk = match.group(0)\n",
    "        start_offset, end_offset = match.span()\n",
    "        \n",
    "        # 计算当前token所在行号\n",
    "        while current_line <= len(lines) and current_pos + len(lines[current_line-1]) <= start_offset:\n",
    "            current_pos += len(lines[current_line-1])\n",
    "            current_line += 1\n",
    "            \n",
    "        tokens_with_offset.append((start_offset, end_offset, tk, current_line))\n",
    "\n",
    "    return tokens_with_offset\n",
    "\n",
    "###############################################################################\n",
    "# 1) 根据全局 tokens 填充 label\n",
    "###############################################################################\n",
    "def fill_ast_labels(ast_node: dict, code: str, global_tokens: List[Tuple[int,int,str,int]]) -> None:\n",
    "    \"\"\"\n",
    "    把节点的 (start_token, end_token) 当作【token下标】，\n",
    "    去 global_tokens 里拿对应的字符 offset，再到 code 中截取。\n",
    "    存到 ast_node[\"label\"]。\n",
    "    \"\"\"\n",
    "    st = ast_node.get(\"start_token\", -1)\n",
    "    et = ast_node.get(\"end_token\", -1)\n",
    "\n",
    "    snippet = \"\"\n",
    "    if (\n",
    "        0 <= st <= et\n",
    "        and st < len(global_tokens)\n",
    "        and et < len(global_tokens)\n",
    "    ):\n",
    "        start_offset = global_tokens[st][0]\n",
    "        end_offset   = global_tokens[et][1]\n",
    "        if 0 <= start_offset < end_offset <= len(code):\n",
    "            snippet = code[start_offset:end_offset]\n",
    "\n",
    "    ast_node[\"label\"] = snippet\n",
    "\n",
    "    for child in ast_node.get(\"children\", []):\n",
    "        fill_ast_labels(child, code, global_tokens)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) 扁平化：去掉 function_placeholder->module->(唯一子节点)\n",
    "###############################################################################\n",
    "def safe_flatten_function_placeholders(node: dict) -> dict:\n",
    "    \"\"\"\n",
    "    新建节点，避免循环引用。\n",
    "    如果 node.type = function_placeholder/class_placeholder，\n",
    "    并且只有1个child且是 'module'，\n",
    "    且 'module' 有1个child => 直接返回该child 并拷贝 placeholder 的字段\n",
    "    \"\"\"\n",
    "    if not node:\n",
    "        return {}\n",
    "\n",
    "    node_type = node.get(\"type\", \"\")\n",
    "    original_children = node.get(\"children\", [])\n",
    "\n",
    "    # 先递归处理children\n",
    "    flattened_children = [safe_flatten_function_placeholders(ch) for ch in original_children]\n",
    "\n",
    "    # 构建 new_node（复制非-children字段）\n",
    "    new_node = {}\n",
    "    for key, val in node.items():\n",
    "        if key != \"children\":\n",
    "            new_node[key] = val\n",
    "    new_node[\"children\"] = flattened_children\n",
    "\n",
    "    # 检查占位符结构\n",
    "    if node_type in (\"function_placeholder\", \"class_placeholder\"):\n",
    "        if len(flattened_children) == 1 and flattened_children[0].get(\"type\") == \"module\":\n",
    "            mod_node = flattened_children[0]\n",
    "            mod_kids = mod_node.get(\"children\", [])\n",
    "            if len(mod_kids) == 1:\n",
    "                real_node = mod_kids[0]\n",
    "                # 把 placeholder 上的一些字段复制给最里层\n",
    "                for field in (\"name\", \"start_line\", \"end_line\", \"start_token\", \"end_token\", \"label\"):\n",
    "                    if field in new_node:\n",
    "                        real_node[field] = new_node[field]\n",
    "                return real_node\n",
    "\n",
    "    return new_node\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) 单文件处理 => 根据同名py文件+json => 生成 global_tokens => 填label => 扁平化\n",
    "###############################################################################\n",
    "def process_ast_json(\n",
    "    input_json_path: str,\n",
    "    output_json_path: str,\n",
    "    py_source_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    预期:\n",
    "      input_json_path = \"llm_ast/chunk_block/1.py.json\"\n",
    "      -> 对应 py_file = \"py_source_dir/1.py\"\n",
    "\n",
    "    假设 JSON 结构如下:\n",
    "    {\n",
    "      \"type\": \"module\",\n",
    "      \"start_token\": 0,\n",
    "      \"end_token\": 307,\n",
    "      ...\n",
    "      \"children\": [...]\n",
    "    }\n",
    "    或者更复杂, 但只要 \"type\"、\"start_token\"/\"end_token\"、\"children\" 就可以\n",
    "\n",
    "    We'll:\n",
    "      1) 找到同名的 .py => 读 code\n",
    "      2) tokenize_code_with_lines(code) => global_tokens\n",
    "      3) fill_ast_labels(ast_root, code, global_tokens)\n",
    "      4) safe_flatten_function_placeholders(ast_root)\n",
    "      5) json.dump()\n",
    "    \"\"\"\n",
    "    base = os.path.basename(input_json_path)  # \"1.cj.json\"\n",
    "    # 拆分 => \"1.cj\" + \".json\"\n",
    "    # 如果你命名方式不同, 需自行改\n",
    "    # 这里假设 input_json_path 的文件名是 \"<something>.cj.json\"\n",
    "    # => python_source = \"<something>.cj\"\n",
    "    if base.endswith(\".cj.json\"):\n",
    "        py_file_name = base[:-5]  # remove \".json\"\n",
    "    else:\n",
    "        # fallback\n",
    "        py_file_name = base\n",
    "\n",
    "    py_full_path = os.path.join(py_source_dir, py_file_name)\n",
    "\n",
    "    if not os.path.isfile(py_full_path):\n",
    "        print(f\"[Warning] No corresponding .cj found for {input_json_path}, skip label fill.\")\n",
    "        code = \"\"\n",
    "        global_tokens = []\n",
    "    else:\n",
    "        # 读取 .py 源码\n",
    "        with open(py_full_path, \"r\", encoding=\"utf-8\") as fpy:\n",
    "            code = fpy.read()\n",
    "        # 分词\n",
    "        global_tokens = tokenize_code_with_lines(code)\n",
    "\n",
    "    # 读取 JSON AST\n",
    "    try:\n",
    "        with open(input_json_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "            ast_data = json.load(fin)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error reading {input_json_path}]: {e}\")\n",
    "        return\n",
    "\n",
    "    # fill label\n",
    "    fill_ast_labels(ast_data, code, global_tokens)\n",
    "\n",
    "    # flatten\n",
    "    flattened_ast = safe_flatten_function_placeholders(ast_data)\n",
    "\n",
    "    # 写出\n",
    "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(flattened_ast, fout, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[Processed] => {output_json_path}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4) main: 遍历 input_json_dir => 对应 .py => output\n",
    "###############################################################################\n",
    "def main():\n",
    "    input_json_dir = \"llm_ast/chunk_block\"              # 你的 AST json 目录\n",
    "    output_json_dir = \"llm_ast/chunk_block_processed\"   # 输出目录\n",
    "    py_source_dir = \"../../dataset/cangjie\"              # 对应的 .cj 文件目录\n",
    "\n",
    "    if not os.path.isdir(input_json_dir):\n",
    "        print(f\"[Error] input dir {input_json_dir} not found.\")\n",
    "        return\n",
    "    if not os.path.isdir(py_source_dir):\n",
    "        print(f\"[Warning] python source dir {py_source_dir} not found. Label fill will be empty.\")\n",
    "\n",
    "    os.makedirs(output_json_dir, exist_ok=True)\n",
    "\n",
    "    # 遍历\n",
    "    for fname in os.listdir(input_json_dir):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        in_path = os.path.join(input_json_dir, fname)\n",
    "        out_path = os.path.join(output_json_dir, fname)\n",
    "\n",
    "        process_ast_json(in_path, out_path, py_source_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
