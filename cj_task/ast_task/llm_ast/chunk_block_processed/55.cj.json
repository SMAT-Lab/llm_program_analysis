{
    "type": "module",
    "start_token": 0,
    "end_token": 1,
    "label": "package tokenizer",
    "children": [
        {
            "type": "packageHeader",
            "start_token": 0,
            "end_token": 0,
            "label": "package",
            "children": []
        },
        {
            "type": "packageNameIdentifier",
            "start_token": 1,
            "end_token": 1,
            "label": "tokenizer",
            "children": []
        },
        {
            "type": "mainDefinition",
            "start_token": 4,
            "end_token": 40,
            "label": "main(): Int64 {\n    let vocab_path = \"./download/Qwen2-0.5B-Instruct/tokenizer.json\";\n    let system_prompt = \"You are a helpful assistant.\";\n    let tokenizer = HuggingfaceTokenizer()\n    tokenizer.load_vocab(file_path: vocab_path);\n    // -- test1 --- //\n    println(\"===== test1 ===== \");\n    let raw_text = \"世界你好，hello world!\";\n    let encode_tokens1 = tokenizer.encode(raw_text);\n    println(\"encode_tokens: ${encode_tokens1}\");\n    let decode_str1 = tokenizer.decode(encode_tokens1, skip_special_tokens: false);\n    println(\"deocode_str: ${decode_str1}\");\n    println(\"===== ===== ===== \");\n\n    // -- test2 -- //\n    println(\"===== test2 ===== \");\n    let raw_text2 = raw_text + \"<|im_end|>\";\n    let encode_tokens2 = tokenizer.encode(raw_text2);\n    println(\"encode_tokens: ${encode_tokens2}\");\n    let decode_str2 = tokenizer.decode(encode_tokens2, skip_special_tokens: false);\n    println(\"deocode_str(with special)    : ${decode_str2}\");\n    let decode_str3 = tokenizer.decode(encode_tokens2, skip_special_tokens: true);\n    println(\"deocode_str(without special) : ${decode_str3}\");\n    println(\"===== ===== ===== \");\n\n    // --- test3 --- //\n    println(\"===== test3 ===== \");\n    let messages = ArrayList<Message>([\n        Message(RoleType.System, system_prompt),\n        Message(RoleType.User, raw_text)\n    ])\n    let new_text = tokenizer.apply_chat_template(messages, add_generation_prompt: true);\n    println(\"new_text:\\n ${new_text}\");\n    return 0\n}",
            "children": [
                {
                    "type": "identifier",
                    "start_token": 4,
                    "end_token": 4,
                    "label": "main",
                    "children": []
                },
                {
                    "type": "parameterList",
                    "start_token": 5,
                    "end_token": 6,
                    "label": "()",
                    "children": []
                },
                {
                    "type": "typeIdentifier",
                    "start_token": 8,
                    "end_token": 8,
                    "label": "Int64",
                    "children": []
                },
                {
                    "type": "block",
                    "start_token": 9,
                    "end_token": -1,
                    "label": "",
                    "children": [
                        {
                            "type": "variableDeclaration",
                            "start_token": 11,
                            "end_token": 15,
                            "label": "let vocab_path = \"./download/Qwen2-0.5B-Instruct/tokenizer.json\";",
                            "children": [
                                {
                                    "type": "identifier",
                                    "start_token": 12,
                                    "end_token": 12,
                                    "label": "vocab_path",
                                    "children": []
                                },
                                {
                                    "type": "stringLiteral",
                                    "start_token": 14,
                                    "end_token": 14,
                                    "label": "\"./download/Qwen2-0.5B-Instruct/tokenizer.json\"",
                                    "children": []
                                }
                            ]
                        },
                        {
                            "type": "variableDeclaration",
                            "start_token": 17,
                            "end_token": 21,
                            "label": "let system_prompt = \"You are a helpful assistant.\";",
                            "children": [
                                {
                                    "type": "identifier",
                                    "start_token": 18,
                                    "end_token": 18,
                                    "label": "system_prompt",
                                    "children": []
                                },
                                {
                                    "type": "stringLiteral",
                                    "start_token": 20,
                                    "end_token": 20,
                                    "label": "\"You are a helpful assistant.\"",
                                    "children": []
                                }
                            ]
                        },
                        {
                            "type": "variableDeclaration",
                            "start_token": 23,
                            "end_token": 28,
                            "label": "let tokenizer = HuggingfaceTokenizer()",
                            "children": [
                                {
                                    "type": "identifier",
                                    "start_token": 24,
                                    "end_token": 24,
                                    "label": "tokenizer",
                                    "children": []
                                },
                                {
                                    "type": "callExpression",
                                    "start_token": 26,
                                    "end_token": 28,
                                    "label": "HuggingfaceTokenizer()",
                                    "children": [
                                        {
                                            "type": "identifier",
                                            "start_token": 26,
                                            "end_token": 26,
                                            "label": "HuggingfaceTokenizer",
                                            "children": []
                                        },
                                        {
                                            "type": "argumentList",
                                            "start_token": 27,
                                            "end_token": 28,
                                            "label": "()",
                                            "children": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "type": "callExpression",
                            "start_token": 30,
                            "end_token": 38,
                            "label": "tokenizer.load_vocab(file_path: vocab_path);",
                            "children": [
                                {
                                    "type": "fieldExpression",
                                    "start_token": 30,
                                    "end_token": 32,
                                    "label": "tokenizer.load_vocab",
                                    "children": [
                                        {
                                            "type": "identifier",
                                            "start_token": 30,
                                            "end_token": 30,
                                            "label": "tokenizer",
                                            "children": []
                                        },
                                        {
                                            "type": "identifier",
                                            "start_token": 32,
                                            "end_token": 32,
                                            "label": "load_vocab",
                                            "children": []
                                        }
                                    ]
                                },
                                {
                                    "type": "argumentList",
                                    "start_token": 33,
                                    "end_token": 37,
                                    "label": "(file_path: vocab_path)",
                                    "children": [
                                        {
                                            "type": "identifier",
                                            "start_token": 34,
                                            "end_token": 34,
                                            "label": "file_path",
                                            "children": []
                                        },
                                        {
                                            "type": "identifier",
                                            "start_token": 36,
                                            "end_token": 36,
                                            "label": "vocab_path",
                                            "children": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "type": "comment",
                            "start_token": 40,
                            "end_token": 40,
                            "label": "// -- test1 --- //\n    println(\"===== test1 ===== \");\n    let raw_text = \"世界你好，hello world!\";\n    let encode_tokens1 = tokenizer.encode(raw_text);\n    println(\"encode_tokens: ${encode_tokens1}\");\n    let decode_str1 = tokenizer.decode(encode_tokens1, skip_special_tokens: false);\n    println(\"deocode_str: ${decode_str1}\");\n    println(\"===== ===== ===== \");\n\n    // -- test2 -- //\n    println(\"===== test2 ===== \");\n    let raw_text2 = raw_text + \"<|im_end|>\";\n    let encode_tokens2 = tokenizer.encode(raw_text2);\n    println(\"encode_tokens: ${encode_tokens2}\");\n    let decode_str2 = tokenizer.decode(encode_tokens2, skip_special_tokens: false);\n    println(\"deocode_str(with special)    : ${decode_str2}\");\n    let decode_str3 = tokenizer.decode(encode_tokens2, skip_special_tokens: true);\n    println(\"deocode_str(without special) : ${decode_str3}\");\n    println(\"===== ===== ===== \");\n\n    // --- test3 --- //\n    println(\"===== test3 ===== \");\n    let messages = ArrayList<Message>([\n        Message(RoleType.System, system_prompt),\n        Message(RoleType.User, raw_text)\n    ])\n    let new_text = tokenizer.apply_chat_template(messages, add_generation_prompt: true);\n    println(\"new_text:\\n ${new_text}\");\n    return 0\n}",
                            "children": []
                        },
                        {
                            "type": "variableDeclaration",
                            "start_token": -1,
                            "end_token": -1,
                            "label": "",
                            "children": [
                                {
                                    "type": "identifier",
                                    "start_token": -1,
                                    "end_token": -1,
                                    "label": "",
                                    "children": []
                                },
                                {
                                    "type": "stringLiteral",
                                    "start_token": -1,
                                    "end_token": -1,
                                    "label": "",
                                    "children": []
                                }
                            ]
                        },
                        {
                            "type": "variableDeclaration",
                            "start_token": -1,
                            "end_token": -1,
                            "label": "",
                            "children": [
                                {
                                    "type": "identifier",
                                    "start_token": -1,
                                    "end_token": -1,
                                    "label": "",
                                    "children": []
                                },
                                {
                                    "type": "callExpression",
                                    "start_token": -1,
                                    "end_token": -1,
                                    "label": "",
                                    "children": [
                                        {
                                            "type": "identifier",
                                            "start_token": -1,
                                            "end_token": -1,
                                            "label": "",
                                            "children": []
                                        },
                                        {
                                            "type": "argumentList",
                                            "start_token": -1,
                                            "end_token": -1,
                                            "label": "",
                                            "children": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "type": "callExpression",
                            "start_token": -1,
                            "end_token": -1,
                            "label": "",
                            "children": [
                                {
                                    "type": "fieldExpression",
                                    "start_token": -1,
                                    "end_token": -1,
                                    "label": "",
                                    "children": [
                                        {
                                            "type": "identifier",
                                            "start_token": -1,
                                            "end_token": -1,
                                            "label": "",
                                            "children": []
                                        },
                                        {
                                            "type": "identifier",
                                            "start_token": -1,
                                            "end_token": -1,
                                            "label": "",
                                            "children": []
                                        }
                                    ]
                                },
                                {
                                    "type": "argumentList",
                                    "start_token": -1,
                                    "end_token": -1,
                                    "label": "",
                                    "children": [
                                        {
                                            "type": "identifier",
                                            "start_token": -1,
                                            "end_token": -1,
                                            "label": "",
                                            "children": []
                                        },
                                        {
                                            "type": "variableDeclaration",
                                            "start_token": -1,
                                            "end_token": -1,
                                            "label": "",
                                            "children": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "type": "returnExpression",
                            "start_token": -1,
                            "end_token": -1,
                            "label": "",
                            "children": []
                        }
                    ]
                }
            ],
            "name": "main",
            "start_line": 3,
            "end_line": 37
        }
    ]
}