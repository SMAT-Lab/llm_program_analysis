{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= 2) py-tree-sitter 相关 ============= #\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import re\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from tqdm import tqdm\n",
    "## need to change the version of tree-sitter 0.19.0 here!!!\n",
    "def create_parser():\n",
    "    cangjie_parser = Parser()\n",
    "    cangjie_language = Language('../cj.so', 'char')\n",
    "    cangjie_parser.set_language(cangjie_language)\n",
    "    return cangjie_parser\n",
    "\n",
    "parser = create_parser()\n",
    "\n",
    "# ============= 3) Tokenization & Utility Functions ============= #\n",
    "import re\n",
    "\n",
    "def tokenize_code(code: str) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    使用正则分词，直接在原始代码上通过 finditer() 获取匹配位置，\n",
    "    并保留换行符，让 LLM 能识别多行结构。\n",
    "    返回形式: [(start_offset, end_offset, token_text), ...]\n",
    "    \"\"\"\n",
    "    token_pattern = (\n",
    "        r'[A-Za-z_]\\w*|'  # 标识符\n",
    "        r'[0-9]+|'        # 数字\n",
    "        r'\"[^\"]*\"|'       # 双引号字符串\n",
    "        r\"'[^']*'|\"       # 单引号字符串\n",
    "        r'\\\\[ntr]|'       # 转义符 \\n \\t \\r\n",
    "        r'//.*|'          # 单行注释 (如 C++/Java/JS 风格)\n",
    "        r'/\\*.*?\\*/|'     # 多行注释 (如 C 风格)\n",
    "        r'\\n|\\r|\\t|'      # 换行/回车/制表符\n",
    "        r'\\S'             # 其他符号(如 +, -, {, }, 以及任何其它非空白字符)\n",
    "    )\n",
    "\n",
    "    tokens_with_offset = []\n",
    "    for match in re.finditer(token_pattern, code, re.MULTILINE | re.DOTALL):\n",
    "        tk = match.group(0)\n",
    "        start_offset, end_offset = match.span()\n",
    "        tokens_with_offset.append((start_offset, end_offset, tk))\n",
    "\n",
    "    return tokens_with_offset\n",
    "\n",
    "# ============= 6) Tree-sitter-based Static AST (带 start_token/end_token) ============= #\n",
    "\n",
    "def find_token_index_for_byte(byte_offset: int, tokens: List[Tuple[int, int, str]], is_start=True) -> int:\n",
    "    \"\"\"\n",
    "    在 tokens 列表里找出对应 byte_offset 所处的 token 下标。\n",
    "    is_start=True 表示在找 start_token，False 则是 end_token。\n",
    "    简易的线性搜索，若需要效率可改为二分。\n",
    "    \"\"\"\n",
    "    for i, (tk_start, tk_end, _) in enumerate(tokens):\n",
    "        if is_start:\n",
    "            # start_token: tk_start <= byte_offset < tk_end\n",
    "            if tk_start <= byte_offset < tk_end:\n",
    "                return i\n",
    "        else:\n",
    "            # end_token: tk_start < byte_offset <= tk_end\n",
    "            if tk_start < byte_offset <= tk_end:\n",
    "                return i\n",
    "    return -1\n",
    "\n",
    "class PyTreeSitterStaticHandler:\n",
    "    \"\"\"\n",
    "    生成一棵 AST：{type, label, start_token, end_token, children}，\n",
    "    其中 start_token/end_token 通过与 tokenize_code 的结果对应。\n",
    "    \"\"\"\n",
    "    def __init__(self, code: str):\n",
    "        self.parser = parser\n",
    "        self.code = code\n",
    "        # 与 LLM 相同的 tokenize 函数\n",
    "        self.tokens = tokenize_code(code)\n",
    "\n",
    "    def generate_static_ast(self) -> Dict[str, Any]:\n",
    "        tree = self.parser.parse(self.code.encode())\n",
    "        root_node = tree.root_node\n",
    "        return self.ts_node_to_dict(root_node)\n",
    "\n",
    "    def ts_node_to_dict(self, node) -> Dict[str, Any]:\n",
    "        # 跳过未命名节点\n",
    "        if not node.is_named:\n",
    "            return None\n",
    "\n",
    "        node_type = node.type\n",
    "        node_text = (node.text or b\"\").decode(\"utf-8\")\n",
    "\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "\n",
    "        start_token_idx = find_token_index_for_byte(start_byte, self.tokens, is_start=True)\n",
    "        end_token_idx = find_token_index_for_byte(end_byte, self.tokens, is_start=False)\n",
    "\n",
    "        custom = {\n",
    "            \"type\": node_type,\n",
    "            \"label\": node_text,  # 这里保留整段源码片段(可自行决定是否截断换行等)\n",
    "            \"start_token\": start_token_idx,\n",
    "            \"end_token\": end_token_idx,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        for i in range(node.child_count):\n",
    "            child = node.child(i)\n",
    "            child_dict = self.ts_node_to_dict(child)\n",
    "            if child_dict:\n",
    "                custom[\"children\"].append(child_dict)\n",
    "\n",
    "        return custom\n",
    "\n",
    "def generate_tree_sitter_ast(code: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    对外暴露的函数: 解析 Python 代码并生成带有 (start_token, end_token) 的静态 AST。\n",
    "    \"\"\"\n",
    "    handler = PyTreeSitterStaticHandler(code)\n",
    "    return handler.generate_static_ast()\n",
    "\n",
    "def process_static_ast(code: str, file_path: str) -> dict:\n",
    "    \"\"\"处理树莓坐（tree-sitter）AST生成和保存.\"\"\"\n",
    "    ts_ast = generate_tree_sitter_ast(code)\n",
    "    ts_out_dir = \"cangjie_ast\"\n",
    "    os.makedirs(ts_out_dir, exist_ok=True)\n",
    "    ts_json_path = os.path.join(ts_out_dir, os.path.basename(file_path) + \".json\")\n",
    "    with open(ts_json_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(ts_ast, fout, indent=4, ensure_ascii=False)\n",
    "    print(f\"[TS AST] => {ts_json_path}\")\n",
    "    return ts_ast\n",
    "\n",
    "def process_single_file(file_path: str):\n",
    "    \"\"\"\n",
    "    1) 读取代码\n",
    "    2) 生成静态 AST (tree-sitter)\n",
    "    3) 保存 JSON\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            code = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] reading {file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # generate static AST\n",
    "    ts_ast = process_static_ast(code, file_path)\n",
    "\n",
    "# ============= 8) 主函数：多线程并行处理 ============= #\n",
    "\n",
    "def main():\n",
    "    source_dir = \"cangjie\"\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"[Error] Directory {source_dir} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # 示例：只处理前 10 个 .py 文件，可按需修改\n",
    "    files = [f for f in os.listdir(source_dir) if f.endswith(\".cj\")]\n",
    "\n",
    "    # process_single_file(os.path.join(source_dir, files[0]))\n",
    "\n",
    "    # 并行处理\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = []\n",
    "        pbar = tqdm(total=len(files), desc=\"处理文件\")\n",
    "        for fname in files:\n",
    "            full_path = os.path.join(source_dir, fname)\n",
    "            future = executor.submit(process_single_file, full_path)\n",
    "            future.add_done_callback(lambda _: pbar.update(1))\n",
    "            futures.append(future)\n",
    "        concurrent.futures.wait(futures)\n",
    "        pbar.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有文件中提取的唯一类型:\n",
      "- ABSTRACT\n",
      "- AS\n",
      "- BOOLEAN\n",
      "- BREAK\n",
      "- CASE\n",
      "- CATCH\n",
      "- CHAR\n",
      "- CLASS\n",
      "- CONST\n",
      "- CONTINUE\n",
      "- DO\n",
      "- ELSE\n",
      "- ENUM\n",
      "- ERROR\n",
      "- EXTEND\n",
      "- FALSE\n",
      "- FINALLY\n",
      "- FOR\n",
      "- FOREIGN\n",
      "- FROM\n",
      "- FUNC\n",
      "- IF\n",
      "- IMPORT\n",
      "- IN\n",
      "- INIT\n",
      "- INOUT\n",
      "- INTERFACE\n",
      "- INTNATIVE\n",
      "- IS\n",
      "- LET\n",
      "- MACRO\n",
      "- MAIN\n",
      "- MATCH\n",
      "- MUT\n",
      "- OPEN\n",
      "- OPERATOR\n",
      "- OVERRIDE\n",
      "- PACKAGE\n",
      "- PRIVATE\n",
      "- PROP\n",
      "- PROTECTED\n",
      "- PUBLIC\n",
      "- QUOTE\n",
      "- REDEF\n",
      "- RETURN\n",
      "- SEALED\n",
      "- SPAWN\n",
      "- STATIC\n",
      "- STRUCT\n",
      "- SUPER\n",
      "- SYNCHRONIZED\n",
      "- THIS\n",
      "- THROW\n",
      "- TRUE\n",
      "- TRY\n",
      "- TYPE\n",
      "- UINTNATIVE\n",
      "- UNIT\n",
      "- UNSAFE\n",
      "- VAR\n",
      "- WHERE\n",
      "- WHILE\n",
      "- argumentList\n",
      "- arrayLiteral\n",
      "- arrowType\n",
      "- assignmentExpression\n",
      "- binaryExpression\n",
      "- block\n",
      "- body\n",
      "- booleanLiteral\n",
      "- breakExpression\n",
      "- builtinFunction\n",
      "- callExpression\n",
      "- caseBody\n",
      "- charLangTypes\n",
      "- characterLiteral\n",
      "- classDefinition\n",
      "- classType\n",
      "- collectionLiteral\n",
      "- comment\n",
      "- constantPattern\n",
      "- continueExpression\n",
      "- dollarIdentifier\n",
      "- element\n",
      "- elements\n",
      "- enumBody\n",
      "- enumDefinition\n",
      "- enumPattern\n",
      "- enumPatternParameters\n",
      "- escapeSeq\n",
      "- exceptionTypePattern\n",
      "- extendBody\n",
      "- extendDefinition\n",
      "- extendMemberDeclaration\n",
      "- fieldExpression\n",
      "- floatLiteral\n",
      "- forInExpression\n",
      "- foreignDeclaration\n",
      "- foreignMemberDeclaration\n",
      "- functionDefinition\n",
      "- functionParameters\n",
      "- genericConstraints\n",
      "- genericsType\n",
      "- identifier\n",
      "- ifExpression\n",
      "- ifLetExpression\n",
      "- importAll\n",
      "- importContent\n",
      "- importList\n",
      "- importSpecified\n",
      "- incDecExpression\n",
      "- initBody\n",
      "- initialize\n",
      "- integerLiteral\n",
      "- interfaceBody\n",
      "- interfaceDefinition\n",
      "- interfaceMemberDeclaration\n",
      "- lambdaExpression\n",
      "- lambdaParameter\n",
      "- lambdaParameters\n",
      "- lineStringContent\n",
      "- lineStringExpression\n",
      "- lineStringLiteral\n",
      "- macroAttrExpr\n",
      "- macroDecl\n",
      "- macroDefinition\n",
      "- macroExpression\n",
      "- macroInputExprWithParens\n",
      "- macroInputExprWithoutParens\n",
      "- macroWithAttrParam\n",
      "- macroWithoutAttrParam\n",
      "- mainDefinition\n",
      "- matchCase\n",
      "- matchExpression\n",
      "- memberDeclaration\n",
      "- modifiers\n",
      "- multiLineStringContent\n",
      "- multiLineStringExpression\n",
      "- multiLineStringLiteral\n",
      "- multilineRawStringLiteral\n",
      "- operatorFunctionDefinition\n",
      "- overloadedOperators\n",
      "- packageHeader\n",
      "- packageNameIdentifier\n",
      "- parameter\n",
      "- parameterList\n",
      "- parenthesizedExpression\n",
      "- patternGuard\n",
      "- prefixType\n",
      "- primaryInit\n",
      "- propertyBody\n",
      "- propertyDefinition\n",
      "- propertyMemberDeclaration\n",
      "- quoteClose\n",
      "- quoteExpression\n",
      "- quoteOpen\n",
      "- quoteParameters\n",
      "- quoteParametersToken\n",
      "- quoteToken\n",
      "- rangeExpression\n",
      "- resourceSpecification\n",
      "- resourceSpecifications\n",
      "- returnExpression\n",
      "- sourceFile\n",
      "- spawnExpression\n",
      "- stringLiteral\n",
      "- structDefinition\n",
      "- subscriptExpression\n",
      "- superInterfaces\n",
      "- synchronizedExpression\n",
      "- throwExpression\n",
      "- tripleQuoteClose\n",
      "- tripleQuoteOpen\n",
      "- tryExpression\n",
      "- tupleLiteral\n",
      "- tuplePattern\n",
      "- tupleType\n",
      "- typeAlias\n",
      "- typeArguments\n",
      "- typeExpression\n",
      "- typeIdentifier\n",
      "- typeParameters\n",
      "- typePattern\n",
      "- unaryExpression\n",
      "- unitLiteral\n",
      "- unsafeExpression\n",
      "- upperBounds\n",
      "- userType\n",
      "- varBindingPattern\n",
      "- variableDeclaration\n",
      "- variableModifiers\n",
      "- whileExpression\n",
      "- wildcardPattern\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, Set\n",
    "\n",
    "def extract_types_from_cfg(cfg: Dict) -> set:\n",
    "    \"\"\"从AST配置中提取所有可能的类型\"\"\"\n",
    "    types = set()\n",
    "    \n",
    "    def traverse(node):\n",
    "        if isinstance(node, dict):\n",
    "            if \"type\" in node:\n",
    "                types.add(node[\"type\"])\n",
    "            for value in node.values():\n",
    "                traverse(value)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                traverse(item)\n",
    "                \n",
    "    traverse(cfg)\n",
    "    return types\n",
    "\n",
    "# 遍历所有json文件并去重\n",
    "all_types = set()\n",
    "processed_files = set()\n",
    "\n",
    "for root in [\"../../dataset/cangjie_ast\"]:\n",
    "    for file in os.listdir(root):\n",
    "        if not file.endswith('.json'):\n",
    "            continue\n",
    "            \n",
    "        # 检查文件是否已处理过(通过文件名去重)\n",
    "        if file in processed_files:\n",
    "            continue\n",
    "            \n",
    "        processed_files.add(file)\n",
    "        file_path = os.path.join(root, file)\n",
    "        \n",
    "        with open(file_path) as f:\n",
    "            cfg = json.load(f)\n",
    "            types = extract_types_from_cfg(cfg)\n",
    "            all_types.update(types)\n",
    "\n",
    "print(\"所有文件中提取的唯一类型:\")\n",
    "for t in sorted(all_types):\n",
    "    print(f\"- {t}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4cfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
