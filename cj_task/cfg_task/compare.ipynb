{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   0%|          | 0/189 [00:00<?, ?it/s]/home/miniconda3/envs/llm_analysis/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "处理CFG文件: 100%|██████████| 189/189 [00:04<00:00, 45.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic Evaluation Summary:\n",
      "Total CFGs compared: 189\n",
      "Average Edge Coverage: 0.37\n",
      "Average Content Similarity: 0.21\n",
      "Average Structure Similarity: 0.49\n",
      "Total GT edges: 891\n",
      "Total LLM edges: 1686\n",
      "Total Matched edges: 506\n",
      "Precision: 0.3001186239620403\n",
      "Recall: 0.5679012345679012\n",
      "F1 Score: 0.3927046953822274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CFGSimilarityResult:\n",
    "    \"\"\"存储CFG比较结果的数据类\"\"\"\n",
    "    filename: str\n",
    "    edge_coverage: float\n",
    "    content_similarity: float\n",
    "    structure_similarity: float\n",
    "    matched_edges: int\n",
    "    gt_edges: int\n",
    "    llm_edges: int\n",
    "    nested_results: Optional[Dict[str, 'CFGSimilarityResult']] = None\n",
    "\n",
    "\n",
    "class CFGComparator:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化CFG比较器\"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def count_edges(cfg: Dict) -> int:\n",
    "        \"\"\"\n",
    "        递归计算CFG中的边数量，包括所有嵌套结构的边\n",
    "        \"\"\"\n",
    "        def count_edges_recursive(blocks):\n",
    "            edge_count = 0\n",
    "            for block in blocks:\n",
    "                successors = block.get(\"successors\", [])\n",
    "                edge_count += len(successors)  # 当前 block 的边数量\n",
    "                # 递归计算 successors 的边\n",
    "                for successor in successors:\n",
    "                    if isinstance(successor, dict):  # 确保 successor 是嵌套结构\n",
    "                        edge_count += count_edges_recursive(successor.get(\"blocks\", []))\n",
    "            return edge_count\n",
    "\n",
    "        # 统计顶层 blocks 的边\n",
    "        edge_count = count_edges_recursive(cfg.get(\"blocks\", []))\n",
    "\n",
    "        # 递归统计嵌套函数和类的边\n",
    "        for func in cfg.get(\"functions\", []):\n",
    "            edge_count += CFGComparator.count_edges(func)\n",
    "        for cls in cfg.get(\"classes\", []):\n",
    "            edge_count += CFGComparator.count_edges(cls)\n",
    "\n",
    "        return edge_count\n",
    "\n",
    "    def structure_similarity(self, llm_cfg: Dict, static_cfg: Dict) -> float:\n",
    "        \"\"\"\n",
    "        计算两个CFG的结构相似度（基于边的数量）\n",
    "        \"\"\"\n",
    "        llm_edges = self.count_edges(llm_cfg)\n",
    "        static_edges = self.count_edges(static_cfg)\n",
    "\n",
    "        if llm_edges == 0 and static_edges == 0:\n",
    "            return 1.0\n",
    "        if llm_edges == 0 or static_edges == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return min(llm_edges, static_edges) / max(llm_edges, static_edges)\n",
    "\n",
    "    def content_similarity(self, llm_cfg: Dict, static_cfg: Dict) -> float:\n",
    "        \"\"\"\n",
    "        计算两个CFG的内容相似度（基于 blocks 的内容）\n",
    "        \"\"\"\n",
    "        llm_blocks = llm_cfg.get(\"blocks\", [])\n",
    "        static_blocks = static_cfg.get(\"blocks\", [])\n",
    "\n",
    "        if not llm_blocks and not static_blocks:\n",
    "            return 1.0\n",
    "        if not llm_blocks or not static_blocks:\n",
    "            return 0.0\n",
    "\n",
    "        llm_code = \"\\n\".join(block.get(\"label\", \"\") for block in llm_blocks)\n",
    "        static_code = \"\\n\".join(block.get(\"label\", \"\") for block in static_blocks)\n",
    "\n",
    "        def custom_tokenizer(text):\n",
    "            return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "        def custom_preprocessor(text):\n",
    "            text = re.sub(r'#.*', '', text)  # 删除注释\n",
    "            return text\n",
    "\n",
    "        vectorizer = CountVectorizer(\n",
    "            tokenizer=custom_tokenizer,\n",
    "            preprocessor=custom_preprocessor,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "        vectors = vectorizer.fit_transform([llm_code, static_code])\n",
    "        similarity = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "        return similarity\n",
    "\n",
    "    def compare_cfgs(self, llm_cfg: Dict, static_cfg: Dict, name: str) -> CFGSimilarityResult:\n",
    "        \"\"\"递归比较两个CFG并返回相似度结果\"\"\"\n",
    "        structure_sim = self.structure_similarity(llm_cfg, static_cfg)\n",
    "        content_sim = self.content_similarity(llm_cfg, static_cfg)\n",
    "\n",
    "        gt_edges = self.count_edges(static_cfg)\n",
    "        llm_edges = self.count_edges(llm_cfg)\n",
    "        matched_edges = int(structure_sim * min(gt_edges, llm_edges))\n",
    "        edge_coverage = matched_edges / gt_edges if gt_edges > 0 else 0\n",
    "\n",
    "        nested_results = {}\n",
    "\n",
    "        llm_functions = {f[\"name\"]: f for f in llm_cfg.get(\"functions\", [])}\n",
    "        static_functions = {f[\"name\"]: f for f in static_cfg.get(\"functions\", [])}\n",
    "        common_functions = set(llm_functions.keys()) & set(static_functions.keys())\n",
    "\n",
    "        for func_name in common_functions:\n",
    "            nested_results[f\"function_{func_name}\"] = self.compare_cfgs(\n",
    "                llm_functions[func_name],\n",
    "                static_functions[func_name],\n",
    "                func_name\n",
    "            )\n",
    "\n",
    "        llm_classes = {c[\"name\"]: c for c in llm_cfg.get(\"classes\", [])}\n",
    "        static_classes = {c[\"name\"]: c for c in static_cfg.get(\"classes\", [])}\n",
    "        common_classes = set(llm_classes.keys()) & set(static_classes.keys())\n",
    "\n",
    "        for class_name in common_classes:\n",
    "            nested_results[f\"class_{class_name}\"] = self.compare_cfgs(\n",
    "                llm_classes[class_name],\n",
    "                static_classes[class_name],\n",
    "                class_name\n",
    "            )\n",
    "\n",
    "        return CFGSimilarityResult(\n",
    "            filename=name,\n",
    "            edge_coverage=edge_coverage,\n",
    "            content_similarity=content_sim,\n",
    "            structure_similarity=structure_sim,\n",
    "            matched_edges=matched_edges,\n",
    "            gt_edges=gt_edges,\n",
    "            llm_edges=llm_edges,\n",
    "            nested_results=nested_results if nested_results else None\n",
    "        )\n",
    "\n",
    "\n",
    "class CFGEvaluator:\n",
    "    def __init__(self, llm_cfg_dir: str, static_cfg_dir: str, result_file: str):\n",
    "        self.llm_cfg_dir = Path(llm_cfg_dir)\n",
    "        self.static_cfg_dir = Path(static_cfg_dir)\n",
    "        self.result_file = Path(result_file)\n",
    "        self.comparator = CFGComparator()\n",
    "        self.results = []\n",
    "\n",
    "    def process_file(self, llm_cfg_path: Path) -> Optional[CFGSimilarityResult]:\n",
    "        static_cfg_path = self.static_cfg_dir / llm_cfg_path.name\n",
    "        if not static_cfg_path.exists():\n",
    "            return None\n",
    "\n",
    "        with open(llm_cfg_path) as f:\n",
    "            llm_cfg = json.load(f)\n",
    "        with open(static_cfg_path) as f:\n",
    "            static_cfg = json.load(f)\n",
    "\n",
    "        result = self.comparator.compare_cfgs(llm_cfg, static_cfg, llm_cfg_path.name)\n",
    "        self.results.append(result)\n",
    "        self.save_results()\n",
    "        return result\n",
    "\n",
    "    def save_results(self):\n",
    "        with open(self.result_file, \"w\") as f:\n",
    "            json.dump(\n",
    "                [self._result_to_dict(r) for r in self.results],\n",
    "                f,\n",
    "                indent=2\n",
    "            )\n",
    "\n",
    "    def evaluate_all(self) -> List[CFGSimilarityResult]:\n",
    "        llm_cfg_paths = list(self.llm_cfg_dir.glob(\"*.json\"))\n",
    "        for llm_cfg_path in tqdm(llm_cfg_paths, desc=\"处理CFG文件\"):\n",
    "            self.process_file(llm_cfg_path)\n",
    "        return self.results\n",
    "\n",
    "    @staticmethod\n",
    "    def _result_to_dict(result: CFGSimilarityResult) -> Dict:\n",
    "        return {\n",
    "            \"filename\": result.filename,\n",
    "            \"edge_coverage\": result.edge_coverage,\n",
    "            \"content_similarity\": result.content_similarity,\n",
    "            \"structure_similarity\": result.structure_similarity,\n",
    "            \"matched_edges\": result.matched_edges,\n",
    "            \"gt_edges\": result.gt_edges,\n",
    "            \"llm_edges\": result.llm_edges,\n",
    "            \"nested_results\": {\n",
    "                k: CFGEvaluator._result_to_dict(v)\n",
    "                for k, v in result.nested_results.items()\n",
    "            } if result.nested_results else None\n",
    "        }\n",
    "\n",
    "\n",
    "def calculate_aggregate_metrics(results: List[CFGSimilarityResult]) -> Dict:\n",
    "    metrics = {\n",
    "        \"total_cfgs_compared\": len(results),\n",
    "        \"average_edge_coverage\": np.mean([r.edge_coverage for r in results]),\n",
    "        \"average_content_similarity\": np.mean([r.content_similarity for r in results]),\n",
    "        \"average_structure_similarity\": np.mean([r.structure_similarity for r in results]),\n",
    "        \"total_gt_edges\": sum(r.gt_edges for r in results),\n",
    "        \"total_llm_edges\": sum(r.llm_edges for r in results),\n",
    "        \"total_matched_edges\": sum(r.matched_edges for r in results)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    evaluator = CFGEvaluator(\n",
    "        llm_cfg_dir=\"llm_cfg\",\n",
    "        static_cfg_dir=\"../../dataset/cangjie_cfg\",\n",
    "        result_file=\"evaluation_results.json\"\n",
    "    )\n",
    "\n",
    "    results = evaluator.evaluate_all()\n",
    "    metrics = calculate_aggregate_metrics(results)\n",
    "\n",
    "    print(\"\\nAutomatic Evaluation Summary:\")\n",
    "    print(f\"Total CFGs compared: {metrics['total_cfgs_compared']}\")\n",
    "    print(f\"Average Edge Coverage: {metrics['average_edge_coverage']:.2f}\")\n",
    "    print(f\"Average Content Similarity: {metrics['average_content_similarity']:.2f}\")\n",
    "    print(f\"Average Structure Similarity: {metrics['average_structure_similarity']:.2f}\")\n",
    "\n",
    "    print(f\"Total GT edges: {metrics['total_gt_edges']}\")\n",
    "    print(f\"Total LLM edges: {metrics['total_llm_edges']}\")\n",
    "    print(f\"Total Matched edges: {metrics['total_matched_edges']}\")\n",
    "\n",
    "    print(f\"Precision: {metrics['total_matched_edges'] / metrics['total_llm_edges']}\")\n",
    "    print(f\"Recall: {metrics['total_matched_edges'] / metrics['total_gt_edges']}\")\n",
    "    print(f\"F1 Score: {2 * metrics['total_matched_edges'] / (metrics['total_llm_edges'] + metrics['total_gt_edges'])}\")\n",
    "\n",
    "    with open(\"evaluation_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files for index 34: ['merged_llm_cfg_with_line_no/34.json']\n",
      "Missing files for index 53: ['merged_llm_cfg_with_line_no/53.json']\n",
      "Missing files for index 55: ['merged_llm_cfg_with_line_no/55.json']\n",
      "Missing files for index 62: ['merged_llm_cfg_with_line_no/62.json']\n",
      "Missing files for index 85: ['merged_llm_cfg_with_line_no/85.json']\n",
      "Missing files for index 99: ['merged_llm_cfg_with_line_no/99.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CFGs:   2%|▏         | 3/200 [00:01<02:03,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files for index 160: ['merged_llm_cfg_with_line_no/160.json']\n",
      "Missing files for index 164: ['merged_llm_cfg_with_line_no/164.json']\n",
      "Missing files for index 172: ['merged_llm_cfg_with_line_no/172.json']\n",
      "Missing files for index 186: ['merged_llm_cfg_with_line_no/186.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CFGs: 100%|██████████| 200/200 [00:46<00:00,  4.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm import tqdm\n",
    "# 假设 get_llm_answers 从您的 llm 模块导入\n",
    "from llm import get_llm_answers  \n",
    "\n",
    "from typing import Set, Tuple\n",
    "\n",
    "def convert_cfg_json_to_edges(cfg_json: dict) -> list[str]:\n",
    "    \"\"\"基于代码块内容生成控制流边，不依赖节点ID\"\"\"\n",
    "    edges = []\n",
    "    edge_counter = 0\n",
    "\n",
    "    def get_block_label(block: dict) -> str:\n",
    "        \"\"\"提取标准化的块标签\"\"\"\n",
    "        label = block.get('label', '')\n",
    "        # 确保label是字符串类型\n",
    "        if isinstance(label, list):\n",
    "            label = ' '.join(label)\n",
    "        elif not isinstance(label, str):\n",
    "            label = str(label)\n",
    "        # 清理换行和多余空格\n",
    "        return ' '.join(label.replace('\\n', '\\\\n').strip().split())\n",
    "\n",
    "    def process_entity(entity: dict):\n",
    "        \"\"\"递归处理各类实体（函数/类/块）\"\"\"\n",
    "        # 处理当前实体的直接blocks\n",
    "        for block in entity.get('blocks', []):\n",
    "            process_block(block)\n",
    "\n",
    "        # 递归处理嵌套结构\n",
    "        for key in ['functions', 'classes']:\n",
    "            for sub_entity in entity.get(key, []):\n",
    "                process_entity(sub_entity)\n",
    "\n",
    "    def process_block(block: dict):\n",
    "        \"\"\"处理单个代码块及其后继\"\"\"\n",
    "        nonlocal edge_counter\n",
    "        \n",
    "        # 获取当前块的规范化标签\n",
    "        source_label = get_block_label(block)\n",
    "        if not source_label:\n",
    "            return\n",
    "\n",
    "        # 处理直接后继\n",
    "        for succ in block.get('successors', []):\n",
    "            target_label = get_block_label(succ)\n",
    "            if target_label:\n",
    "                edges.append(\n",
    "                    f\"Edge {edge_counter}: [Source] {source_label} => \"\n",
    "                    f\"[Target] {target_label}\"\n",
    "                )\n",
    "                edge_counter += 1\n",
    "\n",
    "        # 递归处理嵌套blocks（如if/else内的块）\n",
    "        for key in ['blocks', 'successors']:\n",
    "            for sub_block in block.get(key, []):\n",
    "                process_block(sub_block)\n",
    "\n",
    "    # 从根节点开始处理\n",
    "    process_entity(cfg_json)\n",
    "    return edges\n",
    "\n",
    "def get_prompt(code: str, llm_cfg: dict, static_cfg: dict):\n",
    "    prompt = f\"\"\"\n",
    "Role: Control Flow Graph Validation Specialist\n",
    "Objective: Accurately compare CFG structures between static analysis (ground truth) and LLM generation\n",
    "\n",
    "### JSON Structure Definition\n",
    "Ground Truth (static_cfg) & LLM Output (llm_cfg) follow:\n",
    "[\n",
    "    \"Edge 0: [Source] node_A -> [Target] node_B\",\n",
    "    \"Edge 1: [Source] node_C -> [Target] node_D\",\n",
    "    ...\n",
    "]\n",
    "\n",
    "### Comparison Criteria\n",
    "1. Structure Matching:\n",
    "   Match edges when:\n",
    "   - Same branching pattern (sequential/conditional/loop)\n",
    "   - Equivalent depth in nested structure\n",
    "   - Matching control flow order\n",
    "\n",
    "2. Mismatch Conditions:\n",
    "   - Different number of successors in equivalent blocks\n",
    "   - Inconsistent branch types (e.g., true/false vs multiple)\n",
    "   - Missing/extra exception handling flows\n",
    "\n",
    "### Analysis Task\n",
    "1. For static_cfg:\n",
    "   - Count total edges (ground truth)\n",
    "   - Map control flow patterns\n",
    "\n",
    "2. For llm_cfg:\n",
    "   - Count total generated edges\n",
    "   - Identify structurally matched edges\n",
    "   - Traverse each edge and check if it corresponds to static analysis\n",
    "\n",
    "3. Output (JSON):\n",
    "{{\n",
    "  \"edge_analysis\": {{\n",
    "    \"static_total\": \"Number of edges from static analysis\",\n",
    "    \"llm_total\": \"Number of edges generated by LLM\",\n",
    "    \"matched_edges\": {{\n",
    "      \"exact_matches\": \"Number of exactly matched edges (type + position)\", \n",
    "      \"partial_matches\": \"Number of type-matched edges with different positions\"\n",
    "    }},\n",
    "    \"accuracy_metrics\": {{\n",
    "      \"precision\": \"exact_matches / llm_total\",\n",
    "      \"recall\": \"exact_matches / static_total\", \n",
    "      \"f1_score\": \"2*(precision*recall)/(precision+recall)\"\n",
    "    }}\n",
    "  }},\n",
    "  \"structure_validation\": {{\n",
    "    \"missing_blocks\": [\"Unmatched static block IDs\"],\n",
    "    \"extra_blocks\": [\"Extra LLM block IDs\"]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "### Input Data\n",
    "TypeScript Code:\n",
    "{code}\n",
    "\n",
    "Static Analysis CFG (Ground Truth):\n",
    "{json.dumps(static_cfg, indent=2)}\n",
    "\n",
    "LLM Generated CFG:\n",
    "{json.dumps(convert_cfg_json_to_edges(llm_cfg), indent=2)}\n",
    "\n",
    "Output JSON analysis ONLY.\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "### Output Schema\n",
    "# {{\n",
    "#   \"total_gt_edges\": \"Count of all DOT edges\",\n",
    "#   \"total_llm_edges\": \"Count of all JSON edges\",\n",
    "#   \"total_matched_edges\": \"Structurally aligned edges\"\n",
    "# }}\n",
    "\n",
    "def process_file(i):\n",
    "    \"\"\"处理单个文件的CFG对比\"\"\"\n",
    "    # 路径配置\n",
    "    code_path = f\"../../dataset/cangjie/{i}.cj\"\n",
    "    llm_cfg_path = f\"merged_llm_cfg_with_line_no/{i}.json\"\n",
    "    static_cfg_path = f\"../../dataset/cangjie_cfg/{i}.json\"  # 改为JSON路径\n",
    "    result_path = f\"results/{i}.json\"\n",
    "\n",
    "    # 跳过已处理文件\n",
    "    if os.path.exists(result_path):\n",
    "        return\n",
    "\n",
    "    # 校验文件存在性\n",
    "    missing_files = [\n",
    "        p for p in [code_path, llm_cfg_path, static_cfg_path]\n",
    "        if not os.path.exists(p)\n",
    "    ]\n",
    "    if missing_files:\n",
    "        print(f\"Missing files for index {i}: {missing_files}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 读取并解析输入数据\n",
    "        code = open(code_path).read()\n",
    "        with open(llm_cfg_path) as f:\n",
    "            llm_cfg = json.load(f)\n",
    "        with open(static_cfg_path) as f:  # 读取JSON格式的static_cfg\n",
    "            static_cfg = json.load(f)\n",
    "\n",
    "        # 生成prompt并获取结果\n",
    "        prompt = get_prompt(code, llm_cfg, static_cfg)\n",
    "        #print(prompt)\n",
    "        result = json.loads(get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True))\n",
    "\n",
    "        #print(json.dumps(result, indent=2))\n",
    "        \n",
    "        # 保存结果\n",
    "        with open(result_path, \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON解析失败 {i}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {i} 时出错: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # 初始化结果目录\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "    #process_file(1)\n",
    "    \n",
    "    # 创建任务列表\n",
    "    file_indices = list(range(200))  # 根据实际文件数量调整\n",
    "    \n",
    "    # 并行处理\n",
    "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        tasks = executor.map(process_file, file_indices)\n",
    "        list(tqdm(tasks, total=len(file_indices), desc=\"Processing CFGs\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG 评估报告\n",
      "==================================================\n",
      "分析文件总数: 187\n",
      "\n",
      "[边匹配分析]\n",
      "静态分析总边数: 550\n",
      "LLM生成总边数: 1248\n",
      "精确匹配边数: 228 (41.45%)\n",
      "部分匹配边数: 74 (13.45%)\n",
      "总匹配边数: 302 (54.91%)\n",
      "精确率 (Precision): 0.2420\n",
      "召回率 (Recall): 0.5491\n",
      "F1 值: 0.3359\n",
      "\n",
      "[结构验证]\n",
      "缺失块总数: 269 (示例: Edge 50, Edge 2: [Source] public func mapErr<F>(f: (E) -> F): Result<T, F> { ... => [Target] case Ok(t) => Ok(t), Edge 32: [Source] func getRubyTOKENFromStr(str: String): RUBYTOKEN { ... => [Target] case \"else\" => RUBYTOKEN.ELSE, Edge 113, Edge 7 (static_cfg))\n",
      "多余块总数: 610 (示例: Edge 0: [Source] class Brower {\\n var metadata: Metadata\\n var links: Links\\n var navigation: Navigation\\nvar groups: ArrayList<Group> = ArrayList<Group>() => [Target] }, Edge 34: [Source] match (this.int64) { => [Target] case Option.Some(v) => return v, Edge 2: [Source] if (sum != 0) {\\n println(sum)\\n return sum\\n }\\nsum = tesSparkMD5_02()\\n if (sum != 0) { => [Target] println(sum)\\n return sum\\n }\\nsum = tesSparkMD5_03()\\n if (sum != 0) {, let result = ArrayList<String>() => [Target] for (i in 0..indices.size:2) {\\n result.append(text[indices[i]..indices[i + 1]])\\n }, Edge 10: [Source] for (reval in context.valDecls) {\\n body.append(quote(\\n dms.add(field($(reval.identifier.value), this.$(reval.identifier)))\\n ))\\n } => [Target] let serializeFunc = FuncDecl(quote(\\n public func serialize(): DataModel {\\n let dms = DataModelStruct()\\n $(body)\\n return dms\\n }\\n ))\\n context.structDecl.body.decls.append(serializeFunc)\\n })\n",
      "\n",
      "[数据质量问题]\n",
      "错误文件数: 3\n",
      "示例错误:\n",
      "  - 193.json: unsupported operand type(s) for +=: 'int' and 'str'\n",
      "  - 101.json: 'extra_blocks'\n",
      "  - 184.json: unsupported operand type(s) for +=: 'int' and 'str'\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def calculate_global_metrics(results_dir: str) -> dict:\n",
    "    \"\"\"精确的全局指标统计\"\"\"\n",
    "    metrics = {\n",
    "        \"total_files\": 0,\n",
    "        \"gt_edges\": 0,\n",
    "        \"llm_edges\": 0,\n",
    "        \"exact_matches\": 0,\n",
    "        \"partial_matches\": 0,\n",
    "        \"missing_blocks\": set(),\n",
    "        \"extra_blocks\": set(),\n",
    "        \"file_errors\": []\n",
    "    }\n",
    "\n",
    "    # 遍历结果目录\n",
    "    for filename in os.listdir(results_dir):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(results_dir, filename)\n",
    "        try:\n",
    "            with open(filepath) as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # 核心指标累加\n",
    "            edge_analysis = data[\"edge_analysis\"]\n",
    "            metrics[\"gt_edges\"] += edge_analysis[\"static_total\"]\n",
    "            metrics[\"llm_edges\"] += edge_analysis[\"llm_total\"]\n",
    "            metrics[\"exact_matches\"] += edge_analysis[\"matched_edges\"][\"exact_matches\"]\n",
    "            metrics[\"partial_matches\"] += edge_analysis[\"matched_edges\"][\"partial_matches\"]\n",
    "\n",
    "            # 结构验证统计\n",
    "            structure = data[\"structure_validation\"]\n",
    "            metrics[\"missing_blocks\"].update(map(str, structure[\"missing_blocks\"]))\n",
    "            metrics[\"extra_blocks\"].update(map(str, structure[\"extra_blocks\"]))\n",
    "\n",
    "            metrics[\"total_files\"] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            metrics[\"file_errors\"].append(f\"{filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 计算衍生指标\n",
    "    total_matched = metrics[\"exact_matches\"] + metrics[\"partial_matches\"]\n",
    "    \n",
    "    precision = total_matched / metrics[\"llm_edges\"] if metrics[\"llm_edges\"] > 0 else 0\n",
    "    recall = total_matched / metrics[\"gt_edges\"] if metrics[\"gt_edges\"] > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"file_count\": metrics[\"total_files\"],\n",
    "        \"edge_metrics\": {\n",
    "            \"static_total\": metrics[\"gt_edges\"],\n",
    "            \"llm_total\": metrics[\"llm_edges\"],\n",
    "            \"exact_matches\": metrics[\"exact_matches\"],\n",
    "            \"partial_matches\": metrics[\"partial_matches\"],\n",
    "            \"total_matched\": total_matched,\n",
    "            \"precision\": round(precision, 4),\n",
    "            \"recall\": round(recall, 4),\n",
    "            \"f1_score\": round(f1, 4)\n",
    "        },\n",
    "        \"structure_metrics\": {\n",
    "            \"missing_blocks_count\": len(metrics[\"missing_blocks\"]),\n",
    "            \"extra_blocks_count\": len(metrics[\"extra_blocks\"]),\n",
    "            \"missing_block_samples\": list(metrics[\"missing_blocks\"])[:5],  # 示例显示前5个\n",
    "            \"extra_block_samples\": list(metrics[\"extra_blocks\"])[:5]\n",
    "        },\n",
    "        \"data_quality\": {\n",
    "            \"error_files\": len(metrics[\"file_errors\"]),\n",
    "            \"error_samples\": metrics[\"file_errors\"][:3]  # 示例显示前3个错误\n",
    "        }\n",
    "    }\n",
    "\n",
    "def print_report(metrics: dict):\n",
    "    \"\"\"格式化输出报告\"\"\"\n",
    "    print(\"CFG 评估报告\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"分析文件总数: {metrics['file_count']}\")\n",
    "    \n",
    "    print(\"\\n[边匹配分析]\")\n",
    "    em = metrics['edge_metrics']\n",
    "    print(f\"静态分析总边数: {em['static_total']}\")\n",
    "    print(f\"LLM生成总边数: {em['llm_total']}\")\n",
    "    print(f\"精确匹配边数: {em['exact_matches']} ({em['exact_matches']/em['static_total']:.2%})\")\n",
    "    print(f\"部分匹配边数: {em['partial_matches']} ({em['partial_matches']/em['static_total']:.2%})\")\n",
    "    print(f\"总匹配边数: {em['total_matched']} ({em['total_matched']/em['static_total']:.2%})\")\n",
    "    print(f\"精确率 (Precision): {em['precision']:.4f}\")\n",
    "    print(f\"召回率 (Recall): {em['recall']:.4f}\")\n",
    "    print(f\"F1 值: {em['f1_score']:.4f}\")\n",
    "\n",
    "    print(\"\\n[结构验证]\")\n",
    "    sm = metrics['structure_metrics']\n",
    "    print(f\"缺失块总数: {sm['missing_blocks_count']} (示例: {', '.join(sm['missing_block_samples'])})\")\n",
    "    print(f\"多余块总数: {sm['extra_blocks_count']} (示例: {', '.join(sm['extra_block_samples'])})\")\n",
    "\n",
    "    if metrics['data_quality']['error_files'] > 0:\n",
    "        print(\"\\n[数据质量问题]\")\n",
    "        print(f\"错误文件数: {metrics['data_quality']['error_files']}\")\n",
    "        print(\"示例错误:\")\n",
    "        for err in metrics['data_quality']['error_samples']:\n",
    "            print(f\"  - {err}\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# 使用示例\n",
    "results_dir = \"results\"\n",
    "metrics = calculate_global_metrics(results_dir)\n",
    "print_report(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
