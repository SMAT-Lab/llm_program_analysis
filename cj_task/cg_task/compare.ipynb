{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_cg_dir = \"llm_cg\"\n",
    "import os\n",
    "import json\n",
    "for file in os.listdir(llm_cg_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        llm_cg = json.load(open(os.path.join(llm_cg_dir, file)))\n",
    "        # 过滤掉重复的值,只保留一个\n",
    "        llm_cg = {k: list(set(v)) for k, v in llm_cg.items()}\n",
    "        json.dump(llm_cg, open(os.path.join(llm_cg_dir, file), \"w\"), indent=4, ensure_ascii=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "平均指标:\n",
      "Precision: 0.8003\n",
      "Recall: 0.6488\n",
      "F1 Score: 0.7166\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def normalize_name(call):\n",
    "    \"\"\"\n",
    "    Normalize a function/method name for comparison.\n",
    "    - Removes redundant module prefixes if possible.\n",
    "    - Converts the name to a comparable base representation.\n",
    "    \"\"\"\n",
    "    return call.split('.')[-1]  # Use only the last part (e.g., \"backend.data.graph.Node\" -> \"Node\")\n",
    "\n",
    "\n",
    "def compare_call_graphs(generated, ground_truth, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Compares the generated call graph with the ground truth and calculates metrics.\n",
    "\n",
    "    Parameters:\n",
    "        generated (dict): The call graph generated by the analyzer.\n",
    "        ground_truth (dict): The static ground truth call graph.\n",
    "        similarity_threshold (float): Threshold for considering two function names as similar.\n",
    "\n",
    "    Returns:\n",
    "        dict: A detailed comparison report including metrics and mismatches.\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"missing_keys\": [],\n",
    "        \"extra_keys\": [],\n",
    "        \"mismatched_calls\": [],\n",
    "        \"metrics\": {\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"f1_score\": 0.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Helper function for similarity matching\n",
    "    def is_similar(call1, call2):\n",
    "        call1 = call1.split(\".\")[-1]\n",
    "        call2 = call2.split(\".\")[-1]\n",
    "        return SequenceMatcher(None, normalize_name(call1), normalize_name(call2)).ratio() >= similarity_threshold\n",
    "\n",
    "    # 过滤掉builtin函数\n",
    "    generated = {k: [c for c in v if 'builtin' not in c] for k, v in generated.items()}\n",
    "    ground_truth = {k: [c for c in v if 'builtin' not in c] for k, v in ground_truth.items()}\n",
    "\n",
    "    # Convert keys to sets\n",
    "    generated_keys = set(generated.keys())\n",
    "    ground_truth_keys = set(ground_truth.keys())\n",
    "\n",
    "    # Detect missing and extra keys\n",
    "    report[\"missing_keys\"] = list(ground_truth_keys - generated_keys)\n",
    "    report[\"extra_keys\"] = list(generated_keys - ground_truth_keys)\n",
    "\n",
    "    # Check mismatched calls for common keys\n",
    "    total_matches = 0\n",
    "    total_ground_truth_calls = 0\n",
    "    total_generated_calls = 0\n",
    "\n",
    "    common_keys = ground_truth_keys.intersection(generated_keys)\n",
    "    for key in common_keys:\n",
    "        ground_truth_calls = ground_truth[key]\n",
    "        generated_calls = generated[key]\n",
    "        total_ground_truth_calls += len(ground_truth_calls)\n",
    "        total_generated_calls += len(generated_calls)\n",
    "\n",
    "        matched_calls = set()\n",
    "        unmatched_generated = []\n",
    "        unmatched_ground_truth = []\n",
    "\n",
    "        for gt_call in ground_truth_calls:\n",
    "            found = False\n",
    "            for gen_call in generated_calls:\n",
    "                if is_similar(gt_call, gen_call):\n",
    "                    matched_calls.add(gt_call)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                unmatched_ground_truth.append(gt_call)\n",
    "\n",
    "        for gen_call in generated_calls:\n",
    "            if not any(is_similar(gen_call, gt_call) for gt_call in ground_truth_calls):\n",
    "                unmatched_generated.append(gen_call)\n",
    "\n",
    "        total_matches += len(matched_calls)\n",
    "        if unmatched_generated or unmatched_ground_truth:\n",
    "            report[\"mismatched_calls\"].append({\n",
    "                \"key\": key,\n",
    "                \"missing_calls\": unmatched_ground_truth,\n",
    "                \"extra_calls\": unmatched_generated\n",
    "            })\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    if total_generated_calls > 0:\n",
    "        precision = total_matches / total_generated_calls\n",
    "    else:\n",
    "        if total_ground_truth_calls > 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            precision = 1.0\n",
    "\n",
    "    if total_ground_truth_calls > 0:\n",
    "        recall = total_matches / total_ground_truth_calls\n",
    "    else:\n",
    "        recall = 1.0\n",
    "\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "\n",
    "    report[\"metrics\"][\"precision\"] = round(precision, 4)\n",
    "    report[\"metrics\"][\"recall\"] = round(recall, 4)\n",
    "    report[\"metrics\"][\"f1_score\"] = round(f1_score, 4)\n",
    "    report[\"metrics\"][\"total_matches\"] = total_matches\n",
    "    report[\"metrics\"][\"total_ground_truth_calls\"] = total_ground_truth_calls\n",
    "    report[\"metrics\"][\"total_generated_calls\"] = total_generated_calls\n",
    "\n",
    "    return report\n",
    "\n",
    "# Example usage\n",
    "import os\n",
    "if __name__ == \"__main__\":\n",
    "    total_matches = 0\n",
    "    total_ground_truth_calls = 0\n",
    "    total_generated_calls = 0\n",
    "    \n",
    "    for i in range(200):\n",
    "        if not os.path.exists(f\"llm_cg/{i}.json\"):\n",
    "            continue\n",
    "        if not os.path.exists(f\"../../dataset/cangjie_cg/{i}.json\"):\n",
    "            continue\n",
    "            \n",
    "        generated = json.load(open(f\"llm_cg/{i}.json\"))\n",
    "        ground_truth = json.load(open(f\"../../dataset/cangjie_cg/{i}.json\"))\n",
    "        \n",
    "        differences = compare_call_graphs(generated, ground_truth)\n",
    "        metrics = differences[\"metrics\"]\n",
    "        \n",
    "        total_matches += metrics[\"total_matches\"]\n",
    "        total_ground_truth_calls += metrics[\"total_ground_truth_calls\"]\n",
    "        total_generated_calls += metrics[\"total_generated_calls\"]\n",
    "            \n",
    "        \n",
    "        # print(f\"file {i} metrics:\")\n",
    "        # print(json.dumps(metrics, indent=4))\n",
    "        # 如果precision低于0.5,删除该文件\n",
    "        # if metrics[\"precision\"] < 0.5:\n",
    "        #     print(f\"file {i} metrics:\")\n",
    "        #     print(json.dumps(metrics, indent=4))\n",
    "            # os.remove(f\"llm_cg/{i}.json\")\n",
    "            # print(f\"已删除文件 llm_cg/{i}.json (precision={metrics['precision']})\")\n",
    "    \n",
    "    # 使用总体数据计算指标\n",
    "    precision = round(total_matches / total_generated_calls, 4) if total_generated_calls > 0 else 0\n",
    "    recall = round(total_matches / total_ground_truth_calls, 4) if total_ground_truth_calls > 0 else 0\n",
    "    f1_score = round(2 * (precision * recall) / (precision + recall), 4) if precision + recall > 0 else 0\n",
    "    \n",
    "    print(\"\\n平均指标:\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")  \n",
    "    print(f\"F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_cg_dir = \"llm_cg\"\n",
    "source_dir = \"../../dataset/cangjie\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "datas = []\n",
    "\n",
    "for i in range(200):\n",
    "    llm_cg_path = os.path.join(llm_cg_dir, f\"{i}.json\")\n",
    "    # static_cg_path = os.path.join(static_cg_dir, f\"{i}.json\")\n",
    "    source_path = os.path.join(source_dir, f\"{i}.cj\")\n",
    "\n",
    "    if not os.path.exists(llm_cg_path) or not os.path.exists(source_path):\n",
    "        continue\n",
    "\n",
    "    with open(source_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        source_code = f.read()\n",
    "\n",
    "    with open(llm_cg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        llm_cg = json.load(f)  \n",
    "\n",
    "    static_cg = {}\n",
    "\n",
    "    data = {    \n",
    "        \"source_code\": source_code,\n",
    "        \"llm_cg\": llm_cg,\n",
    "        \"static_cg\": static_cg\n",
    "    }\n",
    "    datas.append(data)\n",
    "\n",
    "with open(\"cg_task.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in datas:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "总体指标:\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "F1 Score: 0\n"
     ]
    }
   ],
   "source": [
    "from llm import get_llm_answers\n",
    "import json\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "def process_line(line):\n",
    "    return{\n",
    "        \"sum_call_from_static\": 0,\n",
    "        \"correct_call_from_llm\": 0,\n",
    "        \"missing_call_from_llm\": 0,\n",
    "        \"extra_call_from_llm\": 0\n",
    "    }\n",
    "\n",
    "jsonl_file = \"cg_task.jsonl\"\n",
    "results = []\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(\"results_local\", exist_ok=True)\n",
    "\n",
    "with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    # 解析每一行获取文件名\n",
    "    files = [f\"{i}.cj\" for i in range(200)]\n",
    "    \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    future_to_line = {executor.submit(process_line, line): i for i, line in enumerate(lines)}\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(future_to_line):\n",
    "        line_index = future_to_line[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            # 添加文件名到结果中\n",
    "            result[\"file_name\"] = files[line_index]\n",
    "            results.append(result)\n",
    "            # 保存单个结果\n",
    "            with open(f\"results_local/result_{line_index}.json\", \"w\") as f:\n",
    "                json.dump(result, f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f'处理第 {line_index} 行时发生错误: {str(e)}')\n",
    "\n",
    "# 按照file_name排序\n",
    "results.sort(key=lambda x: x[\"file_name\"])\n",
    "\n",
    "# 保存所有结果\n",
    "with open(\"results_local/all_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# 计算总体指标\n",
    "total_correct = sum(r[\"correct_call_from_llm\"] for r in results)\n",
    "total_static = sum(r[\"sum_call_from_static\"] for r in results)\n",
    "total_llm = sum(r[\"correct_call_from_llm\"] + r[\"extra_call_from_llm\"] for r in results)\n",
    "\n",
    "precision = round(total_correct / total_llm, 4) if total_llm > 0 else 0\n",
    "recall = round(total_correct / total_static, 4) if total_static > 0 else 0\n",
    "f1_score = round(2 * (precision * recall) / (precision + recall), 4) if precision + recall > 0 else 0\n",
    "\n",
    "print(\"\\n总体指标:\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
