{
    "Tokenizer.createTokenizer": [
        "HuggingfaceTokenizer.HuggingfaceTokenizer",
        "HuggingfaceTokenizer.load_vocab",
        "println"
    ],
    "Tokenizer.is_stop": [],
    "Tokenizer.is_special": [],
    "Tokenizer.encode": [
        "println"
    ],
    "Tokenizer.decode": [
        "println"
    ],
    "Tokenizer.load_special": [
        "println"
    ],
    "Tokenizer.load_vocab": [
        "File.exists",
        "Exception",
        "println"
    ],
    "Tiktoken.decode": [],
    "Tiktoken.load_vocab": [
        "File.exists",
        "Exception"
    ],
    "Tiktoken.encode": [
        "ArrayList.ArrayList",
        "HuggingfaceTokenizer.encode"
    ],
    "HashPairString.HashPairString": [],
    "HashPairString.hashCode": [],
    "HashPairString.==": [],
    "HashPairUInt32.HashPairUInt32": [],
    "HashPairUInt32.toString": [],
    "HashPairUInt32.hashCode": [],
    "HashPairUInt32.==": [],
    "HuggingfaceTokenizer.HuggingfaceTokenizer": [
        "HuggingfaceTokenizer.get_byte_char",
        "HashPairUInt32.HashPairUInt32",
        "HuggingfaceTokenizer.put",
        "HashPairString",
        "HuggingfaceTokenizer.put"
    ],
    "HuggingfaceTokenizer.get_byte_char": [
        "ArrayList.ArrayList",
        "pow",
        "HashMap.HashMap",
        "HashMap.put"
    ],
    "HuggingfaceTokenizer.load_vocab": [
        "File.exists",
        "Exception",
        "File.openRead",
        "File.readToEnd",
        "File.close",
        "ByteArrayStream.ByteArrayStream",
        "ByteArrayStream.write",
        "JsonReader.JsonReader",
        "TokenizerJson.fromJson",
        "HuggingfaceTokenizer.get_byte_char",
        "HashMap.put",
        "HashMap.contains",
        "Exception"
    ],
    "HuggingfaceTokenizer.token_to_id": [],
    "HuggingfaceTokenizer.id_to_token": [],
    "HuggingfaceTokenizer.apply_chat_template": [
        "Exception"
    ],
    "HuggingfaceTokenizer.decode": [
        "HashMap.contains",
        "HashMap.put",
        "HuggingfaceTokenizer.Rune.toRuneArray",
        "HuggingfaceTokenizer.rune2byte",
        "HuggingfaceTokenizer.ByteArrayStream.toArray",
        "String.fromUtf8"
    ],
    "HuggingfaceTokenizer.encode": [
        "HuggingfaceTokenizer.token_to_id",
        "HuggingfaceTokenizer.get_pairs",
        "Exception",
        "HuggingfaceTokenizer.contains",
        "HuggingfaceTokenizer.put",
        "HuggingfaceTokenizer.bpe",
        "HuggingfaceTokenizer.contains",
        "HuggingfaceTokenizer.put",
        "HashMap.contains",
        "Exception",
        "HashMap.put",
        "HashMap.contains",
        "Regex.Regex",
        "Regex.matcher",
        "MatchData.iterator",
        "HashMap.put",
        "HashSet.contains",
        "HuggingfaceTokenizer.vocab.get",
        "HuggingfaceTokenizer.Rune.toRuneArray",
        "HuggingfaceTokenizer.getPairs",
        "HashSet.contains",
        "ArrayList.iterator",
        "ArrayList.append",
        "HashMap.getOrThrow",
        "HashMap.put"
    ],
    "HuggingfaceTokenizer.get_pairs": [],
    "HuggingfaceTokenizer.bpe": [
        "ArrayList.ArrayList",
        "HashSet.HashSet",
        "HashSet.contains",
        "HashSet.contains",
        "HashSet.put",
        "HuggingfaceTokenizer.HashPairString.contains",
        "HuggingfaceTokenizer.HashPairString.contains",
        "HuggingfaceTokenizer.left_index"
    ]
}