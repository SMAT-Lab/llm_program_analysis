{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code_dir = \"../../dataset/ts\"\n",
    "static_ast_dir = \"../../dataset/ts_ast\"\n",
    "\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "def tokenize_code_with_lines(code: str) -> List[Tuple[int, int, str, int]]:\n",
    "    \"\"\"\n",
    "    使用正则分词，直接在原始代码上通过 finditer() 获取匹配位置，\n",
    "    并保留换行符，让 LLM 能识别多行结构。\n",
    "    返回形式: [(start_offset, end_offset, token_text, line_number), ...]\n",
    "    \"\"\"\n",
    "    token_pattern = (\n",
    "        r'[A-Za-z_]\\w*|'  # 标识符\n",
    "        r'[0-9]+|'        # 数字\n",
    "        r'\"[^\"]*\"|'       # 双引号字符串\n",
    "        r\"'[^']*'|\"       # 单引号字符串\n",
    "        r'\\\\[ntr]|'       # 转义符 \\n \\t \\r\n",
    "        r'//.*|'          # 单行注释 (如 C++/Java/JS 风格)\n",
    "        r'/\\*.*?\\*/|'     # 多行注释 (如 C 风格)\n",
    "        r'\\n|\\r|\\t|'      # 换行/回车/制表符\n",
    "        r'\\S'             # 其他符号(如 +, -, {, }, 以及任何其它非空白字符)\n",
    "    )\n",
    "\n",
    "    tokens_with_offset = []\n",
    "    lines = code.splitlines(keepends=True)\n",
    "    current_line = 1\n",
    "    current_pos = 0\n",
    "    \n",
    "    for match in re.finditer(token_pattern, code, re.MULTILINE | re.DOTALL):\n",
    "        tk = match.group(0)\n",
    "        start_offset, end_offset = match.span()\n",
    "        \n",
    "        # 计算当前token所在行号\n",
    "        while current_line <= len(lines) and current_pos + len(lines[current_line-1]) <= start_offset:\n",
    "            current_pos += len(lines[current_line-1])\n",
    "            current_line += 1\n",
    "            \n",
    "        tokens_with_offset.append((start_offset, end_offset, tk, current_line))\n",
    "\n",
    "    return tokens_with_offset\n",
    "\n",
    "import os\n",
    "import json\n",
    "for i in range(200):\n",
    "    file_path = os.path.join(source_code_dir, f\"{i}.ts\")\n",
    "    if not os.path.exists(file_path):\n",
    "        continue\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        source_code = file.read()\n",
    "\n",
    "    ast_file_path = os.path.join(static_ast_dir, f\"{i}.json\")\n",
    "    if not os.path.exists(ast_file_path):\n",
    "        continue\n",
    "    with open(ast_file_path, 'r', encoding='utf-8') as file:\n",
    "        ast = json.load(file)\n",
    "\n",
    "    # 根据AST节点的name找到对应的token范围\n",
    "    tokens = tokenize_code_with_lines(source_code)\n",
    "    \n",
    "    def find_token_range(node, tokens):\n",
    "        \"\"\"递归查找AST节点对应的token范围\"\"\"\n",
    "        if not isinstance(node, dict):\n",
    "            return\n",
    "            \n",
    "        # 获取节点文本\n",
    "        label = node.get('label', '')\n",
    "        \n",
    "        # 在tokens中查找匹配的范围\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        \n",
    "        # 遍历所有tokens,找到最长的匹配\n",
    "        for i, (start, end, text, line) in enumerate(tokens):\n",
    "            # 如果当前token匹配label的开头\n",
    "            if label.startswith(text):\n",
    "                start_token = i\n",
    "                # 从当前位置继续向后匹配,直到完整匹配label\n",
    "                current_text = text\n",
    "                current_end = i\n",
    "                while current_end < len(tokens)-1 and len(current_text) < len(label):\n",
    "                    current_end += 1\n",
    "                    current_text += tokens[current_end][2]\n",
    "                    if current_text == label:\n",
    "                        end_token = current_end\n",
    "                        break\n",
    "                if end_token is not None:\n",
    "                    break\n",
    "                    \n",
    "        # 更新节点信息\n",
    "        if start_token is not None and end_token is not None:\n",
    "            node['start_token'] = start_token\n",
    "            node['end_token'] = end_token\n",
    "            \n",
    "        # 递归处理子节点\n",
    "        children = node.get('children', [])\n",
    "        for child in children:\n",
    "            find_token_range(child, tokens)\n",
    "            \n",
    "    # 处理AST\n",
    "    find_token_range(ast, tokens)\n",
    "    \n",
    "    # 保存更新后的AST\n",
    "    with open(ast_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ast, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有文件中提取的唯一类型:\n",
      "- AmpersandAmpersandToken\n",
      "- AnyKeyword\n",
      "- ArrayLiteralExpression\n",
      "- ArrayType\n",
      "- ArrowFunction\n",
      "- AsExpression\n",
      "- AsteriskToken\n",
      "- AsyncKeyword\n",
      "- AwaitExpression\n",
      "- BarBarToken\n",
      "- BarEqualsToken\n",
      "- BarToken\n",
      "- BinaryExpression\n",
      "- Block\n",
      "- BooleanKeyword\n",
      "- BreakStatement\n",
      "- CallExpression\n",
      "- CaseBlock\n",
      "- CaseClause\n",
      "- CatchClause\n",
      "- ClassDeclaration\n",
      "- ColonToken\n",
      "- ConditionalExpression\n",
      "- Constructor\n",
      "- ContinueStatement\n",
      "- DefaultClause\n",
      "- DefaultKeyword\n",
      "- DotDotDotToken\n",
      "- ElementAccessExpression\n",
      "- EmptyStatement\n",
      "- EndOfFileToken\n",
      "- EnumDeclaration\n",
      "- EnumMember\n",
      "- EqualsEqualsEqualsToken\n",
      "- EqualsEqualsToken\n",
      "- EqualsGreaterThanToken\n",
      "- ExclamationEqualsEqualsToken\n",
      "- ExclamationEqualsToken\n",
      "- ExportAssignment\n",
      "- ExportKeyword\n",
      "- ExpressionStatement\n",
      "- ExpressionWithTypeArguments\n",
      "- FalseKeyword\n",
      "- FirstAssignment\n",
      "- FirstBinaryOperator\n",
      "- FirstCompoundAssignment\n",
      "- FirstContextualKeyword\n",
      "- FirstLiteralToken\n",
      "- FirstNode\n",
      "- FirstStatement\n",
      "- FirstTemplateToken\n",
      "- ForInStatement\n",
      "- ForOfStatement\n",
      "- ForStatement\n",
      "- FunctionDeclaration\n",
      "- FunctionExpression\n",
      "- FunctionType\n",
      "- GreaterThanEqualsToken\n",
      "- GreaterThanGreaterThanToken\n",
      "- GreaterThanToken\n",
      "- HeritageClause\n",
      "- Identifier\n",
      "- IfStatement\n",
      "- ImportClause\n",
      "- ImportDeclaration\n",
      "- ImportSpecifier\n",
      "- InKeyword\n",
      "- InterfaceDeclaration\n",
      "- LastTemplateToken\n",
      "- LessThanEqualsToken\n",
      "- LessThanLessThanToken\n",
      "- LiteralType\n",
      "- MethodDeclaration\n",
      "- MethodSignature\n",
      "- MinusEqualsToken\n",
      "- MinusToken\n",
      "- NamedImports\n",
      "- NewExpression\n",
      "- NullKeyword\n",
      "- NumberKeyword\n",
      "- ObjectKeyword\n",
      "- ObjectLiteralExpression\n",
      "- Parameter\n",
      "- ParenthesizedExpression\n",
      "- PercentToken\n",
      "- PlusToken\n",
      "- PostfixUnaryExpression\n",
      "- PrefixUnaryExpression\n",
      "- PrivateKeyword\n",
      "- PropertyAccessExpression\n",
      "- PropertyAssignment\n",
      "- PropertyDeclaration\n",
      "- PropertySignature\n",
      "- ProtectedKeyword\n",
      "- PublicKeyword\n",
      "- QuestionDotToken\n",
      "- QuestionQuestionToken\n",
      "- QuestionToken\n",
      "- ReadonlyKeyword\n",
      "- RegularExpressionLiteral\n",
      "- ReturnStatement\n",
      "- SemicolonClassElement\n",
      "- SlashToken\n",
      "- SourceFile\n",
      "- StaticKeyword\n",
      "- StringKeyword\n",
      "- StringLiteral\n",
      "- SuperKeyword\n",
      "- SwitchStatement\n",
      "- TemplateExpression\n",
      "- TemplateHead\n",
      "- TemplateMiddle\n",
      "- TemplateSpan\n",
      "- ThisKeyword\n",
      "- ThrowStatement\n",
      "- TrueKeyword\n",
      "- TryStatement\n",
      "- TypeAliasDeclaration\n",
      "- TypeLiteral\n",
      "- TypeParameter\n",
      "- TypeReference\n",
      "- UndefinedKeyword\n",
      "- UnionType\n",
      "- VariableDeclaration\n",
      "- VariableDeclarationList\n",
      "- VoidKeyword\n",
      "- WhileStatement\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, Set\n",
    "\n",
    "def extract_types_from_cfg(cfg: Dict) -> set:\n",
    "    \"\"\"从AST配置中提取所有可能的类型\"\"\"\n",
    "    types = set()\n",
    "    \n",
    "    def traverse(node):\n",
    "        if isinstance(node, dict):\n",
    "            if \"type\" in node:\n",
    "                types.add(node[\"type\"])\n",
    "            for value in node.values():\n",
    "                traverse(value)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                traverse(item)\n",
    "                \n",
    "    traverse(cfg)\n",
    "    return types\n",
    "\n",
    "# 遍历所有json文件并去重\n",
    "all_types = set()\n",
    "processed_files = set()\n",
    "\n",
    "for root in [\"../../dataset/ts_ast\"]:\n",
    "    for file in os.listdir(root):\n",
    "        if not file.endswith('.json'):\n",
    "            continue\n",
    "            \n",
    "        # 检查文件是否已处理过(通过文件名去重)\n",
    "        if file in processed_files:\n",
    "            continue\n",
    "            \n",
    "        processed_files.add(file)\n",
    "        file_path = os.path.join(root, file)\n",
    "        \n",
    "        with open(file_path) as f:\n",
    "            cfg = json.load(f)\n",
    "            types = extract_types_from_cfg(cfg)\n",
    "            all_types.update(types)\n",
    "\n",
    "print(\"所有文件中提取的唯一类型:\")\n",
    "for t in sorted(all_types):\n",
    "    print(f\"- {t}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4cfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
