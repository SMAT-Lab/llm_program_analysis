{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import re\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tokenize_code_with_lines(code: str) -> List[Tuple[int, int, str, int]]:\n",
    "    \"\"\"\n",
    "    使用正则分词，直接在原始代码上通过 finditer() 获取匹配位置，\n",
    "    并保留换行符，让 LLM 能识别多行结构。\n",
    "    返回形式: [(start_offset, end_offset, token_text, line_number), ...]\n",
    "    \"\"\"\n",
    "    token_pattern = (\n",
    "        r'[A-Za-z_]\\w*|'       # 标识符\n",
    "        r'[0-9]+(?:\\.[0-9]+)?|'# 数字(包含小数)\n",
    "        r'\"[^\"]*\"|'            # 双引号字符串\n",
    "        r\"'[^']*'|\"            # 单引号字符串\n",
    "        r'`[^`]*`|'            # 模板字符串\n",
    "        r'//.*?(?=\\n|$)|'      # 单行注释(到行尾)\n",
    "        r'/\\*[\\s\\S]*?\\*/|'     # 多行注释\n",
    "        r'=>|'                 # 箭头函数\n",
    "        r'===|!==|==|!=|'      # 相等性操作符\n",
    "        r'&&|\\|\\||'            # 逻辑操作符\n",
    "        r'[-+*/=<>!&|^~?:;,.(){}[\\]]|' # 其他操作符和分隔符\n",
    "        r'\\n|\\r|\\t|'           # 换行/回车/制表符\n",
    "        r'\\s+'                 # 其他空白字符\n",
    "    )\n",
    "\n",
    "    tokens_with_offset = []\n",
    "    lines = code.splitlines(keepends=True)\n",
    "    current_line = 1\n",
    "    current_pos = 0\n",
    "    \n",
    "    for match in re.finditer(token_pattern, code, re.MULTILINE | re.DOTALL):\n",
    "        tk = match.group(0)\n",
    "        start_offset, end_offset = match.span()\n",
    "        \n",
    "        # 计算当前token所在行号\n",
    "        while current_line <= len(lines) and current_pos + len(lines[current_line-1]) <= start_offset:\n",
    "            current_pos += len(lines[current_line-1])\n",
    "            current_line += 1\n",
    "            \n",
    "        # 跳过纯空白token\n",
    "        if not tk.isspace():\n",
    "            tokens_with_offset.append((start_offset, end_offset, tk, current_line))\n",
    "\n",
    "    return tokens_with_offset\n",
    "\n",
    "with open(\"../../dataset/ts/0.ts\", \"r\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "tokens = tokenize_code_with_lines(code)\n",
    "print(len(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
