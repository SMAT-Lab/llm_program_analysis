{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 按行数分类的覆盖率统计 ===\n",
      "\n",
      "Category: 0-50 (0~50 行), file_count=85\n",
      "[NoParent-All] static=7651, llm=8445, matched=5897 => Cov(st)=77.07%, Cov(llm)=69.83%\n",
      "[NoParent-Leaf] static=4251, llm=5180, matched=3626 => Cov(st)=85.30%, Cov(llm)=70.00%\n",
      "[FlexParent-All] static=7651, llm=8445, matched=5575 => Cov(st)=72.87%, Cov(llm)=66.02%\n",
      "[FlexParent-Leaf] static=4251, llm=5180, matched=3387 => Cov(st)=79.68%, Cov(llm)=65.39%\n",
      "\n",
      "Category: 51-200 (51~200 行), file_count=91\n",
      "[NoParent-All] static=34705, llm=29010, matched=20737 => Cov(st)=59.75%, Cov(llm)=71.48%\n",
      "[NoParent-Leaf] static=19068, llm=17449, matched=12127 => Cov(st)=63.60%, Cov(llm)=69.50%\n",
      "[FlexParent-All] static=34705, llm=29010, matched=19754 => Cov(st)=56.92%, Cov(llm)=68.09%\n",
      "[FlexParent-Leaf] static=19068, llm=17449, matched=11503 => Cov(st)=60.33%, Cov(llm)=65.92%\n",
      "\n",
      "Category: 201-9999999 (201~9999999 行), file_count=22\n",
      "[NoParent-All] static=63619, llm=20804, matched=13982 => Cov(st)=21.98%, Cov(llm)=67.21%\n",
      "[NoParent-Leaf] static=33334, llm=12560, matched=8286 => Cov(st)=24.86%, Cov(llm)=65.97%\n",
      "[FlexParent-All] static=63619, llm=20804, matched=13019 => Cov(st)=20.46%, Cov(llm)=62.58%\n",
      "[FlexParent-Leaf] static=33334, llm=12560, matched=7631 => Cov(st)=22.89%, Cov(llm)=60.76%\n",
      "\n",
      "Category: all (0~9999999 行), file_count=198\n",
      "[NoParent-All] static=105975, llm=58259, matched=40616 => Cov(st)=38.33%, Cov(llm)=69.72%\n",
      "[NoParent-Leaf] static=56653, llm=35189, matched=24039 => Cov(st)=42.43%, Cov(llm)=68.31%\n",
      "[FlexParent-All] static=105975, llm=58259, matched=38348 => Cov(st)=36.19%, Cov(llm)=65.82%\n",
      "[FlexParent-Leaf] static=56653, llm=35189, matched=22521 => Cov(st)=39.75%, Cov(llm)=64.00%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "###############################################################################\n",
    "# 覆盖率比较逻辑 (与你已有的保持一致)\n",
    "###############################################################################\n",
    "def collect_node_ranges(ast_node: dict, only_leaves: bool=False) -> set:\n",
    "    result = set()\n",
    "    st = ast_node.get(\"start_token\", -1)\n",
    "    et = ast_node.get(\"end_token\", -1)\n",
    "    children = ast_node.get(\"children\", [])\n",
    "    is_leaf = (len(children) == 0)\n",
    "\n",
    "    if (not only_leaves) or (only_leaves and is_leaf):\n",
    "        if st >= 0 and et >= 0:\n",
    "            result.add((st, et))\n",
    "\n",
    "    for child in children:\n",
    "        result.update(collect_node_ranges(child, only_leaves))\n",
    "    return result\n",
    "\n",
    "def compare_ast_just_tokenRanges(ast_static: dict, ast_llm: dict, only_leaves: bool=False):\n",
    "    \"\"\"\n",
    "    不考虑父节点, 只看 (start_token, end_token) 一致即可。\n",
    "    可以只统计叶子(only_leaves=True)或全部节点。\n",
    "    \"\"\"\n",
    "    static_set = collect_node_ranges(ast_static, only_leaves)\n",
    "    llm_set = collect_node_ranges(ast_llm, only_leaves)\n",
    "    inter = static_set.intersection(llm_set)\n",
    "\n",
    "    total_static = len(static_set)\n",
    "    total_llm = len(llm_set)\n",
    "    matched = len(inter)\n",
    "\n",
    "    cov_static = (matched / total_static) if total_static else 0.0\n",
    "    cov_llm = (matched / total_llm) if total_llm else 0.0\n",
    "\n",
    "    return {\n",
    "        \"total_static\": total_static,\n",
    "        \"total_llm\": total_llm,\n",
    "        \"matched\": matched,\n",
    "        \"coverage_static\": cov_static,\n",
    "        \"coverage_llm\": cov_llm\n",
    "    }\n",
    "\n",
    "def collect_node_ancestors(ast_node: dict, ancestors: list, only_leaves: bool=False) -> dict:\n",
    "    results = {}\n",
    "    st = ast_node.get(\"start_token\", -1)\n",
    "    et = ast_node.get(\"end_token\", -1)\n",
    "    children = ast_node.get(\"children\", [])\n",
    "    node_is_leaf = (len(children)==0)\n",
    "\n",
    "    if (not only_leaves) or (only_leaves and node_is_leaf):\n",
    "        if st>=0 and et>=0:\n",
    "            results[(st, et)] = {\n",
    "                \"ancestors\": set(ancestors),\n",
    "                \"is_leaf\": node_is_leaf\n",
    "            }\n",
    "\n",
    "    new_ancestors = ancestors[:]\n",
    "    if st>=0 and et>=0:\n",
    "        new_ancestors.append((st,et))\n",
    "\n",
    "    for child in children:\n",
    "        submap = collect_node_ancestors(child, new_ancestors, only_leaves)\n",
    "        for k,v in submap.items():\n",
    "            results[k] = v\n",
    "\n",
    "    return results\n",
    "\n",
    "def build_static_parent_map(ast_node: dict, parent: dict, store: dict, only_leaves: bool=False):\n",
    "    st = ast_node.get(\"start_token\",-1)\n",
    "    et = ast_node.get(\"end_token\",-1)\n",
    "    children = ast_node.get(\"children\",[])\n",
    "    node_is_leaf = (len(children)==0)\n",
    "\n",
    "    if parent:\n",
    "        pst = parent.get(\"start_token\",-1)\n",
    "        pet = parent.get(\"end_token\",-1)\n",
    "    else:\n",
    "        pst, pet = -1, -1\n",
    "\n",
    "    if (not only_leaves) or (only_leaves and node_is_leaf):\n",
    "        if st>=0 and et>=0:\n",
    "            store[(st,et)] = (pst,pet)\n",
    "\n",
    "    for ch in children:\n",
    "        build_static_parent_map(ch, ast_node, store, only_leaves)\n",
    "\n",
    "def compare_ast_flexible_parent(ast_static: dict, ast_llm: dict, only_leaves: bool=False):\n",
    "    static_map = {}\n",
    "    build_static_parent_map(ast_static, None, static_map, only_leaves)\n",
    "    llm_map = collect_node_ancestors(ast_llm, [], only_leaves)\n",
    "\n",
    "    matched = 0\n",
    "    total_static = len(static_map)\n",
    "    total_llm = len(llm_map)\n",
    "\n",
    "    for (node_st,node_et), (pst,pet) in static_map.items():\n",
    "        if (node_st,node_et) in llm_map:\n",
    "            ancset = llm_map[(node_st,node_et)][\"ancestors\"]\n",
    "            if (pst,pet)==(-1,-1):\n",
    "                matched += 1\n",
    "            else:\n",
    "                # 对每个祖先节点范围进行宽松匹配\n",
    "                for anc_st,anc_et in ancset:\n",
    "                    # 允许父节点范围有小幅度偏差(比如注释导致的token偏移)\n",
    "                    if abs(anc_st - pst) <= 3 and abs(anc_et - pet) <= 3:\n",
    "                        matched += 1\n",
    "                        break\n",
    "\n",
    "    cov_static = (matched / total_static) if total_static>0 else 0\n",
    "    cov_llm = (matched / total_llm) if total_llm>0 else 0\n",
    "\n",
    "    return {\n",
    "        \"total_static\": total_static,\n",
    "        \"total_llm\": total_llm,\n",
    "        \"matched\": matched,\n",
    "        \"coverage_static\": cov_static,\n",
    "        \"coverage_llm\": cov_llm\n",
    "    }\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) main: 行数分类 + 覆盖率统计\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Python 源文件目录\n",
    "    source_dir = \"../../dataset/ts\"\n",
    "    # LLM AST 目录\n",
    "    llm_json_dir = \"./llm_ast/chunk_block_processed\"\n",
    "    # 静态 AST 目录\n",
    "    static_json_dir = \"../../dataset/ts_ast\"\n",
    "\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"[Error] Source dir not found: {source_dir}\")\n",
    "        return\n",
    "\n",
    "    # 定义行数范围\n",
    "    categories = [\n",
    "        (\"0-50\", 0, 50),\n",
    "        (\"51-200\", 51, 200),\n",
    "        (\"201-9999999\", 201, 9999999),\n",
    "        (\"all\", 0, 9999999)  # 新增整体统计类别\n",
    "    ]\n",
    "\n",
    "    # 为每个分类，存储 4种统计\n",
    "    # np_all, np_leaf, fp_all, fp_leaf\n",
    "    # 每种统计包括 total_static_sum, total_llm_sum, matched_sum, file_count\n",
    "    stats_map = {}\n",
    "    for cat_name, low, high in categories:\n",
    "        stats_map[cat_name] = {\n",
    "            \"file_count\": 0,\n",
    "            # no-parent(all)\n",
    "            \"np_all_static\": 0, \"np_all_llm\":0, \"np_all_match\":0,\n",
    "            # no-parent(leaf)\n",
    "            \"np_leaf_static\": 0, \"np_leaf_llm\":0, \"np_leaf_match\":0,\n",
    "            # flex-parent(all)\n",
    "            \"fp_all_static\": 0, \"fp_all_llm\":0, \"fp_all_match\":0,\n",
    "            # flex-parent(leaf)\n",
    "            \"fp_leaf_static\": 0, \"fp_leaf_llm\":0, \"fp_leaf_match\":0\n",
    "        }\n",
    "\n",
    "    # 遍历 cangjie 源文件 => line_count => cat\n",
    "    for fname in [f\"{i}.ts\" for i in range(200)]:\n",
    "        if not fname.endswith(\".ts\"):\n",
    "            continue\n",
    "        py_path = os.path.join(source_dir, fname)\n",
    "\n",
    "        # 读取行数\n",
    "        try:\n",
    "            with open(py_path,\"r\",encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "            line_count = len(lines)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error reading {py_path}]: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 判断分类\n",
    "        cat_name = None\n",
    "        for (cname, low, high) in categories[:-1]:  # 不包括 \"all\" 类别\n",
    "            if low <= line_count <= high:\n",
    "                cat_name = cname\n",
    "                break\n",
    "        if not cat_name:\n",
    "            continue  # 不在任何区间\n",
    "\n",
    "        # 找对应 json => <fname> + \".json\"\n",
    "        # e.g. \"1.cj\" => \"1.cj.json\"\n",
    "        json_name = fname + \".json\"\n",
    "        llm_file = os.path.join(llm_json_dir, json_name)\n",
    "        static_file = os.path.join(static_json_dir, json_name).replace(\".ts\", \"\")\n",
    "\n",
    "        if not os.path.exists(llm_file) or not os.path.exists(static_file):\n",
    "            # print(f\"[Warn] Missing AST for {fname}, skip.\")\n",
    "            continue\n",
    "\n",
    "        # 读取 JSON\n",
    "        try:\n",
    "            with open(llm_file,\"r\",encoding=\"utf-8\") as f1:\n",
    "                ast_llm = json.load(f1)\n",
    "            with open(static_file,\"r\",encoding=\"utf-8\") as f2:\n",
    "                ast_static = json.load(f2)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error reading AST for {fname}]: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 计算4种覆盖率\n",
    "        # no-parent(all)\n",
    "        r_np_all = compare_ast_just_tokenRanges(ast_static, ast_llm, only_leaves=False)\n",
    "        # no-parent(leaf)\n",
    "        r_np_leaf = compare_ast_just_tokenRanges(ast_static, ast_llm, only_leaves=True)\n",
    "        # flex-parent(all)\n",
    "        r_fp_all = compare_ast_flexible_parent(ast_static, ast_llm, only_leaves=False)\n",
    "        # flex-parent(leaf)\n",
    "        r_fp_leaf = compare_ast_flexible_parent(ast_static, ast_llm, only_leaves=True)\n",
    "\n",
    "        # 累加到 stats_map[cat_name] 和 stats_map[\"all\"]\n",
    "        for stat_cat in [cat_name, \"all\"]:\n",
    "            sdat = stats_map[stat_cat]\n",
    "            sdat[\"file_count\"] += 1\n",
    "\n",
    "            # np_all\n",
    "            sdat[\"np_all_static\"] += r_np_all[\"total_static\"]\n",
    "            sdat[\"np_all_llm\"] += r_np_all[\"total_llm\"]\n",
    "            sdat[\"np_all_match\"] += r_np_all[\"matched\"]\n",
    "\n",
    "            # np_leaf\n",
    "            sdat[\"np_leaf_static\"] += r_np_leaf[\"total_static\"]\n",
    "            sdat[\"np_leaf_llm\"] += r_np_leaf[\"total_llm\"]\n",
    "            sdat[\"np_leaf_match\"] += r_np_leaf[\"matched\"]\n",
    "\n",
    "            # fp_all\n",
    "            sdat[\"fp_all_static\"] += r_fp_all[\"total_static\"]\n",
    "            sdat[\"fp_all_llm\"] += r_fp_all[\"total_llm\"]\n",
    "            sdat[\"fp_all_match\"] += r_fp_all[\"matched\"]\n",
    "\n",
    "            # fp_leaf\n",
    "            sdat[\"fp_leaf_static\"] += r_fp_leaf[\"total_static\"]\n",
    "            sdat[\"fp_leaf_llm\"] += r_fp_leaf[\"total_llm\"]\n",
    "            sdat[\"fp_leaf_match\"] += r_fp_leaf[\"matched\"]\n",
    "\n",
    "    # 输出每个分类结果\n",
    "    print(\"\\n=== 按行数分类的覆盖率统计 ===\")\n",
    "    for (cat_name, low, high) in categories:\n",
    "        sdat = stats_map[cat_name]\n",
    "        fcount = sdat[\"file_count\"]\n",
    "        if fcount==0:\n",
    "            print(f\"\\nCategory: {cat_name} ({low}~{high}) => 无文件。\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nCategory: {cat_name} ({low}~{high} 行), file_count={fcount}\")\n",
    "\n",
    "        # np_all\n",
    "        a_s = sdat[\"np_all_static\"]\n",
    "        a_l = sdat[\"np_all_llm\"]\n",
    "        a_m = sdat[\"np_all_match\"]\n",
    "        cov_s = (a_m/a_s)*100 if a_s>0 else 0\n",
    "        cov_l = (a_m/a_l)*100 if a_l>0 else 0\n",
    "        print(\"[NoParent-All] static=%d, llm=%d, matched=%d => Cov(st)=%.2f%%, Cov(llm)=%.2f%%\" % (a_s,a_l,a_m,cov_s,cov_l))\n",
    "\n",
    "        # np_leaf\n",
    "        b_s = sdat[\"np_leaf_static\"]\n",
    "        b_l = sdat[\"np_leaf_llm\"]\n",
    "        b_m = sdat[\"np_leaf_match\"]\n",
    "        cov_s = (b_m/b_s)*100 if b_s>0 else 0\n",
    "        cov_l = (b_m/b_l)*100 if b_l>0 else 0\n",
    "        print(\"[NoParent-Leaf] static=%d, llm=%d, matched=%d => Cov(st)=%.2f%%, Cov(llm)=%.2f%%\" % (b_s,b_l,b_m,cov_s,cov_l))\n",
    "\n",
    "        # fp_all\n",
    "        c_s = sdat[\"fp_all_static\"]\n",
    "        c_l = sdat[\"fp_all_llm\"]\n",
    "        c_m = sdat[\"fp_all_match\"]\n",
    "        cov_s = (c_m/c_s)*100 if c_s>0 else 0\n",
    "        cov_l = (c_m/c_l)*100 if c_l>0 else 0\n",
    "        print(\"[FlexParent-All] static=%d, llm=%d, matched=%d => Cov(st)=%.2f%%, Cov(llm)=%.2f%%\" % (c_s,c_l,c_m,cov_s,cov_l))\n",
    "\n",
    "        # fp_leaf\n",
    "        d_s = sdat[\"fp_leaf_static\"]\n",
    "        d_l = sdat[\"fp_leaf_llm\"]\n",
    "        d_m = sdat[\"fp_leaf_match\"]\n",
    "        cov_s = (d_m/d_s)*100 if d_s>0 else 0\n",
    "        cov_l = (d_m/d_l)*100 if d_l>0 else 0\n",
    "        print(\"[FlexParent-Leaf] static=%d, llm=%d, matched=%d => Cov(st)=%.2f%%, Cov(llm)=%.2f%%\" % (d_s,d_l,d_m,cov_s,cov_l))\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
