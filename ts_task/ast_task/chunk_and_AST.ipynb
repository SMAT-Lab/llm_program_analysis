{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  88%|████████▊ | 114/129 [04:59<01:55,  7.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请求失败,正在进行第1次重试...\n",
      "请求失败,正在进行第1次重试...\n",
      "请求失败,正在进行第1次重试...\n",
      "请求失败,正在进行第1次重试...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  89%|████████▉ | 115/129 [07:07<02:47, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请求失败,正在进行第1次重试...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  90%|████████▉ | 116/129 [07:20<02:36, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 1629 column 1 (char 15098)\n",
      "请求失败,正在进行第2次重试...\n",
      "请求失败,正在进行第2次重试...\n",
      "请求失败,正在进行第2次重试...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  95%|█████████▍| 122/129 [14:51<04:12, 36.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请求失败,正在进行第1次重试...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  95%|█████████▌| 123/129 [15:28<03:38, 36.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: 请求失败,已重试3次: Request timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  99%|█████████▉| 128/129 [31:10<02:59, 179.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请求失败,正在进行第1次重试...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-threaded script for Python code analysis (with parent range fix):\n",
    "\n",
    "1) LLM-based AST generation (via CFG + partial block approach):\n",
    "   - parse_cfg_structure() => get line ranges of classes/functions\n",
    "   - build_ast_from_cfg() => recursively exclude child function/class lines from the parent block,\n",
    "     parse only the remaining lines, then insert function/class placeholders back in the correct position.\n",
    "     *Now we fix the parent's start_token/end_token to include children.*\n",
    "   - Each node has global-level start_token/end_token, thanks to a single global tokenize_code_with_lines().\n",
    "\n",
    "2) Tree-sitter-based static AST:\n",
    "   - generate_tree_sitter_ast() => returns {type, label, children}. (Not fully implemented here)\n",
    "\n",
    "3) Compare snippet-level labels (optional).\n",
    "\n",
    "4) Save both ASTs as JSON.\n",
    "\n",
    "5) Multi-file parallel processing with ThreadPoolExecutor.\n",
    "\n",
    "See the \"llm_build_ast_from_tokens\" function's prompt – we keep it intact as requested.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import re\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "###############################################################################\n",
    "#                           LLM interface (stub)                              #\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Replace 'get_llm_answers' with your actual LLM API call or function.\n",
    "Here we just provide a stub or minimal placeholder.\n",
    "\"\"\"\n",
    "try:\n",
    "    from llm import get_llm_answers\n",
    "except ImportError:\n",
    "    # If no llm.py found, define a placeholder\n",
    "    def get_llm_answers(prompt, model_name=\"\", require_json=False, temperature=0):\n",
    "        # Return a minimal JSON for demonstration\n",
    "        # In reality, you'd implement the actual LLM call (OpenAI, local model, etc.)\n",
    "        return \"{}\"\n",
    "\n",
    "###############################################################################\n",
    "#                   1) 全局分词，含行号 -> global_tokens                       #\n",
    "###############################################################################\n",
    "def tokenize_code_with_lines(code: str) -> List[Tuple[int, int, str, int]]:\n",
    "    \"\"\"\n",
    "    使用正则分词，直接在原始代码上通过 finditer() 获取匹配位置，\n",
    "    并保留换行符，让 LLM 能识别多行结构。\n",
    "    返回形式: [(start_offset, end_offset, token_text, line_number), ...]\n",
    "    \"\"\"\n",
    "    token_pattern = (\n",
    "        r'[A-Za-z_]\\w*|'       # 标识符\n",
    "        r'[0-9]+(?:\\.[0-9]+)?|'# 数字(包含小数)\n",
    "        r'\"[^\"]*\"|'            # 双引号字符串\n",
    "        r\"'[^']*'|\"            # 单引号字符串\n",
    "        r'`[^`]*`|'            # 模板字符串\n",
    "        r'//.*?(?=\\n|$)|'      # 单行注释(到行尾)\n",
    "        r'/\\*[\\s\\S]*?\\*/|'     # 多行注释\n",
    "        r'=>|'                 # 箭头函数\n",
    "        r'===|!==|==|!=|'      # 相等性操作符\n",
    "        r'&&|\\|\\||'            # 逻辑操作符\n",
    "        r'[-+*/=<>!&|^~?:;,.(){}[\\]]|' # 其他操作符和分隔符\n",
    "        r'\\n|\\r|\\t|'           # 换行/回车/制表符\n",
    "        r'\\s+'                 # 其他空白字符\n",
    "    )\n",
    "\n",
    "    tokens_with_offset = []\n",
    "    lines = code.splitlines(keepends=True)\n",
    "    current_line = 1\n",
    "    current_pos = 0\n",
    "    \n",
    "    for match in re.finditer(token_pattern, code, re.MULTILINE | re.DOTALL):\n",
    "        tk = match.group(0)\n",
    "        start_offset, end_offset = match.span()\n",
    "        \n",
    "        # 计算当前token所在行号\n",
    "        while current_line <= len(lines) and current_pos + len(lines[current_line-1]) <= start_offset:\n",
    "            current_pos += len(lines[current_line-1])\n",
    "            current_line += 1\n",
    "            \n",
    "        # 跳过纯空白token\n",
    "        if not tk.isspace():\n",
    "            tokens_with_offset.append((start_offset, end_offset, tk, current_line))\n",
    "\n",
    "    return tokens_with_offset\n",
    "\n",
    "###############################################################################\n",
    "#                      2) 获取 CFG (class/function) 行范围                    #\n",
    "###############################################################################\n",
    "def get_structure_prompt(code_text: str) -> str:\n",
    "    \"\"\"\n",
    "    构造提示给 LLM，让其解析出脚本中的类/函数行范围。\n",
    "    \"\"\"\n",
    "    lines = code_text.splitlines()\n",
    "    lines_json = [{\"line\": i+1, \"code\": line} for i, line in enumerate(lines)]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are given Python code lines (with line numbers). Identify all top-level and nested classes/functions,\n",
    "and return a JSON with this structure:\n",
    "\n",
    "{{\n",
    "  \"name\": \"example_script\",\n",
    "  \"type\": \"CFG\",\n",
    "  \"start_line\": 1,\n",
    "  \"end_line\": {len(lines)},\n",
    "  \"functions\": [\n",
    "    {{\n",
    "      \"name\": \"function_name\",\n",
    "      \"type\": \"function\",\n",
    "      \"start_line\": 10,\n",
    "      \"end_line\": 20,\n",
    "      \"functions\": [],\n",
    "      \"classes\": []\n",
    "    }}\n",
    "  ],\n",
    "  \"classes\": [\n",
    "    {{\n",
    "      \"name\": \"class_name\",\n",
    "      \"type\": \"class\",\n",
    "      \"start_line\": 30,\n",
    "      \"end_line\": 40,\n",
    "      \"functions\": [...],\n",
    "      \"classes\": [...]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Do not omit or rename fields.\n",
    "Here is the code lines:\n",
    "{json.dumps(lines_json, indent=2)}\n",
    "\n",
    "Return valid JSON only.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def parse_cfg_structure(code_text: str) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    调用LLM，获取CFG JSON结构\n",
    "    \"\"\"\n",
    "    prompt = get_structure_prompt(code_text)\n",
    "    try:\n",
    "        raw = get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True, temperature=0)\n",
    "        return json.loads(raw)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] parse_cfg_structure: {e}\")\n",
    "        lines = code_text.split('\\n')\n",
    "        return {\n",
    "            \"type\": \"CFG\",\n",
    "            \"name\": \"fallback\",\n",
    "            \"start_line\": 1,\n",
    "            \"end_line\": len(lines),\n",
    "            \"functions\": [],\n",
    "            \"classes\": []\n",
    "        }\n",
    "\n",
    "###############################################################################\n",
    "#        3) 在 block 内排除子函数/类行, 用 LLM 局部解析 => remap 索引          #\n",
    "###############################################################################\n",
    "def filter_tokens_for_block(\n",
    "    global_tokens: List[Tuple[int,int,str,int]],\n",
    "    block_start_line: int,\n",
    "    block_end_line: int,\n",
    "    excluded_line_ranges: List[Tuple[int,int]]\n",
    ") -> Tuple[List[Tuple[int,int,str,int]], List[int]]:\n",
    "    \"\"\"\n",
    "    在 [block_start_line..block_end_line] 内，排除excluded_line_ranges，\n",
    "    返回局部token列表 local_tokens 及其到全局的 mapping。\n",
    "    \"\"\"\n",
    "    def in_excluded(line_no: int) -> bool:\n",
    "        for (ex_st, ex_end) in excluded_line_ranges:\n",
    "            if ex_st <= line_no <= ex_end:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    filtered = []\n",
    "    mapping = []\n",
    "    for global_idx, (so, eo, tk, ln) in enumerate(global_tokens):\n",
    "        if ln < block_start_line or ln > block_end_line:\n",
    "            continue\n",
    "        if in_excluded(ln):\n",
    "            continue\n",
    "        filtered.append((so, eo, tk, ln))\n",
    "        mapping.append(global_idx)\n",
    "    return filtered, mapping\n",
    "\n",
    "def remap_ast_local_to_global(ast_node: Dict[str,Any], mapping: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    递归把局部下标 (start_token/end_token) => 全局下标\n",
    "    mapping[ local_idx ] = global_idx\n",
    "    \"\"\"\n",
    "    st_local = ast_node.get(\"start_token\", -1)\n",
    "    et_local = ast_node.get(\"end_token\", -1)\n",
    "\n",
    "    if 0 <= st_local < len(mapping):\n",
    "        ast_node[\"start_token\"] = mapping[st_local]\n",
    "    else:\n",
    "        ast_node[\"start_token\"] = -1\n",
    "\n",
    "    if 0 <= et_local < len(mapping):\n",
    "        ast_node[\"end_token\"] = mapping[et_local]\n",
    "    else:\n",
    "        ast_node[\"end_token\"] = -1\n",
    "\n",
    "    for c in ast_node.get(\"children\", []):\n",
    "        remap_ast_local_to_global(c, mapping)\n",
    "\n",
    "###############################################################################\n",
    "#   4) 保持原样: llm_build_ast_from_tokens() (prompt 不变, 勿改)              #\n",
    "###############################################################################\n",
    "def llm_build_ast_from_tokens(tokens_with_offset: List[Tuple[int, int, str]], top_level=True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    给定 tokens列表 => 调用 LLM => 生成 JSON AST.\n",
    "    - top_level: 是否最外层(只有最外层允许 'module'), 否则用 'block' 等\n",
    "    \"\"\"\n",
    "    indexed_tokens = [(i, t[2]) for i, t in enumerate(tokens_with_offset)]\n",
    "    token_info = \"\\n\".join(f\"{i}: {text}\" for (i, text) in indexed_tokens)\n",
    "\n",
    "    # 构造 prompt\n",
    "    allowed_types = [\n",
    "        \"AmpersandAmpersandToken\", \"AnyKeyword\", \"ArrayLiteralExpression\", \"ArrayType\", \"ArrowFunction\",\n",
    "        \"AsExpression\", \"AsteriskToken\", \"AsyncKeyword\", \"AwaitExpression\", \"BarBarToken\",\n",
    "        \"BarEqualsToken\", \"BarToken\", \"BinaryExpression\", \"Block\", \"BooleanKeyword\",\n",
    "        \"BreakStatement\", \"CallExpression\", \"CaseBlock\", \"CaseClause\", \"CatchClause\",\n",
    "        \"ClassDeclaration\", \"ColonToken\", \"ConditionalExpression\", \"Constructor\", \"ContinueStatement\",\n",
    "        \"DefaultClause\", \"DefaultKeyword\", \"DotDotDotToken\", \"ElementAccessExpression\", \"EmptyStatement\",\n",
    "        \"EndOfFileToken\", \"EnumDeclaration\", \"EnumMember\", \"EqualsEqualsEqualsToken\", \"EqualsEqualsToken\",\n",
    "        \"EqualsGreaterThanToken\", \"ExclamationEqualsEqualsToken\", \"ExclamationEqualsToken\",\n",
    "        \"ExportAssignment\", \"ExportKeyword\", \"ExpressionStatement\", \"ExpressionWithTypeArguments\",\n",
    "        \"FalseKeyword\", \"FirstAssignment\", \"FirstBinaryOperator\", \"FirstCompoundAssignment\",\n",
    "        \"FirstContextualKeyword\", \"FirstLiteralToken\", \"FirstNode\", \"FirstStatement\",\n",
    "        \"FirstTemplateToken\", \"ForInStatement\", \"ForOfStatement\", \"ForStatement\", \"FunctionDeclaration\",\n",
    "        \"FunctionExpression\", \"FunctionType\", \"GreaterThanEqualsToken\", \"GreaterThanGreaterThanToken\",\n",
    "        \"GreaterThanToken\", \"HeritageClause\", \"Identifier\", \"IfStatement\", \"ImportClause\",\n",
    "        \"ImportDeclaration\", \"ImportSpecifier\", \"InKeyword\", \"InterfaceDeclaration\", \"LastTemplateToken\",\n",
    "        \"LessThanEqualsToken\", \"LessThanLessThanToken\", \"LiteralType\", \"MethodDeclaration\",\n",
    "        \"MethodSignature\", \"MinusEqualsToken\", \"MinusToken\", \"NamedImports\", \"NewExpression\",\n",
    "        \"NullKeyword\", \"NumberKeyword\", \"ObjectKeyword\", \"ObjectLiteralExpression\", \"Parameter\",\n",
    "        \"ParenthesizedExpression\", \"PercentToken\", \"PlusToken\", \"PostfixUnaryExpression\",\n",
    "        \"PrefixUnaryExpression\", \"PrivateKeyword\", \"PropertyAccessExpression\", \"PropertyAssignment\",\n",
    "        \"PropertyDeclaration\", \"PropertySignature\", \"ProtectedKeyword\", \"PublicKeyword\",\n",
    "        \"QuestionDotToken\", \"QuestionQuestionToken\", \"QuestionToken\", \"ReadonlyKeyword\",\n",
    "        \"RegularExpressionLiteral\", \"ReturnStatement\", \"SemicolonClassElement\", \"SlashToken\",\n",
    "        \"SourceFile\", \"StaticKeyword\", \"StringKeyword\", \"StringLiteral\", \"SuperKeyword\",\n",
    "        \"SwitchStatement\", \"TemplateExpression\", \"TemplateHead\", \"TemplateMiddle\", \"TemplateSpan\",\n",
    "        \"ThisKeyword\", \"ThrowStatement\", \"TrueKeyword\", \"TryStatement\", \"TypeAliasDeclaration\",\n",
    "        \"TypeLiteral\", \"TypeParameter\", \"TypeReference\", \"UndefinedKeyword\", \"UnionType\",\n",
    "        \"VariableDeclaration\", \"VariableDeclarationList\", \"VoidKeyword\", \"WhileStatement\"\n",
    "    ]\n",
    "    allowed_types_str = \", \".join(allowed_types)\n",
    "\n",
    "    top_level_instruction = \"Exactly one 'module' node can appear at the root. Use 'block' if nested.\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        \"Below is a list of tokens (index -> token_string) for a code snippet:\\n\"\n",
    "        f\"{token_info}\\n\\n\"\n",
    "        \"Create a JSON-based AST with these fields:\\n\"\n",
    "        f\"- 'type': must be in {{{allowed_types_str}}}\\n\"\n",
    "        \"- 'start_token', 'end_token'\\n\"\n",
    "        \"- 'children' (array)\\n\\n\"\n",
    "        \"Leaf nodes => start_token == end_token.\\n\"\n",
    "        \"No overlapping sibling token ranges.\\n\"\n",
    "        \"Return valid JSON only.\\n\"\n",
    "    )\n",
    "    if top_level:\n",
    "        prompt += \"\\nAt the root, use 'module'. Do not nest multiple 'module'.\\n\" + top_level_instruction\n",
    "    else:\n",
    "        prompt += \"\\nInside blocks, do not produce 'module'. Use 'block' or suitable type.\\n\" + top_level_instruction\n",
    "\n",
    "    try:\n",
    "        llm_output = get_llm_answers(\n",
    "            prompt,\n",
    "            model_name=\"gpt-4o\",\n",
    "            require_json=True,\n",
    "            temperature=0\n",
    "        )\n",
    "        ast_dict = json.loads(llm_output)\n",
    "        return ast_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] llm_build_ast_from_tokens: {e}\")\n",
    "        return {\n",
    "            \"type\": \"ErrorNode\",\n",
    "            \"start_token\": -1,\n",
    "            \"end_token\": -1,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "def llm_parse_block_ast(local_tokens: List[Tuple[int,int,str,int]], top_level=True) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    使用 llm_build_ast_from_tokens 来解析局部token，得到局部AST(下标0..N-1)。\n",
    "    \"\"\"\n",
    "    # tokens_with_offset 只需要 (start_offset, end_offset, text)，忽略行号\n",
    "    tokens_for_llm = [(so, eo, tk) for (so, eo, tk, ln) in local_tokens]\n",
    "    return llm_build_ast_from_tokens(tokens_for_llm, top_level=top_level)\n",
    "\n",
    "###############################################################################\n",
    "#    5) 递归构建AST: 处理本块普通语句 + 子函数/类 => 插占位 => merge & sort      #\n",
    "###############################################################################\n",
    "def find_first_token_index(global_tokens: List[Tuple[int,int,str,int]], line_start: int) -> int:\n",
    "    \"\"\"\n",
    "    找到在全局tokens里，行号 >= line_start 的第一个token的索引\n",
    "    \"\"\"\n",
    "    for i,(s_off,e_off,tk,ln) in enumerate(global_tokens):\n",
    "        if ln >= line_start:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def find_last_token_index(global_tokens: List[Tuple[int,int,str,int]], line_end: int) -> int:\n",
    "    \"\"\"\n",
    "    找到在全局tokens里，行号 <= line_end 的最后一个token的索引\n",
    "    \"\"\"\n",
    "    idx = -1\n",
    "    for i,(s_off,e_off,tk,ln) in enumerate(global_tokens):\n",
    "        if ln <= line_end:\n",
    "            idx = i\n",
    "        else:\n",
    "            break\n",
    "    return idx\n",
    "\n",
    "def build_ast_from_cfg(\n",
    "    cfg_node: Dict[str,Any],\n",
    "    global_tokens: List[Tuple[int,int,str,int]],\n",
    "    top_level=True\n",
    ") -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    递归构建 AST:\n",
    "      1) 在 [start_line..end_line] 排除 functions/classes 行\n",
    "      2) 解析剩余 => block_ast_local\n",
    "      3) remap => 全局索引\n",
    "      4) 对每个子函数/类递归 build => 占位节点 => 插入\n",
    "      5) 合并children并排序\n",
    "      6) 修正父节点 start_token/end_token, 使之至少覆盖所有子节点\n",
    "    \"\"\"\n",
    "    st_line = cfg_node.get(\"start_line\", 1)\n",
    "    ed_line = cfg_node.get(\"end_line\", 1)\n",
    "\n",
    "    # 收集排除行\n",
    "    excluded_line_ranges = []\n",
    "    for fn in cfg_node.get(\"functions\", []):\n",
    "        excluded_line_ranges.append((fn[\"start_line\"], fn[\"end_line\"]))\n",
    "    for cl in cfg_node.get(\"classes\", []):\n",
    "        excluded_line_ranges.append((cl[\"start_line\"], cl[\"end_line\"]))\n",
    "\n",
    "    # 1) filter\n",
    "    local_tokens, mapping = filter_tokens_for_block(global_tokens, st_line, ed_line, excluded_line_ranges)\n",
    "    # 2) LLM解析 => block_ast_local\n",
    "    block_ast_local = llm_parse_block_ast(local_tokens, top_level=top_level)\n",
    "    # 3) remap\n",
    "    remap_ast_local_to_global(block_ast_local, mapping)\n",
    "\n",
    "    if \"children\" not in block_ast_local:\n",
    "        block_ast_local[\"children\"] = []\n",
    "\n",
    "    # 4) 处理子函数/类 => 占位\n",
    "    placeholders = []\n",
    "    for fn_cfg in cfg_node.get(\"functions\", []):\n",
    "        fn_ast = build_ast_from_cfg(fn_cfg, global_tokens, top_level=False)\n",
    "        placeholders.append({\n",
    "            \"type\": \"function_placeholder\",\n",
    "            \"name\": fn_cfg[\"name\"],\n",
    "            \"start_line\": fn_cfg[\"start_line\"],\n",
    "            \"end_line\": fn_cfg[\"end_line\"],\n",
    "            \"start_token\": find_first_token_index(global_tokens, fn_cfg[\"start_line\"]),\n",
    "            \"end_token\": find_last_token_index(global_tokens, fn_cfg[\"end_line\"]),\n",
    "            \"children\": [fn_ast]\n",
    "        })\n",
    "    for cl_cfg in cfg_node.get(\"classes\", []):\n",
    "        cl_ast = build_ast_from_cfg(cl_cfg, global_tokens, top_level=False)\n",
    "        placeholders.append({\n",
    "            \"type\": \"class_placeholder\",\n",
    "            \"name\": cl_cfg[\"name\"],\n",
    "            \"start_line\": cl_cfg[\"start_line\"],\n",
    "            \"end_line\": cl_cfg[\"end_line\"],\n",
    "            \"start_token\": find_first_token_index(global_tokens, cl_cfg[\"start_line\"]),\n",
    "            \"end_token\": find_last_token_index(global_tokens, cl_cfg[\"end_line\"]),\n",
    "            \"children\": [cl_ast]\n",
    "        })\n",
    "\n",
    "    # 5) 合并 & 排序\n",
    "    merged_children = block_ast_local[\"children\"] + placeholders\n",
    "    merged_children.sort(key=lambda n: n.get(\"start_token\", -1))\n",
    "    block_ast_local[\"children\"] = merged_children\n",
    "\n",
    "    # 6) 修正父节点的 start_token/end_token，使其至少覆盖子节点\n",
    "    fix_parent_token_range(block_ast_local)\n",
    "\n",
    "    if top_level:\n",
    "        block_ast_local[\"type\"] = \"module\"\n",
    "\n",
    "    return block_ast_local\n",
    "\n",
    "def fix_parent_token_range(node: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    在生成父节点 AST 后，确保父节点 token 区间能覆盖所有子节点。\n",
    "    如果初始 parent_start 或 parent_end 为 -1，则直接用子节点范围。\n",
    "    \"\"\"\n",
    "    if \"children\" not in node:\n",
    "        return\n",
    "\n",
    "    parent_start = node.get(\"start_token\", -1)\n",
    "    parent_end = node.get(\"end_token\", -1)\n",
    "\n",
    "    child_starts = []\n",
    "    child_ends = []\n",
    "    for child in node[\"children\"]:\n",
    "        c_st = child.get(\"start_token\", -1)\n",
    "        c_ed = child.get(\"end_token\", -1)\n",
    "        if c_st >= 0:\n",
    "            child_starts.append(c_st)\n",
    "        if c_ed >= 0:\n",
    "            child_ends.append(c_ed)\n",
    "\n",
    "    if child_starts:\n",
    "        min_st = min(child_starts)\n",
    "        if parent_start < 0:\n",
    "            parent_start = min_st\n",
    "        else:\n",
    "            parent_start = min(parent_start, min_st)\n",
    "\n",
    "    if child_ends:\n",
    "        max_ed = max(child_ends)\n",
    "        if parent_end < 0:\n",
    "            parent_end = max_ed\n",
    "        else:\n",
    "            parent_end = max(parent_end, max_ed)\n",
    "\n",
    "    node[\"start_token\"] = parent_start\n",
    "    node[\"end_token\"] = parent_end\n",
    "\n",
    "###############################################################################\n",
    "#            7) 最终对外：生成 LLM AST (via CFG)                              #\n",
    "###############################################################################\n",
    "def generate_llm_ast_via_cfg(code: str) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    最终对外函数: \n",
    "    1) 全局分词 => global_tokens\n",
    "    2) parse_cfg_structure => cfg\n",
    "    3) build_ast_from_cfg => AST(全局下标, 并修正父范围)\n",
    "    \"\"\"\n",
    "    global_tokens = tokenize_code_with_lines(code)\n",
    "    cfg_root = parse_cfg_structure(code)\n",
    "    llm_ast = build_ast_from_cfg(cfg_root, global_tokens, top_level=True)\n",
    "    return llm_ast\n",
    "\n",
    "###############################################################################\n",
    "#      8) 单文件处理：生成并保存 LLM AST（可选对比 Tree-sitter AST）           #\n",
    "###############################################################################\n",
    "def process_llm_ast(code: str, file_path: str) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    生成 LLM AST, 并保存到 JSON\n",
    "    \"\"\"\n",
    "    llm_dir = \"llm_ast/chunk_block\"\n",
    "    os.makedirs(llm_dir, exist_ok=True)\n",
    "    llm_path = os.path.join(llm_dir, os.path.basename(file_path) + \".json\")\n",
    "\n",
    "    # 如果已存在，则直接读缓存，避免重复调用 LLM\n",
    "    if os.path.exists(llm_path):\n",
    "        with open(llm_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            llm_ast = json.load(f)\n",
    "        return llm_ast\n",
    "\n",
    "    # 否则调用解析\n",
    "    llm_ast = generate_llm_ast_via_cfg(code)\n",
    "    with open(llm_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(llm_ast, fout, indent=4, ensure_ascii=False)\n",
    "    return llm_ast\n",
    "\n",
    "def process_single_file(file_path: str):\n",
    "    \"\"\"\n",
    "    1) 读取文件\n",
    "    2) 生成 LLM AST\n",
    "    3) TODO: Tree-sitter AST (可自行实现)\n",
    "    4) 可选进行对比\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            code = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error reading {file_path}]: {e}\")\n",
    "        return\n",
    "\n",
    "    process_llm_ast(code, file_path)\n",
    "\n",
    "###############################################################################\n",
    "#                            9) main() 并行处理                               #\n",
    "###############################################################################\n",
    "def main():\n",
    "    source_dir = \"../../dataset/ts\"  # 修改为实际源文件目录\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"[Error] Directory {source_dir} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # 示例：只处理一个 ts 文件\n",
    "    # full_path = os.path.join(source_dir, \"0.ts\")\n",
    "    # process_single_file(full_path)\n",
    "\n",
    "    # 如果要并行处理所有文件，可以再启用下面的逻辑\n",
    "    files = [f for f in os.listdir(source_dir) if f.endswith(\".ts\")]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = []\n",
    "        pbar = tqdm(total=len(files), desc=\"Processing files\")\n",
    "        for fname in files:\n",
    "            fp = os.path.join(source_dir, fname)\n",
    "            future = executor.submit(process_single_file, fp)\n",
    "            future.add_done_callback(lambda _: pbar.update(1))\n",
    "            futures.append(future)\n",
    "        concurrent.futures.wait(futures)\n",
    "        pbar.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processed] => llm_ast/chunk_block_processed/58.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/19.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/42.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/23.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/49.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/27.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/60.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/96.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/10.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/118.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/67.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/29.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/117.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/13.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/30.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/76.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/40.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/51.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/35.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/65.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/39.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/86.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/107.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/94.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/6.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/103.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/82.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/111.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/8.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/34.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/31.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/71.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/64.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/1.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/44.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/128.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/108.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/5.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/75.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/18.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/91.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/119.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/2.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/83.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/120.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/41.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/0.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/90.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/4.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/87.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/127.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/99.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/74.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/25.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/101.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/22.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/115.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/38.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/57.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/95.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/62.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/116.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/33.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/78.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/69.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/66.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/110.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/106.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/37.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/63.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/46.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/20.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/73.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/123.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/59.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/85.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/16.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/26.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/84.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/45.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/72.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/3.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/28.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/54.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/92.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/93.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/102.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/43.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/88.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/125.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/24.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/11.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/81.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/9.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/17.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/50.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/122.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/32.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/68.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/109.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/121.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/97.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/104.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/77.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/70.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/114.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/61.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/112.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/80.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/79.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/52.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/100.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/14.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/7.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/113.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/55.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/126.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/53.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/21.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/47.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/48.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/124.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/15.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/105.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/36.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/89.ts.json\n",
      "[Processed] => llm_ast/chunk_block_processed/98.ts.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#                   1) 全局分词，含行号 -> global_tokens                       #\n",
    "###############################################################################\n",
    "def tokenize_code_with_lines(code: str) -> List[Tuple[int, int, str, int]]:\n",
    "    \"\"\"\n",
    "    使用正则分词，直接在原始代码上通过 finditer() 获取匹配位置，\n",
    "    并保留换行符，让 LLM 能识别多行结构。\n",
    "    返回形式: [(start_offset, end_offset, token_text, line_number), ...]\n",
    "    \"\"\"\n",
    "    token_pattern = (\n",
    "        r'[A-Za-z_]\\w*|'  # 标识符\n",
    "        r'[0-9]+|'        # 数字\n",
    "        r'\"[^\"]*\"|'       # 双引号字符串\n",
    "        r\"'[^']*'|\"       # 单引号字符串\n",
    "        r'\\\\[ntr]|'       # 转义符 \\n \\t \\r\n",
    "        r'//.*|'          # 单行注释 (如 C++/Java/JS 风格)\n",
    "        r'/\\*.*?\\*/|'     # 多行注释 (如 C 风格)\n",
    "        r'\\n|\\r|\\t|'      # 换行/回车/制表符\n",
    "        r'\\S'             # 其他符号(如 +, -, {, }, 以及任何其它非空白字符)\n",
    "    )\n",
    "\n",
    "    tokens_with_offset = []\n",
    "    lines = code.splitlines(keepends=True)\n",
    "    current_line = 1\n",
    "    current_pos = 0\n",
    "    \n",
    "    for match in re.finditer(token_pattern, code, re.MULTILINE | re.DOTALL):\n",
    "        tk = match.group(0)\n",
    "        start_offset, end_offset = match.span()\n",
    "        \n",
    "        # 计算当前token所在行号\n",
    "        while current_line <= len(lines) and current_pos + len(lines[current_line-1]) <= start_offset:\n",
    "            current_pos += len(lines[current_line-1])\n",
    "            current_line += 1\n",
    "            \n",
    "        tokens_with_offset.append((start_offset, end_offset, tk, current_line))\n",
    "\n",
    "    return tokens_with_offset\n",
    "\n",
    "###############################################################################\n",
    "# 1) 根据全局 tokens 填充 label\n",
    "###############################################################################\n",
    "def fill_ast_labels(ast_node: dict, code: str, global_tokens: List[Tuple[int,int,str,int]]) -> None:\n",
    "    \"\"\"\n",
    "    把节点的 (start_token, end_token) 当作【token下标】，\n",
    "    去 global_tokens 里拿对应的字符 offset，再到 code 中截取。\n",
    "    存到 ast_node[\"label\"]。\n",
    "    \"\"\"\n",
    "    st = ast_node.get(\"start_token\", -1)\n",
    "    et = ast_node.get(\"end_token\", -1)\n",
    "\n",
    "    snippet = \"\"\n",
    "    if (\n",
    "        0 <= st <= et\n",
    "        and st < len(global_tokens)\n",
    "        and et < len(global_tokens)\n",
    "    ):\n",
    "        start_offset = global_tokens[st][0]\n",
    "        end_offset   = global_tokens[et][1]\n",
    "        if 0 <= start_offset < end_offset <= len(code):\n",
    "            snippet = code[start_offset:end_offset]\n",
    "\n",
    "    ast_node[\"label\"] = snippet\n",
    "\n",
    "    for child in ast_node.get(\"children\", []):\n",
    "        fill_ast_labels(child, code, global_tokens)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) 扁平化：去掉 function_placeholder->module->(唯一子节点)\n",
    "###############################################################################\n",
    "def safe_flatten_function_placeholders(node: dict) -> dict:\n",
    "    \"\"\"\n",
    "    新建节点，避免循环引用。\n",
    "    如果 node.type = function_placeholder/class_placeholder，\n",
    "    并且只有1个child且是 'module'，\n",
    "    且 'module' 有1个child => 直接返回该child 并拷贝 placeholder 的字段\n",
    "    \"\"\"\n",
    "    if not node:\n",
    "        return {}\n",
    "\n",
    "    node_type = node.get(\"type\", \"\")\n",
    "    original_children = node.get(\"children\", [])\n",
    "\n",
    "    # 先递归处理children\n",
    "    flattened_children = [safe_flatten_function_placeholders(ch) for ch in original_children]\n",
    "\n",
    "    # 构建 new_node（复制非-children字段）\n",
    "    new_node = {}\n",
    "    for key, val in node.items():\n",
    "        if key != \"children\":\n",
    "            new_node[key] = val\n",
    "    new_node[\"children\"] = flattened_children\n",
    "\n",
    "    # 检查占位符结构\n",
    "    if node_type in (\"function_placeholder\", \"class_placeholder\"):\n",
    "        if len(flattened_children) == 1 and flattened_children[0].get(\"type\") == \"module\":\n",
    "            mod_node = flattened_children[0]\n",
    "            mod_kids = mod_node.get(\"children\", [])\n",
    "            if len(mod_kids) == 1:\n",
    "                real_node = mod_kids[0]\n",
    "                # 把 placeholder 上的一些字段复制给最里层\n",
    "                for field in (\"name\", \"start_line\", \"end_line\", \"start_token\", \"end_token\", \"label\"):\n",
    "                    if field in new_node:\n",
    "                        real_node[field] = new_node[field]\n",
    "                return real_node\n",
    "\n",
    "    return new_node\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) 单文件处理 => 根据同名py文件+json => 生成 global_tokens => 填label => 扁平化\n",
    "###############################################################################\n",
    "def process_ast_json(\n",
    "    input_json_path: str,\n",
    "    output_json_path: str,\n",
    "    py_source_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    预期:\n",
    "      input_json_path = \"llm_ast/chunk_block/1.py.json\"\n",
    "      -> 对应 py_file = \"py_source_dir/1.py\"\n",
    "\n",
    "    假设 JSON 结构如下:\n",
    "    {\n",
    "      \"type\": \"module\",\n",
    "      \"start_token\": 0,\n",
    "      \"end_token\": 307,\n",
    "      ...\n",
    "      \"children\": [...]\n",
    "    }\n",
    "    或者更复杂, 但只要 \"type\"、\"start_token\"/\"end_token\"、\"children\" 就可以\n",
    "\n",
    "    We'll:\n",
    "      1) 找到同名的 .py => 读 code\n",
    "      2) tokenize_code_with_lines(code) => global_tokens\n",
    "      3) fill_ast_labels(ast_root, code, global_tokens)\n",
    "      4) safe_flatten_function_placeholders(ast_root)\n",
    "      5) json.dump()\n",
    "    \"\"\"\n",
    "    base = os.path.basename(input_json_path)  # \"1.cj.json\"\n",
    "    # 拆分 => \"1.cj\" + \".json\"\n",
    "    # 如果你命名方式不同, 需自行改\n",
    "    # 这里假设 input_json_path 的文件名是 \"<something>.cj.json\"\n",
    "    # => python_source = \"<something>.cj\"\n",
    "    if base.endswith(\".ts.json\"):\n",
    "        py_file_name = base[:-5]  # remove \".json\"\n",
    "    else:\n",
    "        # fallback\n",
    "        py_file_name = base\n",
    "\n",
    "    py_full_path = os.path.join(py_source_dir, py_file_name)\n",
    "\n",
    "    if not os.path.isfile(py_full_path):\n",
    "        print(f\"[Warning] No corresponding .ts found for {input_json_path}, skip label fill.\")\n",
    "        code = \"\"\n",
    "        global_tokens = []\n",
    "    else:\n",
    "        # 读取 .py 源码\n",
    "        with open(py_full_path, \"r\", encoding=\"utf-8\") as fpy:\n",
    "            code = fpy.read()\n",
    "        # 分词\n",
    "        global_tokens = tokenize_code_with_lines(code)\n",
    "\n",
    "    # 读取 JSON AST\n",
    "    try:\n",
    "        with open(input_json_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "            ast_data = json.load(fin)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error reading {input_json_path}]: {e}\")\n",
    "        return\n",
    "\n",
    "    # fill label\n",
    "    fill_ast_labels(ast_data, code, global_tokens)\n",
    "\n",
    "    # flatten\n",
    "    flattened_ast = safe_flatten_function_placeholders(ast_data)\n",
    "\n",
    "    # 写出\n",
    "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(flattened_ast, fout, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[Processed] => {output_json_path}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4) main: 遍历 input_json_dir => 对应 .py => output\n",
    "###############################################################################\n",
    "def main():\n",
    "    input_json_dir = \"llm_ast/chunk_block\"              # 你的 AST json 目录\n",
    "    output_json_dir = \"llm_ast/chunk_block_processed\"   # 输出目录\n",
    "    py_source_dir = \"../../dataset/ts\"              # 对应的 .ts 文件目录\n",
    "\n",
    "    if not os.path.isdir(input_json_dir):\n",
    "        print(f\"[Error] input dir {input_json_dir} not found.\")\n",
    "        return\n",
    "    if not os.path.isdir(py_source_dir):\n",
    "        print(f\"[Warning] python source dir {py_source_dir} not found. Label fill will be empty.\")\n",
    "\n",
    "    os.makedirs(output_json_dir, exist_ok=True)\n",
    "\n",
    "    # 遍历\n",
    "    for fname in os.listdir(input_json_dir):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        in_path = os.path.join(input_json_dir, fname)\n",
    "        out_path = os.path.join(output_json_dir, fname)\n",
    "\n",
    "        process_ast_json(in_path, out_path, py_source_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清除异常数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Remove] 96.ts.json contains ErrorNode\n",
      "[Remove] 95.ts.json contains ErrorNode\n",
      "[Remove] 93.ts.json contains ErrorNode\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def has_error_node(ast_data):\n",
    "    \"\"\"递归检查AST中是否包含ErrorNode\"\"\"\n",
    "    if isinstance(ast_data, dict):\n",
    "        if ast_data.get(\"type\") == \"ErrorNode\":\n",
    "            return True\n",
    "        for value in ast_data.values():\n",
    "            if has_error_node(value):\n",
    "                return True\n",
    "    elif isinstance(ast_data, list):\n",
    "        for item in ast_data:\n",
    "            if has_error_node(item):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def clean_error_files():\n",
    "    \"\"\"清理包含ErrorNode的文件\"\"\"\n",
    "    input_dir = \"llm_ast/chunk_block\"\n",
    "    \n",
    "    for fname in os.listdir(input_dir):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(input_dir, fname)\n",
    "        \n",
    "        # 读取JSON\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                ast_data = json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[Error] Failed to parse {fname}: {str(e)}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to process {fname}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        # 检查是否包含ErrorNode\n",
    "        if has_error_node(ast_data):\n",
    "            print(f\"[Remove] {fname} contains ErrorNode\")\n",
    "            os.remove(file_path)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    clean_error_files()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
