{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Generate CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果不太好，还是需要我们一步步进行处理！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1 先将文件的嵌套类，方法给找到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/0.ts\n",
      "Processing ../../dataset/ts/1.ts\n",
      "Processing ../../dataset/ts/2.ts\n",
      "Processing ../../dataset/ts/3.ts\n",
      "Processing ../../dataset/ts/4.ts\n",
      "Processing ../../dataset/ts/5.ts\n",
      "Processing ../../dataset/ts/6.ts\n",
      "Processing ../../dataset/ts/7.ts\n",
      "Processing ../../dataset/ts/8.ts\n",
      "Processing ../../dataset/ts/9.ts\n",
      "Processing ../../dataset/ts/10.ts\n",
      "Processing ../../dataset/ts/11.ts\n",
      "Processing ../../dataset/ts/12.ts\n",
      "Processing ../../dataset/ts/13.ts\n",
      "Processing ../../dataset/ts/14.ts\n",
      "Processing ../../dataset/ts/15.ts\n",
      "Processing ../../dataset/ts/16.ts\n",
      "Processing ../../dataset/ts/17.ts\n",
      "Processing ../../dataset/ts/18.ts\n",
      "Processing ../../dataset/ts/19.ts\n",
      "Processing ../../dataset/ts/20.ts\n",
      "Processing ../../dataset/ts/22.ts\n",
      "Processing ../../dataset/ts/23.ts\n",
      "Processing ../../dataset/ts/24.ts\n",
      "Processing ../../dataset/ts/25.ts\n",
      "Processing ../../dataset/ts/21.ts\n",
      "Processing ../../dataset/ts/27.ts\n",
      "Processing ../../dataset/ts/26.ts\n",
      "Processing ../../dataset/ts/28.ts\n",
      "Processing ../../dataset/ts/29.ts\n",
      "Processing ../../dataset/ts/30.ts\n",
      "Processing ../../dataset/ts/31.ts\n",
      "Processing ../../dataset/ts/32.ts\n",
      "Processing ../../dataset/ts/33.ts\n",
      "Processing ../../dataset/ts/34.ts\n",
      "Processing ../../dataset/ts/35.ts\n",
      "Processing ../../dataset/ts/36.ts\n",
      "Processing ../../dataset/ts/37.ts\n",
      "Processing ../../dataset/ts/38.ts\n",
      "Processing ../../dataset/ts/39.ts\n",
      "Processing ../../dataset/ts/41.ts\n",
      "Processing ../../dataset/ts/40.ts\n",
      "Processing ../../dataset/ts/42.ts\n",
      "Processing ../../dataset/ts/43.ts\n",
      "Processing ../../dataset/ts/44.ts\n",
      "Processing ../../dataset/ts/45.ts\n",
      "Processing ../../dataset/ts/46.ts\n",
      "Processing ../../dataset/ts/47.ts\n",
      "Processing ../../dataset/ts/48.ts\n",
      "Processing ../../dataset/ts/49.ts\n",
      "Processing ../../dataset/ts/50.ts\n",
      "Processing ../../dataset/ts/51.ts\n",
      "Processing ../../dataset/ts/52.ts\n",
      "Processing ../../dataset/ts/54.ts\n",
      "Processing ../../dataset/ts/53.ts\n",
      "Processing ../../dataset/ts/55.ts\n",
      "Processing ../../dataset/ts/56.ts\n",
      "Processing ../../dataset/ts/57.ts\n",
      "Processing ../../dataset/ts/58.ts\n",
      "Processing ../../dataset/ts/59.ts\n",
      "Processing ../../dataset/ts/60.ts\n",
      "Processing ../../dataset/ts/61.ts\n",
      "Processing ../../dataset/ts/62.ts\n",
      "Processing ../../dataset/ts/63.ts\n",
      "Processing ../../dataset/ts/64.ts\n",
      "Processing ../../dataset/ts/67.ts\n",
      "Processing ../../dataset/ts/66.ts\n",
      "Processing ../../dataset/ts/65.ts\n",
      "Processing ../../dataset/ts/68.ts\n",
      "Processing ../../dataset/ts/69.ts\n",
      "Processing ../../dataset/ts/70.ts\n",
      "Processing ../../dataset/ts/72.ts\n",
      "Processing ../../dataset/ts/71.ts\n",
      "Processing ../../dataset/ts/73.ts\n",
      "Processing ../../dataset/ts/74.ts\n",
      "Processing ../../dataset/ts/75.ts\n",
      "Processing ../../dataset/ts/76.ts\n",
      "Processing ../../dataset/ts/77.ts\n",
      "Processing ../../dataset/ts/79.ts\n",
      "Processing ../../dataset/ts/80.ts\n",
      "Processing ../../dataset/ts/81.ts\n",
      "Processing ../../dataset/ts/78.ts\n",
      "Processing ../../dataset/ts/82.ts\n",
      "Processing ../../dataset/ts/83.ts\n",
      "Processing ../../dataset/ts/84.ts\n",
      "Processing ../../dataset/ts/85.ts\n",
      "Processing ../../dataset/ts/86.ts\n",
      "Processing ../../dataset/ts/87.ts\n",
      "Processing ../../dataset/ts/88.ts\n",
      "Processing ../../dataset/ts/89.ts\n",
      "Processing ../../dataset/ts/90.ts\n",
      "Processing ../../dataset/ts/91.ts\n",
      "Processing ../../dataset/ts/93.ts\n",
      "Processing ../../dataset/ts/92.ts\n",
      "Processing ../../dataset/ts/95.ts\n",
      "Processing ../../dataset/ts/94.ts\n",
      "Processing ../../dataset/ts/96.ts\n",
      "Processing ../../dataset/ts/97.ts\n",
      "Processing ../../dataset/ts/98.ts\n",
      "Processing ../../dataset/ts/99.ts\n",
      "Processing ../../dataset/ts/100.ts\n",
      "Processing ../../dataset/ts/101.ts\n",
      "Processing ../../dataset/ts/102.ts\n",
      "Processing ../../dataset/ts/103.ts\n",
      "Processing ../../dataset/ts/104.ts\n",
      "Processing ../../dataset/ts/105.ts\n",
      "Processing ../../dataset/ts/106.ts\n",
      "Processing ../../dataset/ts/107.ts\n",
      "Processing ../../dataset/ts/108.ts\n",
      "Processing ../../dataset/ts/109.ts\n",
      "Processing ../../dataset/ts/110.ts\n",
      "Processing ../../dataset/ts/111.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   0%|          | 1/200 [00:10<33:51, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/112.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   2%|▏         | 3/200 [00:11<08:26,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/113.ts\n",
      "Processing ../../dataset/ts/114.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   2%|▏         | 4/200 [00:11<05:29,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/115.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   2%|▎         | 5/200 [00:12<04:42,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/116.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   3%|▎         | 6/200 [00:13<04:19,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/117.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   4%|▎         | 7/200 [00:13<03:11,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/118.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   4%|▍         | 8/200 [00:14<03:10,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/119.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   4%|▍         | 9/200 [00:15<02:51,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/120.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   5%|▌         | 10/200 [00:16<03:03,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/121.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   6%|▌         | 12/200 [00:17<01:48,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/122.ts\n",
      "Processing ../../dataset/ts/123.ts\n",
      "Processing ../../dataset/ts/124.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   7%|▋         | 14/200 [00:17<01:09,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/125.ts\n",
      "Processing ../../dataset/ts/126.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   8%|▊         | 16/200 [00:17<00:53,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/127.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   8%|▊         | 17/200 [00:18<00:56,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/128.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  45%|████▌     | 90/200 [00:18<00:01, 69.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/ts/129.ts\n",
      "Processing ../../dataset/ts/130.ts\n",
      "Processing ../../dataset/ts/131.ts\n",
      "Processing ../../dataset/ts/132.ts\n",
      "Processing ../../dataset/ts/133.ts\n",
      "Processing ../../dataset/ts/134.ts\n",
      "Processing ../../dataset/ts/135.ts\n",
      "Processing ../../dataset/ts/136.ts\n",
      "Processing ../../dataset/ts/137.ts\n",
      "Processing ../../dataset/ts/138.ts\n",
      "Processing ../../dataset/ts/139.ts\n",
      "Processing ../../dataset/ts/140.ts\n",
      "Processing ../../dataset/ts/141.ts\n",
      "Processing ../../dataset/ts/142.ts\n",
      "Processing ../../dataset/ts/143.ts\n",
      "Processing ../../dataset/ts/144.ts\n",
      "Processing ../../dataset/ts/145.ts\n",
      "Processing ../../dataset/ts/146.ts\n",
      "Processing ../../dataset/ts/147.ts\n",
      "Processing ../../dataset/ts/148.ts\n",
      "Processing ../../dataset/ts/149.ts\n",
      "Processing ../../dataset/ts/150.ts\n",
      "Processing ../../dataset/ts/151.ts\n",
      "Processing ../../dataset/ts/152.ts\n",
      "Processing ../../dataset/ts/153.ts\n",
      "Processing ../../dataset/ts/154.ts\n",
      "Processing ../../dataset/ts/155.ts\n",
      "Processing ../../dataset/ts/156.ts\n",
      "Processing ../../dataset/ts/157.ts\n",
      "Processing ../../dataset/ts/158.ts\n",
      "Processing ../../dataset/ts/159.ts\n",
      "Processing ../../dataset/ts/160.ts\n",
      "Processing ../../dataset/ts/161.ts\n",
      "Processing ../../dataset/ts/162.ts\n",
      "Processing ../../dataset/ts/163.ts\n",
      "Processing ../../dataset/ts/164.ts\n",
      "Processing ../../dataset/ts/165.ts\n",
      "Processing ../../dataset/ts/166.ts\n",
      "Processing ../../dataset/ts/167.ts\n",
      "Processing ../../dataset/ts/168.ts\n",
      "Processing ../../dataset/ts/169.ts\n",
      "Processing ../../dataset/ts/170.ts\n",
      "Processing ../../dataset/ts/171.ts\n",
      "Processing ../../dataset/ts/172.ts\n",
      "Processing ../../dataset/ts/173.ts\n",
      "Processing ../../dataset/ts/174.ts\n",
      "Processing ../../dataset/ts/175.ts\n",
      "Processing ../../dataset/ts/176.ts\n",
      "Processing ../../dataset/ts/177.ts\n",
      "Processing ../../dataset/ts/178.ts\n",
      "Processing ../../dataset/ts/179.ts\n",
      "Processing ../../dataset/ts/180.ts\n",
      "Processing ../../dataset/ts/181.ts\n",
      "Processing ../../dataset/ts/182.ts\n",
      "Processing ../../dataset/ts/183.ts\n",
      "Processing ../../dataset/ts/184.ts\n",
      "Processing ../../dataset/ts/185.ts\n",
      "Processing ../../dataset/ts/186.ts\n",
      "Processing ../../dataset/ts/187.ts\n",
      "Processing ../../dataset/ts/188.ts\n",
      "Processing ../../dataset/ts/189.ts\n",
      "Processing ../../dataset/ts/190.ts\n",
      "Processing ../../dataset/ts/191.ts\n",
      "Processing ../../dataset/ts/192.ts\n",
      "Processing ../../dataset/ts/193.ts\n",
      "Processing ../../dataset/ts/194.ts\n",
      "Processing ../../dataset/ts/195.ts\n",
      "Processing ../../dataset/ts/196.ts\n",
      "Processing ../../dataset/ts/197.ts\n",
      "Processing ../../dataset/ts/198.ts\n",
      "Processing ../../dataset/ts/199.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件: 100%|██████████| 200/200 [14:51<00:00,  4.46s/it] \n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import as_completed\n",
    "from multiprocessing import cpu_count\n",
    "from llm import get_llm_answers\n",
    "import json\n",
    "\n",
    "def get_step1_prompt(code_text: str, program_language: str):\n",
    "    \"\"\"\n",
    "    生成第一步的Prompt\n",
    "    \"\"\"\n",
    "    code_lines = code_text.splitlines()\n",
    "    code_lines_json = [{\n",
    "        \"line\": i + 1,\n",
    "        \"code\": line\n",
    "    } for i, line in enumerate(code_lines)]\n",
    "\n",
    "    prompt = \"\"\"\n",
    "You are given a piece of \"\"\" +  program_language + \"\"\" code. Your goal is to find all the nested classes and methods in the code.\n",
    "\n",
    "Please return the result in JSON format, your output should be the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"example_script\",  // Name of the script or function\n",
    "    \"type\": \"CFG\",\n",
    "    \"start_line\": number,\n",
    "    \"end_line\": number,\n",
    "    \"functions\": [\n",
    "      {\n",
    "        \"name\": \"function_name\",\n",
    "        \"type\": \"function\",\n",
    "        \"start_line\": number,\n",
    "        \"end_line\": number,\n",
    "        \"functions\": [],         // Nested functions\n",
    "        \"classes\": []            // Nested classes\n",
    "      }\n",
    "    ],\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"name\": \"class_name\",\n",
    "        \"type\": \"class\",\n",
    "        \"start_line\": number,\n",
    "        \"end_line\": number,\n",
    "        \"functions\": [           // Methods of the class\n",
    "          {\n",
    "            \"name\": \"method_name\",\n",
    "            \"type\": \"function\",\n",
    "            \"start_line\": number,\n",
    "            \"end_line\": number,\n",
    "            \"functions\": [],     // Nested functions\n",
    "            \"classes\": []        // Nested classes\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "The code lines are:\n",
    "\"\"\" + json.dumps(code_lines_json, indent=2) + \"\"\"\n",
    "IMPORTANT: Make sure that the nested classes and methods are in the correct level. For example, if a function is nested in another class, the function should be in the nested class's functions list. \n",
    "Besides, if a class is nested in another class, the class should be in the nested class's classes list.\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def find_nested_classes_and_methods(code_text: str, program_language):\n",
    "    \"\"\"\n",
    "    找到文件中的嵌套类，方法\n",
    "    \"\"\"\n",
    "    prompt = get_step1_prompt(code_text, program_language)\n",
    "    response = get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True)\n",
    "    nested_classes_and_methods = json.loads(response)\n",
    "    return nested_classes_and_methods\n",
    "\n",
    "def process_file_with_chain_of_thought(code_text: str, program_language: str):\n",
    "    \"\"\"\n",
    "    读取 Python 文件 -> 生成’思维链’式Prompt -> 调用大模型 -> 写入结果JSON\n",
    "    \"\"\"\n",
    "    # with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    #     code_text = f.read()\n",
    "\n",
    "    # 找到文件中的嵌套类，方法\n",
    "    step1_result = find_nested_classes_and_methods(code_text, program_language)\n",
    "    # print(json.dumps(step1_result, indent=2))\n",
    "\n",
    "    return step1_result\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def get_code_by_line_range(code_block, code):\n",
    "    code_lines = code.splitlines()\n",
    "    start_line = code_block[\"start_line\"]\n",
    "    end_line = code_block[\"end_line\"] + 1\n",
    "\n",
    "    ## start_line 到 end_line 之间的代码， 但是要减去自身class和function的代码\n",
    "    line_set = set(range(start_line, end_line))\n",
    "    for func in code_block.get(\"functions\", []):\n",
    "        func_start_line = func.get(\"start_line\", 0)\n",
    "        func_end_line = func.get(\"end_line\", 0)\n",
    "        line_set.difference_update(range(func_start_line, func_end_line))\n",
    "\n",
    "    for cls in code_block.get(\"classes\", []):\n",
    "        cls_start_line = cls.get(\"start_line\", 0)\n",
    "        cls_end_line = cls.get(\"end_line\", 0)\n",
    "        line_set.difference_update(range(cls_start_line, cls_end_line))\n",
    "\n",
    "    # 将line_set转换为有序列表并排序,确保按行号顺序\n",
    "    ordered_lines = sorted(list(line_set))\n",
    "    sum_code = \"\\n\".join([code_lines[i-1] for i in ordered_lines])\n",
    "\n",
    "    code_block[\"simplified_code\"] = sum_code\n",
    "\n",
    "def recursive_get_code_by_line_range(code_block, code):\n",
    "    get_code_by_line_range(code_block, code)\n",
    "    for func in code_block.get(\"functions\", []):\n",
    "        recursive_get_code_by_line_range(func, code)\n",
    "    for cls in code_block.get(\"classes\", []):\n",
    "        recursive_get_code_by_line_range(cls, code)\n",
    "\n",
    "\n",
    "def print_simplified_code(code_block: dict, indent=0):\n",
    "    \"\"\"\n",
    "    递归遍历并打印 simplified_code\n",
    "    \"\"\"\n",
    "    print(\" \" * indent + \"简化后的代码:\")\n",
    "    print(\" \" * indent + code_block.get(\"simplified_code\", \"\").strip())\n",
    "\n",
    "    # 递归处理嵌套的类\n",
    "    for class_block in code_block.get(\"classes\", []):\n",
    "        print(\" \" * indent + f\"\\n类 {class_block.get('name', '')}:\")\n",
    "        print_simplified_code(class_block, indent + 2)\n",
    "\n",
    "    # 递归处理嵌套的函数\n",
    "    for function_block in code_block.get(\"functions\", []):\n",
    "        print(\" \" * indent + f\"\\n函数 {function_block.get('name', '')}:\")\n",
    "        print_simplified_code(function_block, indent + 2)\n",
    "\n",
    "def get_code_cfg_prompt(code, program_language):\n",
    "    \"\"\"\n",
    "    生成代码的CFG的Prompt\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You will be given a piece of **{program_language}** code. Your goal is to generate a **Control Flow Graph (CFG)** for this code and output the result as **JSON**. Here are the specific requirements:\n",
    "\n",
    "1. **Definition of Basic Blocks**:\n",
    "   - A basic block can contain one or more “continuous and unbranched” statements.\n",
    "   - Whenever you encounter a statement that causes a flow jump or branch (e.g., `if-else`, `for-while`, `try-except-finally`, `with-as`, `match-case`, `break-continue-return`, etc.), you should start a new basic block.\n",
    "\n",
    "2. **JSON Output Structure**:\n",
    "   - Your output must strictly follow this JSON format, with no additional text or explanation:\n",
    "```json\n",
    "{{\n",
    "  \"blocks\": [\n",
    "    {{\n",
    "      \"id\": 1,\n",
    "      \"label\": \"if a > 2:\",\n",
    "      \"successors\": [\n",
    "        {{\n",
    "          \"id\": 2,\n",
    "          \"label\": \"    print(a)\",\n",
    "          \"successors\": [\n",
    "            {{\n",
    "              \"id\": 4,\n",
    "              \"label\": \"print(\\\\\"done\\\\\")\",\n",
    "              \"successors\": []\n",
    "            }}\n",
    "          ]\n",
    "        }},\n",
    "        {{\n",
    "          \"id\": 3,\n",
    "          \"label\": \"    print(1)\",\n",
    "          \"successors\": [\n",
    "            {{\n",
    "              \"id\": 4,\n",
    "              \"label\": \"print(\\\\\"done\\\\\")\",\n",
    "              \"successors\": []\n",
    "            }}\n",
    "          ]\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "   - **id**: an integer starting from 1, incrementing by 1 for each block.\n",
    "   - **label**: the exact code inside this block, unchanged.\n",
    "   - **successors**: a list of nested blocks that may execute after this block. Each item in this list is itself a block with the same structure: \"id\", \"label\", and \"successors\".\n",
    "\n",
    "3. **Branch Structures**:\n",
    "   - **if-else**: for `if condition: ... else: ...`, both the `if` body and the `else` body should be separate blocks. The `if` block’s `\"successors\"` should include both branches as nested block objects.\n",
    "   - **for-while**: the loop body and the statement(s) following the loop should be in different blocks, with correct flow back to the loop condition if it continues, or forward to the next block if it terminates. Again, these should appear as nested block objects in `\"successors\"`.\n",
    "   - **try-except-finally**: each `try`, `except`, and `finally` block should be identified separately, showing normal and exceptional flows by nesting them in `\"successors\"`.\n",
    "   - **with-as**: the code inside the `with` statement and the code after the `with` block should be separate blocks, reflected as nested structures.\n",
    "   - **match-case**: treat each `case` body as a separate nested block in `\"successors\"`.\n",
    "   - **break-continue-return**: these statements jump to outside of the loop, back to the loop condition, or end the function. If the function ends, the `\"successors\"` can be an empty list.\n",
    "\n",
    "4. **Final Output**:\n",
    "   - Ensure your output is valid JSON (only one root object, containing `\"blocks\"`).\n",
    "   - Do not add extra text or explanation—only the JSON object itself.\n",
    "\n",
    "---\n",
    "**Below is the code you need to analyze. Replace `{program_language}` with the actual language and insert the given code where indicated. Then generate the final JSON output strictly following the above rules:**\n",
    "\n",
    "```python\n",
    "# Example input code\n",
    "{code}\n",
    "```\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_single_block_cfg(code_block, program_languge):\n",
    "    \"\"\"\n",
    "    获取每个代码块的CFG\n",
    "    \"\"\"\n",
    "    prompt = get_code_cfg_prompt(code_block[\"simplified_code\"], program_languge)\n",
    "    response = get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True)\n",
    "    blocks = json.loads(response)[\"blocks\"]\n",
    "    code_block[\"blocks\"] = blocks\n",
    "    \n",
    "def recursive_get_each_block_cfg(code_block, program_language):\n",
    "    \"\"\"\n",
    "    递归获取每个代码块的CFG\n",
    "    \"\"\"\n",
    "    get_single_block_cfg(code_block, program_language)\n",
    "    for block in code_block.get(\"classes\", []):\n",
    "        recursive_get_each_block_cfg(block, program_language)\n",
    "    for block in code_block.get(\"functions\", []):\n",
    "        recursive_get_each_block_cfg(block, program_language)\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "def main():\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    from functools import partial\n",
    "    def process_single_file(file, target_file):\n",
    "        if os.path.exists(target_file):\n",
    "            return\n",
    "        print(\"Processing \" + file)\n",
    "        with open(file, 'r') as f:\n",
    "            code = f.read()\n",
    "        step1_result = process_file_with_chain_of_thought(code, \"ts\")\n",
    "        recursive_get_code_by_line_range(step1_result, code)\n",
    "        recursive_get_each_block_cfg(step1_result, \"ts\")\n",
    "\n",
    "        def remove_duplicate_blocks(code_block):\n",
    "            \"\"\"\n",
    "            删除同一层级中start_line和end_line相同的重复块,仅保留最前面的一个\n",
    "            \"\"\"\n",
    "            if \"blocks\" in code_block:\n",
    "                # 用于记录已经出现过的(start_line, end_line)组合\n",
    "                seen = set()\n",
    "                # 用于存储不重复的blocks\n",
    "                unique_blocks = []\n",
    "                \n",
    "                for block in code_block[\"blocks\"]:\n",
    "                    # 如果block有start_line和end_line属性\n",
    "                    if \"start_line\" in block and \"end_line\" in block:\n",
    "                        key = (block[\"start_line\"], block[\"end_line\"])\n",
    "                        if key not in seen:\n",
    "                            seen.add(key)\n",
    "                            unique_blocks.append(block)\n",
    "                    else:\n",
    "                        # 如果没有这些属性,保留该block\n",
    "                        unique_blocks.append(block)\n",
    "                        \n",
    "                code_block[\"blocks\"] = unique_blocks\n",
    "            \n",
    "            # 递归处理子块\n",
    "            for sub_block in code_block.get(\"classes\", []):\n",
    "                remove_duplicate_blocks(sub_block)\n",
    "            for sub_block in code_block.get(\"functions\", []):\n",
    "                remove_duplicate_blocks(sub_block)\n",
    "                \n",
    "        # 处理整个CFG\n",
    "        remove_duplicate_blocks(step1_result)\n",
    "        \n",
    "        with open(target_file, \"w\") as f:\n",
    "            json.dump(step1_result, f, indent=2)\n",
    "\n",
    "    target_dir = \"llm_cfg\"\n",
    "    source_code_dir = \"../../dataset/ts\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    files = [(source_code_dir + \"/\" + str(i) + \".ts\", target_dir + \"/\" + str(i) + \".json\") for i in range(200)]\n",
    "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = []\n",
    "        for file_pair in files:\n",
    "            futures.append(executor.submit(process_single_file, *file_pair))\n",
    "        \n",
    "        for _ in tqdm(as_completed(futures), total=len(files), desc=\"处理CFG文件\"):\n",
    "            pass\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM生成的代码可能可以合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 95.json\n",
      "Processing 110.json\n",
      "Processing 94.json\n",
      "Processing 38.json\n",
      "Processing 21.json\n",
      "Processing 121.json\n",
      "Processing 72.json\n",
      "Processing 67.json\n",
      "Processing 4.json\n",
      "Processing 74.json\n",
      "Processing 116.json\n",
      "Processing 40.json\n",
      "Processing 14.json\n",
      "Processing 7.json\n",
      "Processing 17.json\n",
      "Processing 107.json\n",
      "Processing 89.json\n",
      "Processing 80.json\n",
      "Processing 98.json\n",
      "Processing 106.json\n",
      "Processing 96.json\n",
      "Processing 123.json\n",
      "Processing 28.json\n",
      "Processing 45.json\n",
      "Processing 13.json\n",
      "Processing 2.json\n",
      "Processing 90.json\n",
      "Processing 34.json\n",
      "Processing 37.json\n",
      "Processing 82.json\n",
      "Processing 105.json\n",
      "Processing 122.json\n",
      "Processing 24.json\n",
      "Processing 73.json\n",
      "Processing 79.json\n",
      "Processing 118.json\n",
      "Processing 87.json\n",
      "Processing 54.json\n",
      "Processing 59.json\n",
      "Processing 85.json\n",
      "Processing 97.json\n",
      "Processing 71.json\n",
      "Processing 8.json\n",
      "Processing 10.json\n",
      "Processing 26.json\n",
      "Processing 33.json\n",
      "Processing 88.json\n",
      "Processing 104.json\n",
      "Processing 39.json\n",
      "Processing 102.json\n",
      "Processing 3.json\n",
      "Processing 42.json\n",
      "Processing 56.json\n",
      "Processing 117.json\n",
      "Processing 50.json\n",
      "Processing 91.json\n",
      "Processing 93.json\n",
      "Processing 101.json\n",
      "Processing 81.json\n",
      "Processing 20.json\n",
      "Processing 120.json\n",
      "Processing 0.json\n",
      "Processing 68.json\n",
      "Processing 36.json\n",
      "Processing 16.json\n",
      "Processing 76.json\n",
      "Processing 69.json\n",
      "Processing 25.json\n",
      "Processing 75.json\n",
      "Processing 44.json\n",
      "Processing 66.json\n",
      "Processing 49.json\n",
      "Processing 63.json\n",
      "Processing 114.json\n",
      "Processing 115.json\n",
      "Processing 127.json\n",
      "Processing 86.json\n",
      "Processing 5.json\n",
      "Processing 22.json\n",
      "Processing 70.json\n",
      "Processing 15.json\n",
      "Processing 84.json\n",
      "Processing 27.json\n",
      "Processing 11.json\n",
      "Processing 65.json\n",
      "Processing 43.json\n",
      "Processing 78.json\n",
      "Processing 32.json\n",
      "Processing 58.json\n",
      "Processing 111.json\n",
      "Processing 124.json\n",
      "Processing 51.json\n",
      "Processing 57.json\n",
      "Processing 29.json\n",
      "Processing 35.json\n",
      "Processing 83.json\n",
      "Processing 1.json\n",
      "Processing 126.json\n",
      "Processing 109.json\n",
      "Processing 41.json\n",
      "Processing 19.json\n",
      "Processing 92.json\n",
      "Processing 52.json\n",
      "Processing 64.json\n",
      "Processing 55.json\n",
      "Processing 112.json\n",
      "Processing 46.json\n",
      "Processing 128.json\n",
      "Processing 6.json\n",
      "Processing 60.json\n",
      "Processing 47.json\n",
      "Processing 77.json\n",
      "Processing 119.json\n",
      "Processing 9.json\n",
      "Processing 99.json\n",
      "Processing 113.json\n",
      "Processing 100.json\n",
      "Processing 62.json\n",
      "Processing 61.json\n",
      "Processing 12.json\n",
      "Processing 125.json\n",
      "Processing 30.json\n",
      "Processing 23.json\n",
      "Processing 108.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def process_cfg(cfg):\n",
    "    \"\"\"\n",
    "    Process a CFG that uses a *nested successors* structure.\n",
    "    We will:\n",
    "      1. Remove unreachable blocks (only keep blocks reachable from the root).\n",
    "      2. Separate loop headers from loop bodies (if desired).\n",
    "      3. Merge consecutive linear blocks that have only one successor and one predecessor.\n",
    "      4. Recursively process functions/classes if they exist.\n",
    "    \"\"\"\n",
    "\n",
    "    #=== 1. 过滤不可达节点: 我们假设 blocks[0] 是 CFG 的根节点 ===#\n",
    "    def filter_connected_blocks(blocks):\n",
    "        \"\"\"\n",
    "        Given a list of blocks (in nested form), return only those reachable\n",
    "        from the 'root' block (which we assume is blocks[0]) by traversing\n",
    "        nested successors.\n",
    "        \"\"\"\n",
    "\n",
    "        visited_ids = set()\n",
    "        # 为了方便在后面快速通过 id 找到对应的 block 对象，我们先做一个 {id: block} 的映射\n",
    "        # 同时存储所有 block 的引用（因为是嵌套的，需要把内部 successors 里的 block 也加入到此映射）\n",
    "        id_to_block = {}\n",
    "\n",
    "        def collect_all_blocks(block_list):\n",
    "            for b in block_list:\n",
    "                id_to_block[b[\"id\"]] = b\n",
    "                if \"successors\" in b:\n",
    "                    collect_all_blocks(b[\"successors\"])\n",
    "\n",
    "        collect_all_blocks(blocks)\n",
    "\n",
    "        # 深度优先搜索，查找所有可达节点\n",
    "        def dfs(block):\n",
    "            if block[\"id\"] in visited_ids:\n",
    "                return\n",
    "            visited_ids.add(block[\"id\"])\n",
    "            for succ_block in block.get(\"successors\", []):\n",
    "                dfs(succ_block)\n",
    "\n",
    "        # 假定 blocks[0] 是 root\n",
    "        if blocks:\n",
    "            root_block = blocks[0]\n",
    "            dfs(root_block)\n",
    "\n",
    "        # 现在我们只保留被 visited_ids 覆盖到的节点，并且需要“剪枝”不在 visited_ids 中的后继\n",
    "        def filter_nested(block_list):\n",
    "            \"\"\"在嵌套结构中移除不可达节点。\"\"\"\n",
    "            filtered = []\n",
    "            for b in block_list:\n",
    "                if b[\"id\"] in visited_ids:\n",
    "                    # 递归处理 successors\n",
    "                    new_successors = filter_nested(b.get(\"successors\", []))\n",
    "                    filtered.append({\n",
    "                        \"id\": b[\"id\"],\n",
    "                        \"label\": b[\"label\"],\n",
    "                        \"successors\": new_successors\n",
    "                    })\n",
    "            return filtered\n",
    "\n",
    "        return filter_nested(blocks)\n",
    "\n",
    "    #=== 2. 判断循环头（示例仅以 \"for\" / \"while\" 关键字简单判断） ===#\n",
    "    def is_loop_header(block):\n",
    "        \"\"\"\n",
    "        A naive check: if the block's label starts with 'for' or 'while'\n",
    "        (or contains those keywords in a relevant way), treat it as a loop header.\n",
    "        \"\"\"\n",
    "        code_str = block[\"label\"].strip()\n",
    "        if code_str.startswith(\"for \") or code_str.startswith(\"while \"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    #=== 3. 合并逻辑（循环头和循环体暂时不做复杂拆分，仅演示思路） ===#\n",
    "    # 在嵌套结构中，“连续的线性块”通常表现为一个 block 有且仅有 1 个 successor，且该 successor 只有该一个 predecessor。\n",
    "    # 但是在嵌套结构里，我们无法简易地统计 predecessor 数量，需要自行设计。\n",
    "    #\n",
    "    # 示例逻辑：深度遍历 + 遇到 loop header 不合并；遇到多 successor 不合并；否则合并到下一个 block。\n",
    "    def merge_blocks_in_place(block):\n",
    "        \"\"\"\n",
    "        递归地合并一个 block 的线性后继。\n",
    "        当遇到循环头或分支时，不再合并。\n",
    "        \"\"\"\n",
    "        successors = block.get(\"successors\", [])\n",
    "        if not successors:\n",
    "            # 无后继，直接返回\n",
    "            return block\n",
    "\n",
    "        # 如果存在多个 successor，说明是分支点，不合并任何后继\n",
    "        if len(successors) > 1:\n",
    "            # 递归处理每个 successor\n",
    "            for i, succ in enumerate(successors):\n",
    "                successors[i] = merge_blocks_in_place(succ)\n",
    "            block[\"successors\"] = successors\n",
    "            return block\n",
    "\n",
    "        # 如果只有 1 个 successor，则尝试合并\n",
    "        single_succ = successors[0]\n",
    "        if is_loop_header(block):\n",
    "            # 如果当前 block 是 loop header，不向后合并，只是递归处理后继\n",
    "            block[\"successors\"][0] = merge_blocks_in_place(single_succ)\n",
    "            return block\n",
    "        if is_loop_header(single_succ):\n",
    "            # 如果后继是 loop header，也不合并，只是递归处理后继\n",
    "            block[\"successors\"][0] = merge_blocks_in_place(single_succ)\n",
    "            return block\n",
    "\n",
    "        # 到这里，意味着我们可以把 single_succ 跟当前块合并\n",
    "        block[\"label\"] = block[\"label\"] + \"\\n\" + single_succ[\"label\"]\n",
    "        # 把 single_succ 的 successors 赋给当前块\n",
    "        block[\"successors\"] = single_succ.get(\"successors\", [])\n",
    "\n",
    "        # 递归处理“合并后”依然存在的后继（可能还是一个 list）\n",
    "        if block[\"successors\"]:\n",
    "            new_succ_list = []\n",
    "            for succ in block[\"successors\"]:\n",
    "                new_succ_list.append(merge_blocks_in_place(succ))\n",
    "            block[\"successors\"] = new_succ_list\n",
    "\n",
    "        return block\n",
    "\n",
    "    #=== 4. 针对最外层的 blocks 做处理 ===#\n",
    "    #  4.1 过滤掉不可达节点\n",
    "    if \"blocks\" in cfg:\n",
    "        cfg[\"blocks\"] = filter_connected_blocks(cfg[\"blocks\"])\n",
    "\n",
    "    #  4.2 合并块：因为是多 block，需要逐个处理，然后再把处理结果放回 cfg[\"blocks\"] \n",
    "    #      同时，新的根块可能因为合并也会改变，所以我们需要重新搜集并替换\n",
    "    if \"blocks\" in cfg and cfg[\"blocks\"]:\n",
    "        merged = []\n",
    "        for b in cfg[\"blocks\"]:\n",
    "            merged_block = merge_blocks_in_place(b)\n",
    "            merged.append(merged_block)\n",
    "        cfg[\"blocks\"] = merged\n",
    "\n",
    "    #=== 5. 递归处理 functions 与 classes ===#\n",
    "    if \"functions\" in cfg:\n",
    "        for func in cfg[\"functions\"]:\n",
    "            process_cfg(func)\n",
    "\n",
    "    if \"classes\" in cfg:\n",
    "        for cls in cfg[\"classes\"]:\n",
    "            process_cfg(cls)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "#=============================\n",
    "# 下面是示例读取并处理文件的逻辑\n",
    "#=============================\n",
    "import os\n",
    "import json\n",
    "\n",
    "for file in os.listdir(\"llm_cfg\"):\n",
    "    path = os.path.join(\"llm_cfg\", file)\n",
    "    if not os.path.isfile(path):\n",
    "        continue\n",
    "\n",
    "    print(\"Processing\", file)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            llm_cfg = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        process_cfg(llm_cfg)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    os.makedirs(\"merged_llm_cfg\", exist_ok=True)\n",
    "    output_path = os.path.join(\"merged_llm_cfg\", file)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(llm_cfg, f, indent=2, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scalpel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
