{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "平均指标:\n",
      "Precision: 0.4821\n",
      "Recall: 0.6806\n",
      "F1 Score: 0.5644\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def normalize_name(call):\n",
    "    \"\"\"\n",
    "    Normalize a function/method name for comparison.\n",
    "    - Removes redundant module prefixes if possible.\n",
    "    - Converts the name to a comparable base representation.\n",
    "    \"\"\"\n",
    "    return call.split('.')[-1]  # Use only the last part (e.g., \"backend.data.graph.Node\" -> \"Node\")\n",
    "\n",
    "\n",
    "def compare_call_graphs(generated, ground_truth, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Compares the generated call graph with the ground truth and calculates metrics.\n",
    "\n",
    "    Parameters:\n",
    "        generated (dict): The call graph generated by the analyzer.\n",
    "        ground_truth (dict): The static ground truth call graph.\n",
    "        similarity_threshold (float): Threshold for considering two function names as similar.\n",
    "\n",
    "    Returns:\n",
    "        dict: A detailed comparison report including metrics and mismatches.\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"missing_keys\": [],\n",
    "        \"extra_keys\": [],\n",
    "        \"mismatched_calls\": [],\n",
    "        \"metrics\": {\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"f1_score\": 0.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Helper function for similarity matching\n",
    "    def is_similar(call1, call2):\n",
    "        return SequenceMatcher(None, normalize_name(call1), normalize_name(call2)).ratio() >= similarity_threshold\n",
    "\n",
    "    # Convert keys to sets\n",
    "    generated_keys = set(generated.keys())\n",
    "    ground_truth_keys = set(ground_truth.keys())\n",
    "\n",
    "    # Detect missing and extra keys\n",
    "    report[\"missing_keys\"] = list(ground_truth_keys - generated_keys)\n",
    "    report[\"extra_keys\"] = list(generated_keys - ground_truth_keys)\n",
    "\n",
    "    # Check mismatched calls for common keys\n",
    "    total_matches = 0\n",
    "    total_ground_truth_calls = 0\n",
    "    total_generated_calls = 0\n",
    "\n",
    "    common_keys = ground_truth_keys.intersection(generated_keys)\n",
    "    for key in common_keys:\n",
    "        ground_truth_calls = ground_truth[key]\n",
    "        generated_calls = generated[key]\n",
    "        total_ground_truth_calls += len(ground_truth_calls)\n",
    "        total_generated_calls += len(generated_calls)\n",
    "\n",
    "        matched_calls = set()\n",
    "        unmatched_generated = []\n",
    "        unmatched_ground_truth = []\n",
    "\n",
    "        for gt_call in ground_truth_calls:\n",
    "            found = False\n",
    "            for gen_call in generated_calls:\n",
    "                if is_similar(gt_call, gen_call):\n",
    "                    matched_calls.add(gt_call)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                unmatched_ground_truth.append(gt_call)\n",
    "\n",
    "        for gen_call in generated_calls:\n",
    "            if not any(is_similar(gen_call, gt_call) for gt_call in ground_truth_calls):\n",
    "                unmatched_generated.append(gen_call)\n",
    "\n",
    "        total_matches += len(matched_calls)\n",
    "        if unmatched_generated or unmatched_ground_truth:\n",
    "            report[\"mismatched_calls\"].append({\n",
    "                \"key\": key,\n",
    "                \"missing_calls\": unmatched_ground_truth,\n",
    "                \"extra_calls\": unmatched_generated\n",
    "            })\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    if total_generated_calls > 0:\n",
    "        precision = total_matches / total_generated_calls\n",
    "    else:\n",
    "        if total_ground_truth_calls > 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            precision = 1.0\n",
    "\n",
    "    if total_ground_truth_calls > 0:\n",
    "        recall = total_matches / total_ground_truth_calls\n",
    "    else:\n",
    "        recall = 1.0\n",
    "\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "\n",
    "    report[\"metrics\"][\"precision\"] = round(precision, 4)\n",
    "    report[\"metrics\"][\"recall\"] = round(recall, 4)\n",
    "    report[\"metrics\"][\"f1_score\"] = round(f1_score, 4)\n",
    "    report[\"metrics\"][\"total_matches\"] = total_matches\n",
    "    report[\"metrics\"][\"total_ground_truth_calls\"] = total_ground_truth_calls\n",
    "    report[\"metrics\"][\"total_generated_calls\"] = total_generated_calls\n",
    "\n",
    "    return report\n",
    "\n",
    "# Example usage\n",
    "import os\n",
    "if __name__ == \"__main__\":\n",
    "    total_matches = 0\n",
    "    total_ground_truth_calls = 0\n",
    "    total_generated_calls = 0\n",
    "    \n",
    "    for i in range(200):\n",
    "        if not os.path.exists(f\"llm_cg/{i}.json\"):\n",
    "            continue\n",
    "        if not os.path.exists(f\"../../dataset/ast_cg/{i}.json\"):\n",
    "            continue\n",
    "            \n",
    "        generated = json.load(open(f\"llm_cg/{i}.json\"))\n",
    "        ground_truth = json.load(open(f\"../../dataset/ast_cg/{i}.json\"))\n",
    "        \n",
    "        differences = compare_call_graphs(generated, ground_truth)\n",
    "        metrics = differences[\"metrics\"]\n",
    "        \n",
    "        total_matches += metrics[\"total_matches\"]\n",
    "        total_ground_truth_calls += metrics[\"total_ground_truth_calls\"]\n",
    "        total_generated_calls += metrics[\"total_generated_calls\"]\n",
    "            \n",
    "        \n",
    "        # print(f\"file {i} metrics:\")\n",
    "        # print(json.dumps(metrics, indent=4))\n",
    "        # 如果precision低于0.5,删除该文件\n",
    "        # if metrics[\"precision\"] < 0.5:\n",
    "        #     print(f\"file {i} metrics:\")\n",
    "        #     print(json.dumps(metrics, indent=4))\n",
    "            # os.remove(f\"llm_cg/{i}.json\")\n",
    "            # print(f\"已删除文件 llm_cg/{i}.json (precision={metrics['precision']})\")\n",
    "    \n",
    "    # 使用总体数据计算指标\n",
    "    precision = round(total_matches / total_generated_calls, 4) if total_generated_calls > 0 else 0\n",
    "    recall = round(total_matches / total_ground_truth_calls, 4) if total_ground_truth_calls > 0 else 0\n",
    "    f1_score = round(2 * (precision * recall) / (precision + recall), 4) if precision + recall > 0 else 0\n",
    "    \n",
    "    print(\"\\n平均指标:\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")  \n",
    "    print(f\"F1 Score: {f1_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
