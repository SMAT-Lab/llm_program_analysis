{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_cg_dir = \"llm_cg\"\n",
    "static_cg_dir = \"../../dataset/ts_cg\"\n",
    "source_dir = \"../../dataset/ts\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "datas = []\n",
    "\n",
    "for i in range(200):\n",
    "    llm_cg_path = os.path.join(llm_cg_dir, f\"{i}.json\")\n",
    "    static_cg_path = os.path.join(static_cg_dir, f\"{i}.ts.json\")\n",
    "    source_path = os.path.join(source_dir, f\"{i}.ts\")\n",
    "\n",
    "    if not os.path.exists(llm_cg_path) or not os.path.exists(static_cg_path) or not os.path.exists(source_path):\n",
    "        continue\n",
    "\n",
    "    with open(source_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        source_code = f.read()\n",
    "\n",
    "    with open(llm_cg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        llm_cg = json.load(f)  \n",
    "    with open(static_cg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        static_cg = json.load(f)\n",
    "\n",
    "    data = {    \n",
    "        \"source_code\": source_code,\n",
    "        \"llm_cg\": llm_cg,\n",
    "        \"static_cg\": static_cg\n",
    "    }\n",
    "    datas.append(data)\n",
    "\n",
    "with open(\"cg_task.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in datas:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import get_llm_answers\n",
    "import json\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "def process_line(line):\n",
    "    data = json.loads(line)\n",
    "    source_code = data[\"source_code\"]\n",
    "    llm_cg = data[\"llm_cg\"]\n",
    "    static_cg = data[\"static_cg\"]\n",
    "\n",
    "    prompt = f\"\"\" I will give you a typescript code and its call graph generated by llm and static analysis.\n",
    "You can treat the static call graph as the ground truth.\n",
    "Please compare the two call graphs and evaluate the llm call graph.\n",
    "TypeScript code:\n",
    "{source_code}\n",
    "LLM call graph:\n",
    "{llm_cg}\n",
    "Static call graph:\n",
    "{static_cg} \n",
    "\n",
    "Sum of LLM call graph: {sum(len(v) for v in llm_cg.values())}\n",
    "Sum of Static call graph: {sum(len(v) for v in static_cg.values())}\n",
    "\n",
    "However, not all the calls in the static call graph need to be considered.\n",
    "We just need to consider the calls that **defined** in the source code. The calls such as **imported** are not considered.\n",
    "\n",
    "Attention:\n",
    "The format of the call graph is a dictionary, where the key is the function name and the value is a list of function calls.\n",
    "\n",
    "Don't just compare the function name, maybe the function name is different but the function is the same.\n",
    "\n",
    "Your output should be a json with the following format:\n",
    "Don't just compare the function name, maybe the function name is different but the function is the same.\n",
    "\n",
    "Your output should be a json with the following format:\n",
    "{{\n",
    "    \"sum_call_from_static\": number,\n",
    "    \"correct_call_from_llm\": number,\n",
    "    \"missing_call_from_llm\": number,\n",
    "    \"extra_call_from_llm\": number\n",
    "}}\n",
    "\"\"\"\n",
    "    return json.loads(get_llm_answers(prompt, model_name=\"deepseek-chat\", require_json=True))\n",
    "\n",
    "jsonl_file = \"cg_task.jsonl\"\n",
    "results = []\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(\"results_local\", exist_ok=True)\n",
    "\n",
    "with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    # 解析每一行获取文件名\n",
    "    files = [f\"ts_{i}.ts\" for i in range(200)]\n",
    "    \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    future_to_line = {executor.submit(process_line, line): i for i, line in enumerate(lines)}\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(future_to_line):\n",
    "        line_index = future_to_line[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            # 添加文件名到结果中\n",
    "            result[\"file_name\"] = files[line_index]\n",
    "            results.append(result)\n",
    "            # 保存单个结果\n",
    "            with open(f\"results_local/result_{line_index}.json\", \"w\") as f:\n",
    "                json.dump(result, f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f'处理第 {line_index} 行时发生错误: {str(e)}')\n",
    "\n",
    "# 保存所有结果\n",
    "with open(\"results_local/all_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "总体指标:\n",
      "Total files: 128\n",
      "Total calls from static: 307\n",
      "Total calls from llm: 772\n",
      "Total calls from llm matched: 227\n",
      "Precision: 0.294\n",
      "Recall: 0.7394\n",
      "F1 Score: 0.4207\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "results = json.load(open(\"results_local/all_results.json\"))\n",
    "\n",
    "# 计算总体指标\n",
    "total_correct = sum(r[\"correct_call_from_llm\"] for r in results)\n",
    "total_static = sum(r[\"sum_call_from_static\"] for r in results)\n",
    "total_llm = sum(r[\"correct_call_from_llm\"] + r[\"extra_call_from_llm\"] for r in results)\n",
    "\n",
    "precision = round(total_correct / total_llm, 4) if total_llm > 0 else 0\n",
    "recall = round(total_correct / total_static, 4) if total_static > 0 else 0\n",
    "f1_score = round(2 * (precision * recall) / (precision + recall), 4) if precision + recall > 0 else 0\n",
    "\n",
    "print(\"\\n总体指标:\")\n",
    "print(f\"Total files: {len(results)}\")\n",
    "print(f\"Total calls from static: {total_static}\")\n",
    "print(f\"Total calls from llm: {total_llm}\")\n",
    "print(f\"Total calls from llm matched: {total_correct}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
