{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from llm import get_llm_answers\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def tokenize_code_with_lines(code: str) -> List[Tuple[int, int, str, int]]:\n",
    "    \"\"\"\n",
    "    使用正则分词，直接在原始代码上通过 finditer() 获取匹配位置，\n",
    "    并保留换行符，让 LLM 能识别多行结构。\n",
    "    返回形式: [(start_offset, end_offset, token_text, line_number), ...]\n",
    "    \"\"\"\n",
    "    token_pattern = (\n",
    "        r'[A-Za-z_]\\w*|'           # 标识符\n",
    "        r'[0-9]+(?:\\.[0-9]+)?|'    # 整数和浮点数\n",
    "        r'\"(?:\\\\.|[^\"\\\\])*\"|'      # 双引号字符串(支持转义)\n",
    "        r\"'(?:\\\\.|[^'\\\\])*'|\"      # 单引号字符串(支持转义)\n",
    "        r'`[^`]*`|'                # 反引号字符串(如Go/JS模板字符串)\n",
    "        r'#.*|'                    # Python风格单行注释\n",
    "        r'//.*|'                   # C++/Java风格单行注释\n",
    "        r'/\\*[\\s\\S]*?\\*/|'         # C风格多行注释\n",
    "        r'\"\"\"[\\s\\S]*?\"\"\"|'         # Python三引号多行字符串\n",
    "        r\"'''[\\s\\S]*?'''|\"         # Python三引号多行字符串\n",
    "        r'\\\\[ntr]|'                # 常见转义字符\n",
    "        r'\\$\\{[^}]*\\}|'            # 字符串插值语法\n",
    "        r'\\n|\\r|\\t|'               # 换行/回车/制表符\n",
    "        r'\\S'                      # 其他任意非空白字符\n",
    "    )\n",
    "\n",
    "    tokens_with_offset = []\n",
    "    lines = code.splitlines(keepends=True)\n",
    "    current_line = 1\n",
    "    current_pos = 0\n",
    "    \n",
    "    for match in re.finditer(token_pattern, code, re.MULTILINE | re.DOTALL):\n",
    "        tk = match.group(0)\n",
    "        start_offset, end_offset = match.span()\n",
    "        \n",
    "        # 计算当前token所在行号\n",
    "        while current_line <= len(lines) and current_pos + len(lines[current_line-1]) <= start_offset:\n",
    "            current_pos += len(lines[current_line-1])\n",
    "            current_line += 1\n",
    "            \n",
    "        tokens_with_offset.append((start_offset, end_offset, tk, current_line))\n",
    "\n",
    "    return tokens_with_offset\n",
    "\n",
    "def get_ast_prompt(tokens, language):\n",
    "    allowed_types = [\n",
    "        \"ABSTRACT\", \"AS\", \"BOOLEAN\", \"BREAK\", \"CASE\", \"CATCH\", \"CHAR\", \"CLASS\", \"CONST\", \"CONTINUE\",\n",
    "        \"DO\", \"ELSE\", \"ENUM\", \"ERROR\", \"EXTEND\", \"FALSE\", \"FINALLY\", \"FOR\", \"FOREIGN\", \"FROM\", \n",
    "        \"FUNC\", \"IF\", \"IMPORT\", \"IN\", \"INIT\", \"INOUT\", \"INTERFACE\", \"INTNATIVE\", \"IS\", \"LET\",\n",
    "        \"MACRO\", \"MAIN\", \"MATCH\", \"MUT\", \"OPEN\", \"OPERATOR\", \"OVERRIDE\", \"PACKAGE\", \"PRIVATE\",\n",
    "        \"PROP\", \"PROTECTED\", \"PUBLIC\", \"QUOTE\", \"REDEF\", \"RETURN\", \"SEALED\", \"SPAWN\", \"STATIC\",\n",
    "        \"STRUCT\", \"SUPER\", \"SYNCHRONIZED\", \"THIS\", \"THROW\", \"TRUE\", \"TRY\", \"TYPE\", \"UINTNATIVE\",\n",
    "        \"UNIT\", \"UNSAFE\", \"VAR\", \"WHERE\", \"WHILE\", \"argumentList\", \"arrayLiteral\", \"arrowType\",\n",
    "        \"assignmentExpression\", \"binaryExpression\", \"block\", \"body\", \"booleanLiteral\",\n",
    "        \"breakExpression\", \"builtinFunction\", \"callExpression\", \"caseBody\", \"charLangTypes\",\n",
    "        \"characterLiteral\", \"classDefinition\", \"classType\", \"collectionLiteral\", \"comment\",\n",
    "        \"constantPattern\", \"continueExpression\", \"dollarIdentifier\", \"element\", \"elements\",\n",
    "        \"enumBody\", \"enumDefinition\", \"enumPattern\", \"enumPatternParameters\", \"escapeSeq\",\n",
    "        \"exceptionTypePattern\", \"extendBody\", \"extendDefinition\", \"extendMemberDeclaration\",\n",
    "        \"fieldExpression\", \"floatLiteral\", \"forInExpression\", \"foreignDeclaration\",\n",
    "        \"foreignMemberDeclaration\", \"functionDefinition\", \"functionParameters\", \"genericConstraints\",\n",
    "        \"genericsType\", \"identifier\", \"ifExpression\", \"ifLetExpression\", \"importAll\", \"importContent\",\n",
    "        \"importList\", \"importSpecified\", \"incDecExpression\", \"initBody\", \"initialize\",\n",
    "        \"integerLiteral\", \"interfaceBody\", \"interfaceDefinition\", \"interfaceMemberDeclaration\",\n",
    "        \"lambdaExpression\", \"lambdaParameter\", \"lambdaParameters\", \"lineStringContent\",\n",
    "        \"lineStringExpression\", \"lineStringLiteral\", \"macroAttrExpr\", \"macroDecl\", \"macroDefinition\",\n",
    "        \"macroExpression\", \"macroInputExprWithParens\", \"macroInputExprWithoutParens\",\n",
    "        \"macroWithAttrParam\", \"macroWithoutAttrParam\", \"mainDefinition\", \"matchCase\",\n",
    "        \"matchExpression\", \"memberDeclaration\", \"modifiers\", \"multiLineStringContent\",\n",
    "        \"multiLineStringExpression\", \"multiLineStringLiteral\", \"multilineRawStringLiteral\",\n",
    "        \"operatorFunctionDefinition\", \"overloadedOperators\", \"packageHeader\", \"packageNameIdentifier\",\n",
    "        \"parameter\", \"parameterList\", \"parenthesizedExpression\", \"patternGuard\", \"prefixType\",\n",
    "        \"primaryInit\", \"propertyBody\", \"propertyDefinition\", \"propertyMemberDeclaration\",\n",
    "        \"quoteClose\", \"quoteExpression\", \"quoteOpen\", \"quoteParameters\", \"quoteParametersToken\",\n",
    "        \"quoteToken\", \"rangeExpression\", \"resourceSpecification\", \"resourceSpecifications\",\n",
    "        \"returnExpression\", \"sourceFile\", \"spawnExpression\", \"stringLiteral\", \"structDefinition\",\n",
    "        \"subscriptExpression\", \"superInterfaces\", \"synchronizedExpression\", \"throwExpression\",\n",
    "        \"tripleQuoteClose\", \"tripleQuoteOpen\", \"tryExpression\", \"tupleLiteral\", \"tuplePattern\",\n",
    "        \"tupleType\", \"typeAlias\", \"typeArguments\", \"typeExpression\", \"typeIdentifier\",\n",
    "        \"typeParameters\", \"typePattern\", \"unaryExpression\", \"unitLiteral\", \"unsafeExpression\",\n",
    "        \"upperBounds\", \"userType\", \"varBindingPattern\", \"variableDeclaration\", \"variableModifiers\",\n",
    "        \"whileExpression\", \"wildcardPattern\"\n",
    "    ]\n",
    "    allowed_types_str = \", \".join(allowed_types)\n",
    "\n",
    "    prompt = f\"\"\"## Task Objective\n",
    "Convert the provided {language} code into a maximally detailed Abstract Syntax Tree (AST) with atomic-level decomposition. The AST must reach the smallest parsable units per language grammar.\n",
    "\n",
    "## Node Expansion Rules\n",
    "1. **MANDATORY Decomposition** to these atomic units:\n",
    "   - Literals (integerLiteral/stringLiteral/booleanLiteral/etc.)\n",
    "   - Operators (operator='+', '-', etc.)\n",
    "   - Identifiers (variable/function names)\n",
    "   - Type annotations\n",
    "   - Individual parameters in parameter lists\n",
    "   - Sub-expressions in complex expressions\n",
    "\n",
    "2. **STRICTLY FORBIDDEN** Merging:\n",
    "   - Compound expressions must split into operator+left+right structure\n",
    "   - Function parameters must be individual nodes\n",
    "   - Multi-element statements (e.g., comma-separated imports) require separate nodes\n",
    "\n",
    "## Type Constraints\n",
    "VALID NODE TYPES ({len(allowed_types)} allowed):\n",
    "{allowed_types_str}\n",
    "\n",
    "- Root node MUST be type=module (exactly one)\n",
    "- Control flow requires exact typing (ifStatement/forStatement/etc.)\n",
    "- Expression types must specify subcategories (binaryExpression/assignmentExpression/etc.)\n",
    "\n",
    "## Structural Validation\n",
    "Your output MUST satisfy:\n",
    "1. Parent-Child Containment: parent.start_token ≤ child.start_token AND parent.end_token ≥ child.end_token\n",
    "2. Sibling Order: Sequential token ranges without overlap\n",
    "3. Leaf Nodes: Must have start_token == end_token AND empty children array\n",
    "\n",
    "## Required Output\n",
    "ONLY output raw JSON with this structure:\n",
    "{{\n",
    "  \"type\": \"module\",\n",
    "  \"start_token\": 0,\n",
    "  \"end_token\": 42,\n",
    "  \"children\": [\n",
    "    {{\n",
    "      \"type\": \"functionDefinition\",\n",
    "      \"start_token\": 3,\n",
    "      \"end_token\": 40,\n",
    "      \"children\": [...] \n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "## Input Format\n",
    "A dictionary of tokens with key as token index and value as token text.\n",
    "The tokenized code:\n",
    "{tokens}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_label_according_to_token_recursively(ast, code, tokenize_code):\n",
    "    start_token = ast[\"start_token\"]\n",
    "    end_token = ast[\"end_token\"]\n",
    "\n",
    "    # 从tokenize_code中提取对应的代码片段\n",
    "    start_offset = tokenize_code[start_token][0]\n",
    "    end_offset = tokenize_code[end_token][1]\n",
    "    label = code[start_offset:end_offset]\n",
    "    \n",
    "    # 将提取的代码片段添加到ast中\n",
    "    # 保存原始属性\n",
    "    original_type = ast[\"type\"]\n",
    "    original_start = ast[\"start_token\"] \n",
    "    original_end = ast[\"end_token\"]\n",
    "    original_children = ast.get(\"children\", [])\n",
    "    \n",
    "    # 重新按顺序构建字典\n",
    "    ast.clear()\n",
    "    ast[\"type\"] = original_type\n",
    "    ast[\"label\"] = label\n",
    "    ast[\"start_token\"] = original_start\n",
    "    ast[\"end_token\"] = original_end\n",
    "    if original_children:\n",
    "        ast[\"children\"] = original_children\n",
    "    \n",
    "    if \"children\" in ast:\n",
    "        for child in ast[\"children\"]:\n",
    "            get_label_according_to_token_recursively(child, code, tokenize_code)\n",
    "    return ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import get_llm_answers\n",
    "\n",
    "\n",
    "\n",
    "def get_label_according_to_lines_recursively(cfg, code_lines):\n",
    "    # 处理 entryBlock\n",
    "    if \"entryBlock\" in cfg:\n",
    "        entry = cfg[\"entryBlock\"]\n",
    "        if \"start_line\" in entry and \"end_line\" in entry:\n",
    "            # 确保 label 在 id 和 type 后面\n",
    "            label = code_lines[entry[\"start_line\"]-1:entry[\"end_line\"]]\n",
    "            entry_new = {\n",
    "                \"id\": entry[\"id\"],\n",
    "                \"type\": entry[\"type\"],\n",
    "                \"label\": label\n",
    "            }\n",
    "            for k,v in entry.items():\n",
    "                if k not in [\"id\", \"type\", \"label\"]:\n",
    "                    entry_new[k] = v\n",
    "            cfg[\"entryBlock\"] = entry_new\n",
    "    \n",
    "    # 处理 blocks\n",
    "    if \"blocks\" in cfg:\n",
    "        for i, block in enumerate(cfg[\"blocks\"]):\n",
    "            if \"start_line\" in block and \"end_line\" in block:\n",
    "                # 确保 label 在 id 和 type 后面\n",
    "                label = code_lines[block[\"start_line\"]-1:block[\"end_line\"]]\n",
    "                block_new = {\n",
    "                    \"id\": block[\"id\"],\n",
    "                    \"type\": block[\"type\"], \n",
    "                    \"label\": label\n",
    "                }\n",
    "                for k,v in block.items():\n",
    "                    if k not in [\"id\", \"type\", \"label\"]:\n",
    "                        block_new[k] = v\n",
    "                cfg[\"blocks\"][i] = block_new\n",
    "            # 递归处理 subCFG\n",
    "            if \"subCFG\" in block:\n",
    "                get_label_according_to_lines_recursively(block[\"subCFG\"], code_lines)\n",
    "    \n",
    "    return cfg\n",
    "\n",
    "def get_cfg_prompt(code_lines, language, ast):\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "# Control Flow Graph Generation Protocol  \n",
    "**Objective**: Convert source code into standardized CFG JSON format according to the Abstract Syntax Tree (AST)\n",
    "\n",
    "## Input Requirements  \n",
    "Submit code and language using this pattern:  \n",
    "```\n",
    "[CODE]  \n",
    "{code_lines}\n",
    "[/CODE]  \n",
    "\n",
    "[AST]  \n",
    "{ast}\n",
    "[/AST]  \n",
    "\n",
    "[LANGUAGE]  \n",
    "{language}  \n",
    "[/LANGUAGE]  \n",
    "```\n",
    "\n",
    "## Output Specifications  \n",
    "### Required JSON Structure  \n",
    "{{\n",
    "  \"name\": \"function_name\",  \n",
    "  \"entryBlock\": {{  \n",
    "    \"id\": \"B0\",  \n",
    "    \"start_line\": 1,\n",
    "    \"end_line\": 1,\n",
    "    \"type\": \"branch|loop|normal|error\"  \n",
    "  }},  \n",
    "  \"blocks\": [  \n",
    "    {{  \n",
    "      \"id\": \"B1\",  \n",
    "      \"start_line\": 1,\n",
    "      \"end_line\": 1,\n",
    "      \"type\": \"branch\",  \n",
    "      \"subCFG\": [{{ /* Nested structure */ }}]  \n",
    "    }}  \n",
    "  ],  \n",
    "  \"edges\": [  \n",
    "    {{  \n",
    "      \"sourceId\": \"B0\",  \n",
    "      \"targetId\": \"B1\", \n",
    "      \"label\": \"edge_description\",  \n",
    "      \"isError\": false  \n",
    "    }}  \n",
    "  ]  \n",
    "}}  \n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [05:00<2:18:01, 42.91s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'start_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m)):\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mprocess_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# process_code(0)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# with tqdm(total=20) as pbar:\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#     with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#             pbar.update(1)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#         executor.map(process_and_update, range(20))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mprocess_code\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     25\u001b[0m prompt \u001b[38;5;241m=\u001b[39m get_ast_prompt(tokens, language)\n\u001b[1;32m     26\u001b[0m answer \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(get_llm_answers(prompt, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-r1:70b\u001b[39m\u001b[38;5;124m\"\u001b[39m, require_json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m---> 27\u001b[0m ast \u001b[38;5;241m=\u001b[39m \u001b[43mget_label_according_to_token_recursively\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m ast_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ast_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ast_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[1], line 144\u001b[0m, in \u001b[0;36mget_label_according_to_token_recursively\u001b[0;34m(ast, code, tokenize_code)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_label_according_to_token_recursively\u001b[39m(ast, code, tokenize_code):\n\u001b[0;32m--> 144\u001b[0m     start_token \u001b[38;5;241m=\u001b[39m \u001b[43mast\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    145\u001b[0m     end_token \u001b[38;5;241m=\u001b[39m ast[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_token\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# 从tokenize_code中提取对应的代码片段\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start_token'"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "import os\n",
    "import json\n",
    "\n",
    "language = \"cangjie\"\n",
    "source_code_dir = \"../dataset/100rows/cangjie\"\n",
    "ast_dir = \"../dataset/100rows/cangjie/ast\"\n",
    "cfg_dir = \"../dataset/100rows/cangjie/cfg\"\n",
    "\n",
    "os.makedirs(ast_dir, exist_ok=True)\n",
    "os.makedirs(cfg_dir, exist_ok=True)\n",
    "\n",
    "def process_code(index):\n",
    "    code_path = os.path.join(source_code_dir, f\"{index}.cj\")\n",
    "    if not os.path.exists(code_path):\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(os.path.join(ast_dir, f\"{index}.json\")) and os.path.exists(os.path.join(cfg_dir, f\"{index}.json\")):\n",
    "        return\n",
    "    \n",
    "    code = open(code_path, \"r\").read()\n",
    "    \n",
    "    tokenize_code = tokenize_code_with_lines(code)\n",
    "    tokens = {i: tokenize_code[i][2] for i in range(len(tokenize_code))}\n",
    "    prompt = get_ast_prompt(tokens, language)\n",
    "    answer = json.loads(get_llm_answers(prompt, model_name=\"deepseek-r1:70b\", require_json=True))\n",
    "    ast = get_label_according_to_token_recursively(answer, code, tokenize_code)\n",
    "    \n",
    "    ast_path = os.path.join(ast_dir, f\"{index}.json\")\n",
    "\n",
    "    with open(ast_path, \"w\") as f:\n",
    "        json.dump(ast, f, indent=4)\n",
    "        \n",
    "    code_lines = \"\\n\".join([f\"{i+1} {line}\" for i, line in enumerate(code.splitlines())])\n",
    "    prompt = get_cfg_prompt(code_lines, language, ast)\n",
    "    # print(prompt)\n",
    "\n",
    "    answer = get_llm_answers(prompt, model_name=\"deepseek-r1:70b\", require_json=True)\n",
    "    cfg = json.loads(answer)\n",
    "    cfg = get_label_according_to_lines_recursively(cfg, code.splitlines())\n",
    "    with open(os.path.join(cfg_dir, f\"{index}.json\"), \"w\") as f:\n",
    "        json.dump(cfg, f, indent=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from tqdm import tqdm\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    for i in tqdm(range(200)):\n",
    "        process_code(i)\n",
    "    # process_code(0)\n",
    "    # with tqdm(total=20) as pbar:\n",
    "    #     with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "    #         def process_and_update(i):\n",
    "    #             process_code(i)\n",
    "    #             pbar.update(1)\n",
    "    #         executor.map(process_and_update, range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source_code_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m code_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43msource_code_dir\u001b[49m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7.cj\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(code_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      4\u001b[0m tokenize_code \u001b[38;5;241m=\u001b[39m tokenize_code_with_lines(code)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_code_dir' is not defined"
     ]
    }
   ],
   "source": [
    "code_path = os.path.join(source_code_dir, f\"7.cj\")\n",
    "code = open(code_path, \"r\").read()\n",
    "\n",
    "tokenize_code = tokenize_code_with_lines(code)\n",
    "tokens = {i: tokenize_code[i][2] for i in range(len(tokenize_code))}\n",
    "prompt = get_ast_prompt(tokens, language)\n",
    "answer = json.loads(get_llm_answers(prompt, model_name=\"deepseek-r1:70b\"))\n",
    "ast = get_label_according_to_token_recursively(answer, code, tokenize_code)\n",
    "\n",
    "ast_path = os.path.join(ast_dir, f\"{i}.json\")\n",
    "with open(ast_path, \"w\") as f:\n",
    "    json.dump(ast, f, indent=4)\n",
    "    \n",
    "code_lines = \"\\n\".join([f\"{i+1} {line}\" for i, line in enumerate(code.splitlines())])\n",
    "prompt = get_cfg_prompt(code_lines, language, ast)\n",
    "# print(prompt)\n",
    "answer = get_llm_answers(prompt, model_name=\"deepseek-r1:70b\")\n",
    "cfg = json.loads(answer)\n",
    "cfg = get_label_according_to_lines_recursively(cfg, code.splitlines())\n",
    "with open(os.path.join(cfg_dir, f\"{i}.json\"), \"w\") as f:\n",
    "    json.dump(cfg, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
