{
  "name": "example_script",
  "type": "CFG",
  "start_line": 1,
  "end_line": 264,
  "functions": [],
  "classes": [
    {
      "name": "SamplingMethod",
      "type": "class",
      "start_line": 10,
      "end_line": 18,
      "functions": [],
      "classes": [],
      "simplified_code": "class SamplingMethod(str, Enum):\n    RANDOM = \"random\"\n    SYSTEMATIC = \"systematic\"\n    TOP = \"top\"\n    BOTTOM = \"bottom\"\n    STRATIFIED = \"stratified\"\n    WEIGHTED = \"weighted\"\n    RESERVOIR = \"reservoir\"\n    CLUSTER = \"cluster\"",
      "blocks": [
        {
          "id": 1,
          "label": "class SamplingMethod(str, Enum):\nRANDOM = \"random\"\nSYSTEMATIC = \"systematic\"\nTOP = \"top\"\nBOTTOM = \"bottom\"\nSTRATIFIED = \"stratified\"\nWEIGHTED = \"weighted\"\nRESERVOIR = \"reservoir\"\nCLUSTER = \"cluster\"",
          "successors": []
        }
      ]
    },
    {
      "name": "DataSamplingBlock",
      "type": "class",
      "start_line": 21,
      "end_line": 264,
      "functions": [
        {
          "name": "__init__",
          "type": "function",
          "start_line": 65,
          "end_line": 94,
          "functions": [],
          "classes": [],
          "simplified_code": "    def __init__(self):\n        super().__init__(\n            id=\"4a448883-71fa-49cf-91cf-70d793bd7d87\",\n            description=\"This block samples data from a given dataset using various sampling methods.\",\n            categories={BlockCategory.LOGIC},\n            input_schema=DataSamplingBlock.Input,\n            output_schema=DataSamplingBlock.Output,\n            test_input={\n                \"data\": [\n                    {\"id\": i, \"value\": chr(97 + i), \"group\": i % 3} for i in range(10)\n                ],\n                \"sample_size\": 3,\n                \"sampling_method\": SamplingMethod.STRATIFIED,\n                \"accumulate\": False,\n                \"random_seed\": 42,\n                \"stratify_key\": \"group\",\n            },\n            test_output=[\n                (\n                    \"sampled_data\",\n                    [\n                        {\"id\": 0, \"value\": \"a\", \"group\": 0},\n                        {\"id\": 1, \"value\": \"b\", \"group\": 1},\n                        {\"id\": 8, \"value\": \"i\", \"group\": 2},\n                    ],\n                ),\n                (\"sample_indices\", [0, 1, 8]),\n            ],\n        )\n        self.accumulated_data = []",
          "blocks": [
            {
              "id": 1,
              "label": "def __init__(self):\nsuper().__init__(...)\nself.accumulated_data = []",
              "successors": []
            }
          ]
        },
        {
          "name": "run",
          "type": "function",
          "start_line": 96,
          "end_line": 264,
          "functions": [],
          "classes": [],
          "simplified_code": "    def run(self, input_data: Input, **kwargs) -> BlockOutput:\n        if input_data.accumulate:\n            if isinstance(input_data.data, dict):\n                self.accumulated_data.append(input_data.data)\n            elif isinstance(input_data.data, list):\n                self.accumulated_data.extend(input_data.data)\n            else:\n                raise ValueError(f\"Unsupported data type: {type(input_data.data)}\")\n\n            # If we don't have enough data yet, return without sampling\n            if len(self.accumulated_data) < input_data.sample_size:\n                return\n\n            data_to_sample = self.accumulated_data\n        else:\n            # If not accumulating, use the input data directly\n            data_to_sample = (\n                input_data.data\n                if isinstance(input_data.data, list)\n                else [input_data.data]\n            )\n\n        if input_data.random_seed is not None:\n            random.seed(input_data.random_seed)\n\n        data_size = len(data_to_sample)\n\n        if input_data.sample_size > data_size:\n            raise ValueError(\n                f\"Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).\"\n            )\n\n        indices = []\n\n        if input_data.sampling_method == SamplingMethod.RANDOM:\n            indices = random.sample(range(data_size), input_data.sample_size)\n        elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:\n            step = data_size // input_data.sample_size\n            start = random.randint(0, step - 1)\n            indices = list(range(start, data_size, step))[: input_data.sample_size]\n        elif input_data.sampling_method == SamplingMethod.TOP:\n            indices = list(range(input_data.sample_size))\n        elif input_data.sampling_method == SamplingMethod.BOTTOM:\n            indices = list(range(data_size - input_data.sample_size, data_size))\n        elif input_data.sampling_method == SamplingMethod.STRATIFIED:\n            if not input_data.stratify_key:\n                raise ValueError(\n                    \"Stratify key must be provided for stratified sampling.\"\n                )\n            strata = defaultdict(list)\n            for i, item in enumerate(data_to_sample):\n                if isinstance(item, dict):\n                    strata_value = item.get(input_data.stratify_key)\n                elif hasattr(item, input_data.stratify_key):\n                    strata_value = getattr(item, input_data.stratify_key)\n                else:\n                    raise ValueError(\n                        f\"Stratify key '{input_data.stratify_key}' not found in item {item}\"\n                    )\n\n                if strata_value is None:\n                    raise ValueError(\n                        f\"Stratify value for key '{input_data.stratify_key}' is None\"\n                    )\n\n                strata[str(strata_value)].append(i)\n\n            # Calculate the number of samples to take from each stratum\n            stratum_sizes = {\n                k: max(1, int(len(v) / data_size * input_data.sample_size))\n                for k, v in strata.items()\n            }\n\n            # Adjust sizes to ensure we get exactly sample_size samples\n            while sum(stratum_sizes.values()) != input_data.sample_size:\n                if sum(stratum_sizes.values()) < input_data.sample_size:\n                    stratum_sizes[\n                        max(stratum_sizes, key=lambda k: stratum_sizes[k])\n                    ] += 1\n                else:\n                    stratum_sizes[\n                        max(stratum_sizes, key=lambda k: stratum_sizes[k])\n                    ] -= 1\n\n            for stratum, size in stratum_sizes.items():\n                indices.extend(random.sample(strata[stratum], size))\n        elif input_data.sampling_method == SamplingMethod.WEIGHTED:\n            if not input_data.weight_key:\n                raise ValueError(\"Weight key must be provided for weighted sampling.\")\n            weights = []\n            for item in data_to_sample:\n                if isinstance(item, dict):\n                    weight = item.get(input_data.weight_key)\n                elif hasattr(item, input_data.weight_key):\n                    weight = getattr(item, input_data.weight_key)\n                else:\n                    raise ValueError(\n                        f\"Weight key '{input_data.weight_key}' not found in item {item}\"\n                    )\n\n                if weight is None:\n                    raise ValueError(\n                        f\"Weight value for key '{input_data.weight_key}' is None\"\n                    )\n                try:\n                    weights.append(float(weight))\n                except ValueError:\n                    raise ValueError(\n                        f\"Weight value '{weight}' cannot be converted to a number\"\n                    )\n\n            if not weights:\n                raise ValueError(\n                    f\"No valid weights found using key '{input_data.weight_key}'\"\n                )\n\n            indices = random.choices(\n                range(data_size), weights=weights, k=input_data.sample_size\n            )\n        elif input_data.sampling_method == SamplingMethod.RESERVOIR:\n            indices = list(range(input_data.sample_size))\n            for i in range(input_data.sample_size, data_size):\n                j = random.randint(0, i)\n                if j < input_data.sample_size:\n                    indices[j] = i\n        elif input_data.sampling_method == SamplingMethod.CLUSTER:\n            if not input_data.cluster_key:\n                raise ValueError(\"Cluster key must be provided for cluster sampling.\")\n            clusters = defaultdict(list)\n            for i, item in enumerate(data_to_sample):\n                if isinstance(item, dict):\n                    cluster_value = item.get(input_data.cluster_key)\n                elif hasattr(item, input_data.cluster_key):\n                    cluster_value = getattr(item, input_data.cluster_key)\n                else:\n                    raise TypeError(\n                        f\"Item {item} does not have the cluster key '{input_data.cluster_key}'\"\n                    )\n\n                clusters[str(cluster_value)].append(i)\n\n            # Randomly select clusters until we have enough samples\n            selected_clusters = []\n            while (\n                sum(len(clusters[c]) for c in selected_clusters)\n                < input_data.sample_size\n            ):\n                available_clusters = [c for c in clusters if c not in selected_clusters]\n                if not available_clusters:\n                    break\n                selected_clusters.append(random.choice(available_clusters))\n\n            for cluster in selected_clusters:\n                indices.extend(clusters[cluster])\n\n            # If we have more samples than needed, randomly remove some\n            if len(indices) > input_data.sample_size:\n                indices = random.sample(indices, input_data.sample_size)\n        else:\n            raise ValueError(f\"Unknown sampling method: {input_data.sampling_method}\")\n\n        sampled_data = [data_to_sample[i] for i in indices]\n\n        # Clear accumulated data after sampling if accumulation is enabled\n        if input_data.accumulate:\n            self.accumulated_data = []\n\n        yield \"sampled_data\", sampled_data\n        yield \"sample_indices\", indices",
          "blocks": [
            {
              "id": 1,
              "label": "if input_data.accumulate:\nif isinstance(input_data.data, dict):\nself.accumulated_data.append(input_data.data)",
              "successors": [
                7
              ]
            },
            {
              "id": 4,
              "label": "elif isinstance(input_data.data, list):\nself.accumulated_data.extend(input_data.data)",
              "successors": [
                7
              ]
            },
            {
              "id": 6,
              "label": "raise ValueError(f\"Unsupported data type: {type(input_data.data)}\")",
              "successors": [
                7
              ]
            },
            {
              "id": 7,
              "label": "if len(self.accumulated_data) < input_data.sample_size:\nreturn",
              "successors": []
            },
            {
              "id": 9,
              "label": "data_to_sample = self.accumulated_data",
              "successors": [
                15
              ]
            },
            {
              "id": 14,
              "label": "data_to_sample = (input_data.data if isinstance(input_data.data, list) else [input_data.data])",
              "successors": [
                15
              ]
            },
            {
              "id": 15,
              "label": "if input_data.random_seed is not None:\nrandom.seed(input_data.random_seed)",
              "successors": [
                17
              ]
            },
            {
              "id": 17,
              "label": "data_size = len(data_to_sample)\nif input_data.sample_size > data_size:\nraise ValueError(f\"Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).\")",
              "successors": [
                20
              ]
            },
            {
              "id": 20,
              "label": "indices = []\nif input_data.sampling_method == SamplingMethod.RANDOM:\nindices = random.sample(range(data_size), input_data.sample_size)",
              "successors": [
                66
              ]
            },
            {
              "id": 23,
              "label": "elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:\nstep = data_size // input_data.sample_size",
              "successors": [
                25
              ]
            },
            {
              "id": 25,
              "label": "start = random.randint(0, step - 1)\nindices = list(range(start, data_size, step))[: input_data.sample_size]",
              "successors": [
                66
              ]
            },
            {
              "id": 66,
              "label": "elif input_data.sampling_method == SamplingMethod.CLUSTER:\nif not input_data.cluster_key: raise ValueError(\"Cluster key must be provided for cluster sampling.\")\nclusters = defaultdict(list)\nfor i, item in enumerate(data_to_sample):\nif isinstance(item, dict): cluster_value = item.get(input_data.cluster_key)",
              "successors": [
                71
              ]
            },
            {
              "id": 71,
              "label": "elif hasattr(item, input_data.cluster_key): cluster_value = getattr(item, input_data.cluster_key)\nelse: raise TypeError(f\"Item {item} does not have the cluster key '{input_data.cluster_key}')",
              "successors": [
                75
              ]
            },
            {
              "id": 73,
              "label": "clusters[str(cluster_value)].append(i)\nselected_clusters = []",
              "successors": [
                75
              ]
            },
            {
              "id": 75,
              "label": "while (sum(len(clusters[c]) for c in selected_clusters) < input_data.sample_size):\navailable_clusters = [c for c in clusters if c not in selected_clusters]",
              "successors": [
                78
              ]
            },
            {
              "id": 77,
              "label": "if not available_clusters: break",
              "successors": [
                78
              ]
            },
            {
              "id": 78,
              "label": "selected_clusters.append(random.choice(available_clusters))\nfor cluster in selected_clusters: indices.extend(clusters[cluster])\nif len(indices) > input_data.sample_size: indices = random.sample(indices, input_data.sample_size)\nelse: raise ValueError(f\"Unknown sampling method: {input_data.sampling_method}\")\nsampled_data = [data_to_sample[i] for i in indices]\nif input_data.accumulate: self.accumulated_data = []\nyield \"sampled_data\", sampled_data\nyield \"sample_indices\", indices",
              "successors": []
            }
          ]
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "class",
          "start_line": 22,
          "end_line": 55,
          "functions": [],
          "classes": [],
          "simplified_code": "    class Input(BlockSchema):\n        data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(\n            description=\"The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists.\",\n            placeholder=\"{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]\",\n        )\n        sample_size: int = SchemaField(\n            description=\"The number of samples to take from the dataset.\",\n            placeholder=\"10\",\n            default=10,\n        )\n        sampling_method: SamplingMethod = SchemaField(\n            description=\"The method to use for sampling.\",\n            default=SamplingMethod.RANDOM,\n        )\n        accumulate: bool = SchemaField(\n            description=\"Whether to accumulate data before sampling.\",\n            default=False,\n        )\n        random_seed: Optional[int] = SchemaField(\n            description=\"Seed for random number generator (optional).\",\n            default=None,\n        )\n        stratify_key: Optional[str] = SchemaField(\n            description=\"Key to use for stratified sampling (required for stratified sampling).\",\n            default=None,\n        )\n        weight_key: Optional[str] = SchemaField(\n            description=\"Key to use for weighted sampling (required for weighted sampling).\",\n            default=None,\n        )\n        cluster_key: Optional[str] = SchemaField(\n            description=\"Key to use for cluster sampling (required for cluster sampling).\",\n            default=None,\n        )",
          "blocks": [
            {
              "id": 1,
              "label": "class Input(BlockSchema):\ndata = SchemaField(...)\nsample_size = SchemaField(...)\nsampling_method = SchemaField(...)\naccumulate = SchemaField(...)\nrandom_seed = SchemaField(...)\nstratify_key = SchemaField(...)\nweight_key = SchemaField(...)\ncluster_key = SchemaField(...)",
              "successors": []
            }
          ]
        },
        {
          "name": "Output",
          "type": "class",
          "start_line": 57,
          "end_line": 63,
          "functions": [],
          "classes": [],
          "simplified_code": "    class Output(BlockSchema):\n        sampled_data: List[Union[dict, List[Any]]] = SchemaField(\n            description=\"The sampled subset of the input data.\"\n        )\n        sample_indices: List[int] = SchemaField(\n            description=\"The indices of the sampled data in the original dataset.\"\n        )",
          "blocks": [
            {
              "id": 1,
              "label": "class Output(BlockSchema):\nsampled_data: List[Union[dict, List[Any]]] = SchemaField(description=\"The sampled subset of the input data.\")",
              "successors": [
                3
              ]
            },
            {
              "id": 3,
              "label": "sample_indices: List[int] = SchemaField(description=\"The indices of the sampled data in the original dataset.\")",
              "successors": []
            }
          ]
        }
      ],
      "simplified_code": "class DataSamplingBlock(Block):\n        )\n\n        )\n\n        self.accumulated_data = []\n\n        yield \"sample_indices\", indices",
      "blocks": [
        {
          "id": 1,
          "label": "class DataSamplingBlock(Block):\ndef accumulate_data(self, data):",
          "successors": [
            3
          ]
        },
        {
          "id": 3,
          "label": "for datum in data:\nself.accumulated_data.append(datum)",
          "successors": [
            3
          ]
        },
        {
          "id": 5,
          "label": "return self.accumulated_data",
          "successors": []
        }
      ]
    }
  ],
  "simplified_code": "import random\nfrom collections import defaultdict\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema\nfrom backend.data.model import SchemaField\n\n\n    CLUSTER = \"cluster\"\n\n\n        yield \"sample_indices\", indices",
  "blocks": [
    {
      "id": 1,
      "label": "import random\nfrom collections import defaultdict\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union\nfrom backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema\nfrom backend.data.model import SchemaField\nCLUSTER = \"cluster\"\nyield \"sample_indices\", indices",
      "successors": []
    }
  ]
}