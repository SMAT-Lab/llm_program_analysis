{
  "name": "186.py",
  "type": "CFG",
  "blocks": [
    {
      "id": 1,
      "label": "#1\nimport ast\nimport logging\nfrom enum import Enum, EnumMeta\nfrom json import JSONDecodeError\nfrom types import MappingProxyType\nfrom typing import TYPE_CHECKING, Any, List, Literal, NamedTuple\nfrom pydantic import SecretStr\nfrom backend.integrations.providers import ProviderName\nif TYPE_CHECKING:",
      "successors": [
        {
          "id": 2,
          "label": "#2\nfrom enum import _EnumMemberT",
          "successors": [
            {
              "id": 3,
              "label": "#3\nimport anthropic\nimport ollama\nimport openai\nfrom groq import Groq\nfrom backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema\nfrom backend.data.model import APIKeyCredentials, CredentialsField, CredentialsMetaInput, SchemaField\nfrom backend.util import json\nfrom backend.util.settings import BehaveAs, Settings\nlogger = logging.getLogger(__name__)\nLLMProviderName = Literal[ProviderName.ANTHROPIC, ProviderName.GROQ,\n    ProviderName.OLLAMA, ProviderName.OPENAI, ProviderName.OPEN_ROUTER]\nAICredentials = CredentialsMetaInput[LLMProviderName, Literal['api_key']]\nTEST_CREDENTIALS = APIKeyCredentials(id=\n    'ed55ac19-356e-4243-a6cb-bc599e9b716f', provider='openai', api_key=\n    SecretStr('mock-openai-api-key'), title='Mock OpenAI API key',\n    expires_at=None)\nTEST_CREDENTIALS_INPUT = {'provider': TEST_CREDENTIALS.provider, 'id':\n    TEST_CREDENTIALS.id, 'type': TEST_CREDENTIALS.type, 'title':\n    TEST_CREDENTIALS.title}\nMODEL_METADATA = {LlmModel.O1_PREVIEW: ModelMetadata('openai', 32000),\n    LlmModel.O1_MINI: ModelMetadata('openai', 62000), LlmModel.GPT4O_MINI:\n    ModelMetadata('openai', 128000), LlmModel.GPT4O: ModelMetadata('openai',\n    128000), LlmModel.GPT4_TURBO: ModelMetadata('openai', 128000), LlmModel\n    .GPT3_5_TURBO: ModelMetadata('openai', 16385), LlmModel.\n    CLAUDE_3_5_SONNET: ModelMetadata('anthropic', 200000), LlmModel.\n    CLAUDE_3_HAIKU: ModelMetadata('anthropic', 200000), LlmModel.LLAMA3_8B:\n    ModelMetadata('groq', 8192), LlmModel.LLAMA3_70B: ModelMetadata('groq',\n    8192), LlmModel.MIXTRAL_8X7B: ModelMetadata('groq', 32768), LlmModel.\n    GEMMA_7B: ModelMetadata('groq', 8192), LlmModel.GEMMA2_9B:\n    ModelMetadata('groq', 8192), LlmModel.LLAMA3_1_405B: ModelMetadata(\n    'groq', 8192), LlmModel.LLAMA3_1_70B: ModelMetadata('groq', 131072),\n    LlmModel.LLAMA3_1_8B: ModelMetadata('groq', 131072), LlmModel.\n    OLLAMA_LLAMA3_8B: ModelMetadata('ollama', 8192), LlmModel.\n    OLLAMA_LLAMA3_405B: ModelMetadata('ollama', 8192), LlmModel.\n    OLLAMA_DOLPHIN: ModelMetadata('ollama', 32768), LlmModel.\n    GEMINI_FLASH_1_5_8B: ModelMetadata('open_router', 8192), LlmModel.\n    GROK_BETA: ModelMetadata('open_router', 8192), LlmModel.MISTRAL_NEMO:\n    ModelMetadata('open_router', 4000), LlmModel.COHERE_COMMAND_R_08_2024:\n    ModelMetadata('open_router', 4000), LlmModel.\n    COHERE_COMMAND_R_PLUS_08_2024: ModelMetadata('open_router', 4000),\n    LlmModel.EVA_QWEN_2_5_32B: ModelMetadata('open_router', 4000), LlmModel\n    .DEEPSEEK_CHAT: ModelMetadata('open_router', 8192), LlmModel.\n    PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: ModelMetadata(\n    'open_router', 8192), LlmModel.QWEN_QWQ_32B_PREVIEW: ModelMetadata(\n    'open_router', 4000), LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B:\n    ModelMetadata('open_router', 4000), LlmModel.\n    NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: ModelMetadata('open_router', 4000),\n    LlmModel.AMAZON_NOVA_LITE_V1: ModelMetadata('open_router', 4000),\n    LlmModel.AMAZON_NOVA_MICRO_V1: ModelMetadata('open_router', 4000),\n    LlmModel.AMAZON_NOVA_PRO_V1: ModelMetadata('open_router', 4000),\n    LlmModel.MICROSOFT_WIZARDLM_2_8X22B: ModelMetadata('open_router', 4000),\n    LlmModel.GRYPHE_MYTHOMAX_L2_13B: ModelMetadata('open_router', 4000)}",
              "successors": [
                {
                  "id": 37,
                  "label": "#37\nfor model in LlmModel:",
                  "successors": [
                    {
                      "id": 38,
                      "label": "#38\nif model not in MODEL_METADATA:",
                      "successors": [
                        {
                          "id": 40,
                          "label": "#40\nraise ValueError(f'Missing MODEL_METADATA metadata for model: {model}')",
                          "successors": []
                        }
                      ]
                    },
                    {
                      "id": 39,
                      "label": "#39",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    }
  ],
  "functions": [
    {
      "name": "AICredentialsField",
      "type": "CFG",
      "blocks": [
        {
          "id": 5,
          "label": "#5\nreturn CredentialsField(description='API key for the LLM provider.',\n    discriminator='model', discriminator_mapping={model.value: model.\n    metadata.provider for model in LlmModel})",
          "successors": []
        }
      ],
      "functions": [],
      "classes": []
    }
  ],
  "classes": [
    {
      "name": "ModelMetadata",
      "type": "CFG",
      "blocks": [
        {
          "id": 9,
          "label": "#9\nprovider: str\ncontext_window: int",
          "successors": []
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "LlmModelMeta",
      "type": "CFG",
      "blocks": [
        {
          "id": 12,
          "label": "#12",
          "successors": []
        }
      ],
      "functions": [
        {
          "name": "__members__",
          "type": "CFG",
          "blocks": [
            {
              "id": 14,
              "label": "#14\nif Settings().config.behave_as == BehaveAs.LOCAL:",
              "successors": [
                {
                  "id": 15,
                  "label": "#15\nmembers = super().__members__\nreturn members",
                  "successors": []
                },
                {
                  "id": 17,
                  "label": "#17\nremoved_providers = ['ollama']\nexisting_members = super().__members__\nmembers = {name: member for name, member in existing_members.items() if \n    LlmModel[name].provider not in removed_providers}\nreturn MappingProxyType(members)",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": []
    },
    {
      "name": "LlmModel",
      "type": "CFG",
      "blocks": [
        {
          "id": 23,
          "label": "#23\nO1_PREVIEW = 'o1-preview'\nO1_MINI = 'o1-mini'\nGPT4O_MINI = 'gpt-4o-mini'\nGPT4O = 'gpt-4o'\nGPT4_TURBO = 'gpt-4-turbo'\nGPT3_5_TURBO = 'gpt-3.5-turbo'\nCLAUDE_3_5_SONNET = 'claude-3-5-sonnet-latest'\nCLAUDE_3_HAIKU = 'claude-3-haiku-20240307'\nLLAMA3_8B = 'llama3-8b-8192'\nLLAMA3_70B = 'llama3-70b-8192'\nMIXTRAL_8X7B = 'mixtral-8x7b-32768'\nGEMMA_7B = 'gemma-7b-it'\nGEMMA2_9B = 'gemma2-9b-it'\nLLAMA3_1_405B = 'llama-3.1-405b-reasoning'\nLLAMA3_1_70B = 'llama-3.1-70b-versatile'\nLLAMA3_1_8B = 'llama-3.1-8b-instant'\nOLLAMA_LLAMA3_8B = 'llama3'\nOLLAMA_LLAMA3_405B = 'llama3.1:405b'\nOLLAMA_DOLPHIN = 'dolphin-mistral:latest'\nGEMINI_FLASH_1_5_8B = 'google/gemini-flash-1.5'\nGROK_BETA = 'x-ai/grok-beta'\nMISTRAL_NEMO = 'mistralai/mistral-nemo'\nCOHERE_COMMAND_R_08_2024 = 'cohere/command-r-08-2024'\nCOHERE_COMMAND_R_PLUS_08_2024 = 'cohere/command-r-plus-08-2024'\nEVA_QWEN_2_5_32B = 'eva-unit-01/eva-qwen-2.5-32b'\nDEEPSEEK_CHAT = 'deepseek/deepseek-chat'\nPERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE = (\n    'perplexity/llama-3.1-sonar-large-128k-online')\nQWEN_QWQ_32B_PREVIEW = 'qwen/qwq-32b-preview'\nNOUSRESEARCH_HERMES_3_LLAMA_3_1_405B = 'nousresearch/hermes-3-llama-3.1-405b'\nNOUSRESEARCH_HERMES_3_LLAMA_3_1_70B = 'nousresearch/hermes-3-llama-3.1-70b'\nAMAZON_NOVA_LITE_V1 = 'amazon/nova-lite-v1'\nAMAZON_NOVA_MICRO_V1 = 'amazon/nova-micro-v1'\nAMAZON_NOVA_PRO_V1 = 'amazon/nova-pro-v1'\nMICROSOFT_WIZARDLM_2_8X22B = 'microsoft/wizardlm-2-8x22b'\nGRYPHE_MYTHOMAX_L2_13B = 'gryphe/mythomax-l2-13b'",
          "successors": []
        }
      ],
      "functions": [
        {
          "name": "metadata",
          "type": "CFG",
          "blocks": [
            {
              "id": 25,
              "label": "#25\nreturn MODEL_METADATA[self]",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "provider",
          "type": "CFG",
          "blocks": [
            {
              "id": 29,
              "label": "#29\nreturn self.metadata.provider",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "context_window",
          "type": "CFG",
          "blocks": [
            {
              "id": 33,
              "label": "#33\nreturn self.metadata.context_window",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": []
    },
    {
      "name": "MessageRole",
      "type": "CFG",
      "blocks": [
        {
          "id": 44,
          "label": "#44\nSYSTEM = 'system'\nUSER = 'user'\nASSISTANT = 'assistant'",
          "successors": []
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "Message",
      "type": "CFG",
      "blocks": [
        {
          "id": 47,
          "label": "#47\nrole: MessageRole\ncontent: str",
          "successors": []
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "AIStructuredResponseGeneratorBlock",
      "type": "CFG",
      "blocks": [
        {
          "id": 50,
          "label": "#50",
          "successors": []
        }
      ],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 58,
              "label": "#58\nsuper().__init__(id='ed55ac19-356e-4243-a6cb-bc599e9b716f', description=\n    'Call a Large Language Model (LLM) to generate formatted object based on the given prompt.'\n    , categories={BlockCategory.AI}, input_schema=\n    AIStructuredResponseGeneratorBlock.Input, output_schema=\n    AIStructuredResponseGeneratorBlock.Output, test_input={'model':\n    LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT,\n    'expected_format': {'key1': 'value1', 'key2': 'value2'}, 'prompt':\n    'User prompt'}, test_credentials=TEST_CREDENTIALS, test_output=(\n    'response', {'key1': 'key1Value', 'key2': 'key2Value'}), test_mock={\n    'llm_call': lambda *args, **kwargs: (json.dumps({'key1': 'key1Value',\n    'key2': 'key2Value'}), 0, 0)})",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 61,
              "label": "#61\n\"\"\"\n        Args:\n            api_key: API key for the LLM provider.\n            llm_model: The LLM model to use.\n            prompt: The prompt to send to the LLM.\n            json_format: Whether the response should be in JSON format.\n            max_tokens: The maximum number of tokens to generate in the chat completion.\n            ollama_host: The host for ollama to use\n\n        Returns:\n            The response from the LLM.\n            The number of tokens used in the prompt.\n            The number of tokens used in the completion.\n        \"\"\"\nprovider = llm_model.metadata.provider\nif provider == 'openai':",
              "successors": [
                {
                  "id": 62,
                  "label": "#62\noai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())\nresponse_format = None\nif llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:",
                  "successors": [
                    {
                      "id": 104,
                      "label": "#104\nsys_messages = [p['content'] for p in prompt if p['role'] == 'system']\nusr_messages = [p['content'] for p in prompt if p['role'] != 'system']\nprompt = [{'role': 'user', 'content': '\\n'.join(sys_messages)}, {'role':\n    'user', 'content': '\\n'.join(usr_messages)}]",
                      "successors": [
                        {
                          "id": 105,
                          "label": "#105\nresponse = oai_client.chat.completions.create(model=llm_model.value,\n    messages=prompt, response_format=response_format, max_completion_tokens\n    =max_tokens)\nreturn response.choices[0\n    ].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0",
                          "successors": []
                        }
                      ]
                    },
                    {
                      "id": 106,
                      "label": "#106\nif json_format:",
                      "successors": [
                        {
                          "id": 107,
                          "label": "#107\nresponse_format = {'type': 'json_object'}",
                          "successors": []
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": 64,
                  "label": "#64\nif provider == 'anthropic':",
                  "successors": [
                    {
                      "id": 65,
                      "label": "#65\nsystem_messages = [p['content'] for p in prompt if p['role'] == 'system']\nsysprompt = ' '.join(system_messages)\nmessages = []\nlast_role = None",
                      "successors": [
                        {
                          "id": 88,
                          "label": "#88\nfor p in prompt:",
                          "successors": [
                            {
                              "id": 89,
                              "label": "#89\nif p['role'] in ['user', 'assistant']:",
                              "successors": [
                                {
                                  "id": 91,
                                  "label": "#91\nif p['role'] != last_role:",
                                  "successors": [
                                    {
                                      "id": 93,
                                      "label": "#93\nmessages.append({'role': p['role'], 'content': p['content']})\nlast_role = p['role']",
                                      "successors": []
                                    },
                                    {
                                      "id": 95,
                                      "label": "#95\nmessages[-1]['content'] += '\\n' + p['content']",
                                      "successors": []
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "id": 90,
                              "label": "#90\nclient = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())\ntry:",
                              "successors": [
                                {
                                  "id": 96,
                                  "label": "#96\nresp = client.messages.create(model=llm_model.value, system=sysprompt,\n    messages=messages, max_tokens=max_tokens or 8192)\nif not resp.content:",
                                  "successors": [
                                    {
                                      "id": 99,
                                      "label": "#99\nraise ValueError('No content returned from Anthropic.')",
                                      "successors": []
                                    },
                                    {
                                      "id": 100,
                                      "label": "#100\nreturn resp.content[0].name if isinstance(resp.content[0], anthropic.types.\n    ToolUseBlock) else resp.content[0\n    ].text, resp.usage.input_tokens, resp.usage.output_tokens",
                                      "successors": []
                                    }
                                  ]
                                },
                                {
                                  "id": 97,
                                  "label": "#97\nerror_message = f'Anthropic API error: {str(e)}'\nlogger.error(error_message)\nraise ValueError(error_message)",
                                  "successors": []
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": 67,
                      "label": "#67\nif provider == 'groq':",
                      "successors": [
                        {
                          "id": 68,
                          "label": "#68\nclient = Groq(api_key=credentials.api_key.get_secret_value())\nresponse_format = {'type': 'json_object'} if json_format else None\nresponse = client.chat.completions.create(model=llm_model.value, messages=\n    prompt, response_format=response_format, max_tokens=max_tokens)\nreturn response.choices[0\n    ].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0",
                          "successors": []
                        },
                        {
                          "id": 70,
                          "label": "#70\nif provider == 'ollama':",
                          "successors": [
                            {
                              "id": 71,
                              "label": "#71\nclient = ollama.Client(host=ollama_host)\nsys_messages = [p['content'] for p in prompt if p['role'] == 'system']\nusr_messages = [p['content'] for p in prompt if p['role'] != 'system']\nresponse = client.generate(model=llm_model.value, prompt=\n    f\"\"\"{sys_messages}\n\n{usr_messages}\"\"\", stream=False)\nreturn response.get('response') or '', response.get('prompt_eval_count'\n    ) or 0, response.get('eval_count') or 0",
                              "successors": []
                            },
                            {
                              "id": 73,
                              "label": "#73\nif provider == 'open_router':",
                              "successors": [
                                {
                                  "id": 74,
                                  "label": "#74\nclient = openai.OpenAI(base_url='https://openrouter.ai/api/v1', api_key=\n    credentials.api_key.get_secret_value())\nresponse = client.chat.completions.create(extra_headers={'HTTP-Referer':\n    'https://agpt.co', 'X-Title': 'AutoGPT'}, model=llm_model.value,\n    messages=prompt, max_tokens=max_tokens)\nif not response.choices:",
                                  "successors": [
                                    {
                                      "id": 78,
                                      "label": "#78\nif response:",
                                      "successors": [
                                        {
                                          "id": 80,
                                          "label": "#80\nraise ValueError(f'OpenRouter error: {response}')",
                                          "successors": []
                                        },
                                        {
                                          "id": 82,
                                          "label": "#82\nraise ValueError('No response from OpenRouter.')",
                                          "successors": []
                                        }
                                      ]
                                    },
                                    {
                                      "id": 79,
                                      "label": "#79\nreturn response.choices[0\n    ].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0",
                                      "successors": []
                                    }
                                  ]
                                },
                                {
                                  "id": 76,
                                  "label": "#76\nraise ValueError(f'Unsupported LLM provider: {provider}')",
                                  "successors": []
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 112,
              "label": "#112\nlogger.debug(f'Calling LLM with input data: {input_data}')\nprompt = [p.model_dump() for p in input_data.conversation_history]\nvalues = input_data.prompt_values\nif values:",
              "successors": [
                {
                  "id": 117,
                  "label": "#117\ninput_data.prompt = input_data.prompt.format(**values)\ninput_data.sys_prompt = input_data.sys_prompt.format(**values)",
                  "successors": [
                    {
                      "id": 118,
                      "label": "#118\nif input_data.sys_prompt:",
                      "successors": [
                        {
                          "id": 119,
                          "label": "#119\nprompt.append({'role': 'system', 'content': input_data.sys_prompt})",
                          "successors": [
                            {
                              "id": 120,
                              "label": "#120\nif input_data.expected_format:",
                              "successors": [
                                {
                                  "id": 121,
                                  "label": "#121\nexpected_format = [f'\"{k}\": \"{v}\"' for k, v in input_data.expected_format.\n    items()]\nformat_prompt = ',\\n  '.join(expected_format)\nsys_prompt = trim_prompt(\n    f\"\"\"\n                  |Reply strictly only in the following JSON format:\n                  |{{\n                  |  {format_prompt}\n                  |}}\n                \"\"\"\n    )\nprompt.append({'role': 'system', 'content': sys_prompt})",
                                  "successors": [
                                    {
                                      "id": 122,
                                      "label": "#122\nif input_data.prompt:",
                                      "successors": [
                                        {
                                          "id": 123,
                                          "label": "#123\nprompt.append({'role': 'user', 'content': input_data.prompt})",
                                          "successors": [
                                            {
                                              "id": 124,
                                              "label": "#124\nlogger.info(f'LLM request: {prompt}')\nretry_prompt = ''\nllm_model = input_data.model",
                                              "successors": [
                                                {
                                                  "id": 139,
                                                  "label": "#139\nfor retry_count in range(input_data.retry):",
                                                  "successors": [
                                                    {
                                                      "id": 140,
                                                      "label": "#140\ntry:",
                                                      "successors": [
                                                        {
                                                          "id": 142,
                                                          "label": "#142\nresponse_text, input_token, output_token = self.llm_call(credentials=\n    credentials, llm_model=llm_model, prompt=prompt, json_format=bool(\n    input_data.expected_format), ollama_host=input_data.ollama_host,\n    max_tokens=input_data.max_tokens)\nself.merge_stats({'input_token_count': input_token, 'output_token_count':\n    output_token})\nlogger.info(f'LLM attempt-{retry_count} response: {response_text}')\nif input_data.expected_format:",
                                                          "successors": [
                                                            {
                                                              "id": 145,
                                                              "label": "#145\nparsed_dict, parsed_error = parse_response(response_text)\nif not parsed_error:",
                                                              "successors": [
                                                                {
                                                                  "id": 150,
                                                                  "label": "#150\nyield 'response', {k: (json.loads(v) if isinstance(v, str) and v.startswith\n    ('[') and v.endswith(']') else ', '.join(v) if isinstance(v, list) else\n    v) for k, v in parsed_dict.items()}",
                                                                  "successors": [
                                                                    {
                                                                      "id": 152,
                                                                      "label": "#152\nreturn",
                                                                      "successors": []
                                                                    }
                                                                  ]
                                                                },
                                                                {
                                                                  "id": 146,
                                                                  "label": "#146\nretry_prompt = trim_prompt(\n    f\"\"\"\n                  |This is your previous error response:\n                  |--\n                  |{response_text}\n                  |--\n                  |\n                  |And this is the error:\n                  |--\n                  |{parsed_error}\n                  |--\n                \"\"\"\n    )\nprompt.append({'role': 'user', 'content': retry_prompt})",
                                                                  "successors": []
                                                                }
                                                              ]
                                                            },
                                                            {
                                                              "id": 147,
                                                              "label": "#147\nyield 'response', {'response': response_text}",
                                                              "successors": [
                                                                {
                                                                  "id": 148,
                                                                  "label": "#148\nreturn",
                                                                  "successors": []
                                                                }
                                                              ]
                                                            }
                                                          ]
                                                        },
                                                        {
                                                          "id": 143,
                                                          "label": "#143\nlogger.exception(f'Error calling LLM: {e}')\nretry_prompt = f'Error calling LLM: {e}'",
                                                          "successors": []
                                                        }
                                                      ]
                                                    },
                                                    {
                                                      "id": 141,
                                                      "label": "#141\nraise RuntimeError(retry_prompt)",
                                                      "successors": []
                                                    }
                                                  ]
                                                }
                                              ]
                                            }
                                          ]
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [
            {
              "name": "trim_prompt",
              "type": "CFG",
              "blocks": [
                {
                  "id": 114,
                  "label": "#114\nlines = s.strip().split('\\n')\nreturn '\\n'.join([line.strip().lstrip('|') for line in lines])",
                  "successors": []
                }
              ],
              "functions": [],
              "classes": []
            },
            {
              "name": "parse_response",
              "type": "CFG",
              "blocks": [
                {
                  "id": 126,
                  "label": "#126\ntry:",
                  "successors": [
                    {
                      "id": 127,
                      "label": "#127\nparsed = json.loads(resp)\nif not isinstance(parsed, dict):",
                      "successors": [
                        {
                          "id": 130,
                          "label": "#130\nreturn {}, f'Expected a dictionary, but got {type(parsed)}'",
                          "successors": []
                        },
                        {
                          "id": 131,
                          "label": "#131\nmiss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\nif miss_keys:",
                          "successors": [
                            {
                              "id": 133,
                              "label": "#133\nreturn parsed, f'Missing keys: {miss_keys}'",
                              "successors": []
                            },
                            {
                              "id": 134,
                              "label": "#134\nreturn parsed, None",
                              "successors": []
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": 128,
                      "label": "#128\nreturn {}, f'JSON decode error: {e}'",
                      "successors": []
                    }
                  ]
                }
              ],
              "functions": [],
              "classes": []
            }
          ],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 52,
              "label": "#52\nprompt: str = SchemaField(description=\n    'The prompt to send to the language model.', placeholder=\nexpected_format: dict[str, str] = SchemaField(description=\n    'Expected format of the response. If provided, the response will be validated against this format. The keys should be the expected fields in the response, and the values should be the description of the field.'\n    )\nmodel: LlmModel = SchemaField(title='LLM Model', default=LlmModel.\n    GPT4_TURBO, description=\n    'The language model to use for answering the prompt.', advanced=False)\ncredentials: AICredentials = AICredentialsField()\nsys_prompt: str = SchemaField(title='System Prompt', default='',\n    description='The system prompt to provide additional context to the model.'\n    )\nconversation_history: list[Message] = SchemaField(default=[], description=\n    'The conversation history to provide context for the prompt.')\nretry: int = SchemaField(title='Retry Count', default=3, description=\n    'Number of times to retry the LLM call if the response does not match the expected format.'\n    )\nprompt_values: dict[str, str] = SchemaField(advanced=False, default={},\n    description='Values used to fill in the prompt.')\nmax_tokens: int | None = SchemaField(advanced=True, default=None,\n    description=\n    'The maximum number of tokens to generate in the chat completion.')\nollama_host: str = SchemaField(advanced=True, default='localhost:11434',\n    description='Ollama host for local  models')",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [
            {
              "id": 55,
              "label": "#55\nresponse: dict[str, Any] = SchemaField(description=\n    'The response object generated by the language model.')\nerror: str = SchemaField(description='Error message if the API call failed.')",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "AITextGeneratorBlock",
      "type": "CFG",
      "blocks": [
        {
          "id": 158,
          "label": "#158",
          "successors": []
        }
      ],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 166,
              "label": "#166\nsuper().__init__(id='1f292d4a-41a4-4977-9684-7c8d560b9f91', description=\n    'Call a Large Language Model (LLM) to generate a string based on the given prompt.'\n    , categories={BlockCategory.AI}, input_schema=AITextGeneratorBlock.\n    Input, output_schema=AITextGeneratorBlock.Output, test_input={'prompt':\n    'User prompt', 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials\n    =TEST_CREDENTIALS, test_output=('response', 'Response text'), test_mock\n    ={'llm_call': lambda *args, **kwargs: 'Response text'})",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 169,
              "label": "#169\nblock = AIStructuredResponseGeneratorBlock()\nresponse = block.run_once(input_data, 'response', credentials=credentials)\nself.merge_stats(block.execution_stats)\nreturn response['response']",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 173,
              "label": "#173\nobject_input_data = AIStructuredResponseGeneratorBlock.Input(**{attr:\n    getattr(input_data, attr) for attr in input_data.model_fields},\n    expected_format={})\nyield 'response', self.llm_call(object_input_data, credentials)",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 160,
              "label": "#160\nprompt: str = SchemaField(description=\n    'The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces.'\nmodel: LlmModel = SchemaField(title='LLM Model', default=LlmModel.\n    GPT4_TURBO, description=\n    'The language model to use for answering the prompt.', advanced=False)\ncredentials: AICredentials = AICredentialsField()\nsys_prompt: str = SchemaField(title='System Prompt', default='',\n    description='The system prompt to provide additional context to the model.'\n    )\nretry: int = SchemaField(title='Retry Count', default=3, description=\n    'Number of times to retry the LLM call if the response does not match the expected format.'\n    )\nprompt_values: dict[str, str] = SchemaField(advanced=False, default={},\n    description='Values used to fill in the prompt.')\nollama_host: str = SchemaField(advanced=True, default='localhost:11434',\n    description='Ollama host for local  models')\nmax_tokens: int | None = SchemaField(advanced=True, default=None,\n    description=\n    'The maximum number of tokens to generate in the chat completion.')",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [
            {
              "id": 163,
              "label": "#163\nresponse: str = SchemaField(description=\n    'The response generated by the language model.')\nerror: str = SchemaField(description='Error message if the API call failed.')",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "SummaryStyle",
      "type": "CFG",
      "blocks": [
        {
          "id": 178,
          "label": "#178\nCONCISE = 'concise'\nDETAILED = 'detailed'\nBULLET_POINTS = 'bullet points'\nNUMBERED_LIST = 'numbered list'",
          "successors": []
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "AITextSummarizerBlock",
      "type": "CFG",
      "blocks": [
        {
          "id": 181,
          "label": "#181",
          "successors": []
        }
      ],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 189,
              "label": "#189\nsuper().__init__(id='a0a69be1-4528-491c-a85a-a4ab6873e3f0', description=\n    'Utilize a Large Language Model (LLM) to summarize a long text.',\n    categories={BlockCategory.AI, BlockCategory.TEXT}, input_schema=\n    AITextSummarizerBlock.Input, output_schema=AITextSummarizerBlock.Output,\n    TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output\n    =('summary', 'Final summary of a long text'), test_mock={'llm_call': lambda\n    input_data, credentials: {'final_summary':\n    'Final summary of a long text'} if 'final_summary' in input_data.\n    expected_format else {'summary': 'Summary of a chunk of text'}})",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 192,
              "label": "#192\nfor output in self._run(input_data, credentials):",
              "successors": [
                {
                  "id": 193,
                  "label": "#193\nyield output",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_run",
          "type": "CFG",
          "blocks": [
            {
              "id": 198,
              "label": "#198\nchunks = self._split_text(input_data.text, input_data.max_tokens,\n    input_data.chunk_overlap)\nsummaries = []",
              "successors": [
                {
                  "id": 199,
                  "label": "#199\nfor chunk in chunks:",
                  "successors": [
                    {
                      "id": 200,
                      "label": "#200\nchunk_summary = self._summarize_chunk(chunk, input_data, credentials)\nsummaries.append(chunk_summary)",
                      "successors": []
                    },
                    {
                      "id": 201,
                      "label": "#201\nfinal_summary = self._combine_summaries(summaries, input_data, credentials)\nyield 'summary', final_summary",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_split_text",
          "type": "CFG",
          "blocks": [
            {
              "id": 205,
              "label": "#205\nwords = text.split()\nchunks = []\nchunk_size = max_tokens - overlap",
              "successors": [
                {
                  "id": 206,
                  "label": "#206\nfor i in range(0, len(words), chunk_size):",
                  "successors": [
                    {
                      "id": 207,
                      "label": "#207\nchunk = ' '.join(words[i:i + max_tokens])\nchunks.append(chunk)",
                      "successors": []
                    },
                    {
                      "id": 208,
                      "label": "#208\nreturn chunks",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 212,
              "label": "#212\nblock = AIStructuredResponseGeneratorBlock()\nresponse = block.run_once(input_data, 'response', credentials=credentials)\nself.merge_stats(block.execution_stats)\nreturn response",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_summarize_chunk",
          "type": "CFG",
          "blocks": [
            {
              "id": 216,
              "label": "#216\nprompt = f\"\"\"Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\n\n```{chunk}```\"\"\"\nllm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(\n    prompt=prompt, credentials=input_data.credentials, model=input_data.\n    model, expected_format={'summary': 'The summary of the given text.'}),\n    credentials=credentials)\nreturn llm_response['summary']",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_combine_summaries",
          "type": "CFG",
          "blocks": [
            {
              "id": 220,
              "label": "#220\ncombined_text = '\\n\\n'.join(summaries)\nif len(combined_text.split()) <= input_data.max_tokens:",
              "successors": [
                {
                  "id": 221,
                  "label": "#221\nprompt = f\"\"\"Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\n\n ```{combined_text}```\n\n Just respond with the final_summary in the format specified.\"\"\"\nllm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(\n    prompt=prompt, credentials=input_data.credentials, model=input_data.\n    model, expected_format={'final_summary':\n    'The final summary of all provided summaries.'}), credentials=credentials)\nreturn llm_response['final_summary']",
                  "successors": []
                },
                {
                  "id": 223,
                  "label": "#223\nreturn self._run(AITextSummarizerBlock.Input(text=combined_text,\n    credentials=input_data.credentials, model=input_data.model, max_tokens=\n    input_data.max_tokens, chunk_overlap=input_data.chunk_overlap),\n    credentials=credentials).send(None)[1]",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 183,
              "label": "#183\ntext: str = SchemaField(description='The text to summarize.', placeholder=\nmodel: LlmModel = SchemaField(title='LLM Model', default=LlmModel.\n    GPT4_TURBO, description=\n    'The language model to use for summarizing the text.')\nfocus: str = SchemaField(title='Focus', default='general information',\n    description='The topic to focus on in the summary')\nstyle: SummaryStyle = SchemaField(title='Summary Style', default=\n    SummaryStyle.CONCISE, description='The style of the summary to generate.')\ncredentials: AICredentials = AICredentialsField()\nmax_tokens: int = SchemaField(title='Max Tokens', default=4096, description\n    ='The maximum number of tokens to generate in the chat completion.', ge=1)\nchunk_overlap: int = SchemaField(title='Chunk Overlap', default=100,\n    description=\n    'The number of overlapping tokens between chunks to maintain context.',\n    ge=0)\nollama_host: str = SchemaField(advanced=True, default='localhost:11434',\n    description='Ollama host for local  models')",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [
            {
              "id": 186,
              "label": "#186\nsummary: str = SchemaField(description='The final summary of the text.')\nerror: str = SchemaField(description='Error message if the API call failed.')",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "AIConversationBlock",
      "type": "CFG",
      "blocks": [
        {
          "id": 229,
          "label": "#229",
          "successors": []
        }
      ],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 237,
              "label": "#237\nsuper().__init__(id='32a87eab-381e-4dd4-bdb8-4c47151be35a', description=\n    'Advanced LLM call that takes a list of messages and sends them to the language model.'\n    , categories={BlockCategory.AI}, input_schema=AIConversationBlock.Input,\n    output_schema=AIConversationBlock.Output, test_input={'messages': [{\n    'role': 'system', 'content': 'You are a helpful assistant.'}, {'role':\n    'user', 'content': 'Who won the world series in 2020?'}, {'role':\n    'assistant', 'content':\n    'The Los Angeles Dodgers won the World Series in 2020.'}, {'role':\n    'user', 'content': 'Where was it played?'}], 'model': LlmModel.\n    GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=\n    TEST_CREDENTIALS, test_output=('response',\n    'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'\n    ), test_mock={'llm_call': lambda *args, **kwargs:\n    'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'\n    })",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 240,
              "label": "#240\nblock = AIStructuredResponseGeneratorBlock()\nresponse = block.run_once(input_data, 'response', credentials=credentials)\nself.merge_stats(block.execution_stats)\nreturn response['response']",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 244,
              "label": "#244\nresponse = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt='',\n    credentials=input_data.credentials, model=input_data.model,\n    conversation_history=input_data.messages, max_tokens=input_data.\n    max_tokens, expected_format={}), credentials=credentials)\nyield 'response', response",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 231,
              "label": "#231\nmessages: List[Message] = SchemaField(description=\n    'List of messages in the conversation.', min_length=1)\nmodel: LlmModel = SchemaField(title='LLM Model', default=LlmModel.\n    GPT4_TURBO, description='The language model to use for the conversation.')\ncredentials: AICredentials = AICredentialsField()\nmax_tokens: int | None = SchemaField(advanced=True, default=None,\n    description=\n    'The maximum number of tokens to generate in the chat completion.')\nollama_host: str = SchemaField(advanced=True, default='localhost:11434',\n    description='Ollama host for local  models')",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [
            {
              "id": 234,
              "label": "#234\nresponse: str = SchemaField(description=\n    \"The model's response to the conversation.\")\nerror: str = SchemaField(description='Error message if the API call failed.')",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "AIListGeneratorBlock",
      "type": "CFG",
      "blocks": [
        {
          "id": 249,
          "label": "#249",
          "successors": []
        }
      ],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 257,
              "label": "#257\nsuper().__init__(id='9c0b0450-d199-458b-a731-072189dd6593', description=\n    'Generate a Python list based on the given prompt using a Large Language Model (LLM).'\n    , categories={BlockCategory.AI, BlockCategory.TEXT}, input_schema=\n    AIListGeneratorBlock.Input, output_schema=AIListGeneratorBlock.Output,\n    test_input={'focus': 'planets', 'source_data':\n    \"Zylora Prime is a glowing jungle world with bioluminescent plants, while Kharon-9 is a harsh desert planet with underground cities. Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of fictional worlds.\"\n    , 'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT,\n    'max_retries': 3}, test_credentials=TEST_CREDENTIALS, test_output=[(\n    'generated_list', ['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara',\n    'Draknos']), ('list_item', 'Zylora Prime'), ('list_item', 'Kharon-9'),\n    ('list_item', 'Vortexia'), ('list_item', 'Oceara'), ('list_item',\n    'Draknos')], test_mock={'llm_call': lambda input_data, credentials: {\n    'response':\n    \"['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']\"}})",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 260,
              "label": "#260\nllm_block = AIStructuredResponseGeneratorBlock()\nresponse = llm_block.run_once(input_data, 'response', credentials=credentials)\nreturn response",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "string_to_list",
          "type": "CFG",
          "blocks": [
            {
              "id": 264,
              "label": "#264\n\"\"\"\n        Converts a string representation of a list into an actual Python list object.\n        \"\"\"\nlogger.debug(f'Converting string to list. Input string: {string}')\ntry:",
              "successors": [
                {
                  "id": 265,
                  "label": "#265\npython_list = ast.literal_eval(string)\nif isinstance(python_list, list):",
                  "successors": [
                    {
                      "id": 268,
                      "label": "#268\nlogger.debug(f'Successfully converted string to list: {python_list}')\nreturn python_list",
                      "successors": []
                    },
                    {
                      "id": 270,
                      "label": "#270\nlogger.error(f\"The provided string '{string}' is not a valid list\")\nraise ValueError(f\"The provided string '{string}' is not a valid list.\")",
                      "successors": []
                    }
                  ]
                },
                {
                  "id": 266,
                  "label": "#266\nlogger.error(f'Failed to convert string to list: {e}')\nraise ValueError('Invalid list format. Could not convert to list.')",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 276,
              "label": "#276\nlogger.debug(f'Starting AIListGeneratorBlock.run with input data: {input_data}'\n    )\napi_key_check = credentials.api_key.get_secret_value()\nif not api_key_check:",
              "successors": [
                {
                  "id": 277,
                  "label": "#277\nraise ValueError('No LLM API key provided.')",
                  "successors": []
                },
                {
                  "id": 278,
                  "label": "#278\nsys_prompt = \"\"\"You are a Python list generator. Your task is to generate a Python list based on the user's prompt. \n            |Respond ONLY with a valid python list. \n            |The list can contain strings, numbers, or nested lists as appropriate. \n            |Do not include any explanations or additional text.\n\n            |Valid Example string formats:\n\n            |Example 1:\n            |```\n            |['1', '2', '3', '4']\n            |```\n\n            |Example 2:\n            |```\n            |[['1', '2'], ['3', '4'], ['5', '6']]\n            |```\n\n            |Example 3:\n            |```\n            |['1', ['2', '3'], ['4', ['5', '6']]]\n            |```\n\n            |Example 4:\n            |```\n            |['a', 'b', 'c']\n            |```\n\n            |Example 5:\n            |```\n            |['1', '2.5', 'string', 'True', ['False', 'None']]\n            |```\n\n            |Do not include any explanations or additional text, just respond with the list in the format specified above.\n            \"\"\"\nif input_data.focus:",
                  "successors": [
                    {
                      "id": 280,
                      "label": "#280\nprompt = f\"\"\"Generate a list with the following focus:\n<focus>\n\n{input_data.focus}</focus>\"\"\"",
                      "successors": [
                        {
                          "id": 281,
                          "label": "#281\nif input_data.source_data:",
                          "successors": [
                            {
                              "id": 286,
                              "label": "#286\nprompt += f\"\"\"\n\nUse the following source data to generate the list from:\n\n<source_data>\n\n{input_data.source_data}</source_data>\n\nDo not invent fictional data that is not present in the source data.\"\"\"",
                              "successors": [
                                {
                                  "id": 287,
                                  "label": "#287\nfor attempt in range(input_data.max_retries):",
                                  "successors": [
                                    {
                                      "id": 289,
                                      "label": "#289\ntry:",
                                      "successors": [
                                        {
                                          "id": 291,
                                          "label": "#291\nlogger.debug('Calling LLM')\nllm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(\n    sys_prompt=sys_prompt, prompt=prompt, credentials=input_data.\n    credentials, model=input_data.model, expected_format={}, ollama_host=\n    input_data.ollama_host), credentials=credentials)\nlogger.debug(f'LLM response: {llm_response}')\nresponse_string = llm_response['response']\nlogger.debug(f'Response string: {response_string}')\nlogger.debug('Converting string to Python list')\nparsed_list = self.string_to_list(response_string)\nlogger.debug(f'Parsed list: {parsed_list}')\nlogger.debug('Successfully generated a valid Python list')\nyield 'generated_list', parsed_list",
                                          "successors": [
                                            {
                                              "id": 294,
                                              "label": "#294\nfor item in parsed_list:",
                                              "successors": [
                                                {
                                                  "id": 295,
                                                  "label": "#295\nyield 'list_item', item",
                                                  "successors": []
                                                },
                                                {
                                                  "id": 296,
                                                  "label": "#296\nreturn",
                                                  "successors": []
                                                }
                                              ]
                                            }
                                          ]
                                        },
                                        {
                                          "id": 292,
                                          "label": "#292\nlogger.error(f'Error in attempt {attempt + 1}: {str(e)}')\nif attempt == input_data.max_retries - 1:",
                                          "successors": [
                                            {
                                              "id": 299,
                                              "label": "#299\nlogger.error(\n    f'Failed to generate a valid Python list after {input_data.max_retries} attempts'\n    )\nraise RuntimeError(\n    f'Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}'\n    )",
                                              "successors": []
                                            },
                                            {
                                              "id": 301,
                                              "label": "#301\nlogger.debug('Preparing retry prompt')\nprompt = f\"\"\"\n                    The previous attempt failed due to `{e}`\n                    Generate a valid Python list based on the original prompt.\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\n                    Original prompt: \n                    ```{prompt}```\n                    \n                    Respond only with the list in the format specified with no commentary or apologies.\n                    \"\"\"\nlogger.debug(f'Retry prompt: {prompt}')",
                                              "successors": []
                                            }
                                          ]
                                        }
                                      ]
                                    },
                                    {
                                      "id": 290,
                                      "label": "#290\nlogger.debug('AIListGeneratorBlock.run completed')",
                                      "successors": []
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "id": 288,
                              "label": "#288\nprompt += \"\"\"\n\nInvent the data to generate the list from.\"\"\"",
                              "successors": []
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": 282,
                      "label": "#282\nif input_data.source_data:",
                      "successors": [
                        {
                          "id": 283,
                          "label": "#283\nprompt = \"\"\"Extract the main focus of the source data to a list.\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.\"\"\"",
                          "successors": []
                        },
                        {
                          "id": 285,
                          "label": "#285\nprompt = 'Generate a random list.'",
                          "successors": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 251,
              "label": "#251\nfocus: str | None = SchemaField(description=\n    'The focus of the list to generate.', placeholder=\n    'The top 5 most interesting news stories in the data.', default=None,\n    advanced=False)\nsource_data: str | None = SchemaField(description=\n    'The data to generate the list from.', placeholder=\n    'News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released.'\n    , default=None, advanced=False)\nmodel: LlmModel = SchemaField(title='LLM Model', default=LlmModel.\n    GPT4_TURBO, description=\n    'The language model to use for generating the list.', advanced=True)\ncredentials: AICredentials = AICredentialsField()\nmax_retries: int = SchemaField(default=3, description=\n    'Maximum number of retries for generating a valid list.', ge=1, le=5)\nmax_tokens: int | None = SchemaField(advanced=True, default=None,\n    description=\n    'The maximum number of tokens to generate in the chat completion.')\nollama_host: str = SchemaField(advanced=True, default='localhost:11434',\n    description='Ollama host for local  models')",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [
            {
              "id": 254,
              "label": "#254\ngenerated_list: List[str] = SchemaField(description='The generated list.')\nlist_item: str = SchemaField(description='Each individual item in the list.')\nerror: str = SchemaField(description=\n    'Error message if the list generation failed.')",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ]
    }
  ]
}