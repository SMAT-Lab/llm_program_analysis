{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Generate CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果不太好，还是需要我们一步步进行处理！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1 先将文件的嵌套类，方法给找到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/1.py\n",
      "Processing ../../dataset/python/2.py\n",
      "Processing ../../dataset/python/3.py\n",
      "Processing ../../dataset/python/4.py\n",
      "Processing ../../dataset/python/5.py\n",
      "Processing ../../dataset/python/6.py\n",
      "Processing ../../dataset/python/7.py\n",
      "Processing ../../dataset/python/8.py\n",
      "Processing ../../dataset/python/9.py\n",
      "Processing ../../dataset/python/10.py\n",
      "Processing ../../dataset/python/12.py\n",
      "Processing ../../dataset/python/11.py\n",
      "Processing ../../dataset/python/13.py\n",
      "Processing ../../dataset/python/14.py\n",
      "Processing ../../dataset/python/15.py\n",
      "Processing ../../dataset/python/16.py\n",
      "Processing ../../dataset/python/17.py\n",
      "Processing ../../dataset/python/18.py\n",
      "Processing ../../dataset/python/19.py\n",
      "Processing ../../dataset/python/20.py\n",
      "Processing ../../dataset/python/21.py\n",
      "Processing ../../dataset/python/22.py\n",
      "Processing ../../dataset/python/23.py\n",
      "Processing ../../dataset/python/24.py\n",
      "Processing ../../dataset/python/25.py\n",
      "Processing ../../dataset/python/26.py\n",
      "Processing ../../dataset/python/27.py\n",
      "Processing ../../dataset/python/28.py\n",
      "Processing ../../dataset/python/29.py\n",
      "Processing ../../dataset/python/30.py\n",
      "Processing ../../dataset/python/31.py\n",
      "Processing ../../dataset/python/32.py\n",
      "Processing ../../dataset/python/34.py\n",
      "Processing ../../dataset/python/33.py\n",
      "Processing ../../dataset/python/35.py\n",
      "Processing ../../dataset/python/36.py\n",
      "Processing ../../dataset/python/37.py\n",
      "Processing ../../dataset/python/38.py\n",
      "Processing ../../dataset/python/39.py\n",
      "Processing ../../dataset/python/40.py\n",
      "Processing ../../dataset/python/41.py\n",
      "Processing ../../dataset/python/42.py\n",
      "Processing ../../dataset/python/43.py\n",
      "Processing ../../dataset/python/44.py\n",
      "Processing ../../dataset/python/45.py\n",
      "Processing ../../dataset/python/46.py\n",
      "Processing ../../dataset/python/47.py\n",
      "Processing ../../dataset/python/48.py\n",
      "Processing ../../dataset/python/49.py\n",
      "Processing ../../dataset/python/50.py\n",
      "Processing ../../dataset/python/51.py\n",
      "Processing ../../dataset/python/52.py\n",
      "Processing ../../dataset/python/53.py\n",
      "Processing ../../dataset/python/54.py\n",
      "Processing ../../dataset/python/55.py\n",
      "Processing ../../dataset/python/56.py\n",
      "Processing ../../dataset/python/57.py\n",
      "Processing ../../dataset/python/58.py\n",
      "Processing ../../dataset/python/59.py\n",
      "Processing ../../dataset/python/60.py\n",
      "Processing ../../dataset/python/61.py\n",
      "Processing ../../dataset/python/62.py\n",
      "Processing ../../dataset/python/63.py\n",
      "Processing ../../dataset/python/64.py\n",
      "Processing ../../dataset/python/65.py\n",
      "Processing ../../dataset/python/66.py\n",
      "Processing ../../dataset/python/67.py\n",
      "Processing ../../dataset/python/68.py\n",
      "Processing ../../dataset/python/69.py\n",
      "Processing ../../dataset/python/70.py\n",
      "Processing ../../dataset/python/71.py\n",
      "Processing ../../dataset/python/72.py\n",
      "Processing ../../dataset/python/73.py\n",
      "Processing ../../dataset/python/74.py\n",
      "Processing ../../dataset/python/75.py\n",
      "Processing ../../dataset/python/77.py\n",
      "Processing ../../dataset/python/76.py\n",
      "Processing ../../dataset/python/78.py\n",
      "Processing ../../dataset/python/79.py\n",
      "Processing ../../dataset/python/80.py\n",
      "Processing ../../dataset/python/82.py\n",
      "Processing ../../dataset/python/81.py\n",
      "Processing ../../dataset/python/83.py\n",
      "Processing ../../dataset/python/84.py\n",
      "Processing ../../dataset/python/85.py\n",
      "Processing ../../dataset/python/86.py\n",
      "Processing ../../dataset/python/87.py\n",
      "Processing ../../dataset/python/88.py\n",
      "Processing ../../dataset/python/89.py\n",
      "Processing ../../dataset/python/90.py\n",
      "Processing ../../dataset/python/91.py\n",
      "Processing ../../dataset/python/92.py\n",
      "Processing ../../dataset/python/93.py\n",
      "Processing ../../dataset/python/94.py\n",
      "Processing ../../dataset/python/95.py\n",
      "Processing ../../dataset/python/96.py\n",
      "Processing ../../dataset/python/98.py\n",
      "Processing ../../dataset/python/99.py\n",
      "Processing ../../dataset/python/100.py\n",
      "Processing ../../dataset/python/101.py\n",
      "Processing ../../dataset/python/102.py\n",
      "Processing ../../dataset/python/103.py\n",
      "Processing ../../dataset/python/104.py\n",
      "Processing ../../dataset/python/106.py\n",
      "Processing ../../dataset/python/105.py\n",
      "Processing ../../dataset/python/107.py\n",
      "Processing ../../dataset/python/108.py\n",
      "Processing ../../dataset/python/109.py\n",
      "Processing ../../dataset/python/110.py\n",
      "Processing ../../dataset/python/111.py\n",
      "Processing ../../dataset/python/112.py\n",
      "Processing ../../dataset/python/113.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   2%|▏         | 3/200 [00:10<11:40,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/114.py\n",
      "Processing ../../dataset/python/115.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   2%|▎         | 5/200 [00:13<08:02,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/116.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   3%|▎         | 6/200 [00:15<07:21,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/117.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   4%|▎         | 7/200 [00:16<06:29,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/118.py\n",
      "Processing ../../dataset/python/119.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   4%|▍         | 9/200 [00:16<03:47,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/120.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   5%|▌         | 10/200 [00:17<03:39,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/121.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   6%|▌         | 11/200 [00:18<03:06,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/122.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   6%|▌         | 12/200 [00:20<04:07,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/123.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   6%|▋         | 13/200 [00:20<03:17,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/124.py\n",
      "Processing ../../dataset/python/125.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   8%|▊         | 16/200 [00:22<02:06,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/126.py\n",
      "Processing ../../dataset/python/127.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   8%|▊         | 17/200 [00:22<01:58,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/128.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:   9%|▉         | 18/200 [00:23<02:27,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/129.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  10%|█         | 21/200 [00:26<01:58,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/130.py\n",
      "Processing ../../dataset/python/131.py\n",
      "Processing ../../dataset/python/132.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  11%|█         | 22/200 [00:26<01:51,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/133.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  12%|█▏        | 24/200 [00:28<02:01,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/134.py\n",
      "Processing ../../dataset/python/135.py\n",
      "Processing ../../dataset/python/136.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  13%|█▎        | 26/200 [00:29<01:42,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/137.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  14%|█▍        | 28/200 [00:29<01:11,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/138.py\n",
      "Processing ../../dataset/python/139.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  14%|█▍        | 29/200 [00:30<01:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/140.py\n",
      "Processing ../../dataset/python/141.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  16%|█▌        | 31/200 [00:30<01:02,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/142.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  16%|█▌        | 32/200 [00:31<01:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/143.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  16%|█▋        | 33/200 [00:31<01:12,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/144.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  17%|█▋        | 34/200 [00:32<01:18,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/145.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  18%|█▊        | 35/200 [00:33<01:48,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/146.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  18%|█▊        | 37/200 [00:34<01:15,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/147.py\n",
      "Processing ../../dataset/python/148.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  20%|█▉        | 39/200 [00:34<00:59,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/149.py\n",
      "Processing ../../dataset/python/150.py\n",
      "Processing ../../dataset/python/151.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  20%|██        | 41/200 [00:35<01:09,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/152.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  21%|██        | 42/200 [00:36<01:04,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/153.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  22%|██▏       | 43/200 [00:37<01:26,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/154.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  22%|██▏       | 44/200 [00:37<01:42,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/155.py\n",
      "Processing ../../dataset/python/156.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  23%|██▎       | 46/200 [00:38<01:09,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/157.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  24%|██▎       | 47/200 [00:38<01:06,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/158.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  24%|██▍       | 48/200 [00:38<00:57,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/159.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  25%|██▌       | 50/200 [00:40<01:05,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/160.py\n",
      "Processing ../../dataset/python/161.py\n",
      "Processing ../../dataset/python/162.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  26%|██▌       | 52/200 [00:42<02:05,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/163.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  26%|██▋       | 53/200 [00:44<02:45,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/164.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  27%|██▋       | 54/200 [00:45<02:12,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/165.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  28%|██▊       | 55/200 [00:46<02:46,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/166.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  28%|██▊       | 56/200 [00:49<03:30,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/167.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  28%|██▊       | 57/200 [00:50<03:30,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/168.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  29%|██▉       | 58/200 [00:50<02:40,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/169.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  30%|██▉       | 59/200 [00:51<02:19,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/170.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  30%|███       | 60/200 [00:52<02:09,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/171.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  30%|███       | 61/200 [00:53<02:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/172.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  31%|███       | 62/200 [00:53<01:38,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/173.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  32%|███▏      | 63/200 [00:54<01:33,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/174.py\n",
      "Processing ../../dataset/python/175.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  32%|███▎      | 65/200 [00:54<01:09,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/176.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  33%|███▎      | 66/200 [00:55<01:04,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/177.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  34%|███▎      | 67/200 [00:55<01:12,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/178.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  34%|███▍      | 68/200 [00:57<01:58,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/179.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  34%|███▍      | 69/200 [00:58<01:52,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/180.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  35%|███▌      | 70/200 [00:58<01:32,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/181.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  36%|███▌      | 71/200 [00:59<01:34,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/182.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  36%|███▋      | 73/200 [01:01<01:31,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/183.py\n",
      "Processing ../../dataset/python/184.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  37%|███▋      | 74/200 [01:01<01:12,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/185.py\n",
      "Processing ../../dataset/python/186.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  38%|███▊      | 76/200 [01:02<01:16,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/187.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  38%|███▊      | 77/200 [01:04<02:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/188.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  39%|███▉      | 78/200 [01:05<01:44,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/189.py\n",
      "Processing ../../dataset/python/190.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  40%|████      | 80/200 [01:06<01:15,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/191.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  40%|████      | 81/200 [01:06<01:09,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/192.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  41%|████      | 82/200 [01:07<01:19,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/193.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  42%|████▏     | 83/200 [01:10<02:47,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/195.py\n",
      "Processing ../../dataset/python/194.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件:  43%|████▎     | 86/200 [01:12<01:32,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/python/196.py\n",
      "Processing ../../dataset/python/197.py\n",
      "Processing ../../dataset/python/198.py\n",
      "Processing ../../dataset/python/199.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件: 100%|██████████| 200/200 [10:16<00:00,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from functools import partial\n",
    "\n",
    "# 这里假设你有一个自己封装的 get_llm_answers 函数\n",
    "# 请根据实际情况导入\n",
    "from llm import get_llm_answers\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_step1_prompt(code_text: str, program_language: str):\n",
    "    \"\"\"\n",
    "    生成第一步的 Prompt，用于让大模型识别所有类和函数（以及嵌套关系）。\n",
    "    注意，这里去掉了原先的三重反引号。\n",
    "    \"\"\"\n",
    "    code_lines = code_text.splitlines()\n",
    "    code_lines_json = [{\n",
    "        \"line\": i + 1,\n",
    "        \"code\": line\n",
    "    } for i, line in enumerate(code_lines)]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are given a piece of {program_language} code. Your goal is to find all the nested classes and methods in the code.\n",
    "\n",
    "Please return the result in JSON format, your output should be the following format:\n",
    "\n",
    "{{\n",
    "    \"name\": \"example_script\",  // Name of the script or function\n",
    "    \"type\": \"CFG\",\n",
    "    \"start_line\": number,\n",
    "    \"end_line\": number,\n",
    "    \"functions\": [\n",
    "      {{\n",
    "        \"name\": \"function_name\",\n",
    "        \"type\": \"function\",\n",
    "        \"start_line\": number,\n",
    "        \"end_line\": number,\n",
    "        \"functions\": [],         // Nested functions\n",
    "        \"classes\": []            // Nested classes\n",
    "      }}\n",
    "    ],\n",
    "    \"classes\": [\n",
    "      {{\n",
    "        \"name\": \"class_name\",\n",
    "        \"type\": \"class\",\n",
    "        \"start_line\": number,\n",
    "        \"end_line\": number,\n",
    "        \"functions\": [           // Methods of the class\n",
    "          {{\n",
    "            \"name\": \"method_name\",\n",
    "            \"type\": \"function\",\n",
    "            \"start_line\": number,\n",
    "            \"end_line\": number,\n",
    "            \"functions\": [],     // Nested functions\n",
    "            \"classes\": []        // Nested classes\n",
    "          }}\n",
    "        ]\n",
    "      }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "The code lines are:\n",
    "{json.dumps(code_lines_json, indent=2)}\n",
    "\n",
    "IMPORTANT: Make sure that the nested classes and methods are in the correct level. For example, if a function is nested in another class, the function should be in the nested class's functions list.\n",
    "Besides, if a class is nested in another class, the class should be in the nested class's classes list.\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "def find_nested_classes_and_methods(code_text: str, program_language: str):\n",
    "    \"\"\"\n",
    "    调用 LLM，让其识别文件中的嵌套类、函数，并返回 JSON 结构。\n",
    "    \"\"\"\n",
    "    prompt = get_step1_prompt(code_text, program_language)\n",
    "    response = get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True)\n",
    "    nested_classes_and_methods = json.loads(response)\n",
    "    return nested_classes_and_methods\n",
    "\n",
    "\n",
    "def process_file_with_chain_of_thought(code_text: str, program_language: str):\n",
    "    \"\"\"\n",
    "    读取代码文本 -> 让大模型找出所有嵌套类/函数 -> 返回 JSON 结构\n",
    "    \"\"\"\n",
    "    step1_result = find_nested_classes_and_methods(code_text, program_language)\n",
    "    return step1_result\n",
    "\n",
    "\n",
    "def get_code_by_line_range(code_block, code):\n",
    "    \"\"\"\n",
    "    取出 code_block 代表的行范围(start_line~end_line)，并且排除其内部嵌套类/函数的所有行。\n",
    "    最终保留一组 { \"lineno\", \"line\" } 数组，方便后续做 CFG 时保留原始行号。\n",
    "    \"\"\"\n",
    "    code_lines = code.splitlines()\n",
    "    start_line = code_block[\"start_line\"]\n",
    "    # 这里根据实际情况决定 end_line 是否 +1\n",
    "    end_line = code_block[\"end_line\"]\n",
    "\n",
    "    # 先把区间内每行加入集合\n",
    "    line_set = set(range(start_line, end_line + 1))\n",
    "\n",
    "    # 从集合中排除掉所有嵌套类/函数的行\n",
    "    for func in code_block.get(\"functions\", []):\n",
    "        func_start_line = func.get(\"start_line\", 0)\n",
    "        func_end_line = func.get(\"end_line\", 0)\n",
    "        line_set.difference_update(range(func_start_line, func_end_line + 1))\n",
    "\n",
    "    for cls in code_block.get(\"classes\", []):\n",
    "        cls_start_line = cls.get(\"start_line\", 0)\n",
    "        cls_end_line = cls.get(\"end_line\", 0)\n",
    "        line_set.difference_update(range(cls_start_line, cls_end_line + 1))\n",
    "\n",
    "    # 剩余行号排序后，保存 { lineno, line } 到 simplified_code\n",
    "    ordered_lines = sorted(line_set)\n",
    "    simplified_code_array = []\n",
    "    for lineno in ordered_lines:\n",
    "        if 1 <= lineno <= len(code_lines):\n",
    "            line_content = code_lines[lineno - 1]\n",
    "        else:\n",
    "            line_content = \"\"\n",
    "        simplified_code_array.append({\n",
    "            \"lineno\": lineno,\n",
    "            \"line\": line_content\n",
    "        })\n",
    "\n",
    "    # 将数组存进 code_block 中\n",
    "    code_block[\"simplified_code\"] = simplified_code_array\n",
    "\n",
    "\n",
    "def recursive_get_code_by_line_range(code_block, code):\n",
    "    \"\"\"\n",
    "    递归地为当前块及其所有子类、子函数，计算并存储 simplified_code（带原始行号）。\n",
    "    \"\"\"\n",
    "    get_code_by_line_range(code_block, code)\n",
    "    for func in code_block.get(\"functions\", []):\n",
    "        recursive_get_code_by_line_range(func, code)\n",
    "    for cls in code_block.get(\"classes\", []):\n",
    "        recursive_get_code_by_line_range(cls, code)\n",
    "\n",
    "\n",
    "def print_simplified_code(code_block: dict, indent=0):\n",
    "    \"\"\"\n",
    "    递归打印 simplified_code 的内容（仅用于调试或查看），保留行号和内容。\n",
    "    \"\"\"\n",
    "    prefix = \" \" * indent\n",
    "    simplified_lines = code_block.get(\"simplified_code\", [])\n",
    "    print(prefix + \"简化后的代码 (行号 -> 内容):\")\n",
    "    for item in simplified_lines:\n",
    "        print(prefix + f\"{item['lineno']:4d}: {item['line']}\")\n",
    "\n",
    "    # 递归处理嵌套的类\n",
    "    for class_block in code_block.get(\"classes\", []):\n",
    "        print(prefix + f\"\\n类 {class_block.get('name', '')}:\")\n",
    "        print_simplified_code(class_block, indent + 2)\n",
    "\n",
    "    # 递归处理嵌套的函数\n",
    "    for function_block in code_block.get(\"functions\", []):\n",
    "        print(prefix + f\"\\n函数 {function_block.get('name', '')}:\")\n",
    "        print_simplified_code(function_block, indent + 2)\n",
    "\n",
    "\n",
    "def get_code_cfg_prompt(line_array, program_language):\n",
    "    \"\"\"\n",
    "    给 LLM 的 Prompt，要求其基于该 line_array 生成 CFG 并返回 JSON。\n",
    "    line_array 的格式形如:\n",
    "      [\n",
    "        {\"lineno\": 10, \"line\": \"def foo():\"},\n",
    "        {\"lineno\": 11, \"line\": \"...\"},\n",
    "        ...\n",
    "      ]\n",
    "\n",
    "    去掉了三重反引号。\n",
    "    \"\"\"\n",
    "    code_as_json = json.dumps(line_array, indent=2)\n",
    "    prompt = f\"\"\"\n",
    "You will be given a piece of {program_language} code in the form of a JSON array. Each element has two fields:\n",
    "  - \"lineno\": the original line number in the code\n",
    "  - \"line\": the actual code text on that line\n",
    "\n",
    "Your goal is to generate a Control Flow Graph (CFG) for this code and output the result as JSON. Here are the specific requirements:\n",
    "\n",
    "1. Input Format:\n",
    "   The code is presented as a JSON array of objects, each with \"lineno\" (int) and \"line\" (string). For example:\n",
    "\n",
    "{code_as_json}\n",
    "\n",
    "(This is the code you need to analyze.)\n",
    "\n",
    "2. Definition of Basic Blocks:\n",
    "   - A basic block can contain one or more “continuous and unbranched” statements.\n",
    "   - Whenever you encounter a statement that causes a flow jump or branch (e.g., if-else, for-while, try-except-finally, with-as, match-case, break-continue-return, etc.), you should start a new basic block.\n",
    "\n",
    "3. JSON Output Structure:\n",
    "   Your output must strictly follow this JSON format, with no additional text or explanation:\n",
    "\n",
    "\"blocks\": [\n",
    "  {{\n",
    "    \"id\": 1,\n",
    "    \"start_line\": 1,\n",
    "    \"end_line\": 1,\n",
    "    \"label\": \"... code of block ...\",\n",
    "    \"successors\": [\n",
    "      {{\n",
    "        \"id\": 2,\n",
    "        \"start_line\": 2,\n",
    "        \"end_line\": 3,\n",
    "        \"label\": \"... code of block ...\",\n",
    "        \"successors\": [...]\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "   - id: an integer starting from 1, incrementing by 1 for each block.\n",
    "   - start_line: the first line number (from the input) that belongs to this block.\n",
    "   - end_line: the last line number (from the input) that belongs to this block.\n",
    "   - label: the exact code snippet (all lines) inside this block, unchanged from the input lines.\n",
    "   - successors: a list of nested blocks that may execute after this block. Each item in this list is itself a block with the same structure: \"id\", \"start_line\", \"end_line\", \"label\", and \"successors\".\n",
    "\n",
    "4. Branch Structures:\n",
    "   - if-else: for if condition: ... else: ..., both the if body and the else body should be separate blocks. The if block’s \"successors\" should include both branches as nested block objects.\n",
    "   - for-while: the loop body and the statement(s) following the loop should be in different blocks, with correct flow back to the loop condition if it continues, or forward to the next block if it terminates.\n",
    "   - try-except-finally: each try, except, and finally block should be identified separately, showing normal and exceptional flows in successors.\n",
    "   - with-as: the code inside the with statement and the code after the with block should be separate blocks.\n",
    "   - match-case: treat each case body as a separate nested block in successors.\n",
    "   - break-continue-return: these statements jump to outside of the loop, back to the loop condition, or end the function. If the function ends, successors can be an empty list.\n",
    "\n",
    "5. Final Output:\n",
    "   - Ensure your output is valid JSON (only one root object, containing \"blocks\").\n",
    "   - Do not add extra text or explanation—only the JSON object itself.\n",
    "   - Each block's start_line and end_line must map correctly back to the lineno values in the input JSON array.\n",
    "\n",
    "Your task: Parse the input line-array, identify all basic blocks with correct start_line, end_line, and label, then produce a single JSON object with the structure above.\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "def get_single_block_cfg(code_block, program_language):\n",
    "    \"\"\"\n",
    "    调用 LLM 获取当前 code_block 的 CFG。这里的 simplified_code 是行号和文本的数组。\n",
    "    \"\"\"\n",
    "    line_array = code_block.get(\"simplified_code\", [])\n",
    "    if not line_array:\n",
    "        code_block[\"blocks\"] = []\n",
    "        return\n",
    "\n",
    "    prompt = get_code_cfg_prompt(line_array, program_language)\n",
    "    response = get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True)\n",
    "    blocks_json = json.loads(response)\n",
    "    code_block[\"blocks\"] = blocks_json.get(\"blocks\", [])\n",
    "\n",
    "\n",
    "def recursive_get_each_block_cfg(code_block, program_language):\n",
    "    \"\"\"\n",
    "    递归获取每个代码块（文件级、类级、函数级）的 CFG。\n",
    "    \"\"\"\n",
    "    get_single_block_cfg(code_block, program_language)\n",
    "    for cls in code_block.get(\"classes\", []):\n",
    "        recursive_get_each_block_cfg(cls, program_language)\n",
    "    for func in code_block.get(\"functions\", []):\n",
    "        recursive_get_each_block_cfg(func, program_language)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    演示入口。根据实际需求修改 source_code_dir 和文件范围等。\n",
    "    这里仅示例对 python 文件进行处理，并将结果输出到 JSON。\n",
    "    \"\"\"\n",
    "    source_code_dir = \"../../dataset/python\"\n",
    "    target_dir = \"llm_cfg_with_line_no\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # 示例：我们只处理 0.py ~ 199.py 这 200 个文件\n",
    "    files = []\n",
    "    for i in range(200):\n",
    "        py_file = os.path.join(source_code_dir, f\"{i}.py\")\n",
    "        out_file = os.path.join(target_dir, f\"{i}.json\")\n",
    "        files.append((py_file, out_file))\n",
    "\n",
    "    def process_single_file(source_file, target_file):\n",
    "        if not os.path.exists(source_file):\n",
    "            return\n",
    "        if os.path.exists(target_file):\n",
    "            # 如果目标文件已存在，可以选择跳过，或覆盖，按需决定\n",
    "            return\n",
    "\n",
    "        print(\"Processing\", source_file)\n",
    "        with open(source_file, 'r', encoding='utf-8') as f:\n",
    "            code = f.read()\n",
    "\n",
    "        # Step 1: 让大模型找出所有类 / 函数（包含嵌套）\n",
    "        step1_result = process_file_with_chain_of_thought(code, \"python\")\n",
    "\n",
    "        # Step 2: 给每个类 / 函数（及顶层）提取 simplified code（排除嵌套代码行，但保留原始行号）\n",
    "        recursive_get_code_by_line_range(step1_result, code)\n",
    "\n",
    "        # Step 3: 对每个简化后的代码块，调用 LLM 生成 CFG\n",
    "        recursive_get_each_block_cfg(step1_result, \"python\")\n",
    "\n",
    "        # 可选：去重逻辑，避免出现重复的 blocks\n",
    "        def remove_duplicate_blocks(code_block):\n",
    "            \"\"\"\n",
    "            删除同一层级中 (start_line, end_line) 相同的重复块，仅保留最前面一个\n",
    "            \"\"\"\n",
    "            if \"blocks\" in code_block:\n",
    "                seen = set()\n",
    "                unique_blocks = []\n",
    "                for blk in code_block[\"blocks\"]:\n",
    "                    s_line = blk.get(\"start_line\", -1)\n",
    "                    e_line = blk.get(\"end_line\", -1)\n",
    "                    key = (s_line, e_line)\n",
    "                    if key not in seen:\n",
    "                        seen.add(key)\n",
    "                        unique_blocks.append(blk)\n",
    "                code_block[\"blocks\"] = unique_blocks\n",
    "\n",
    "            for sub_cls in code_block.get(\"classes\", []):\n",
    "                remove_duplicate_blocks(sub_cls)\n",
    "            for sub_func in code_block.get(\"functions\", []):\n",
    "                remove_duplicate_blocks(sub_func)\n",
    "\n",
    "        remove_duplicate_blocks(step1_result)\n",
    "\n",
    "        # 输出到 JSON\n",
    "        with open(target_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "            json.dump(step1_result, fout, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # 多线程处理所有文件（可单线程执行以更好查看输出）\n",
    "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_single_file, src, tgt) for src, tgt in files]\n",
    "        for _ in tqdm(as_completed(futures), total=len(files), desc=\"处理CFG文件\"):\n",
    "            pass\n",
    "\n",
    "    # process_single_file(*files[0])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM生成的代码可能可以合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 95.json\n",
      "Error processing 95.json: 'label'\n",
      "Processing 110.json\n",
      "Processing 160.json\n",
      "Processing 94.json\n",
      "Processing 38.json\n",
      "Processing 21.json\n",
      "Error processing 21.json: 'list' object has no attribute 'strip'\n",
      "Processing 187.json\n",
      "Processing 121.json\n",
      "Processing 72.json\n",
      "Processing 132.json\n",
      "Processing 67.json\n",
      "Processing 149.json\n",
      "Processing 147.json\n",
      "Processing 135.json\n",
      "Processing 4.json\n",
      "Error processing 4.json: 'list' object has no attribute 'strip'\n",
      "Processing 74.json\n",
      "Processing 116.json\n",
      "Processing 40.json\n",
      "Processing 178.json\n",
      "Processing 14.json\n",
      "Processing 7.json\n",
      "Processing 166.json\n",
      "Processing 31.json\n",
      "Processing 17.json\n",
      "Processing 167.json\n",
      "Processing 107.json\n",
      "Processing 156.json\n",
      "Processing 89.json\n",
      "Processing 183.json\n",
      "Processing 193.json\n",
      "Processing 176.json\n",
      "Processing 162.json\n",
      "Processing 80.json\n",
      "Processing 136.json\n",
      "Processing 171.json\n",
      "Processing 98.json\n",
      "Processing 106.json\n",
      "Processing 141.json\n",
      "Processing 152.json\n",
      "Processing 96.json\n",
      "Processing 123.json\n",
      "Processing 28.json\n",
      "Processing 150.json\n",
      "Processing 45.json\n",
      "Processing 13.json\n",
      "Processing 169.json\n",
      "Processing 175.json\n",
      "Processing 198.json\n",
      "Processing 2.json\n",
      "Processing 90.json\n",
      "Processing 177.json\n",
      "Processing 34.json\n",
      "Processing 37.json\n",
      "Processing 138.json\n",
      "Processing 82.json\n",
      "Processing 105.json\n",
      "Processing 122.json\n",
      "Error processing 122.json: 'list' object has no attribute 'strip'\n",
      "Processing 163.json\n",
      "Processing 24.json\n",
      "Processing 73.json\n",
      "Processing 79.json\n",
      "Processing 190.json\n",
      "Processing 118.json\n",
      "Processing 87.json\n",
      "Processing 53.json\n",
      "Processing 54.json\n",
      "Processing 59.json\n",
      "Processing 85.json\n",
      "Processing 194.json\n",
      "Processing 188.json\n",
      "Processing 71.json\n",
      "Processing 8.json\n",
      "Processing 10.json\n",
      "Processing 26.json\n",
      "Processing 164.json\n",
      "Processing 185.json\n",
      "Processing 33.json\n",
      "Processing 88.json\n",
      "Processing 104.json\n",
      "Processing 179.json\n",
      "Processing 199.json\n",
      "Processing 39.json\n",
      "Processing 195.json\n",
      "Processing 102.json\n",
      "Processing 3.json\n",
      "Processing 130.json\n",
      "Processing 42.json\n",
      "Processing 157.json\n",
      "Processing 154.json\n",
      "Processing 56.json\n",
      "Processing 117.json\n",
      "Processing 50.json\n",
      "Processing 48.json\n",
      "Processing 91.json\n",
      "Processing 93.json\n",
      "Processing 184.json\n",
      "Processing 81.json\n",
      "Processing 20.json\n",
      "Processing 0.json\n",
      "Processing 197.json\n",
      "Processing 182.json\n",
      "Processing 140.json\n",
      "Processing 151.json\n",
      "Processing 68.json\n",
      "Processing 36.json\n",
      "Processing 131.json\n",
      "Error processing 131.json: 'label'\n",
      "Processing 16.json\n",
      "Processing 76.json\n",
      "Processing 69.json\n",
      "Processing 165.json\n",
      "Processing 25.json\n",
      "Processing 144.json\n",
      "Processing 189.json\n",
      "Processing 143.json\n",
      "Processing 75.json\n",
      "Processing 142.json\n",
      "Processing 196.json\n",
      "Processing 44.json\n",
      "Processing 66.json\n",
      "Processing 49.json\n",
      "Processing 180.json\n",
      "Processing 63.json\n",
      "Processing 114.json\n",
      "Processing 115.json\n",
      "Processing 127.json\n",
      "Processing 192.json\n",
      "Processing 186.json\n",
      "Processing 86.json\n",
      "Processing 148.json\n",
      "Processing 5.json\n",
      "Processing 22.json\n",
      "Processing 70.json\n",
      "Processing 15.json\n",
      "Processing 84.json\n",
      "Processing 27.json\n",
      "Processing 155.json\n",
      "Processing 11.json\n",
      "Processing 65.json\n",
      "Processing 139.json\n",
      "Processing 43.json\n",
      "Processing 78.json\n",
      "Processing 32.json\n",
      "Processing 174.json\n",
      "Processing 58.json\n",
      "Processing 111.json\n",
      "Processing 124.json\n",
      "Processing 51.json\n",
      "Processing 153.json\n",
      "Processing 161.json\n",
      "Processing 181.json\n",
      "Processing 173.json\n",
      "Processing 57.json\n",
      "Processing 29.json\n",
      "Processing 35.json\n",
      "Processing 83.json\n",
      "Processing 1.json\n",
      "Processing 145.json\n",
      "Processing 137.json\n",
      "Processing 103.json\n",
      "Processing 159.json\n",
      "Processing 134.json\n",
      "Processing 126.json\n",
      "Processing 109.json\n",
      "Processing 41.json\n",
      "Processing 168.json\n",
      "Processing 19.json\n",
      "Processing 92.json\n",
      "Processing 52.json\n",
      "Processing 64.json\n",
      "Processing 18.json\n",
      "Processing 55.json\n",
      "Processing 112.json\n",
      "Processing 191.json\n",
      "Processing 46.json\n",
      "Processing 128.json\n",
      "Processing 6.json\n",
      "Processing 129.json\n",
      "Processing 60.json\n",
      "Processing 47.json\n",
      "Processing 77.json\n",
      "Processing 119.json\n",
      "Processing 9.json\n",
      "Processing 99.json\n",
      "Processing 113.json\n",
      "Processing 100.json\n",
      "Processing 62.json\n",
      "Processing 61.json\n",
      "Processing 12.json\n",
      "Processing 158.json\n",
      "Error processing 158.json: 'id'\n",
      "Processing 125.json\n",
      "Processing 30.json\n",
      "Processing 23.json\n",
      "Processing 170.json\n",
      "Processing 146.json\n",
      "Processing 172.json\n",
      "Processing 108.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def process_cfg(cfg):\n",
    "    \"\"\"\n",
    "    Process a CFG that uses a *nested successors* structure.\n",
    "    We will:\n",
    "      1. Remove unreachable blocks (only keep blocks reachable from the root).\n",
    "      2. Separate loop headers from loop bodies (if desired).\n",
    "      3. Merge consecutive linear blocks that have only one successor and one predecessor.\n",
    "      4. Recursively process functions/classes if they exist.\n",
    "    \"\"\"\n",
    "\n",
    "    #=== 1. 过滤不可达节点: 我们假设 blocks[0] 是 CFG 的根节点 ===#\n",
    "    def filter_connected_blocks(blocks):\n",
    "        \"\"\"\n",
    "        Given a list of blocks (in nested form), return only those reachable\n",
    "        from the 'root' block (which we assume is blocks[0]) by traversing\n",
    "        nested successors.\n",
    "        \"\"\"\n",
    "\n",
    "        visited_ids = set()\n",
    "        # 为了方便在后面快速通过 id 找到对应的 block 对象，我们先做一个 {id: block} 的映射\n",
    "        # 同时存储所有 block 的引用（因为是嵌套的，需要把内部 successors 里的 block 也加入到此映射）\n",
    "        id_to_block = {}\n",
    "\n",
    "        def collect_all_blocks(block_list):\n",
    "            for b in block_list:\n",
    "                id_to_block[b[\"id\"]] = b\n",
    "                if \"successors\" in b:\n",
    "                    collect_all_blocks(b[\"successors\"])\n",
    "\n",
    "        collect_all_blocks(blocks)\n",
    "\n",
    "        # 深度优先搜索，查找所有可达节点\n",
    "        def dfs(block):\n",
    "            if block[\"id\"] in visited_ids:\n",
    "                return\n",
    "            visited_ids.add(block[\"id\"])\n",
    "            for succ_block in block.get(\"successors\", []):\n",
    "                dfs(succ_block)\n",
    "\n",
    "        # 假定 blocks[0] 是 root\n",
    "        if blocks:\n",
    "            root_block = blocks[0]\n",
    "            dfs(root_block)\n",
    "\n",
    "        # 现在我们只保留被 visited_ids 覆盖到的节点，并且需要“剪枝”不在 visited_ids 中的后继\n",
    "        def filter_nested(block_list):\n",
    "            \"\"\"在嵌套结构中移除不可达节点。\"\"\"\n",
    "            filtered = []\n",
    "            for b in block_list:\n",
    "                if b[\"id\"] in visited_ids:\n",
    "                    # 递归处理 successors\n",
    "                    new_successors = filter_nested(b.get(\"successors\", []))\n",
    "                    filtered.append({\n",
    "                        \"id\": b[\"id\"],\n",
    "                        \"label\": b[\"label\"],\n",
    "                        \"successors\": new_successors\n",
    "                    })\n",
    "            return filtered\n",
    "\n",
    "        return filter_nested(blocks)\n",
    "\n",
    "    #=== 2. 判断循环头（示例仅以 \"for\" / \"while\" 关键字简单判断） ===#\n",
    "    def is_loop_header(block):\n",
    "        \"\"\"\n",
    "        A naive check: if the block's label starts with 'for' or 'while'\n",
    "        (or contains those keywords in a relevant way), treat it as a loop header.\n",
    "        \"\"\"\n",
    "        code_str = block[\"label\"].strip()\n",
    "        if code_str.startswith(\"for \") or code_str.startswith(\"while \"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    #=== 3. 合并逻辑（循环头和循环体暂时不做复杂拆分，仅演示思路） ===#\n",
    "    # 在嵌套结构中，“连续的线性块”通常表现为一个 block 有且仅有 1 个 successor，且该 successor 只有该一个 predecessor。\n",
    "    # 但是在嵌套结构里，我们无法简易地统计 predecessor 数量，需要自行设计。\n",
    "    #\n",
    "    # 示例逻辑：深度遍历 + 遇到 loop header 不合并；遇到多 successor 不合并；否则合并到下一个 block。\n",
    "    def merge_blocks_in_place(block):\n",
    "        \"\"\"\n",
    "        递归地合并一个 block 的线性后继。\n",
    "        当遇到循环头或分支时，不再合并。\n",
    "        \"\"\"\n",
    "        successors = block.get(\"successors\", [])\n",
    "        if not successors:\n",
    "            # 无后继，直接返回\n",
    "            return block\n",
    "\n",
    "        # 如果存在多个 successor，说明是分支点，不合并任何后继\n",
    "        if len(successors) > 1:\n",
    "            # 递归处理每个 successor\n",
    "            for i, succ in enumerate(successors):\n",
    "                successors[i] = merge_blocks_in_place(succ)\n",
    "            block[\"successors\"] = successors\n",
    "            return block\n",
    "\n",
    "        # 如果只有 1 个 successor，则尝试合并\n",
    "        single_succ = successors[0]\n",
    "        if is_loop_header(block):\n",
    "            # 如果当前 block 是 loop header，不向后合并，只是递归处理后继\n",
    "            block[\"successors\"][0] = merge_blocks_in_place(single_succ)\n",
    "            return block\n",
    "        if is_loop_header(single_succ):\n",
    "            # 如果后继是 loop header，也不合并，只是递归处理后继\n",
    "            block[\"successors\"][0] = merge_blocks_in_place(single_succ)\n",
    "            return block\n",
    "\n",
    "        # 到这里，意味着我们可以把 single_succ 跟当前块合并\n",
    "        block[\"label\"] = block[\"label\"] + \"\\n\" + single_succ[\"label\"]\n",
    "        # 把 single_succ 的 successors 赋给当前块\n",
    "        block[\"successors\"] = single_succ.get(\"successors\", [])\n",
    "\n",
    "        # 递归处理“合并后”依然存在的后继（可能还是一个 list）\n",
    "        if block[\"successors\"]:\n",
    "            new_succ_list = []\n",
    "            for succ in block[\"successors\"]:\n",
    "                new_succ_list.append(merge_blocks_in_place(succ))\n",
    "            block[\"successors\"] = new_succ_list\n",
    "\n",
    "        return block\n",
    "\n",
    "    #=== 4. 针对最外层的 blocks 做处理 ===#\n",
    "    #  4.1 过滤掉不可达节点\n",
    "    if \"blocks\" in cfg:\n",
    "        cfg[\"blocks\"] = filter_connected_blocks(cfg[\"blocks\"])\n",
    "\n",
    "    #  4.2 合并块：因为是多 block，需要逐个处理，然后再把处理结果放回 cfg[\"blocks\"] \n",
    "    #      同时，新的根块可能因为合并也会改变，所以我们需要重新搜集并替换\n",
    "    if \"blocks\" in cfg and cfg[\"blocks\"]:\n",
    "        merged = []\n",
    "        for b in cfg[\"blocks\"]:\n",
    "            merged_block = merge_blocks_in_place(b)\n",
    "            merged.append(merged_block)\n",
    "        cfg[\"blocks\"] = merged\n",
    "\n",
    "    #=== 5. 递归处理 functions 与 classes ===#\n",
    "    if \"functions\" in cfg:\n",
    "        for func in cfg[\"functions\"]:\n",
    "            process_cfg(func)\n",
    "\n",
    "    if \"classes\" in cfg:\n",
    "        for cls in cfg[\"classes\"]:\n",
    "            process_cfg(cls)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "#=============================\n",
    "# 下面是示例读取并处理文件的逻辑\n",
    "#=============================\n",
    "import os\n",
    "import json\n",
    "\n",
    "for file in os.listdir(\"llm_cfg_with_line_no\"):\n",
    "    path = os.path.join(\"llm_cfg_with_line_no\", file)\n",
    "    if not os.path.isfile(path):\n",
    "        continue\n",
    "\n",
    "    print(\"Processing\", file)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            llm_cfg = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        process_cfg(llm_cfg)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    os.makedirs(\"merged_llm_cfg_with_line_no\", exist_ok=True)\n",
    "    output_path = os.path.join(\"merged_llm_cfg_with_line_no\", file)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(llm_cfg, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0.json\n",
      "Saved figure: visualized_llm_cfg/0/top.png\n",
      "Saved figure: visualized_llm_cfg/0/func_create_test_user.png\n",
      "Saved figure: visualized_llm_cfg/0/func_create_test_graph.png\n",
      "Saved figure: visualized_llm_cfg/0/func_sample_agent.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 如果你用 pygraphviz：\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "# 如果要用 pydot，请改为：\n",
    "# from networkx.drawing.nx_pydot import pydot_layout\n",
    "\n",
    "def flatten_cfg(blocks):\n",
    "    \"\"\"\n",
    "    将嵌套结构的 CFG 转换为 (nodes, edges) 两个列表：\n",
    "      - nodes: [(id, label), ...]\n",
    "      - edges: [(id1, id2), ...] 表示从 id1 -> id2 的有向边\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    def dfs(block):\n",
    "        # 将当前块加入 nodes\n",
    "        nodes.append((block[\"id\"], block[\"label\"]))\n",
    "        # 遍历 successors\n",
    "        for succ in block.get(\"successors\", []):\n",
    "            edges.append((block[\"id\"], succ[\"id\"]))\n",
    "            dfs(succ)\n",
    "\n",
    "    for b in blocks:\n",
    "        dfs(b)\n",
    "\n",
    "    # 去重\n",
    "    nodes = list(dict.fromkeys(nodes))\n",
    "    edges = list(dict.fromkeys(edges))\n",
    "    return nodes, edges\n",
    "\n",
    "def visualize_cfg_blocks(blocks, title=\"CFG\", output_path=None, rankdir=\"TB\"):\n",
    "    \"\"\"\n",
    "    使用 Graphviz (dot) 分层布局，可视化嵌套CFG的 blocks，\n",
    "    并画出带箭头的有向图。这里 label 显示完整，无省略。\n",
    "    \n",
    "    参数:\n",
    "      - blocks: 一个 list，形如 [ {id, label, successors: [...]}, ...]\n",
    "      - title: 生成图片的标题\n",
    "      - output_path: 若指定则保存到该文件，否则 plt.show()\n",
    "      - rankdir: \"TB\"（自上而下）或 \"LR\"（自左向右）\n",
    "    \"\"\"\n",
    "    # 1) flatten\n",
    "    nodes, edges = flatten_cfg(blocks)\n",
    "\n",
    "    # 2) 构建有向图\n",
    "    G = nx.DiGraph()\n",
    "    for node_id, label in nodes:\n",
    "        G.add_node(node_id, label=label)\n",
    "    for src, dst in edges:\n",
    "        G.add_edge(src, dst)\n",
    "\n",
    "    # 3) 不再截断 label，全部显示\n",
    "    labels_dict = {}\n",
    "    for node_id, data in G.nodes(data=True):\n",
    "        full_label = data[\"label\"] or \"\"\n",
    "        # 可以考虑在字符串中替换换行符，避免节点过于高\n",
    "        # full_label = full_label.replace(\"\\n\", \"\\\\n\")\n",
    "        # 或者直接原样显示（某些布局器可能会把换行符当作字符串，而不会自动换行）\n",
    "        labels_dict[node_id] = f\"{node_id}: {full_label}\"\n",
    "\n",
    "    # 4) 使用 dot 布局，指定 rankdir\n",
    "    # prog=\"dot\" 对应分层布局，-Grankdir 控制自上而下(TB)或自左向右(LR)\n",
    "    pos = graphviz_layout(G, prog=\"dot\", args=f\"-Grankdir={rankdir}\")\n",
    "\n",
    "    # 5) 绘图 (带箭头)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=\"lightblue\", node_size=1800, edgecolors=\"black\")\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos,\n",
    "        arrows=True,\n",
    "        arrowstyle=\"->\",\n",
    "        arrowsize=15,\n",
    "        connectionstyle=\"arc3,rad=0.1\"\n",
    "    )\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels_dict, font_size=8)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"Saved figure: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def visualize_cfg_separately(cfg_data, base_name=\"cfg\", output_dir=\"visualized_llm_cfg\", rankdir=\"TB\"):\n",
    "    \"\"\"\n",
    "    将同一个文件中的顶层 blocks、每个 function 的 blocks、每个 class 的 blocks\n",
    "    分别绘制不同子图，存成多个文件。文件保存在 output_dir/base_name/ 下。\n",
    "    \n",
    "    参数:\n",
    "      - cfg_data: 形如 {\"blocks\": [...], \"functions\": [...], \"classes\": [...]} 的字典\n",
    "      - base_name: 例如 \"0\"、\"1\" 等文件名前缀\n",
    "      - output_dir: 例如 \"visualized_llm_cfg\"\n",
    "      - rankdir: \"TB\"（自上而下）或 \"LR\"（自左向右）\n",
    "    \"\"\"\n",
    "    # 先创建子目录: visualized_llm_cfg/0/ 之类\n",
    "    sub_dir = os.path.join(output_dir, base_name)\n",
    "    os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "    # 1) 绘制顶层 blocks\n",
    "    if \"blocks\" in cfg_data and cfg_data[\"blocks\"]:\n",
    "        out_path = os.path.join(sub_dir, f\"top.png\")\n",
    "        visualize_cfg_blocks(\n",
    "            cfg_data[\"blocks\"],\n",
    "            title=f\"{base_name}_top\",\n",
    "            output_path=out_path,\n",
    "            rankdir=rankdir\n",
    "        )\n",
    "\n",
    "    # 2) 绘制每个 function\n",
    "    if \"functions\" in cfg_data and cfg_data[\"functions\"]:\n",
    "        for i, func in enumerate(cfg_data[\"functions\"], start=1):\n",
    "            func_name = func.get(\"name\", f\"func_{i}\")\n",
    "            out_path = os.path.join(sub_dir, f\"func_{func_name}.png\")\n",
    "            if \"blocks\" in func and func[\"blocks\"]:\n",
    "                visualize_cfg_blocks(\n",
    "                    func[\"blocks\"],\n",
    "                    title=f\"{base_name}_func_{func_name}\",\n",
    "                    output_path=out_path,\n",
    "                    rankdir=rankdir\n",
    "                )\n",
    "\n",
    "    # 3) 绘制每个 class\n",
    "    if \"classes\" in cfg_data and cfg_data[\"classes\"]:\n",
    "        for i, cls in enumerate(cfg_data[\"classes\"], start=1):\n",
    "            cls_name = cls.get(\"name\", f\"class_{i}\")\n",
    "            out_path = os.path.join(sub_dir, f\"class_{cls_name}.png\")\n",
    "            if \"blocks\" in cls and cls[\"blocks\"]:\n",
    "                visualize_cfg_blocks(\n",
    "                    cls[\"blocks\"],\n",
    "                    title=f\"{base_name}_class_{cls_name}\",\n",
    "                    output_path=out_path,\n",
    "                    rankdir=rankdir\n",
    "                )\n",
    "\n",
    "def visualize_cfg_file_separately(file_path, output_dir=\"visualized_llm_cfg\", rankdir=\"TB\"):\n",
    "    \"\"\"\n",
    "    给定一个 JSON 文件，分顶层/函数/类分别绘图并保存。\n",
    "    目标目录: output_dir/<文件名前缀>/...\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        llm_cfg = json.load(f)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    visualize_cfg_separately(\n",
    "        llm_cfg,\n",
    "        base_name=base_name,\n",
    "        output_dir=output_dir,\n",
    "        rankdir=rankdir\n",
    "    )\n",
    "\n",
    "# ====== 示例：批量处理 \"merged_llm_cfg\" 文件夹下的所有 JSON 文件 ====== #\n",
    "if __name__ == \"__main__\":\n",
    "    folder = \"merged_llm_cfg\"\n",
    "    for file_name in os.listdir(folder):\n",
    "        if not file_name.lower().endswith(\".json\"):\n",
    "            continue\n",
    "        path = os.path.join(folder, file_name)\n",
    "        print(\"Processing:\", file_name)\n",
    "        try:\n",
    "            # 用 rankdir=\"TB\" 自上而下布局；可改成 rankdir=\"LR\" 自左向右。\n",
    "            visualize_cfg_file_separately(path, output_dir=\"visualized_llm_cfg\", rankdir=\"TB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CFG文件: 100%|██████████| 48/48 [00:18<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic Evaluation Summary:\n",
      "Total CFGs compared: 48\n",
      "Average Edge Coverage: 0.52\n",
      "Average Content Similarity: 0.00\n",
      "Average Structure Similarity: 0.83\n",
      "\n",
      "LLM Evaluation Summary:\n",
      "Average Structure Similarity: 0.84\n",
      "Average Content Similarity: 0.86\n",
      "Average Total Similarity: 0.85\n",
      "Reasonable Percentage: 77.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from llm import get_llm_answers\n",
    "\n",
    "def compare_cfg_similarity(llm_cfg, static_cfg):\n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "You are a CFG evaluator to evaluate whether the generated CFG is correct based on the static CFG.\n",
    "\n",
    "You should first compare the structure of the CFG, then compare the content of the CFG. Focus on the flow of the CFG and ignore the details such as content and block_id, block_name.\n",
    "\n",
    "Your output should be a json with the following format:\n",
    "{{\n",
    "    \"reasonable\": true/false,\n",
    "    \"structure_similarity\": 0.8,\n",
    "    \"content_similarity\": 0.9,\n",
    "    \"total_similarity\": 0.85,\n",
    "    \"reason\": \"\"\n",
    "}}\n",
    "\n",
    "Ground truth:\n",
    "{static_cfg}\n",
    "\n",
    "Generated CFG:\n",
    "{llm_cfg}\n",
    "\"\"\"\n",
    "            similarity = json.loads(get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True))\n",
    "            return similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                raise e\n",
    "            print(f\"重试第{retry_count}次,错误信息:{str(e)}\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class CFGSimilarityResult:\n",
    "    \"\"\"存储CFG比较结果的数据类\"\"\"\n",
    "    filename: str\n",
    "    edge_coverage: float\n",
    "    content_similarity: float\n",
    "    structure_similarity: float\n",
    "    matched_edges: int\n",
    "    gt_edges: int\n",
    "    llm_edges: int\n",
    "    nested_results: Optional[Dict[str, 'CFGSimilarityResult']] = None\n",
    "    llm_similarity: Optional[Dict[str, Union[float, bool]]] = None\n",
    "\n",
    "class CFGComparator:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化CFG比较器\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_edges(cfg: Dict) -> int:\n",
    "        \"\"\"递归计算CFG中的边数量\"\"\"\n",
    "        edge_count = sum(\n",
    "            len(block.get(\"successors\", []))\n",
    "            for block in cfg.get(\"blocks\", [])\n",
    "        )\n",
    "        \n",
    "        # 递归计算嵌套CFG的边\n",
    "        for func in cfg.get(\"functions\", []):  # functions是列表\n",
    "            edge_count += CFGComparator.count_edges(func)\n",
    "        for cls in cfg.get(\"classes\", []):     # classes是列表\n",
    "            edge_count += CFGComparator.count_edges(cls)\n",
    "            \n",
    "        return edge_count\n",
    "\n",
    "    def structure_similarity(self, llm_cfg: Dict, static_cfg: Dict) -> float:\n",
    "        \"\"\"计算两个CFG的结构相似度\"\"\"\n",
    "        # 获取两个CFG的blocks\n",
    "        llm_blocks = llm_cfg.get(\"blocks\", [])\n",
    "        static_blocks = static_cfg.get(\"blocks\", [])\n",
    "        \n",
    "        # 如果两个CFG都没有blocks，返回1.0\n",
    "        if not llm_blocks and not static_blocks:\n",
    "            return 1.0\n",
    "        # 如果其中一个没有blocks，返回0.0\n",
    "        if not llm_blocks or not static_blocks:\n",
    "            return 0.0\n",
    "            \n",
    "        # 计算边的匹配度\n",
    "        llm_edges = sum(len(block.get(\"successors\", [])) for block in llm_blocks)\n",
    "        static_edges = sum(len(block.get(\"successors\", [])) for block in static_blocks)\n",
    "        \n",
    "        if llm_edges == 0 and static_edges == 0:\n",
    "            return 1.0\n",
    "        if llm_edges == 0 or static_edges == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # 使用边数的比例计算相似度\n",
    "        return min(llm_edges, static_edges) / max(llm_edges, static_edges)\n",
    "\n",
    "    def content_similarity(self, llm_cfg: Dict, static_cfg: Dict) -> float:\n",
    "        \"\"\"计算两个CFG的内容相似度\"\"\"\n",
    "        # 获取简化的代码内容\n",
    "        llm_code = llm_cfg.get(\"simplified_code\", \"\")\n",
    "        static_code = static_cfg.get(\"simplified_code\", \"\")\n",
    "        \n",
    "        # 如果两个都为空，返回1.0\n",
    "        if not llm_code and not static_code:\n",
    "            return 1.0\n",
    "        # 如果其中一个为空，返回0.0\n",
    "        if not llm_code or not static_code:\n",
    "            return 0.0\n",
    "        \n",
    "        # 将代码分割成行并去除空白行\n",
    "        llm_lines = [line.strip() for line in llm_code.split(\"\\n\") if line.strip()]\n",
    "        static_lines = [line.strip() for line in static_code.split(\"\\n\") if line.strip()]\n",
    "        \n",
    "        # 计算行的匹配度\n",
    "        common_lines = set(llm_lines) & set(static_lines)\n",
    "        return len(common_lines) / max(len(llm_lines), len(static_lines))\n",
    "\n",
    "    def compare_cfgs(self, llm_cfg: Dict, static_cfg: Dict, name: str) -> CFGSimilarityResult:\n",
    "        \"\"\"递归比较两个CFG并返回相似度结果\"\"\"\n",
    "        # 计算当前层级的相似度\n",
    "        structure_sim = self.structure_similarity(llm_cfg, static_cfg)\n",
    "        content_sim = self.content_similarity(llm_cfg, static_cfg)\n",
    "        \n",
    "        # 计算边的统计信息\n",
    "        gt_edges = self.count_edges(static_cfg)\n",
    "        llm_edges = self.count_edges(llm_cfg)\n",
    "        matched_edges = int(structure_sim * min(gt_edges, llm_edges))\n",
    "        edge_coverage = matched_edges / gt_edges if gt_edges > 0 else 0\n",
    "        \n",
    "        # 递归比较嵌套的CFG\n",
    "        nested_results = {}\n",
    "        \n",
    "        # 比较函数CFG\n",
    "        llm_functions = {f[\"name\"]: f for f in llm_cfg.get(\"functions\", [])}\n",
    "        static_functions = {f[\"name\"]: f for f in static_cfg.get(\"functions\", [])}\n",
    "        common_functions = set(llm_functions.keys()) & set(static_functions.keys())\n",
    "        \n",
    "        for func_name in common_functions:\n",
    "            nested_results[f\"function_{func_name}\"] = self.compare_cfgs(\n",
    "                llm_functions[func_name],\n",
    "                static_functions[func_name],\n",
    "                func_name\n",
    "            )\n",
    "        \n",
    "        # 比较类CFG\n",
    "        llm_classes = {c[\"name\"]: c for c in llm_cfg.get(\"classes\", [])}\n",
    "        static_classes = {c[\"name\"]: c for c in static_cfg.get(\"classes\", [])}\n",
    "        common_classes = set(llm_classes.keys()) & set(static_classes.keys())\n",
    "        \n",
    "        for class_name in common_classes:\n",
    "            nested_results[f\"class_{class_name}\"] = self.compare_cfgs(\n",
    "                llm_classes[class_name],\n",
    "                static_classes[class_name],\n",
    "                class_name\n",
    "            )\n",
    "        \n",
    "        return CFGSimilarityResult(\n",
    "            filename=name,\n",
    "            edge_coverage=edge_coverage,\n",
    "            content_similarity=content_sim,\n",
    "            structure_similarity=structure_sim,\n",
    "            matched_edges=matched_edges,\n",
    "            gt_edges=gt_edges,\n",
    "            llm_edges=llm_edges,\n",
    "            nested_results=nested_results if nested_results else None,\n",
    "            llm_similarity=None  # 将在process_file中设置\n",
    "        )\n",
    "\n",
    "class CFGEvaluator:\n",
    "    def __init__(self, llm_cfg_dir: str, static_cfg_dir: str, result_file: str):\n",
    "        \"\"\"初始化评估器\n",
    "        \n",
    "        Args:\n",
    "            llm_cfg_dir: LLM生成的CFG文件目录\n",
    "            static_cfg_dir: 静态分析生成的CFG文件目录\n",
    "            result_file: 结果保存文件路径\n",
    "        \"\"\"\n",
    "        self.llm_cfg_dir = Path(llm_cfg_dir)\n",
    "        self.static_cfg_dir = Path(static_cfg_dir)\n",
    "        self.result_file = Path(result_file)\n",
    "        self.comparator = CFGComparator()\n",
    "        self.results = []  # 存储所有结果\n",
    "    \n",
    "    def process_file(self, llm_cfg_path: Path) -> Optional[CFGSimilarityResult]:\n",
    "        \"\"\"处理单个CFG文件对的比较\n",
    "        \n",
    "        Args:\n",
    "            llm_cfg_path: LLM生成的CFG文件路径\n",
    "            \n",
    "        Returns:\n",
    "            CFGSimilarityResult 或 None（如果没有对应的静态CFG文件）\n",
    "        \"\"\"\n",
    "        # 获取对应的静态CFG文件路径\n",
    "        static_cfg_path = self.static_cfg_dir / llm_cfg_path.name\n",
    "        if not static_cfg_path.exists():\n",
    "            return None\n",
    "            \n",
    "        # 读取CFG文件\n",
    "        with open(llm_cfg_path) as f:\n",
    "            llm_cfg = json.load(f)\n",
    "        with open(static_cfg_path) as f:\n",
    "            static_cfg = json.load(f)\n",
    "            \n",
    "        # 比较CFG\n",
    "        result = self.comparator.compare_cfgs(llm_cfg, static_cfg, llm_cfg_path.name)\n",
    "        llm_sim = compare_cfg_similarity(llm_cfg, static_cfg)\n",
    "        result.llm_similarity = llm_sim\n",
    "        \n",
    "        # 将结果添加到列表并保存\n",
    "        self.results.append(result)\n",
    "        self.save_results()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"保存当前所有结果到文件\"\"\"\n",
    "        with open(self.result_file, \"w\") as f:\n",
    "            json.dump(\n",
    "                [self._result_to_dict(r) for r in self.results],\n",
    "                f,\n",
    "                indent=2\n",
    "            )\n",
    "    \n",
    "    def evaluate_all(self) -> List[CFGSimilarityResult]:\n",
    "        \"\"\"评估所有CFG文件对\n",
    "        \n",
    "        Returns:\n",
    "            所有比较结果的列表\n",
    "        \"\"\"\n",
    "        # 处理每个LLM生成的CFG文件\n",
    "        llm_cfg_paths = list(self.llm_cfg_dir.glob(\"*.json\"))\n",
    "        \n",
    "        # 使用多线程并行处理\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for llm_cfg_path in llm_cfg_paths:\n",
    "                future = executor.submit(self.process_file, llm_cfg_path)\n",
    "                futures.append(future)\n",
    "            \n",
    "            # 使用tqdm显示进度\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"处理CFG文件\"):\n",
    "                future.result()\n",
    "                \n",
    "        return self.results\n",
    "    \n",
    "    @staticmethod\n",
    "    def _result_to_dict(result: CFGSimilarityResult) -> Dict:\n",
    "        \"\"\"将CFGSimilarityResult转换为可JSON序列化的字典\"\"\"\n",
    "        return {\n",
    "            \"filename\": result.filename,\n",
    "            \"edge_coverage\": result.edge_coverage,\n",
    "            \"content_similarity\": result.content_similarity,\n",
    "            \"structure_similarity\": result.structure_similarity,\n",
    "            \"matched_edges\": result.matched_edges,\n",
    "            \"gt_edges\": result.gt_edges,\n",
    "            \"llm_edges\": result.llm_edges,\n",
    "            \"nested_results\": {\n",
    "                k: CFGEvaluator._result_to_dict(v)\n",
    "                for k, v in result.nested_results.items()\n",
    "            } if result.nested_results else None,\n",
    "            \"llm_similarity\": result.llm_similarity\n",
    "        }\n",
    "\n",
    "def calculate_aggregate_metrics(results: List[CFGSimilarityResult]) -> Dict:\n",
    "    \"\"\"计算聚合指标\n",
    "    \n",
    "    Args:\n",
    "        results: CFGSimilarityResult列表\n",
    "        \n",
    "    Returns:\n",
    "        包含聚合指标的字典\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"total_cfgs_compared\": len(results),\n",
    "        \"average_edge_coverage\": np.mean([r.edge_coverage for r in results]),\n",
    "        \"average_content_similarity\": np.mean([r.content_similarity for r in results]),\n",
    "        \"average_structure_similarity\": np.mean([r.structure_similarity for r in results]),\n",
    "        \"total_gt_edges\": sum(r.gt_edges for r in results),\n",
    "        \"total_llm_edges\": sum(r.llm_edges for r in results),\n",
    "        \"total_matched_edges\": sum(r.matched_edges for r in results)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    evaluator = CFGEvaluator(\n",
    "        llm_cfg_dir=\"merged_llm_cfg_50\",\n",
    "        static_cfg_dir=\"static_cfg\",\n",
    "        result_file=\"evaluation_results_50.json\"\n",
    "    )\n",
    "    \n",
    "    # 评估所有CFG\n",
    "    results = evaluator.evaluate_all()\n",
    "    \n",
    "    # 计算统计指标\n",
    "    metrics = calculate_aggregate_metrics(results)\n",
    "    \n",
    "    # 计算LLM评估的平均值\n",
    "    llm_metrics = {\n",
    "        \"average_llm_structure_similarity\": np.mean([\n",
    "            r.llm_similarity[\"structure_similarity\"] \n",
    "            for r in results if r.llm_similarity\n",
    "        ]),\n",
    "        \"average_llm_content_similarity\": np.mean([\n",
    "            r.llm_similarity[\"content_similarity\"] \n",
    "            for r in results if r.llm_similarity\n",
    "        ]),\n",
    "        \"average_llm_total_similarity\": np.mean([\n",
    "            r.llm_similarity[\"total_similarity\"] \n",
    "            for r in results if r.llm_similarity\n",
    "        ]),\n",
    "        \"reasonable_percentage\": np.mean([\n",
    "            float(r.llm_similarity[\"reasonable\"]) \n",
    "            for r in results if r.llm_similarity\n",
    "        ]) * 100\n",
    "    }\n",
    "    \n",
    "    # 输出评估结果\n",
    "    print(\"\\nAutomatic Evaluation Summary:\")\n",
    "    print(f\"Total CFGs compared: {metrics['total_cfgs_compared']}\")\n",
    "    print(f\"Average Edge Coverage: {metrics['average_edge_coverage']:.2f}\")\n",
    "    print(f\"Average Content Similarity: {metrics['average_content_similarity']:.2f}\")\n",
    "    print(f\"Average Structure Similarity: {metrics['average_structure_similarity']:.2f}\")\n",
    "    \n",
    "    print(\"\\nLLM Evaluation Summary:\")\n",
    "    print(f\"Average Structure Similarity: {llm_metrics['average_llm_structure_similarity']:.2f}\")\n",
    "    print(f\"Average Content Similarity: {llm_metrics['average_llm_content_similarity']:.2f}\")\n",
    "    print(f\"Average Total Similarity: {llm_metrics['average_llm_total_similarity']:.2f}\")\n",
    "    print(f\"Reasonable Percentage: {llm_metrics['reasonable_percentage']:.1f}%\")\n",
    "    \n",
    "    # 保存完整的评估指标\n",
    "    metrics.update(llm_metrics)\n",
    "    with open(\"evaluation_metrics_50.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scalpel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
