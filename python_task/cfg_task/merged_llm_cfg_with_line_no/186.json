{
  "name": "example_script",
  "type": "CFG",
  "start_line": 1,
  "end_line": 1125,
  "functions": [
    {
      "name": "AICredentialsField",
      "type": "function",
      "start_line": 56,
      "end_line": 63,
      "functions": [],
      "classes": [],
      "simplified_code": [
        {
          "lineno": 56,
          "line": "def AICredentialsField() -> AICredentials:"
        },
        {
          "lineno": 57,
          "line": "    return CredentialsField("
        },
        {
          "lineno": 58,
          "line": "        description=\"API key for the LLM provider.\","
        },
        {
          "lineno": 59,
          "line": "        discriminator=\"model\","
        },
        {
          "lineno": 60,
          "line": "        discriminator_mapping={"
        },
        {
          "lineno": 61,
          "line": "            model.value: model.metadata.provider for model in LlmModel"
        },
        {
          "lineno": 62,
          "line": "        },"
        },
        {
          "lineno": 63,
          "line": "    )"
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "def AICredentialsField() -> AICredentials:\n    return CredentialsField(\n        description=\"API key for the LLM provider.\",\n        discriminator=\"model\",\n        discriminator_mapping={\n            model.value: model.metadata.provider for model in LlmModel\n        },\n    )",
          "successors": []
        }
      ]
    }
  ],
  "classes": [
    {
      "name": "ModelMetadata",
      "type": "class",
      "start_line": 66,
      "end_line": 68,
      "functions": [],
      "classes": [],
      "simplified_code": [
        {
          "lineno": 66,
          "line": "class ModelMetadata(NamedTuple):"
        },
        {
          "lineno": 67,
          "line": "    provider: str"
        },
        {
          "lineno": 68,
          "line": "    context_window: int"
        }
      ],
      "blocks": [
        {
          "id": 66,
          "label": "intone",
          "successors": []
        }
      ]
    },
    {
      "name": "LlmModelMeta",
      "type": "class",
      "start_line": 71,
      "end_line": 87,
      "functions": [
        {
          "name": "__members__",
          "type": "function",
          "start_line": 73,
          "end_line": 87,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 73,
              "line": "    def __members__("
            },
            {
              "lineno": 74,
              "line": "        self: type[\"_EnumMemberT\"],"
            },
            {
              "lineno": 75,
              "line": "    ) -> MappingProxyType[str, \"_EnumMemberT\"]:"
            },
            {
              "lineno": 76,
              "line": "        if Settings().config.behave_as == BehaveAs.LOCAL:"
            },
            {
              "lineno": 77,
              "line": "            members = super().__members__"
            },
            {
              "lineno": 78,
              "line": "            return members"
            },
            {
              "lineno": 79,
              "line": "        else:"
            },
            {
              "lineno": 80,
              "line": "            removed_providers = [\"ollama\"]"
            },
            {
              "lineno": 81,
              "line": "            existing_members = super().__members__"
            },
            {
              "lineno": 82,
              "line": "            members = {"
            },
            {
              "lineno": 83,
              "line": "                name: member"
            },
            {
              "lineno": 84,
              "line": "                for name, member in existing_members.items()"
            },
            {
              "lineno": 85,
              "line": "                if LlmModel[name].provider not in removed_providers"
            },
            {
              "lineno": 86,
              "line": "            }"
            },
            {
              "lineno": 87,
              "line": "            return MappingProxyType(members)"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def __members__(\n        self: type[\"_EnumMemberT\"],\n    ) -> MappingProxyType[str, \"_EnumMemberT\"]:",
              "successors": [
                {
                  "id": 2,
                  "label": "        if Settings().config.behave_as == BehaveAs.LOCAL:\n            members = super().__members__\n            return members",
                  "successors": []
                },
                {
                  "id": 4,
                  "label": "        else:\n            removed_providers = [\"ollama\"]\n            existing_members = super().__members__\n            members = {\n                name: member\n                for name, member in existing_members.items()\n                if LlmModel[name].provider not in removed_providers\n            }\n            return MappingProxyType(members)",
                  "successors": []
                }
              ]
            }
          ]
        }
      ],
      "classes": [],
      "simplified_code": [
        {
          "lineno": 71,
          "line": "class LlmModelMeta(EnumMeta):"
        },
        {
          "lineno": 72,
          "line": "    @property"
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "class LlmModelMeta(EnumMeta):\n    @property",
          "successors": []
        }
      ]
    },
    {
      "name": "LlmModel",
      "type": "class",
      "start_line": 90,
      "end_line": 186,
      "functions": [
        {
          "name": "metadata",
          "type": "function",
          "start_line": 136,
          "end_line": 137,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 136,
              "line": "    def metadata(self) -> ModelMetadata:"
            },
            {
              "lineno": 137,
              "line": "        return MODEL_METADATA[self]"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def metadata(self) -> ModelMetadata:\n        return MODEL_METADATA[self]",
              "successors": []
            }
          ]
        },
        {
          "name": "provider",
          "type": "function",
          "start_line": 140,
          "end_line": 141,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 140,
              "line": "    def provider(self) -> str:"
            },
            {
              "lineno": 141,
              "line": "        return self.metadata.provider"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def provider(self) -> str:\n        return self.metadata.provider",
              "successors": []
            }
          ]
        },
        {
          "name": "context_window",
          "type": "function",
          "start_line": 144,
          "end_line": 145,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 144,
              "line": "    def context_window(self) -> int:"
            },
            {
              "lineno": 145,
              "line": "        return self.metadata.context_window"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def context_window(self) -> int:\n        return self.metadata.context_window",
              "successors": []
            }
          ]
        }
      ],
      "classes": [],
      "simplified_code": [
        {
          "lineno": 90,
          "line": "class LlmModel(str, Enum, metaclass=LlmModelMeta):"
        },
        {
          "lineno": 91,
          "line": "    # OpenAI models"
        },
        {
          "lineno": 92,
          "line": "    O1_PREVIEW = \"o1-preview\""
        },
        {
          "lineno": 93,
          "line": "    O1_MINI = \"o1-mini\""
        },
        {
          "lineno": 94,
          "line": "    GPT4O_MINI = \"gpt-4o-mini\""
        },
        {
          "lineno": 95,
          "line": "    GPT4O = \"gpt-4o\""
        },
        {
          "lineno": 96,
          "line": "    GPT4_TURBO = \"gpt-4-turbo\""
        },
        {
          "lineno": 97,
          "line": "    GPT3_5_TURBO = \"gpt-3.5-turbo\""
        },
        {
          "lineno": 98,
          "line": "    # Anthropic models"
        },
        {
          "lineno": 99,
          "line": "    CLAUDE_3_5_SONNET = \"claude-3-5-sonnet-latest\""
        },
        {
          "lineno": 100,
          "line": "    CLAUDE_3_HAIKU = \"claude-3-haiku-20240307\""
        },
        {
          "lineno": 101,
          "line": "    # Groq models"
        },
        {
          "lineno": 102,
          "line": "    LLAMA3_8B = \"llama3-8b-8192\""
        },
        {
          "lineno": 103,
          "line": "    LLAMA3_70B = \"llama3-70b-8192\""
        },
        {
          "lineno": 104,
          "line": "    MIXTRAL_8X7B = \"mixtral-8x7b-32768\""
        },
        {
          "lineno": 105,
          "line": "    GEMMA_7B = \"gemma-7b-it\""
        },
        {
          "lineno": 106,
          "line": "    GEMMA2_9B = \"gemma2-9b-it\""
        },
        {
          "lineno": 107,
          "line": "    # New Groq models (Preview)"
        },
        {
          "lineno": 108,
          "line": "    LLAMA3_1_405B = \"llama-3.1-405b-reasoning\""
        },
        {
          "lineno": 109,
          "line": "    LLAMA3_1_70B = \"llama-3.1-70b-versatile\""
        },
        {
          "lineno": 110,
          "line": "    LLAMA3_1_8B = \"llama-3.1-8b-instant\""
        },
        {
          "lineno": 111,
          "line": "    # Ollama models"
        },
        {
          "lineno": 112,
          "line": "    OLLAMA_LLAMA3_8B = \"llama3\""
        },
        {
          "lineno": 113,
          "line": "    OLLAMA_LLAMA3_405B = \"llama3.1:405b\""
        },
        {
          "lineno": 114,
          "line": "    OLLAMA_DOLPHIN = \"dolphin-mistral:latest\""
        },
        {
          "lineno": 115,
          "line": "    # OpenRouter models"
        },
        {
          "lineno": 116,
          "line": "    GEMINI_FLASH_1_5_8B = \"google/gemini-flash-1.5\""
        },
        {
          "lineno": 117,
          "line": "    GROK_BETA = \"x-ai/grok-beta\""
        },
        {
          "lineno": 118,
          "line": "    MISTRAL_NEMO = \"mistralai/mistral-nemo\""
        },
        {
          "lineno": 119,
          "line": "    COHERE_COMMAND_R_08_2024 = \"cohere/command-r-08-2024\""
        },
        {
          "lineno": 120,
          "line": "    COHERE_COMMAND_R_PLUS_08_2024 = \"cohere/command-r-plus-08-2024\""
        },
        {
          "lineno": 121,
          "line": "    EVA_QWEN_2_5_32B = \"eva-unit-01/eva-qwen-2.5-32b\""
        },
        {
          "lineno": 122,
          "line": "    DEEPSEEK_CHAT = \"deepseek/deepseek-chat\""
        },
        {
          "lineno": 123,
          "line": "    PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE = ("
        },
        {
          "lineno": 124,
          "line": "        \"perplexity/llama-3.1-sonar-large-128k-online\""
        },
        {
          "lineno": 125,
          "line": "    )"
        },
        {
          "lineno": 126,
          "line": "    QWEN_QWQ_32B_PREVIEW = \"qwen/qwq-32b-preview\""
        },
        {
          "lineno": 127,
          "line": "    NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B = \"nousresearch/hermes-3-llama-3.1-405b\""
        },
        {
          "lineno": 128,
          "line": "    NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B = \"nousresearch/hermes-3-llama-3.1-70b\""
        },
        {
          "lineno": 129,
          "line": "    AMAZON_NOVA_LITE_V1 = \"amazon/nova-lite-v1\""
        },
        {
          "lineno": 130,
          "line": "    AMAZON_NOVA_MICRO_V1 = \"amazon/nova-micro-v1\""
        },
        {
          "lineno": 131,
          "line": "    AMAZON_NOVA_PRO_V1 = \"amazon/nova-pro-v1\""
        },
        {
          "lineno": 132,
          "line": "    MICROSOFT_WIZARDLM_2_8X22B = \"microsoft/wizardlm-2-8x22b\""
        },
        {
          "lineno": 133,
          "line": "    GRYPHE_MYTHOMAX_L2_13B = \"gryphe/mythomax-l2-13b\""
        },
        {
          "lineno": 134,
          "line": ""
        },
        {
          "lineno": 135,
          "line": "    @property"
        },
        {
          "lineno": 138,
          "line": ""
        },
        {
          "lineno": 139,
          "line": "    @property"
        },
        {
          "lineno": 142,
          "line": ""
        },
        {
          "lineno": 143,
          "line": "    @property"
        },
        {
          "lineno": 146,
          "line": ""
        },
        {
          "lineno": 147,
          "line": ""
        },
        {
          "lineno": 148,
          "line": "MODEL_METADATA = {"
        },
        {
          "lineno": 149,
          "line": "    LlmModel.O1_PREVIEW: ModelMetadata(\"openai\", 32000),"
        },
        {
          "lineno": 150,
          "line": "    LlmModel.O1_MINI: ModelMetadata(\"openai\", 62000),"
        },
        {
          "lineno": 151,
          "line": "    LlmModel.GPT4O_MINI: ModelMetadata(\"openai\", 128000),"
        },
        {
          "lineno": 152,
          "line": "    LlmModel.GPT4O: ModelMetadata(\"openai\", 128000),"
        },
        {
          "lineno": 153,
          "line": "    LlmModel.GPT4_TURBO: ModelMetadata(\"openai\", 128000),"
        },
        {
          "lineno": 154,
          "line": "    LlmModel.GPT3_5_TURBO: ModelMetadata(\"openai\", 16385),"
        },
        {
          "lineno": 155,
          "line": "    LlmModel.CLAUDE_3_5_SONNET: ModelMetadata(\"anthropic\", 200000),"
        },
        {
          "lineno": 156,
          "line": "    LlmModel.CLAUDE_3_HAIKU: ModelMetadata(\"anthropic\", 200000),"
        },
        {
          "lineno": 157,
          "line": "    LlmModel.LLAMA3_8B: ModelMetadata(\"groq\", 8192),"
        },
        {
          "lineno": 158,
          "line": "    LlmModel.LLAMA3_70B: ModelMetadata(\"groq\", 8192),"
        },
        {
          "lineno": 159,
          "line": "    LlmModel.MIXTRAL_8X7B: ModelMetadata(\"groq\", 32768),"
        },
        {
          "lineno": 160,
          "line": "    LlmModel.GEMMA_7B: ModelMetadata(\"groq\", 8192),"
        },
        {
          "lineno": 161,
          "line": "    LlmModel.GEMMA2_9B: ModelMetadata(\"groq\", 8192),"
        },
        {
          "lineno": 162,
          "line": "    LlmModel.LLAMA3_1_405B: ModelMetadata(\"groq\", 8192),"
        },
        {
          "lineno": 163,
          "line": "    # Limited to 16k during preview"
        },
        {
          "lineno": 164,
          "line": "    LlmModel.LLAMA3_1_70B: ModelMetadata(\"groq\", 131072),"
        },
        {
          "lineno": 165,
          "line": "    LlmModel.LLAMA3_1_8B: ModelMetadata(\"groq\", 131072),"
        },
        {
          "lineno": 166,
          "line": "    LlmModel.OLLAMA_LLAMA3_8B: ModelMetadata(\"ollama\", 8192),"
        },
        {
          "lineno": 167,
          "line": "    LlmModel.OLLAMA_LLAMA3_405B: ModelMetadata(\"ollama\", 8192),"
        },
        {
          "lineno": 168,
          "line": "    LlmModel.OLLAMA_DOLPHIN: ModelMetadata(\"ollama\", 32768),"
        },
        {
          "lineno": 169,
          "line": "    LlmModel.GEMINI_FLASH_1_5_8B: ModelMetadata(\"open_router\", 8192),"
        },
        {
          "lineno": 170,
          "line": "    LlmModel.GROK_BETA: ModelMetadata(\"open_router\", 8192),"
        },
        {
          "lineno": 171,
          "line": "    LlmModel.MISTRAL_NEMO: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 172,
          "line": "    LlmModel.COHERE_COMMAND_R_08_2024: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 173,
          "line": "    LlmModel.COHERE_COMMAND_R_PLUS_08_2024: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 174,
          "line": "    LlmModel.EVA_QWEN_2_5_32B: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 175,
          "line": "    LlmModel.DEEPSEEK_CHAT: ModelMetadata(\"open_router\", 8192),"
        },
        {
          "lineno": 176,
          "line": "    LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: ModelMetadata("
        },
        {
          "lineno": 177,
          "line": "        \"open_router\", 8192"
        },
        {
          "lineno": 178,
          "line": "    ),"
        },
        {
          "lineno": 179,
          "line": "    LlmModel.QWEN_QWQ_32B_PREVIEW: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 180,
          "line": "    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 181,
          "line": "    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 182,
          "line": "    LlmModel.AMAZON_NOVA_LITE_V1: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 183,
          "line": "    LlmModel.AMAZON_NOVA_MICRO_V1: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 184,
          "line": "    LlmModel.AMAZON_NOVA_PRO_V1: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 185,
          "line": "    LlmModel.MICROSOFT_WIZARDLM_2_8X22B: ModelMetadata(\"open_router\", 4000),"
        },
        {
          "lineno": 186,
          "line": "    LlmModel.GRYPHE_MYTHOMAX_L2_13B: ModelMetadata(\"open_router\", 4000),"
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "class LlmModel(str, Enum, metaclass=LlmModelMeta):\n    # OpenAI models\n    O1_PREVIEW = \"o1-preview\"\n    O1_MINI = \"o1-mini\"\n    GPT4O_MINI = \"gpt-4o-mini\"\n    GPT4O = \"gpt-4o\"\n    GPT4_TURBO = \"gpt-4-turbo\"\n    GPT3_5_TURBO = \"gpt-3.5-turbo\"\n    # Anthropic models\n    CLAUDE_3_5_SONNET = \"claude-3-5-sonnet-latest\"\n    CLAUDE_3_HAIKU = \"claude-3-haiku-20240307\"\n    # Groq models\n    LLAMA3_8B = \"llama3-8b-8192\"\n    LLAMA3_70B = \"llama3-70b-8192\"\n    MIXTRAL_8X7B = \"mixtral-8x7b-32768\"\n    GEMMA_7B = \"gemma-7b-it\"\n    GEMMA2_9B = \"gemma2-9b-it\"\n    # New Groq models (Preview)\n    LLAMA3_1_405B = \"llama-3.1-405b-reasoning\"\n    LLAMA3_1_70B = \"llama-3.1-70b-versatile\"\n    LLAMA3_1_8B = \"llama-3.1-8b-instant\"\n    # Ollama models\n    OLLAMA_LLAMA3_8B = \"llama3\"\n    OLLAMA_LLAMA3_405B = \"llama3.1:405b\"\n    OLLAMA_DOLPHIN = \"dolphin-mistral:latest\"\n    # OpenRouter models\n    GEMINI_FLASH_1_5_8B = \"google/gemini-flash-1.5\"\n    GROK_BETA = \"x-ai/grok-beta\"\n    MISTRAL_NEMO = \"mistralai/mistral-nemo\"\n    COHERE_COMMAND_R_08_2024 = \"cohere/command-r-08-2024\"\n    COHERE_COMMAND_R_PLUS_08_2024 = \"cohere/command-r-plus-08-2024\"\n    EVA_QWEN_2_5_32B = \"eva-unit-01/eva-qwen-2.5-32b\"\n    DEEPSEEK_CHAT = \"deepseek/deepseek-chat\"\n    PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE = (\n        \"perplexity/llama-3.1-sonar-large-128k-online\"\n    )\n    QWEN_QWQ_32B_PREVIEW = \"qwen/qwq-32b-preview\"\n    NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B = \"nousresearch/hermes-3-llama-3.1-405b\"\n    NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B = \"nousresearch/hermes-3-llama-3.1-70b\"\n    AMAZON_NOVA_LITE_V1 = \"amazon/nova-lite-v1\"\n    AMAZON_NOVA_MICRO_V1 = \"amazon/nova-micro-v1\"\n    AMAZON_NOVA_PRO_V1 = \"amazon/nova-pro-v1\"\n    MICROSOFT_WIZARDLM_2_8X22B = \"microsoft/wizardlm-2-8x22b\"\n    GRYPHE_MYTHOMAX_L2_13B = \"gryphe/mythomax-l2-13b\"\n\n@property",
          "successors": [
            {
              "id": 3,
              "label": "@property\n@property",
              "successors": [
                {
                  "id": 5,
                  "label": "MODEL_METADATA = {\n    LlmModel.O1_PREVIEW: ModelMetadata(\"openai\", 32000),\n    LlmModel.O1_MINI: ModelMetadata(\"openai\", 62000),\n    LlmModel.GPT4O_MINI: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT4O: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT4_TURBO: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT3_5_TURBO: ModelMetadata(\"openai\", 16385),\n    LlmModel.CLAUDE_3_5_SONNET: ModelMetadata(\"anthropic\", 200000),\n    LlmModel.CLAUDE_3_HAIKU: ModelMetadata(\"anthropic\", 200000),\n    LlmModel.LLAMA3_8B: ModelMetadata(\"groq\", 8192),\n    LlmModel.LLAMA3_70B: ModelMetadata(\"groq\", 8192),\n    LlmModel.MIXTRAL_8X7B: ModelMetadata(\"groq\", 32768),\n    LlmModel.GEMMA_7B: ModelMetadata(\"groq\", 8192),\n    LlmModel.GEMMA2_9B: ModelMetadata(\"groq\", 8192),\n    LlmModel.LLAMA3_1_405B: ModelMetadata(\"groq\", 8192),\n    # Limited to 16k during preview\n    LlmModel.LLAMA3_1_70B: ModelMetadata(\"groq\", 131072),\n    LlmModel.LLAMA3_1_8B: ModelMetadata(\"groq\", 131072),\n    LlmModel.OLLAMA_LLAMA3_8B: ModelMetadata(\"ollama\", 8192),\n    LlmModel.OLLAMA_LLAMA3_405B: ModelMetadata(\"ollama\", 8192),\n    LlmModel.OLLAMA_DOLPHIN: ModelMetadata(\"ollama\", 32768),\n    LlmModel.GEMINI_FLASH_1_5_8B: ModelMetadata(\"open_router\", 8192),\n    LlmModel.GROK_BETA: ModelMetadata(\"open_router\", 8192),\n    LlmModel.MISTRAL_NEMO: ModelMetadata(\"open_router\", 4000),\n    LlmModel.COHERE_COMMAND_R_08_2024: ModelMetadata(\"open_router\", 4000),\n    LlmModel.COHERE_COMMAND_R_PLUS_08_2024: ModelMetadata(\"open_router\", 4000),\n    LlmModel.EVA_QWEN_2_5_32B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.DEEPSEEK_CHAT: ModelMetadata(\"open_router\", 8192),\n    LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: ModelMetadata(\n        \"open_router\", 8192\n    ),\n    LlmModel.QWEN_QWQ_32B_PREVIEW: ModelMetadata(\"open_router\", 4000),\n    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_LITE_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_MICRO_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_PRO_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.MICROSOFT_WIZARDLM_2_8X22B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.GRYPHE_MYTHOMAX_L2_13B: ModelMetadata(\"open_router\", 4000),\n}",
                  "successors": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "name": "MessageRole",
      "type": "class",
      "start_line": 194,
      "end_line": 197,
      "functions": [],
      "classes": [],
      "simplified_code": [
        {
          "lineno": 194,
          "line": "class MessageRole(str, Enum):"
        },
        {
          "lineno": 195,
          "line": "    SYSTEM = \"system\""
        },
        {
          "lineno": 196,
          "line": "    USER = \"user\""
        },
        {
          "lineno": 197,
          "line": "    ASSISTANT = \"assistant\""
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "class MessageRole(str, Enum):\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"",
          "successors": []
        }
      ]
    },
    {
      "name": "Message",
      "type": "class",
      "start_line": 200,
      "end_line": 202,
      "functions": [],
      "classes": [],
      "simplified_code": [
        {
          "lineno": 200,
          "line": "class Message(BlockSchema):"
        },
        {
          "lineno": 201,
          "line": "    role: MessageRole"
        },
        {
          "lineno": 202,
          "line": "    content: str"
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "class Message(BlockSchema):\n    role: MessageRole\n    content: str",
          "successors": []
        }
      ]
    },
    {
      "name": "AIStructuredResponseGeneratorBlock",
      "type": "class",
      "start_line": 205,
      "end_line": 552,
      "functions": [
        {
          "name": "__init__",
          "type": "function",
          "start_line": 257,
          "end_line": 287,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 257,
              "line": "    def __init__(self):"
            },
            {
              "lineno": 258,
              "line": "        super().__init__("
            },
            {
              "lineno": 259,
              "line": "            id=\"ed55ac19-356e-4243-a6cb-bc599e9b716f\","
            },
            {
              "lineno": 260,
              "line": "            description=\"Call a Large Language Model (LLM) to generate formatted object based on the given prompt.\","
            },
            {
              "lineno": 261,
              "line": "            categories={BlockCategory.AI},"
            },
            {
              "lineno": 262,
              "line": "            input_schema=AIStructuredResponseGeneratorBlock.Input,"
            },
            {
              "lineno": 263,
              "line": "            output_schema=AIStructuredResponseGeneratorBlock.Output,"
            },
            {
              "lineno": 264,
              "line": "            test_input={"
            },
            {
              "lineno": 265,
              "line": "                \"model\": LlmModel.GPT4_TURBO,"
            },
            {
              "lineno": 266,
              "line": "                \"credentials\": TEST_CREDENTIALS_INPUT,"
            },
            {
              "lineno": 267,
              "line": "                \"expected_format\": {"
            },
            {
              "lineno": 268,
              "line": "                    \"key1\": \"value1\","
            },
            {
              "lineno": 269,
              "line": "                    \"key2\": \"value2\","
            },
            {
              "lineno": 270,
              "line": "                },"
            },
            {
              "lineno": 271,
              "line": "                \"prompt\": \"User prompt\","
            },
            {
              "lineno": 272,
              "line": "            },"
            },
            {
              "lineno": 273,
              "line": "            test_credentials=TEST_CREDENTIALS,"
            },
            {
              "lineno": 274,
              "line": "            test_output=(\"response\", {\"key1\": \"key1Value\", \"key2\": \"key2Value\"}),"
            },
            {
              "lineno": 275,
              "line": "            test_mock={"
            },
            {
              "lineno": 276,
              "line": "                \"llm_call\": lambda *args, **kwargs: ("
            },
            {
              "lineno": 277,
              "line": "                    json.dumps("
            },
            {
              "lineno": 278,
              "line": "                        {"
            },
            {
              "lineno": 279,
              "line": "                            \"key1\": \"key1Value\","
            },
            {
              "lineno": 280,
              "line": "                            \"key2\": \"key2Value\","
            },
            {
              "lineno": 281,
              "line": "                        }"
            },
            {
              "lineno": 282,
              "line": "                    ),"
            },
            {
              "lineno": 283,
              "line": "                    0,"
            },
            {
              "lineno": 284,
              "line": "                    0,"
            },
            {
              "lineno": 285,
              "line": "                )"
            },
            {
              "lineno": 286,
              "line": "            },"
            },
            {
              "lineno": 287,
              "line": "        )"
            }
          ],
          "blocks": []
        },
        {
          "name": "llm_call",
          "type": "function",
          "start_line": 290,
          "end_line": 437,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 290,
              "line": "    def llm_call("
            },
            {
              "lineno": 291,
              "line": "        credentials: APIKeyCredentials,"
            },
            {
              "lineno": 292,
              "line": "        llm_model: LlmModel,"
            },
            {
              "lineno": 293,
              "line": "        prompt: list[dict],"
            },
            {
              "lineno": 294,
              "line": "        json_format: bool,"
            },
            {
              "lineno": 295,
              "line": "        max_tokens: int | None = None,"
            },
            {
              "lineno": 296,
              "line": "        ollama_host: str = \"localhost:11434\","
            },
            {
              "lineno": 297,
              "line": "    ) -> tuple[str, int, int]:"
            },
            {
              "lineno": 298,
              "line": "        \"\"\""
            },
            {
              "lineno": 299,
              "line": "        Args:"
            },
            {
              "lineno": 300,
              "line": "            api_key: API key for the LLM provider."
            },
            {
              "lineno": 301,
              "line": "            llm_model: The LLM model to use."
            },
            {
              "lineno": 302,
              "line": "            prompt: The prompt to send to the LLM."
            },
            {
              "lineno": 303,
              "line": "            json_format: Whether the response should be in JSON format."
            },
            {
              "lineno": 304,
              "line": "            max_tokens: The maximum number of tokens to generate in the chat completion."
            },
            {
              "lineno": 305,
              "line": "            ollama_host: The host for ollama to use"
            },
            {
              "lineno": 306,
              "line": ""
            },
            {
              "lineno": 307,
              "line": "        Returns:"
            },
            {
              "lineno": 308,
              "line": "            The response from the LLM."
            },
            {
              "lineno": 309,
              "line": "            The number of tokens used in the prompt."
            },
            {
              "lineno": 310,
              "line": "            The number of tokens used in the completion."
            },
            {
              "lineno": 311,
              "line": "        \"\"\""
            },
            {
              "lineno": 312,
              "line": "        provider = llm_model.metadata.provider"
            },
            {
              "lineno": 313,
              "line": ""
            },
            {
              "lineno": 314,
              "line": "        if provider == \"openai\":"
            },
            {
              "lineno": 315,
              "line": "            oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())"
            },
            {
              "lineno": 316,
              "line": "            response_format = None"
            },
            {
              "lineno": 317,
              "line": ""
            },
            {
              "lineno": 318,
              "line": "            if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:"
            },
            {
              "lineno": 319,
              "line": "                sys_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]"
            },
            {
              "lineno": 320,
              "line": "                usr_messages = [p[\"content\"] for p in prompt if p[\"role\"] != \"system\"]"
            },
            {
              "lineno": 321,
              "line": "                prompt = ["
            },
            {
              "lineno": 322,
              "line": "                    {\"role\": \"user\", \"content\": \"\\n\".join(sys_messages)},"
            },
            {
              "lineno": 323,
              "line": "                    {\"role\": \"user\", \"content\": \"\\n\".join(usr_messages)},"
            },
            {
              "lineno": 324,
              "line": "                ]"
            },
            {
              "lineno": 325,
              "line": "            elif json_format:"
            },
            {
              "lineno": 326,
              "line": "                response_format = {\"type\": \"json_object\"}"
            },
            {
              "lineno": 327,
              "line": ""
            },
            {
              "lineno": 328,
              "line": "            response = oai_client.chat.completions.create("
            },
            {
              "lineno": 329,
              "line": "                model=llm_model.value,"
            },
            {
              "lineno": 330,
              "line": "                messages=prompt,  # type: ignore"
            },
            {
              "lineno": 331,
              "line": "                response_format=response_format,  # type: ignore"
            },
            {
              "lineno": 332,
              "line": "                max_completion_tokens=max_tokens,"
            },
            {
              "lineno": 333,
              "line": "            )"
            },
            {
              "lineno": 334,
              "line": ""
            },
            {
              "lineno": 335,
              "line": "            return ("
            },
            {
              "lineno": 336,
              "line": "                response.choices[0].message.content or \"\","
            },
            {
              "lineno": 337,
              "line": "                response.usage.prompt_tokens if response.usage else 0,"
            },
            {
              "lineno": 338,
              "line": "                response.usage.completion_tokens if response.usage else 0,"
            },
            {
              "lineno": 339,
              "line": "            )"
            },
            {
              "lineno": 340,
              "line": "        elif provider == \"anthropic\":"
            },
            {
              "lineno": 341,
              "line": "            system_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]"
            },
            {
              "lineno": 342,
              "line": "            sysprompt = \" \".join(system_messages)"
            },
            {
              "lineno": 343,
              "line": ""
            },
            {
              "lineno": 344,
              "line": "            messages = []"
            },
            {
              "lineno": 345,
              "line": "            last_role = None"
            },
            {
              "lineno": 346,
              "line": "            for p in prompt:"
            },
            {
              "lineno": 347,
              "line": "                if p[\"role\"] in [\"user\", \"assistant\"]:"
            },
            {
              "lineno": 348,
              "line": "                    if p[\"role\"] != last_role:"
            },
            {
              "lineno": 349,
              "line": "                        messages.append({\"role\": p[\"role\"], \"content\": p[\"content\"]})"
            },
            {
              "lineno": 350,
              "line": "                        last_role = p[\"role\"]"
            },
            {
              "lineno": 351,
              "line": "                    else:"
            },
            {
              "lineno": 352,
              "line": "                        # If the role is the same as the last one, combine the content"
            },
            {
              "lineno": 353,
              "line": "                        messages[-1][\"content\"] += \"\\n\" + p[\"content\"]"
            },
            {
              "lineno": 354,
              "line": ""
            },
            {
              "lineno": 355,
              "line": "            client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())"
            },
            {
              "lineno": 356,
              "line": "            try:"
            },
            {
              "lineno": 357,
              "line": "                resp = client.messages.create("
            },
            {
              "lineno": 358,
              "line": "                    model=llm_model.value,"
            },
            {
              "lineno": 359,
              "line": "                    system=sysprompt,"
            },
            {
              "lineno": 360,
              "line": "                    messages=messages,"
            },
            {
              "lineno": 361,
              "line": "                    max_tokens=max_tokens or 8192,"
            },
            {
              "lineno": 362,
              "line": "                )"
            },
            {
              "lineno": 363,
              "line": ""
            },
            {
              "lineno": 364,
              "line": "                if not resp.content:"
            },
            {
              "lineno": 365,
              "line": "                    raise ValueError(\"No content returned from Anthropic.\")"
            },
            {
              "lineno": 366,
              "line": ""
            },
            {
              "lineno": 367,
              "line": "                return ("
            },
            {
              "lineno": 368,
              "line": "                    ("
            },
            {
              "lineno": 369,
              "line": "                        resp.content[0].name"
            },
            {
              "lineno": 370,
              "line": "                        if isinstance(resp.content[0], anthropic.types.ToolUseBlock)"
            },
            {
              "lineno": 371,
              "line": "                        else resp.content[0].text"
            },
            {
              "lineno": 372,
              "line": "                    ),"
            },
            {
              "lineno": 373,
              "line": "                    resp.usage.input_tokens,"
            },
            {
              "lineno": 374,
              "line": "                    resp.usage.output_tokens,"
            },
            {
              "lineno": 375,
              "line": "                )"
            },
            {
              "lineno": 376,
              "line": "            except anthropic.APIError as e:"
            },
            {
              "lineno": 377,
              "line": "                error_message = f\"Anthropic API error: {str(e)}\""
            },
            {
              "lineno": 378,
              "line": "                logger.error(error_message)"
            },
            {
              "lineno": 379,
              "line": "                raise ValueError(error_message)"
            },
            {
              "lineno": 380,
              "line": "        elif provider == \"groq\":"
            },
            {
              "lineno": 381,
              "line": "            client = Groq(api_key=credentials.api_key.get_secret_value())"
            },
            {
              "lineno": 382,
              "line": "            response_format = {\"type\": \"json_object\"} if json_format else None"
            },
            {
              "lineno": 383,
              "line": "            response = client.chat.completions.create("
            },
            {
              "lineno": 384,
              "line": "                model=llm_model.value,"
            },
            {
              "lineno": 385,
              "line": "                messages=prompt,  # type: ignore"
            },
            {
              "lineno": 386,
              "line": "                response_format=response_format,  # type: ignore"
            },
            {
              "lineno": 387,
              "line": "                max_tokens=max_tokens,"
            },
            {
              "lineno": 388,
              "line": "            )"
            },
            {
              "lineno": 389,
              "line": "            return ("
            },
            {
              "lineno": 390,
              "line": "                response.choices[0].message.content or \"\","
            },
            {
              "lineno": 391,
              "line": "                response.usage.prompt_tokens if response.usage else 0,"
            },
            {
              "lineno": 392,
              "line": "                response.usage.completion_tokens if response.usage else 0,"
            },
            {
              "lineno": 393,
              "line": "            )"
            },
            {
              "lineno": 394,
              "line": "        elif provider == \"ollama\":"
            },
            {
              "lineno": 395,
              "line": "            client = ollama.Client(host=ollama_host)"
            },
            {
              "lineno": 396,
              "line": "            sys_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]"
            },
            {
              "lineno": 397,
              "line": "            usr_messages = [p[\"content\"] for p in prompt if p[\"role\"] != \"system\"]"
            },
            {
              "lineno": 398,
              "line": "            response = client.generate("
            },
            {
              "lineno": 399,
              "line": "                model=llm_model.value,"
            },
            {
              "lineno": 400,
              "line": "                prompt=f\"{sys_messages}\\n\\n{usr_messages}\","
            },
            {
              "lineno": 401,
              "line": "                stream=False,"
            },
            {
              "lineno": 402,
              "line": "            )"
            },
            {
              "lineno": 403,
              "line": "            return ("
            },
            {
              "lineno": 404,
              "line": "                response.get(\"response\") or \"\","
            },
            {
              "lineno": 405,
              "line": "                response.get(\"prompt_eval_count\") or 0,"
            },
            {
              "lineno": 406,
              "line": "                response.get(\"eval_count\") or 0,"
            },
            {
              "lineno": 407,
              "line": "            )"
            },
            {
              "lineno": 408,
              "line": "        elif provider == \"open_router\":"
            },
            {
              "lineno": 409,
              "line": "            client = openai.OpenAI("
            },
            {
              "lineno": 410,
              "line": "                base_url=\"https://openrouter.ai/api/v1\","
            },
            {
              "lineno": 411,
              "line": "                api_key=credentials.api_key.get_secret_value(),"
            },
            {
              "lineno": 412,
              "line": "            )"
            },
            {
              "lineno": 413,
              "line": ""
            },
            {
              "lineno": 414,
              "line": "            response = client.chat.completions.create("
            },
            {
              "lineno": 415,
              "line": "                extra_headers={"
            },
            {
              "lineno": 416,
              "line": "                    \"HTTP-Referer\": \"https://agpt.co\","
            },
            {
              "lineno": 417,
              "line": "                    \"X-Title\": \"AutoGPT\","
            },
            {
              "lineno": 418,
              "line": "                },"
            },
            {
              "lineno": 419,
              "line": "                model=llm_model.value,"
            },
            {
              "lineno": 420,
              "line": "                messages=prompt,  # type: ignore"
            },
            {
              "lineno": 421,
              "line": "                max_tokens=max_tokens,"
            },
            {
              "lineno": 422,
              "line": "            )"
            },
            {
              "lineno": 423,
              "line": ""
            },
            {
              "lineno": 424,
              "line": "            # If there's no response, raise an error"
            },
            {
              "lineno": 425,
              "line": "            if not response.choices:"
            },
            {
              "lineno": 426,
              "line": "                if response:"
            },
            {
              "lineno": 427,
              "line": "                    raise ValueError(f\"OpenRouter error: {response}\")"
            },
            {
              "lineno": 428,
              "line": "                else:"
            },
            {
              "lineno": 429,
              "line": "                    raise ValueError(\"No response from OpenRouter.\")"
            },
            {
              "lineno": 430,
              "line": ""
            },
            {
              "lineno": 431,
              "line": "            return ("
            },
            {
              "lineno": 432,
              "line": "                response.choices[0].message.content or \"\","
            },
            {
              "lineno": 433,
              "line": "                response.usage.prompt_tokens if response.usage else 0,"
            },
            {
              "lineno": 434,
              "line": "                response.usage.completion_tokens if response.usage else 0,"
            },
            {
              "lineno": 435,
              "line": "            )"
            },
            {
              "lineno": 436,
              "line": "        else:"
            },
            {
              "lineno": 437,
              "line": "            raise ValueError(f\"Unsupported LLM provider: {provider}\")"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def llm_call(\n        credentials: APIKeyCredentials,\n        llm_model: LlmModel,\n        prompt: list[dict],\n        json_format: bool,\n        max_tokens: int | None = None,\n        ollama_host: str = \"localhost:11434\",\n    ) -> tuple[str, int, int]:\n        \"\"\"\n        Args:\n            api_key: API key for the LLM provider.\n            llm_model: The LLM model to use.\n            prompt: The prompt to send to the LLM.\n            json_format: Whether the response should be in JSON format.\n            max_tokens: The maximum number of tokens to generate in the chat completion.\n            ollama_host: The host for ollama to use\n\n        Returns:\n            The response from the LLM.\n            The number of tokens used in the prompt.\n            The number of tokens used in the completion.\n        \"\"\"\n        provider = llm_model.metadata.provider\n\n        if provider == \"openai\":\n            oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())\n            response_format = None\n\n            if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:\n                sys_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]\n                usr_messages = [p[\"content\"] for p in prompt if p[\"role\"] != \"system\"]\n                prompt = [\n                    {\"role\": \"user\", \"content\": \"\\n\".join(sys_messages)},\n                    {\"role\": \"user\", \"content\": \"\\n\".join(usr_messages)},\n                ]\n            elif json_format:\n                response_format = {\"type\": \"json_object\"}\n\n            response = oai_client.chat.completions.create(\n                model=llm_model.value,\n                messages=prompt,  # type: ignore\n                response_format=response_format,  # type: ignore\n                max_completion_tokens=max_tokens,\n            )\n\n            return (\n                response.choices[0].message.content or \"\",\n                response.usage.prompt_tokens if response.usage else 0,\n                response.usage.completion_tokens if response.usage else 0,\n            )",
              "successors": [
                {
                  "id": 3,
                  "label": "        elif provider == \"anthropic\":\n            system_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]\n            sysprompt = \" \".join(system_messages)\n\n            messages = []\n            last_role = None\n            for p in prompt:\n                if p[\"role\"] in [\"user\", \"assistant\"]:\n                    if p[\"role\"] != last_role:\n                        messages.append({\"role\": p[\"role\"], \"content\": p[\"content\"]})\n                        last_role = p[\"role\"]\n                    else:\n                        # If the role is the same as the last one, combine the content\n                        messages[-1][\"content\"] += \"\\n\" + p[\"content\"]\n\n            client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())\n            try:\n                resp = client.messages.create(\n                    model=llm_model.value,\n                    system=sysprompt,\n                    messages=messages,\n                    max_tokens=max_tokens or 8192,\n                )\n\n                if not resp.content:\n                    raise ValueError(\"No content returned from Anthropic.\")\n\n                return (\n                    (\n                        resp.content[0].name\n                        if isinstance(resp.content[0], anthropic.types.ToolUseBlock)\n                        else resp.content[0].text\n                    ),\n                    resp.usage.input_tokens,\n                    resp.usage.output_tokens,\n                )\n            except anthropic.APIError as e:\n                error_message = f\"Anthropic API error: {str(e)}\"\n                logger.error(error_message)\n                raise ValueError(error_message)\n        elif provider == \"groq\":\n            client = Groq(api_key=credentials.api_key.get_secret_value())\n            response_format = {\"type\": \"json_object\"} if json_format else None\n            response = client.chat.completions.create(\n                model=llm_model.value,\n                messages=prompt,  # type: ignore\n                response_format=response_format,  # type: ignore\n                max_tokens=max_tokens,\n            )\n            return (\n                response.choices[0].message.content or \"\",\n                response.usage.prompt_tokens if response.usage else 0,\n                response.usage.completion_tokens if response.usage else 0,\n            )\n        elif provider == \"ollama\":\n            client = ollama.Client(host=ollama_host)\n            sys_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]\n            usr_messages = [p[\"content\"] for p in prompt if p[\"role\"] != \"system\"]\n            response = client.generate(\n                model=llm_model.value,\n                prompt=f\"{sys_messages}\\n\\n{usr_messages}\",\n                stream=False,\n            )\n            return (\n                response.get(\"response\") or \"\",\n                response.get(\"prompt_eval_count\") or 0,\n                response.get(\"eval_count\") or 0,\n            )\n        elif provider == \"open_router\":\n            client = openai.OpenAI(\n                base_url=\"https://openrouter.ai/api/v1\",\n                api_key=credentials.api_key.get_secret_value(),\n            )\n\n            response = client.chat.completions.create(\n                extra_headers={\n                    \"HTTP-Referer\": \"https://agpt.co\",\n                    \"X-Title\": \"AutoGPT\",\n                },\n                model=llm_model.value,\n                messages=prompt,  # type: ignore\n                max_tokens=max_tokens,\n            )\n\n            # If there's no response, raise an error\n            if not response.choices:\n                if response:\n                    raise ValueError(f\"OpenRouter error: {response}\")\n                else:\n                    raise ValueError(\"No response from OpenRouter.\")\n\n            return (\n                response.choices[0].message.content or \"\",\n                response.usage.prompt_tokens if response.usage else 0,\n                response.usage.completion_tokens if response.usage else 0,\n            )\n        else:\n            raise ValueError(f\"Unsupported LLM provider: {provider}\")",
                  "successors": []
                }
              ]
            }
          ]
        },
        {
          "name": "run",
          "type": "function",
          "start_line": 439,
          "end_line": 551,
          "functions": [
            {
              "name": "trim_prompt",
              "type": "function",
              "start_line": 445,
              "end_line": 447,
              "functions": [],
              "classes": [],
              "simplified_code": [
                {
                  "lineno": 445,
                  "line": "        def trim_prompt(s: str) -> str:"
                },
                {
                  "lineno": 446,
                  "line": "            lines = s.strip().split(\"\\n\")"
                },
                {
                  "lineno": 447,
                  "line": "            return \"\\n\".join([line.strip().lstrip(\"|\") for line in lines])"
                }
              ],
              "blocks": [
                {
                  "id": 1,
                  "label": "        def trim_prompt(s: str) -> str:\n            lines = s.strip().split(\"\\n\")\n            return \"\\n\".join([line.strip().lstrip(\"|\") for line in lines])",
                  "successors": []
                }
              ]
            },
            {
              "name": "parse_response",
              "type": "function",
              "start_line": 475,
              "end_line": 485,
              "functions": [],
              "classes": [],
              "simplified_code": [
                {
                  "lineno": 475,
                  "line": "        def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:"
                },
                {
                  "lineno": 476,
                  "line": "            try:"
                },
                {
                  "lineno": 477,
                  "line": "                parsed = json.loads(resp)"
                },
                {
                  "lineno": 478,
                  "line": "                if not isinstance(parsed, dict):"
                },
                {
                  "lineno": 479,
                  "line": "                    return {}, f\"Expected a dictionary, but got {type(parsed)}\""
                },
                {
                  "lineno": 480,
                  "line": "                miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())"
                },
                {
                  "lineno": 481,
                  "line": "                if miss_keys:"
                },
                {
                  "lineno": 482,
                  "line": "                    return parsed, f\"Missing keys: {miss_keys}\""
                },
                {
                  "lineno": 483,
                  "line": "                return parsed, None"
                },
                {
                  "lineno": 484,
                  "line": "            except JSONDecodeError as e:"
                },
                {
                  "lineno": 485,
                  "line": "                return {}, f\"JSON decode error: {e}\""
                }
              ],
              "blocks": [
                {
                  "id": 1,
                  "label": "        def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:\n            try:\n                parsed = json.loads(resp)\n                if not isinstance(parsed, dict):\n                    return {}, f\"Expected a dictionary, but got {type(parsed)}\"\n                miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\n                if miss_keys:\n                    return parsed, f\"Missing keys: {miss_keys}\"\n                return parsed, None",
                  "successors": [
                    {
                      "id": 3,
                      "label": "            except JSONDecodeError as e:\n                return {}, f\"JSON decode error: {e}\"",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 439,
              "line": "    def run("
            },
            {
              "lineno": 440,
              "line": "        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs"
            },
            {
              "lineno": 441,
              "line": "    ) -> BlockOutput:"
            },
            {
              "lineno": 442,
              "line": "        logger.debug(f\"Calling LLM with input data: {input_data}\")"
            },
            {
              "lineno": 443,
              "line": "        prompt = [p.model_dump() for p in input_data.conversation_history]"
            },
            {
              "lineno": 444,
              "line": ""
            },
            {
              "lineno": 448,
              "line": ""
            },
            {
              "lineno": 449,
              "line": "        values = input_data.prompt_values"
            },
            {
              "lineno": 450,
              "line": "        if values:"
            },
            {
              "lineno": 451,
              "line": "            input_data.prompt = input_data.prompt.format(**values)"
            },
            {
              "lineno": 452,
              "line": "            input_data.sys_prompt = input_data.sys_prompt.format(**values)"
            },
            {
              "lineno": 453,
              "line": ""
            },
            {
              "lineno": 454,
              "line": "        if input_data.sys_prompt:"
            },
            {
              "lineno": 455,
              "line": "            prompt.append({\"role\": \"system\", \"content\": input_data.sys_prompt})"
            },
            {
              "lineno": 456,
              "line": ""
            },
            {
              "lineno": 457,
              "line": "        if input_data.expected_format:"
            },
            {
              "lineno": 458,
              "line": "            expected_format = ["
            },
            {
              "lineno": 459,
              "line": "                f'\"{k}\": \"{v}\"' for k, v in input_data.expected_format.items()"
            },
            {
              "lineno": 460,
              "line": "            ]"
            },
            {
              "lineno": 461,
              "line": "            format_prompt = \",\\n  \".join(expected_format)"
            },
            {
              "lineno": 462,
              "line": "            sys_prompt = trim_prompt("
            },
            {
              "lineno": 463,
              "line": "                f\"\"\""
            },
            {
              "lineno": 464,
              "line": "                  |Reply strictly only in the following JSON format:"
            },
            {
              "lineno": 465,
              "line": "                  |{{"
            },
            {
              "lineno": 466,
              "line": "                  |  {format_prompt}"
            },
            {
              "lineno": 467,
              "line": "                  |}}"
            },
            {
              "lineno": 468,
              "line": "                \"\"\""
            },
            {
              "lineno": 469,
              "line": "            )"
            },
            {
              "lineno": 470,
              "line": "            prompt.append({\"role\": \"system\", \"content\": sys_prompt})"
            },
            {
              "lineno": 471,
              "line": ""
            },
            {
              "lineno": 472,
              "line": "        if input_data.prompt:"
            },
            {
              "lineno": 473,
              "line": "            prompt.append({\"role\": \"user\", \"content\": input_data.prompt})"
            },
            {
              "lineno": 474,
              "line": ""
            },
            {
              "lineno": 486,
              "line": ""
            },
            {
              "lineno": 487,
              "line": "        logger.info(f\"LLM request: {prompt}\")"
            },
            {
              "lineno": 488,
              "line": "        retry_prompt = \"\""
            },
            {
              "lineno": 489,
              "line": "        llm_model = input_data.model"
            },
            {
              "lineno": 490,
              "line": ""
            },
            {
              "lineno": 491,
              "line": "        for retry_count in range(input_data.retry):"
            },
            {
              "lineno": 492,
              "line": "            try:"
            },
            {
              "lineno": 493,
              "line": "                response_text, input_token, output_token = self.llm_call("
            },
            {
              "lineno": 494,
              "line": "                    credentials=credentials,"
            },
            {
              "lineno": 495,
              "line": "                    llm_model=llm_model,"
            },
            {
              "lineno": 496,
              "line": "                    prompt=prompt,"
            },
            {
              "lineno": 497,
              "line": "                    json_format=bool(input_data.expected_format),"
            },
            {
              "lineno": 498,
              "line": "                    ollama_host=input_data.ollama_host,"
            },
            {
              "lineno": 499,
              "line": "                    max_tokens=input_data.max_tokens,"
            },
            {
              "lineno": 500,
              "line": "                )"
            },
            {
              "lineno": 501,
              "line": "                self.merge_stats("
            },
            {
              "lineno": 502,
              "line": "                    {"
            },
            {
              "lineno": 503,
              "line": "                        \"input_token_count\": input_token,"
            },
            {
              "lineno": 504,
              "line": "                        \"output_token_count\": output_token,"
            },
            {
              "lineno": 505,
              "line": "                    }"
            },
            {
              "lineno": 506,
              "line": "                )"
            },
            {
              "lineno": 507,
              "line": "                logger.info(f\"LLM attempt-{retry_count} response: {response_text}\")"
            },
            {
              "lineno": 508,
              "line": ""
            },
            {
              "lineno": 509,
              "line": "                if input_data.expected_format:"
            },
            {
              "lineno": 510,
              "line": "                    parsed_dict, parsed_error = parse_response(response_text)"
            },
            {
              "lineno": 511,
              "line": "                    if not parsed_error:"
            },
            {
              "lineno": 512,
              "line": "                        yield \"response\", {"
            },
            {
              "lineno": 513,
              "line": "                            k: ("
            },
            {
              "lineno": 514,
              "line": "                                json.loads(v)"
            },
            {
              "lineno": 515,
              "line": "                                if isinstance(v, str)"
            },
            {
              "lineno": 516,
              "line": "                                and v.startswith(\"[\")"
            },
            {
              "lineno": 517,
              "line": "                                and v.endswith(\"]\")"
            },
            {
              "lineno": 518,
              "line": "                                else (\", \".join(v) if isinstance(v, list) else v)"
            },
            {
              "lineno": 519,
              "line": "                            )"
            },
            {
              "lineno": 520,
              "line": "                            for k, v in parsed_dict.items()"
            },
            {
              "lineno": 521,
              "line": "                        }"
            },
            {
              "lineno": 522,
              "line": "                        return"
            },
            {
              "lineno": 523,
              "line": "                else:"
            },
            {
              "lineno": 524,
              "line": "                    yield \"response\", {\"response\": response_text}"
            },
            {
              "lineno": 525,
              "line": "                    return"
            },
            {
              "lineno": 526,
              "line": ""
            },
            {
              "lineno": 527,
              "line": "                retry_prompt = trim_prompt("
            },
            {
              "lineno": 528,
              "line": "                    f\"\"\""
            },
            {
              "lineno": 529,
              "line": "                  |This is your previous error response:"
            },
            {
              "lineno": 530,
              "line": "                  |--"
            },
            {
              "lineno": 531,
              "line": "                  |{response_text}"
            },
            {
              "lineno": 532,
              "line": "                  |--"
            },
            {
              "lineno": 533,
              "line": "                  |"
            },
            {
              "lineno": 534,
              "line": "                  |And this is the error:"
            },
            {
              "lineno": 535,
              "line": "                  |--"
            },
            {
              "lineno": 536,
              "line": "                  |{parsed_error}"
            },
            {
              "lineno": 537,
              "line": "                  |--"
            },
            {
              "lineno": 538,
              "line": "                \"\"\""
            },
            {
              "lineno": 539,
              "line": "                )"
            },
            {
              "lineno": 540,
              "line": "                prompt.append({\"role\": \"user\", \"content\": retry_prompt})"
            },
            {
              "lineno": 541,
              "line": "            except Exception as e:"
            },
            {
              "lineno": 542,
              "line": "                logger.exception(f\"Error calling LLM: {e}\")"
            },
            {
              "lineno": 543,
              "line": "                retry_prompt = f\"Error calling LLM: {e}\""
            },
            {
              "lineno": 544,
              "line": "            finally:"
            },
            {
              "lineno": 545,
              "line": "                self.merge_stats("
            },
            {
              "lineno": 546,
              "line": "                    {"
            },
            {
              "lineno": 547,
              "line": "                        \"llm_call_count\": retry_count + 1,"
            },
            {
              "lineno": 548,
              "line": "                        \"llm_retry_count\": retry_count,"
            },
            {
              "lineno": 549,
              "line": "                    }"
            },
            {
              "lineno": 550,
              "line": "                )"
            },
            {
              "lineno": 551,
              "line": ""
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:\n        logger.debug(f\"Calling LLM with input data: {input_data}\")\n        prompt = [p.model_dump() for p in input_data.conversation_history]",
              "successors": [
                {
                  "id": 3,
                  "label": "        values = input_data.prompt_values\n        if values:\n            input_data.prompt = input_data.prompt.format(**values)\n            input_data.sys_prompt = input_data.sys_prompt.format(**values)",
                  "successors": [
                    {
                      "id": 5,
                      "label": "        if input_data.sys_prompt:\n            prompt.append({\"role\": \"system\", \"content\": input_data.sys_prompt})\n        if input_data.expected_format:\n            expected_format = [\n                f'\"{k}\": \"{v}\"' for k, v in input_data.expected_format.items()\n            ]\n            format_prompt = \",\\n  \".join(expected_format)\n            sys_prompt = trim_prompt(\n                f\"\"\"\n                  |Reply strictly only in the following JSON format:\n                  |{{\n                  |  {format_prompt}\n                  |}}\n                \"\"\"\n            )\n            prompt.append({\"role\": \"system\", \"content\": sys_prompt})",
                      "successors": [
                        {
                          "id": 7,
                          "label": "        if input_data.prompt:\n            prompt.append({\"role\": \"user\", \"content\": input_data.prompt})\n        logger.info(f\"LLM request: {prompt}\")\n        retry_prompt = \"\"\n        llm_model = input_data.model",
                          "successors": [
                            {
                              "id": 9,
                              "label": "        for retry_count in range(input_data.retry):\n            try:\n                response_text, input_token, output_token = self.llm_call(\n                    credentials=credentials,\n                    llm_model=llm_model,\n                    prompt=prompt,\n                    json_format=bool(input_data.expected_format),\n                    ollama_host=input_data.ollama_host,\n                    max_tokens=input_data.max_tokens\n                )",
                              "successors": [
                                {
                                  "id": 10,
                                  "label": "                self.merge_stats(\n                    {\n                        \"input_token_count\": input_token,\n                        \"output_token_count\": output_token\n                    }\n                )\n                logger.info(f\"LLM attempt-{retry_count} response: {response_text}\")",
                                  "successors": [
                                    {
                                      "id": 11,
                                      "label": "                if input_data.expected_format:\n                    parsed_dict, parsed_error = parse_response(response_text)\n                    if not parsed_error:\n                        yield \"response\", {\n                            k: (\n                                json.loads(v)\n                                if isinstance(v, str)\n                                and v.startswith(\"[\")\n                                and v.endswith(\"]\")\n                                else (\", \".join(v) if isinstance(v, list) else v)\n                            )\n                            for k, v in parsed_dict.items()\n                        }\n                        return",
                                      "successors": []
                                    },
                                    {
                                      "id": 12,
                                      "label": "                else:\n                    yield \"response\", {\"response\": response_text}\n                    return",
                                      "successors": []
                                    }
                                  ]
                                },
                                {
                                  "id": 13,
                                  "label": "                retry_prompt = trim_prompt(\n                    f\"\"\"\n                  |This is your previous error response:\n                  |--\n                  |{response_text}\n                  |--\n                  |\n                  |And this is the error:\n                  |--\n                  |{parsed_error}\n                  |--\n                \"\"\"\n                )\n                prompt.append({\"role\": \"user\", \"content\": retry_prompt})",
                                  "successors": []
                                },
                                {
                                  "id": 14,
                                  "label": "            except Exception as e:\n                logger.exception(f\"Error calling LLM: {e}\")\n                retry_prompt = f\"Error calling LLM: {e}\"",
                                  "successors": []
                                },
                                {
                                  "id": 15,
                                  "label": "            finally:\n                self.merge_stats(\n                    {\n                        \"llm_call_count\": retry_count + 1,\n                        \"llm_retry_count\": retry_count\n                    }\n                )",
                                  "successors": []
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "class",
          "start_line": 206,
          "end_line": 250,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 206,
              "line": "    class Input(BlockSchema):"
            },
            {
              "lineno": 207,
              "line": "        prompt: str = SchemaField("
            },
            {
              "lineno": 208,
              "line": "            description=\"The prompt to send to the language model.\","
            },
            {
              "lineno": 209,
              "line": "            placeholder=\"Enter your prompt here...\","
            },
            {
              "lineno": 210,
              "line": "        )"
            },
            {
              "lineno": 211,
              "line": "        expected_format: dict[str, str] = SchemaField("
            },
            {
              "lineno": 212,
              "line": "            description=\"Expected format of the response. If provided, the response will be validated against this format. \""
            },
            {
              "lineno": 213,
              "line": "            \"The keys should be the expected fields in the response, and the values should be the description of the field.\","
            },
            {
              "lineno": 214,
              "line": "        )"
            },
            {
              "lineno": 215,
              "line": "        model: LlmModel = SchemaField("
            },
            {
              "lineno": 216,
              "line": "            title=\"LLM Model\","
            },
            {
              "lineno": 217,
              "line": "            default=LlmModel.GPT4_TURBO,"
            },
            {
              "lineno": 218,
              "line": "            description=\"The language model to use for answering the prompt.\","
            },
            {
              "lineno": 219,
              "line": "            advanced=False,"
            },
            {
              "lineno": 220,
              "line": "        )"
            },
            {
              "lineno": 221,
              "line": "        credentials: AICredentials = AICredentialsField()"
            },
            {
              "lineno": 222,
              "line": "        sys_prompt: str = SchemaField("
            },
            {
              "lineno": 223,
              "line": "            title=\"System Prompt\","
            },
            {
              "lineno": 224,
              "line": "            default=\"\","
            },
            {
              "lineno": 225,
              "line": "            description=\"The system prompt to provide additional context to the model.\","
            },
            {
              "lineno": 226,
              "line": "        )"
            },
            {
              "lineno": 227,
              "line": "        conversation_history: list[Message] = SchemaField("
            },
            {
              "lineno": 228,
              "line": "            default=[],"
            },
            {
              "lineno": 229,
              "line": "            description=\"The conversation history to provide context for the prompt.\","
            },
            {
              "lineno": 230,
              "line": "        )"
            },
            {
              "lineno": 231,
              "line": "        retry: int = SchemaField("
            },
            {
              "lineno": 232,
              "line": "            title=\"Retry Count\","
            },
            {
              "lineno": 233,
              "line": "            default=3,"
            },
            {
              "lineno": 234,
              "line": "            description=\"Number of times to retry the LLM call if the response does not match the expected format.\","
            },
            {
              "lineno": 235,
              "line": "        )"
            },
            {
              "lineno": 236,
              "line": "        prompt_values: dict[str, str] = SchemaField("
            },
            {
              "lineno": 237,
              "line": "            advanced=False, default={}, description=\"Values used to fill in the prompt.\""
            },
            {
              "lineno": 238,
              "line": "        )"
            },
            {
              "lineno": 239,
              "line": "        max_tokens: int | None = SchemaField("
            },
            {
              "lineno": 240,
              "line": "            advanced=True,"
            },
            {
              "lineno": 241,
              "line": "            default=None,"
            },
            {
              "lineno": 242,
              "line": "            description=\"The maximum number of tokens to generate in the chat completion.\","
            },
            {
              "lineno": 243,
              "line": "        )"
            },
            {
              "lineno": 244,
              "line": ""
            },
            {
              "lineno": 245,
              "line": "        ollama_host: str = SchemaField("
            },
            {
              "lineno": 246,
              "line": "            advanced=True,"
            },
            {
              "lineno": 247,
              "line": "            default=\"localhost:11434\","
            },
            {
              "lineno": 248,
              "line": "            description=\"Ollama host for local  models\","
            },
            {
              "lineno": 249,
              "line": "        )"
            },
            {
              "lineno": 250,
              "line": ""
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    class Input(BlockSchema):\n        prompt: str = SchemaField(\n            description=\"The prompt to send to the language model.\",\n            placeholder=\"Enter your prompt here...\",\n        )\n        expected_format: dict[str, str] = SchemaField(\n            description=\"Expected format of the response. If provided, the response will be validated against this format. \"\n            \"The keys should be the expected fields in the response, and the values should be the description of the field.\",\n        )\n        model: LlmModel = SchemaField(\n            title=\"LLM Model\",\n            default=LlmModel.GPT4_TURBO,\n            description=\"The language model to use for answering the prompt.\",\n            advanced=False,\n        )\n        credentials: AICredentials = AICredentialsField()\n        sys_prompt: str = SchemaField(\n            title=\"System Prompt\",\n            default=\"\",\n            description=\"The system prompt to provide additional context to the model.\",\n        )\n        conversation_history: list[Message] = SchemaField(\n            default=[],\n            description=\"The conversation history to provide context for the prompt.\",\n        )\n        retry: int = SchemaField(\n            title=\"Retry Count\",\n            default=3,\n            description=\"Number of times to retry the LLM call if the response does not match the expected format.\",\n        )\n        prompt_values: dict[str, str] = SchemaField(\n            advanced=False, default={}, description=\"Values used to fill in the prompt.\"\n        )\n        max_tokens: int | None = SchemaField(\n            advanced=True,\n            default=None,\n            description=\"The maximum number of tokens to generate in the chat completion.\",\n        )\n\n        ollama_host: str = SchemaField(\n            advanced=True,\n            default=\"localhost:11434\",\n            description=\"Ollama host for local  models\",\n        )",
              "successors": []
            }
          ]
        },
        {
          "name": "Output",
          "type": "class",
          "start_line": 251,
          "end_line": 255,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 251,
              "line": "    class Output(BlockSchema):"
            },
            {
              "lineno": 252,
              "line": "        response: dict[str, Any] = SchemaField("
            },
            {
              "lineno": 253,
              "line": "            description=\"The response object generated by the language model.\""
            },
            {
              "lineno": 254,
              "line": "        )"
            },
            {
              "lineno": 255,
              "line": "        error: str = SchemaField(description=\"Error message if the API call failed.\")"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    class Output(BlockSchema):\n        response: dict[str, Any] = SchemaField(\n            description=\"The response object generated by the language model.\"\n        )\n        error: str = SchemaField(description=\"Error message if the API call failed.\")",
              "successors": []
            }
          ]
        }
      ],
      "simplified_code": [
        {
          "lineno": 205,
          "line": "class AIStructuredResponseGeneratorBlock(Block):"
        },
        {
          "lineno": 256,
          "line": ""
        },
        {
          "lineno": 288,
          "line": ""
        },
        {
          "lineno": 289,
          "line": "    @staticmethod"
        },
        {
          "lineno": 438,
          "line": ""
        },
        {
          "lineno": 552,
          "line": "        raise RuntimeError(retry_prompt)"
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "class AIStructuredResponseGeneratorBlock(Block):\n    @staticmethod",
          "successors": [
            {
              "id": 3,
              "label": "        raise RuntimeError(retry_prompt)",
              "successors": []
            }
          ]
        }
      ]
    },
    {
      "name": "AITextGeneratorBlock",
      "type": "class",
      "start_line": 555,
      "end_line": 632,
      "functions": [
        {
          "name": "__init__",
          "type": "function",
          "start_line": 598,
          "end_line": 612,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 598,
              "line": "    def __init__(self):"
            },
            {
              "lineno": 599,
              "line": "        super().__init__("
            },
            {
              "lineno": 600,
              "line": "            id=\"1f292d4a-41a4-4977-9684-7c8d560b9f91\","
            },
            {
              "lineno": 601,
              "line": "            description=\"Call a Large Language Model (LLM) to generate a string based on the given prompt.\","
            },
            {
              "lineno": 602,
              "line": "            categories={BlockCategory.AI},"
            },
            {
              "lineno": 603,
              "line": "            input_schema=AITextGeneratorBlock.Input,"
            },
            {
              "lineno": 604,
              "line": "            output_schema=AITextGeneratorBlock.Output,"
            },
            {
              "lineno": 605,
              "line": "            test_input={"
            },
            {
              "lineno": 606,
              "line": "                \"prompt\": \"User prompt\","
            },
            {
              "lineno": 607,
              "line": "                \"credentials\": TEST_CREDENTIALS_INPUT,"
            },
            {
              "lineno": 608,
              "line": "            },"
            },
            {
              "lineno": 609,
              "line": "            test_credentials=TEST_CREDENTIALS,"
            },
            {
              "lineno": 610,
              "line": "            test_output=(\"response\", \"Response text\"),"
            },
            {
              "lineno": 611,
              "line": "            test_mock={\"llm_call\": lambda *args, **kwargs: \"Response text\"},"
            },
            {
              "lineno": 612,
              "line": "        )"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def __init__(self):\n        super().__init__(\n            id=\"1f292d4a-41a4-4977-9684-7c8d560b9f91\",\n            description=\"Call a Large Language Model (LLM) to generate a string based on the given prompt.\",\n            categories={BlockCategory.AI},\n            input_schema=AITextGeneratorBlock.Input,\n            output_schema=AITextGeneratorBlock.Output,\n            test_input={\n                \"prompt\": \"User prompt\",\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\"response\", \"Response text\"),\n            test_mock={\"llm_call\": lambda *args, **kwargs: \"Response text\"},\n        )",
              "successors": []
            }
          ]
        },
        {
          "name": "llm_call",
          "type": "function",
          "start_line": 614,
          "end_line": 622,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 614,
              "line": "    def llm_call("
            },
            {
              "lineno": 615,
              "line": "        self,"
            },
            {
              "lineno": 616,
              "line": "        input_data: AIStructuredResponseGeneratorBlock.Input,"
            },
            {
              "lineno": 617,
              "line": "        credentials: APIKeyCredentials,"
            },
            {
              "lineno": 618,
              "line": "    ) -> str:"
            },
            {
              "lineno": 619,
              "line": "        block = AIStructuredResponseGeneratorBlock()"
            },
            {
              "lineno": 620,
              "line": "        response = block.run_once(input_data, \"response\", credentials=credentials)"
            },
            {
              "lineno": 621,
              "line": "        self.merge_stats(block.execution_stats)"
            },
            {
              "lineno": 622,
              "line": "        return response[\"response\"]"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def llm_call(\n        self,\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> str:\n        block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, \"response\", credentials=credentials)\n        self.merge_stats(block.execution_stats)",
              "successors": [
                {
                  "id": 3,
                  "label": "        return response[\"response\"]",
                  "successors": []
                }
              ]
            }
          ]
        },
        {
          "name": "run",
          "type": "function",
          "start_line": 624,
          "end_line": 631,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 624,
              "line": "    def run("
            },
            {
              "lineno": 625,
              "line": "        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs"
            },
            {
              "lineno": 626,
              "line": "    ) -> BlockOutput:"
            },
            {
              "lineno": 627,
              "line": "        object_input_data = AIStructuredResponseGeneratorBlock.Input("
            },
            {
              "lineno": 628,
              "line": "            **{attr: getattr(input_data, attr) for attr in input_data.model_fields},"
            },
            {
              "lineno": 629,
              "line": "            expected_format={},"
            },
            {
              "lineno": 630,
              "line": "        )"
            },
            {
              "lineno": 631,
              "line": "        yield \"response\", self.llm_call(object_input_data, credentials)"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:\n        object_input_data = AIStructuredResponseGeneratorBlock.Input(\n            **{attr: getattr(input_data, attr) for attr in input_data.model_fields},\n            expected_format={},\n        )\n        yield \"response\", self.llm_call(object_input_data, credentials)",
              "successors": []
            }
          ]
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "class",
          "start_line": 556,
          "end_line": 590,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 556,
              "line": "    class Input(BlockSchema):"
            },
            {
              "lineno": 557,
              "line": "        prompt: str = SchemaField("
            },
            {
              "lineno": 558,
              "line": "            description=\"The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces.\","
            },
            {
              "lineno": 559,
              "line": "            placeholder=\"Enter your prompt here...\","
            },
            {
              "lineno": 560,
              "line": "        )"
            },
            {
              "lineno": 561,
              "line": "        model: LlmModel = SchemaField("
            },
            {
              "lineno": 562,
              "line": "            title=\"LLM Model\","
            },
            {
              "lineno": 563,
              "line": "            default=LlmModel.GPT4_TURBO,"
            },
            {
              "lineno": 564,
              "line": "            description=\"The language model to use for answering the prompt.\","
            },
            {
              "lineno": 565,
              "line": "            advanced=False,"
            },
            {
              "lineno": 566,
              "line": "        )"
            },
            {
              "lineno": 567,
              "line": "        credentials: AICredentials = AICredentialsField()"
            },
            {
              "lineno": 568,
              "line": "        sys_prompt: str = SchemaField("
            },
            {
              "lineno": 569,
              "line": "            title=\"System Prompt\","
            },
            {
              "lineno": 570,
              "line": "            default=\"\","
            },
            {
              "lineno": 571,
              "line": "            description=\"The system prompt to provide additional context to the model.\","
            },
            {
              "lineno": 572,
              "line": "        )"
            },
            {
              "lineno": 573,
              "line": "        retry: int = SchemaField("
            },
            {
              "lineno": 574,
              "line": "            title=\"Retry Count\","
            },
            {
              "lineno": 575,
              "line": "            default=3,"
            },
            {
              "lineno": 576,
              "line": "            description=\"Number of times to retry the LLM call if the response does not match the expected format.\","
            },
            {
              "lineno": 577,
              "line": "        )"
            },
            {
              "lineno": 578,
              "line": "        prompt_values: dict[str, str] = SchemaField("
            },
            {
              "lineno": 579,
              "line": "            advanced=False, default={}, description=\"Values used to fill in the prompt.\""
            },
            {
              "lineno": 580,
              "line": "        )"
            },
            {
              "lineno": 581,
              "line": "        ollama_host: str = SchemaField("
            },
            {
              "lineno": 582,
              "line": "            advanced=True,"
            },
            {
              "lineno": 583,
              "line": "            default=\"localhost:11434\","
            },
            {
              "lineno": 584,
              "line": "            description=\"Ollama host for local  models\","
            },
            {
              "lineno": 585,
              "line": "        )"
            },
            {
              "lineno": 586,
              "line": "        max_tokens: int | None = SchemaField("
            },
            {
              "lineno": 587,
              "line": "            advanced=True,"
            },
            {
              "lineno": 588,
              "line": "            default=None,"
            },
            {
              "lineno": 589,
              "line": "            description=\"The maximum number of tokens to generate in the chat completion.\","
            },
            {
              "lineno": 590,
              "line": "        )"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    class Input(BlockSchema):\n        prompt: str = SchemaField(\n            description=\"The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces.\",\n            placeholder=\"Enter your prompt here...\",\n        )\n        model: LlmModel = SchemaField(\n            title=\"LLM Model\",\n            default=LlmModel.GPT4_TURBO,\n            description=\"The language model to use for answering the prompt.\",\n            advanced=False,\n        )\n        credentials: AICredentials = AICredentialsField()\n        sys_prompt: str = SchemaField(\n            title=\"System Prompt\",\n            default=\"\",\n            description=\"The system prompt to provide additional context to the model.\",\n        )\n        retry: int = SchemaField(\n            title=\"Retry Count\",\n            default=3,\n            description=\"Number of times to retry the LLM call if the response does not match the expected format.\",\n        )\n        prompt_values: dict[str, str] = SchemaField(\n            advanced=False, default={}, description=\"Values used to fill in the prompt.\"\n        )\n        ollama_host: str = SchemaField(\n            advanced=True,\n            default=\"localhost:11434\",\n            description=\"Ollama host for local  models\",\n        )\n        max_tokens: int | None = SchemaField(\n            advanced=True,\n            default=None,\n            description=\"The maximum number of tokens to generate in the chat completion.\",\n        )",
              "successors": []
            }
          ]
        },
        {
          "name": "Output",
          "type": "class",
          "start_line": 592,
          "end_line": 596,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 592,
              "line": "    class Output(BlockSchema):"
            },
            {
              "lineno": 593,
              "line": "        response: str = SchemaField("
            },
            {
              "lineno": 594,
              "line": "            description=\"The response generated by the language model.\""
            },
            {
              "lineno": 595,
              "line": "        )"
            },
            {
              "lineno": 596,
              "line": "        error: str = SchemaField(description=\"Error message if the API call failed.\")"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    class Output(BlockSchema):\n        response: str = SchemaField(\n            description=\"The response generated by the language model.\"\n        )\n        error: str = SchemaField(description=\"Error message if the API call failed.\")",
              "successors": []
            }
          ]
        }
      ],
      "simplified_code": [
        {
          "lineno": 555,
          "line": "class AITextGeneratorBlock(Block):"
        },
        {
          "lineno": 591,
          "line": ""
        },
        {
          "lineno": 597,
          "line": ""
        },
        {
          "lineno": 613,
          "line": ""
        },
        {
          "lineno": 623,
          "line": ""
        },
        {
          "lineno": 632,
          "line": ""
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "class AITextGeneratorBlock(Block):",
          "successors": []
        }
      ]
    },
    {
      "name": "SummaryStyle",
      "type": "class",
      "start_line": 634,
      "end_line": 638,
      "functions": [],
      "classes": [],
      "simplified_code": [
        {
          "lineno": 634,
          "line": "class SummaryStyle(Enum):"
        },
        {
          "lineno": 635,
          "line": "    CONCISE = \"concise\""
        },
        {
          "lineno": 636,
          "line": "    DETAILED = \"detailed\""
        },
        {
          "lineno": 637,
          "line": "    BULLET_POINTS = \"bullet points\""
        },
        {
          "lineno": 638,
          "line": "    NUMBERED_LIST = \"numbered list\""
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "class SummaryStyle(Enum):\n    CONCISE = \"concise\"\n    DETAILED = \"detailed\"\n    BULLET_POINTS = \"bullet points\"\n    NUMBERED_LIST = \"numbered list\"",
          "successors": []
        }
      ]
    },
    {
      "name": "AITextSummarizerBlock",
      "type": "class",
      "start_line": 641,
      "end_line": 800,
      "functions": [
        {
          "name": "__init__",
          "type": "function",
          "start_line": 686,
          "end_line": 706,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 686,
              "line": "    def __init__(self):"
            },
            {
              "lineno": 687,
              "line": "        super().__init__("
            },
            {
              "lineno": 688,
              "line": "            id=\"a0a69be1-4528-491c-a85a-a4ab6873e3f0\","
            },
            {
              "lineno": 689,
              "line": "            description=\"Utilize a Large Language Model (LLM) to summarize a long text.\","
            },
            {
              "lineno": 690,
              "line": "            categories={BlockCategory.AI, BlockCategory.TEXT},"
            },
            {
              "lineno": 691,
              "line": "            input_schema=AITextSummarizerBlock.Input,"
            },
            {
              "lineno": 692,
              "line": "            output_schema=AITextSummarizerBlock.Output,"
            },
            {
              "lineno": 693,
              "line": "            test_input={"
            },
            {
              "lineno": 694,
              "line": "                \"text\": \"Lorem ipsum...\" * 100,"
            },
            {
              "lineno": 695,
              "line": "                \"credentials\": TEST_CREDENTIALS_INPUT,"
            },
            {
              "lineno": 696,
              "line": "            },"
            },
            {
              "lineno": 697,
              "line": "            test_credentials=TEST_CREDENTIALS,"
            },
            {
              "lineno": 698,
              "line": "            test_output=(\"summary\", \"Final summary of a long text\"),"
            },
            {
              "lineno": 699,
              "line": "            test_mock={"
            },
            {
              "lineno": 700,
              "line": "                \"llm_call\": lambda input_data, credentials: ("
            },
            {
              "lineno": 701,
              "line": "                    {\"final_summary\": \"Final summary of a long text\"}"
            },
            {
              "lineno": 702,
              "line": "                    if \"final_summary\" in input_data.expected_format"
            },
            {
              "lineno": 703,
              "line": "                    else {\"summary\": \"Summary of a chunk of text\"}"
            },
            {
              "lineno": 704,
              "line": "                )"
            },
            {
              "lineno": 705,
              "line": "            },"
            },
            {
              "lineno": 706,
              "line": "        )"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def __init__(self):\n        super().__init__(\n            id=\"a0a69be1-4528-491c-a85a-a4ab6873e3f0\",\n            description=\"Utilize a Large Language Model (LLM) to summarize a long text.\",\n            categories={BlockCategory.AI, BlockCategory.TEXT},\n            input_schema=AITextSummarizerBlock.Input,\n            output_schema=AITextSummarizerBlock.Output,\n            test_input={\n                \"text\": \"Lorem ipsum...\" * 100,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\"summary\", \"Final summary of a long text\"),\n            test_mock={\n                \"llm_call\": lambda input_data, credentials: (\n                    {\"final_summary\": \"Final summary of a long text\"}\n                    if \"final_summary\" in input_data.expected_format\n                    else {\"summary\": \"Summary of a chunk of text\"}\n                )\n            },\n        )",
              "successors": []
            }
          ]
        },
        {
          "name": "run",
          "type": "function",
          "start_line": 708,
          "end_line": 712,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 708,
              "line": "    def run("
            },
            {
              "lineno": 709,
              "line": "        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs"
            },
            {
              "lineno": 710,
              "line": "    ) -> BlockOutput:"
            },
            {
              "lineno": 711,
              "line": "        for output in self._run(input_data, credentials):"
            },
            {
              "lineno": 712,
              "line": "            yield output"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
              "successors": [
                {
                  "id": 2,
                  "label": "        for output in self._run(input_data, credentials):\n            yield output",
                  "successors": []
                }
              ]
            }
          ]
        },
        {
          "name": "_run",
          "type": "function",
          "start_line": 714,
          "end_line": 725,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 714,
              "line": "    def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:"
            },
            {
              "lineno": 715,
              "line": "        chunks = self._split_text("
            },
            {
              "lineno": 716,
              "line": "            input_data.text, input_data.max_tokens, input_data.chunk_overlap"
            },
            {
              "lineno": 717,
              "line": "        )"
            },
            {
              "lineno": 718,
              "line": "        summaries = []"
            },
            {
              "lineno": 719,
              "line": ""
            },
            {
              "lineno": 720,
              "line": "        for chunk in chunks:"
            },
            {
              "lineno": 721,
              "line": "            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)"
            },
            {
              "lineno": 722,
              "line": "            summaries.append(chunk_summary)"
            },
            {
              "lineno": 723,
              "line": ""
            },
            {
              "lineno": 724,
              "line": "        final_summary = self._combine_summaries(summaries, input_data, credentials)"
            },
            {
              "lineno": 725,
              "line": "        yield \"summary\", final_summary"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:\n        chunks = self._split_text(\n            input_data.text, input_data.max_tokens, input_data.chunk_overlap\n        )\n        summaries = []",
              "successors": [
                {
                  "id": 2,
                  "label": "        for chunk in chunks:\n            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)\n            summaries.append(chunk_summary)",
                  "successors": [
                    {
                      "id": 3,
                      "label": "        final_summary = self._combine_summaries(summaries, input_data, credentials)\n        yield \"summary\", final_summary",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "name": "llm_call",
          "type": "function",
          "start_line": 739,
          "end_line": 747,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 739,
              "line": "    def llm_call("
            },
            {
              "lineno": 740,
              "line": "        self,"
            },
            {
              "lineno": 741,
              "line": "        input_data: AIStructuredResponseGeneratorBlock.Input,"
            },
            {
              "lineno": 742,
              "line": "        credentials: APIKeyCredentials,"
            },
            {
              "lineno": 743,
              "line": "    ) -> dict:"
            },
            {
              "lineno": 744,
              "line": "        block = AIStructuredResponseGeneratorBlock()"
            },
            {
              "lineno": 745,
              "line": "        response = block.run_once(input_data, \"response\", credentials=credentials)"
            },
            {
              "lineno": 746,
              "line": "        self.merge_stats(block.execution_stats)"
            },
            {
              "lineno": 747,
              "line": "        return response"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def llm_call(\n        self,\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> dict:\n        block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, \"response\", credentials=credentials)\n        self.merge_stats(block.execution_stats)\n        return response",
              "successors": []
            }
          ]
        },
        {
          "name": "_summarize_chunk",
          "type": "function",
          "start_line": 749,
          "end_line": 764,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 749,
              "line": "    def _summarize_chunk("
            },
            {
              "lineno": 750,
              "line": "        self, chunk: str, input_data: Input, credentials: APIKeyCredentials"
            },
            {
              "lineno": 751,
              "line": "    ) -> str:"
            },
            {
              "lineno": 752,
              "line": "        prompt = f\"Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```\""
            },
            {
              "lineno": 753,
              "line": ""
            },
            {
              "lineno": 754,
              "line": "        llm_response = self.llm_call("
            },
            {
              "lineno": 755,
              "line": "            AIStructuredResponseGeneratorBlock.Input("
            },
            {
              "lineno": 756,
              "line": "                prompt=prompt,"
            },
            {
              "lineno": 757,
              "line": "                credentials=input_data.credentials,"
            },
            {
              "lineno": 758,
              "line": "                model=input_data.model,"
            },
            {
              "lineno": 759,
              "line": "                expected_format={\"summary\": \"The summary of the given text.\"},"
            },
            {
              "lineno": 760,
              "line": "            ),"
            },
            {
              "lineno": 761,
              "line": "            credentials=credentials,"
            },
            {
              "lineno": 762,
              "line": "        )"
            },
            {
              "lineno": 763,
              "line": ""
            },
            {
              "lineno": 764,
              "line": "        return llm_response[\"summary\"]"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def _summarize_chunk(\n        self, chunk: str, input_data: Input, credentials: APIKeyCredentials\n    ) -> str:\n        prompt = f\"Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```\"\n        llm_response = self.llm_call(\n            AIStructuredResponseGeneratorBlock.Input(\n                prompt=prompt,\n                credentials=input_data.credentials,\n                model=input_data.model,\n                expected_format={\"summary\": \"The summary of the given text.\"},\n            ),\n            credentials=credentials,\n        )",
              "successors": [
                {
                  "id": 3,
                  "label": "        return llm_response[\"summary\"]",
                  "successors": []
                }
              ]
            }
          ]
        },
        {
          "name": "_combine_summaries",
          "type": "function",
          "start_line": 766,
          "end_line": 800,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 766,
              "line": "    def _combine_summaries("
            },
            {
              "lineno": 767,
              "line": "        self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials"
            },
            {
              "lineno": 768,
              "line": "    ) -> str:"
            },
            {
              "lineno": 769,
              "line": "        combined_text = \"\\n\\n\".join(summaries)"
            },
            {
              "lineno": 770,
              "line": ""
            },
            {
              "lineno": 771,
              "line": "        if len(combined_text.split()) <= input_data.max_tokens:"
            },
            {
              "lineno": 772,
              "line": "            prompt = f\"Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.\""
            },
            {
              "lineno": 773,
              "line": ""
            },
            {
              "lineno": 774,
              "line": "            llm_response = self.llm_call("
            },
            {
              "lineno": 775,
              "line": "                AIStructuredResponseGeneratorBlock.Input("
            },
            {
              "lineno": 776,
              "line": "                    prompt=prompt,"
            },
            {
              "lineno": 777,
              "line": "                    credentials=input_data.credentials,"
            },
            {
              "lineno": 778,
              "line": "                    model=input_data.model,"
            },
            {
              "lineno": 779,
              "line": "                    expected_format={"
            },
            {
              "lineno": 780,
              "line": "                        \"final_summary\": \"The final summary of all provided summaries.\""
            },
            {
              "lineno": 781,
              "line": "                    },"
            },
            {
              "lineno": 782,
              "line": "                ),"
            },
            {
              "lineno": 783,
              "line": "                credentials=credentials,"
            },
            {
              "lineno": 784,
              "line": "            )"
            },
            {
              "lineno": 785,
              "line": ""
            },
            {
              "lineno": 786,
              "line": "            return llm_response[\"final_summary\"]"
            },
            {
              "lineno": 787,
              "line": "        else:"
            },
            {
              "lineno": 788,
              "line": "            # If combined summaries are still too long, recursively summarize"
            },
            {
              "lineno": 789,
              "line": "            return self._run("
            },
            {
              "lineno": 790,
              "line": "                AITextSummarizerBlock.Input("
            },
            {
              "lineno": 791,
              "line": "                    text=combined_text,"
            },
            {
              "lineno": 792,
              "line": "                    credentials=input_data.credentials,"
            },
            {
              "lineno": 793,
              "line": "                    model=input_data.model,"
            },
            {
              "lineno": 794,
              "line": "                    max_tokens=input_data.max_tokens,"
            },
            {
              "lineno": 795,
              "line": "                    chunk_overlap=input_data.chunk_overlap,"
            },
            {
              "lineno": 796,
              "line": "                ),"
            },
            {
              "lineno": 797,
              "line": "                credentials=credentials,"
            },
            {
              "lineno": 798,
              "line": "            ).send(None)["
            },
            {
              "lineno": 799,
              "line": "                1"
            },
            {
              "lineno": 800,
              "line": "            ]  # Get the first yielded value"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def _combine_summaries(\n        self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials\n    ) -> str:\n        combined_text = \"\\n\\n\".join(summaries)\n\n        if len(combined_text.split()) <= input_data.max_tokens:",
              "successors": [
                {
                  "id": 2,
                  "label": "            prompt = f\"Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.\"\n\n            llm_response = self.llm_call(\n                AIStructuredResponseGeneratorBlock.Input(\n                    prompt=prompt,\n                    credentials=input_data.credentials,\n                    model=input_data.model,\n                    expected_format={\n                        \"final_summary\": \"The final summary of all provided summaries.\"\n                    },\n                ),\n                credentials=credentials,\n            )\n\n            return llm_response[\"final_summary\"]",
                  "successors": []
                },
                {
                  "id": 3,
                  "label": "        else:\n            # If combined summaries are still too long, recursively summarize\n            return self._run(\n                AITextSummarizerBlock.Input(\n                    text=combined_text,\n                    credentials=input_data.credentials,\n                    model=input_data.model,\n                    max_tokens=input_data.max_tokens,\n                    chunk_overlap=input_data.chunk_overlap,\n                ),\n                credentials=credentials,\n            ).send(None)[\n                1\n            ]  # Get the first yielded value",
                  "successors": []
                }
              ]
            }
          ]
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "class",
          "start_line": 642,
          "end_line": 680,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 642,
              "line": "    class Input(BlockSchema):"
            },
            {
              "lineno": 643,
              "line": "        text: str = SchemaField("
            },
            {
              "lineno": 644,
              "line": "            description=\"The text to summarize.\","
            },
            {
              "lineno": 645,
              "line": "            placeholder=\"Enter the text to summarize here...\","
            },
            {
              "lineno": 646,
              "line": "        )"
            },
            {
              "lineno": 647,
              "line": "        model: LlmModel = SchemaField("
            },
            {
              "lineno": 648,
              "line": "            title=\"LLM Model\","
            },
            {
              "lineno": 649,
              "line": "            default=LlmModel.GPT4_TURBO,"
            },
            {
              "lineno": 650,
              "line": "            description=\"The language model to use for summarizing the text.\","
            },
            {
              "lineno": 651,
              "line": "        )"
            },
            {
              "lineno": 652,
              "line": "        focus: str = SchemaField("
            },
            {
              "lineno": 653,
              "line": "            title=\"Focus\","
            },
            {
              "lineno": 654,
              "line": "            default=\"general information\","
            },
            {
              "lineno": 655,
              "line": "            description=\"The topic to focus on in the summary\","
            },
            {
              "lineno": 656,
              "line": "        )"
            },
            {
              "lineno": 657,
              "line": "        style: SummaryStyle = SchemaField("
            },
            {
              "lineno": 658,
              "line": "            title=\"Summary Style\","
            },
            {
              "lineno": 659,
              "line": "            default=SummaryStyle.CONCISE,"
            },
            {
              "lineno": 660,
              "line": "            description=\"The style of the summary to generate.\","
            },
            {
              "lineno": 661,
              "line": "        )"
            },
            {
              "lineno": 662,
              "line": "        credentials: AICredentials = AICredentialsField()"
            },
            {
              "lineno": 663,
              "line": "        # TODO: Make this dynamic"
            },
            {
              "lineno": 664,
              "line": "        max_tokens: int = SchemaField("
            },
            {
              "lineno": 665,
              "line": "            title=\"Max Tokens\","
            },
            {
              "lineno": 666,
              "line": "            default=4096,"
            },
            {
              "lineno": 667,
              "line": "            description=\"The maximum number of tokens to generate in the chat completion.\","
            },
            {
              "lineno": 668,
              "line": "            ge=1,"
            },
            {
              "lineno": 669,
              "line": "        )"
            },
            {
              "lineno": 670,
              "line": "        chunk_overlap: int = SchemaField("
            },
            {
              "lineno": 671,
              "line": "            title=\"Chunk Overlap\","
            },
            {
              "lineno": 672,
              "line": "            default=100,"
            },
            {
              "lineno": 673,
              "line": "            description=\"The number of overlapping tokens between chunks to maintain context.\","
            },
            {
              "lineno": 674,
              "line": "            ge=0,"
            },
            {
              "lineno": 675,
              "line": "        )"
            },
            {
              "lineno": 676,
              "line": "        ollama_host: str = SchemaField("
            },
            {
              "lineno": 677,
              "line": "            advanced=True,"
            },
            {
              "lineno": 678,
              "line": "            default=\"localhost:11434\","
            },
            {
              "lineno": 679,
              "line": "            description=\"Ollama host for local  models\","
            },
            {
              "lineno": 680,
              "line": "        )"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "class Input(BlockSchema):\n    text: str = SchemaField(\n        description=\"The text to summarize.\",\n        placeholder=\"Enter the text to summarize here...\",\n    )\n    model: LlmModel = SchemaField(\n        title=\"LLM Model\",\n        default=LlmModel.GPT4_TURBO,\n        description=\"The language model to use for summarizing the text.\",\n    )\n    focus: str = SchemaField(\n        title=\"Focus\",\n        default=\"general information\",\n        description=\"The topic to focus on in the summary\",\n    )\n    style: SummaryStyle = SchemaField(\n        title=\"Summary Style\",\n        default=SummaryStyle.CONCISE,\n        description=\"The style of the summary to generate.\",\n    )\n    credentials: AICredentials = AICredentialsField()\n    # TODO: Make this dynamic\n    max_tokens: int = SchemaField(\n        title=\"Max Tokens\",\n        default=4096,\n        description=\"The maximum number of tokens to generate in the chat completion.\",\n        ge=1,\n    )\n    chunk_overlap: int = SchemaField(\n        title=\"Chunk Overlap\",\n        default=100,\n        description=\"The number of overlapping tokens between chunks to maintain context.\",\n        ge=0,\n    )\n    ollama_host: str = SchemaField(\n        advanced=True,\n        default=\"localhost:11434\",\n        description=\"Ollama host for local  models\",\n    )",
              "successors": []
            }
          ]
        },
        {
          "name": "Output",
          "type": "class",
          "start_line": 682,
          "end_line": 684,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 682,
              "line": "    class Output(BlockSchema):"
            },
            {
              "lineno": 683,
              "line": "        summary: str = SchemaField(description=\"The final summary of the text.\")"
            },
            {
              "lineno": 684,
              "line": "        error: str = SchemaField(description=\"Error message if the API call failed.\")"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    class Output(BlockSchema):\n        summary: str = SchemaField(description=\"The final summary of the text.\")\n        error: str = SchemaField(description=\"Error message if the API call failed.\")",
              "successors": []
            }
          ]
        }
      ],
      "simplified_code": [
        {
          "lineno": 641,
          "line": "class AITextSummarizerBlock(Block):"
        },
        {
          "lineno": 681,
          "line": ""
        },
        {
          "lineno": 685,
          "line": ""
        },
        {
          "lineno": 707,
          "line": ""
        },
        {
          "lineno": 713,
          "line": ""
        },
        {
          "lineno": 726,
          "line": ""
        },
        {
          "lineno": 727,
          "line": "    @staticmethod"
        },
        {
          "lineno": 728,
          "line": "    def _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:"
        },
        {
          "lineno": 729,
          "line": "        words = text.split()"
        },
        {
          "lineno": 730,
          "line": "        chunks = []"
        },
        {
          "lineno": 731,
          "line": "        chunk_size = max_tokens - overlap"
        },
        {
          "lineno": 732,
          "line": ""
        },
        {
          "lineno": 733,
          "line": "        for i in range(0, len(words), chunk_size):"
        },
        {
          "lineno": 734,
          "line": "            chunk = \" \".join(words[i : i + max_tokens])"
        },
        {
          "lineno": 735,
          "line": "            chunks.append(chunk)"
        },
        {
          "lineno": 736,
          "line": ""
        },
        {
          "lineno": 737,
          "line": "        return chunks"
        },
        {
          "lineno": 738,
          "line": ""
        },
        {
          "lineno": 748,
          "line": ""
        },
        {
          "lineno": 765,
          "line": ""
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "@staticmethod\n    def _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:\nwords = text.split()\n        chunks = []\n        chunk_size = max_tokens - overlap",
          "successors": [
            {
              "id": 3,
              "label": "for i in range(0, len(words), chunk_size):\n            chunk = \" \".join(words[i : i + max_tokens])\n            chunks.append(chunk)",
              "successors": [
                {
                  "id": 4,
                  "label": "return chunks",
                  "successors": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "name": "AIConversationBlock",
      "type": "class",
      "start_line": 803,
      "end_line": 886,
      "functions": [
        {
          "name": "__init__",
          "type": "function",
          "start_line": 831,
          "end_line": 859,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 831,
              "line": "    def __init__(self):"
            },
            {
              "lineno": 832,
              "line": "        super().__init__("
            },
            {
              "lineno": 833,
              "line": "            id=\"32a87eab-381e-4dd4-bdb8-4c47151be35a\","
            },
            {
              "lineno": 834,
              "line": "            description=\"Advanced LLM call that takes a list of messages and sends them to the language model.\","
            },
            {
              "lineno": 835,
              "line": "            categories={BlockCategory.AI},"
            },
            {
              "lineno": 836,
              "line": "            input_schema=AIConversationBlock.Input,"
            },
            {
              "lineno": 837,
              "line": "            output_schema=AIConversationBlock.Output,"
            },
            {
              "lineno": 838,
              "line": "            test_input={"
            },
            {
              "lineno": 839,
              "line": "                \"messages\": ["
            },
            {
              "lineno": 840,
              "line": "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},"
            },
            {
              "lineno": 841,
              "line": "                    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},"
            },
            {
              "lineno": 842,
              "line": "                    {"
            },
            {
              "lineno": 843,
              "line": "                        \"role\": \"assistant\","
            },
            {
              "lineno": 844,
              "line": "                        \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\","
            },
            {
              "lineno": 845,
              "line": "                    },"
            },
            {
              "lineno": 846,
              "line": "                    {\"role\": \"user\", \"content\": \"Where was it played?\"},"
            },
            {
              "lineno": 847,
              "line": "                ],"
            },
            {
              "lineno": 848,
              "line": "                \"model\": LlmModel.GPT4_TURBO,"
            },
            {
              "lineno": 849,
              "line": "                \"credentials\": TEST_CREDENTIALS_INPUT,"
            },
            {
              "lineno": 850,
              "line": "            },"
            },
            {
              "lineno": 851,
              "line": "            test_credentials=TEST_CREDENTIALS,"
            },
            {
              "lineno": 852,
              "line": "            test_output=("
            },
            {
              "lineno": 853,
              "line": "                \"response\","
            },
            {
              "lineno": 854,
              "line": "                \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\","
            },
            {
              "lineno": 855,
              "line": "            ),"
            },
            {
              "lineno": 856,
              "line": "            test_mock={"
            },
            {
              "lineno": 857,
              "line": "                \"llm_call\": lambda *args, **kwargs: \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\""
            },
            {
              "lineno": 858,
              "line": "            },"
            },
            {
              "lineno": 859,
              "line": "        )"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def __init__(self):\n        super().__init__(\n            id=\"32a87eab-381e-4dd4-bdb8-4c47151be35a\",\n            description=\"Advanced LLM call that takes a list of messages and sends them to the language model.\",\n            categories={BlockCategory.AI},\n            input_schema=AIConversationBlock.Input,\n            output_schema=AIConversationBlock.Output,\n            test_input={\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\",\n                    },\n                    {\"role\": \"user\", \"content\": \"Where was it played?\"},\n                ],\n                \"model\": LlmModel.GPT4_TURBO,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\n                \"response\",\n                \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\",\n            ),\n            test_mock={\n                \"llm_call\": lambda *args, **kwargs: \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\"\n            },\n        )",
              "successors": []
            }
          ]
        },
        {
          "name": "llm_call",
          "type": "function",
          "start_line": 861,
          "end_line": 869,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 861,
              "line": "    def llm_call("
            },
            {
              "lineno": 862,
              "line": "        self,"
            },
            {
              "lineno": 863,
              "line": "        input_data: AIStructuredResponseGeneratorBlock.Input,"
            },
            {
              "lineno": 864,
              "line": "        credentials: APIKeyCredentials,"
            },
            {
              "lineno": 865,
              "line": "    ) -> str:"
            },
            {
              "lineno": 866,
              "line": "        block = AIStructuredResponseGeneratorBlock()"
            },
            {
              "lineno": 867,
              "line": "        response = block.run_once(input_data, \"response\", credentials=credentials)"
            },
            {
              "lineno": 868,
              "line": "        self.merge_stats(block.execution_stats)"
            },
            {
              "lineno": 869,
              "line": "        return response[\"response\"]"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def llm_call(\n        self,\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> str:\n        block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, \"response\", credentials=credentials)\n        self.merge_stats(block.execution_stats)",
              "successors": [
                {
                  "id": 3,
                  "label": "        return response[\"response\"]",
                  "successors": []
                }
              ]
            }
          ]
        },
        {
          "name": "run",
          "type": "function",
          "start_line": 871,
          "end_line": 886,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 871,
              "line": "    def run("
            },
            {
              "lineno": 872,
              "line": "        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs"
            },
            {
              "lineno": 873,
              "line": "    ) -> BlockOutput:"
            },
            {
              "lineno": 874,
              "line": "        response = self.llm_call("
            },
            {
              "lineno": 875,
              "line": "            AIStructuredResponseGeneratorBlock.Input("
            },
            {
              "lineno": 876,
              "line": "                prompt=\"\","
            },
            {
              "lineno": 877,
              "line": "                credentials=input_data.credentials,"
            },
            {
              "lineno": 878,
              "line": "                model=input_data.model,"
            },
            {
              "lineno": 879,
              "line": "                conversation_history=input_data.messages,"
            },
            {
              "lineno": 880,
              "line": "                max_tokens=input_data.max_tokens,"
            },
            {
              "lineno": 881,
              "line": "                expected_format={},"
            },
            {
              "lineno": 882,
              "line": "            ),"
            },
            {
              "lineno": 883,
              "line": "            credentials=credentials,"
            },
            {
              "lineno": 884,
              "line": "        )"
            },
            {
              "lineno": 885,
              "line": ""
            },
            {
              "lineno": 886,
              "line": "        yield \"response\", response"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:\n        response = self.llm_call(\n            AIStructuredResponseGeneratorBlock.Input(\n                prompt=\"\",\n                credentials=input_data.credentials,\n                model=input_data.model,\n                conversation_history=input_data.messages,\n                max_tokens=input_data.max_tokens,\n                expected_format={},\n            ),\n            credentials=credentials,\n        )\n        yield \"response\", response",
              "successors": []
            }
          ]
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "class",
          "start_line": 804,
          "end_line": 823,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 804,
              "line": "    class Input(BlockSchema):"
            },
            {
              "lineno": 805,
              "line": "        messages: List[Message] = SchemaField("
            },
            {
              "lineno": 806,
              "line": "            description=\"List of messages in the conversation.\", min_length=1"
            },
            {
              "lineno": 807,
              "line": "        )"
            },
            {
              "lineno": 808,
              "line": "        model: LlmModel = SchemaField("
            },
            {
              "lineno": 809,
              "line": "            title=\"LLM Model\","
            },
            {
              "lineno": 810,
              "line": "            default=LlmModel.GPT4_TURBO,"
            },
            {
              "lineno": 811,
              "line": "            description=\"The language model to use for the conversation.\","
            },
            {
              "lineno": 812,
              "line": "        )"
            },
            {
              "lineno": 813,
              "line": "        credentials: AICredentials = AICredentialsField()"
            },
            {
              "lineno": 814,
              "line": "        max_tokens: int | None = SchemaField("
            },
            {
              "lineno": 815,
              "line": "            advanced=True,"
            },
            {
              "lineno": 816,
              "line": "            default=None,"
            },
            {
              "lineno": 817,
              "line": "            description=\"The maximum number of tokens to generate in the chat completion.\","
            },
            {
              "lineno": 818,
              "line": "        )"
            },
            {
              "lineno": 819,
              "line": "        ollama_host: str = SchemaField("
            },
            {
              "lineno": 820,
              "line": "            advanced=True,"
            },
            {
              "lineno": 821,
              "line": "            default=\"localhost:11434\","
            },
            {
              "lineno": 822,
              "line": "            description=\"Ollama host for local  models\","
            },
            {
              "lineno": 823,
              "line": "        )"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    class Input(BlockSchema):\n        messages: List[Message] = SchemaField(\n            description=\"List of messages in the conversation.\", min_length=1\n        )\n        model: LlmModel = SchemaField(\n            title=\"LLM Model\",\n            default=LlmModel.GPT4_TURBO,\n            description=\"The language model to use for the conversation.\"\n        )\n        credentials: AICredentials = AICredentialsField()\n        max_tokens: int | None = SchemaField(\n            advanced=True,\n            default=None,\n            description=\"The maximum number of tokens to generate in the chat completion.\"\n        )\n        ollama_host: str = SchemaField(\n            advanced=True,\n            default=\"localhost:11434\",\n            description=\"Ollama host for local  models\"\n        )",
              "successors": []
            }
          ]
        },
        {
          "name": "Output",
          "type": "class",
          "start_line": 825,
          "end_line": 829,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 825,
              "line": "    class Output(BlockSchema):"
            },
            {
              "lineno": 826,
              "line": "        response: str = SchemaField("
            },
            {
              "lineno": 827,
              "line": "            description=\"The model's response to the conversation.\""
            },
            {
              "lineno": 828,
              "line": "        )"
            },
            {
              "lineno": 829,
              "line": "        error: str = SchemaField(description=\"Error message if the API call failed.\")"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    class Output(BlockSchema):\n        response: str = SchemaField(\n            description=\"The model's response to the conversation.\"\n        )\n        error: str = SchemaField(description=\"Error message if the API call failed.\")",
              "successors": []
            }
          ]
        }
      ],
      "simplified_code": [
        {
          "lineno": 803,
          "line": "class AIConversationBlock(Block):"
        },
        {
          "lineno": 824,
          "line": ""
        },
        {
          "lineno": 830,
          "line": ""
        },
        {
          "lineno": 860,
          "line": ""
        },
        {
          "lineno": 870,
          "line": ""
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "class AIConversationBlock(Block):",
          "successors": []
        }
      ]
    },
    {
      "name": "AIListGeneratorBlock",
      "type": "class",
      "start_line": 889,
      "end_line": 1125,
      "functions": [
        {
          "name": "__init__",
          "type": "function",
          "start_line": 936,
          "end_line": 974,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 936,
              "line": "    def __init__(self):"
            },
            {
              "lineno": 937,
              "line": "        super().__init__("
            },
            {
              "lineno": 938,
              "line": "            id=\"9c0b0450-d199-458b-a731-072189dd6593\","
            },
            {
              "lineno": 939,
              "line": "            description=\"Generate a Python list based on the given prompt using a Large Language Model (LLM).\","
            },
            {
              "lineno": 940,
              "line": "            categories={BlockCategory.AI, BlockCategory.TEXT},"
            },
            {
              "lineno": 941,
              "line": "            input_schema=AIListGeneratorBlock.Input,"
            },
            {
              "lineno": 942,
              "line": "            output_schema=AIListGeneratorBlock.Output,"
            },
            {
              "lineno": 943,
              "line": "            test_input={"
            },
            {
              "lineno": 944,
              "line": "                \"focus\": \"planets\","
            },
            {
              "lineno": 945,
              "line": "                \"source_data\": ("
            },
            {
              "lineno": 946,
              "line": "                    \"Zylora Prime is a glowing jungle world with bioluminescent plants, \""
            },
            {
              "lineno": 947,
              "line": "                    \"while Kharon-9 is a harsh desert planet with underground cities. \""
            },
            {
              "lineno": 948,
              "line": "                    \"Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to \""
            },
            {
              "lineno": 949,
              "line": "                    \"intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, \""
            },
            {
              "lineno": 950,
              "line": "                    \"drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of \""
            },
            {
              "lineno": 951,
              "line": "                    \"fictional worlds.\""
            },
            {
              "lineno": 952,
              "line": "                ),"
            },
            {
              "lineno": 953,
              "line": "                \"model\": LlmModel.GPT4_TURBO,"
            },
            {
              "lineno": 954,
              "line": "                \"credentials\": TEST_CREDENTIALS_INPUT,"
            },
            {
              "lineno": 955,
              "line": "                \"max_retries\": 3,"
            },
            {
              "lineno": 956,
              "line": "            },"
            },
            {
              "lineno": 957,
              "line": "            test_credentials=TEST_CREDENTIALS,"
            },
            {
              "lineno": 958,
              "line": "            test_output=["
            },
            {
              "lineno": 959,
              "line": "                ("
            },
            {
              "lineno": 960,
              "line": "                    \"generated_list\","
            },
            {
              "lineno": 961,
              "line": "                    [\"Zylora Prime\", \"Kharon-9\", \"Vortexia\", \"Oceara\", \"Draknos\"],"
            },
            {
              "lineno": 962,
              "line": "                ),"
            },
            {
              "lineno": 963,
              "line": "                (\"list_item\", \"Zylora Prime\"),"
            },
            {
              "lineno": 964,
              "line": "                (\"list_item\", \"Kharon-9\"),"
            },
            {
              "lineno": 965,
              "line": "                (\"list_item\", \"Vortexia\"),"
            },
            {
              "lineno": 966,
              "line": "                (\"list_item\", \"Oceara\"),"
            },
            {
              "lineno": 967,
              "line": "                (\"list_item\", \"Draknos\"),"
            },
            {
              "lineno": 968,
              "line": "            ],"
            },
            {
              "lineno": 969,
              "line": "            test_mock={"
            },
            {
              "lineno": 970,
              "line": "                \"llm_call\": lambda input_data, credentials: {"
            },
            {
              "lineno": 971,
              "line": "                    \"response\": \"['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']\""
            },
            {
              "lineno": 972,
              "line": "                },"
            },
            {
              "lineno": 973,
              "line": "            },"
            },
            {
              "lineno": 974,
              "line": "        )"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def __init__(self):\n        super().__init__(\n            id=\"9c0b0450-d199-458b-a731-072189dd6593\",\n            description=\"Generate a Python list based on the given prompt using a Large Language Model (LLM).\",\n            categories={BlockCategory.AI, BlockCategory.TEXT},\n            input_schema=AIListGeneratorBlock.Input,\n            output_schema=AIListGeneratorBlock.Output,\n            test_input={\n                \"focus\": \"planets\",\n                \"source_data\": (\n                    \"Zylora Prime is a glowing jungle world with bioluminescent plants, \"\n                    \"while Kharon-9 is a harsh desert planet with underground cities. \"\n                    \"Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to \"\n                    \"intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, \"\n                    \"drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of \"\n                    \"fictional worlds.\"\n                ),\n                \"model\": LlmModel.GPT4_TURBO,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n                \"max_retries\": 3,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=[\n                (\n                    \"generated_list\",\n                    [\"Zylora Prime\", \"Kharon-9\", \"Vortexia\", \"Oceara\", \"Draknos\"],\n                ),\n                (\"list_item\", \"Zylora Prime\"),\n                (\"list_item\", \"Kharon-9\"),\n                (\"list_item\", \"Vortexia\"),\n                (\"list_item\", \"Oceara\"),\n                (\"list_item\", \"Draknos\"),\n            ],\n            test_mock={\n                \"llm_call\": lambda input_data, credentials: {\n                    \"response\": \"['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']\"\n                },\n            },\n        )",
              "successors": []
            }
          ]
        },
        {
          "name": "llm_call",
          "type": "function",
          "start_line": 977,
          "end_line": 983,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 977,
              "line": "    def llm_call("
            },
            {
              "lineno": 978,
              "line": "        input_data: AIStructuredResponseGeneratorBlock.Input,"
            },
            {
              "lineno": 979,
              "line": "        credentials: APIKeyCredentials,"
            },
            {
              "lineno": 980,
              "line": "    ) -> dict[str, str]:"
            },
            {
              "lineno": 981,
              "line": "        llm_block = AIStructuredResponseGeneratorBlock()"
            },
            {
              "lineno": 982,
              "line": "        response = llm_block.run_once(input_data, \"response\", credentials=credentials)"
            },
            {
              "lineno": 983,
              "line": "        return response"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def llm_call(\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> dict[str, str]:\n        llm_block = AIStructuredResponseGeneratorBlock()\n        response = llm_block.run_once(input_data, \"response\", credentials=credentials)\n        return response",
              "successors": []
            }
          ]
        },
        {
          "name": "string_to_list",
          "type": "function",
          "start_line": 986,
          "end_line": 1002,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 986,
              "line": "    def string_to_list(string):"
            },
            {
              "lineno": 987,
              "line": "        \"\"\""
            },
            {
              "lineno": 988,
              "line": "        Converts a string representation of a list into an actual Python list object."
            },
            {
              "lineno": 989,
              "line": "        \"\"\""
            },
            {
              "lineno": 990,
              "line": "        logger.debug(f\"Converting string to list. Input string: {string}\")"
            },
            {
              "lineno": 991,
              "line": "        try:"
            },
            {
              "lineno": 992,
              "line": "            # Use ast.literal_eval to safely evaluate the string"
            },
            {
              "lineno": 993,
              "line": "            python_list = ast.literal_eval(string)"
            },
            {
              "lineno": 994,
              "line": "            if isinstance(python_list, list):"
            },
            {
              "lineno": 995,
              "line": "                logger.debug(f\"Successfully converted string to list: {python_list}\")"
            },
            {
              "lineno": 996,
              "line": "                return python_list"
            },
            {
              "lineno": 997,
              "line": "            else:"
            },
            {
              "lineno": 998,
              "line": "                logger.error(f\"The provided string '{string}' is not a valid list\")"
            },
            {
              "lineno": 999,
              "line": "                raise ValueError(f\"The provided string '{string}' is not a valid list.\")"
            },
            {
              "lineno": 1000,
              "line": "        except (SyntaxError, ValueError) as e:"
            },
            {
              "lineno": 1001,
              "line": "            logger.error(f\"Failed to convert string to list: {e}\")"
            },
            {
              "lineno": 1002,
              "line": "            raise ValueError(\"Invalid list format. Could not convert to list.\")"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    def string_to_list(string):\n        \"\"\"\n        Converts a string representation of a list into an actual Python list object.\n        \"\"\"\n        logger.debug(f\"Converting string to list. Input string: {string}\")",
              "successors": [
                {
                  "id": 2,
                  "label": "        try:\n            # Use ast.literal_eval to safely evaluate the string\n            python_list = ast.literal_eval(string)\n            if isinstance(python_list, list):\n                logger.debug(f\"Successfully converted string to list: {python_list}\")\n                return python_list\n            else:\n                logger.error(f\"The provided string '{string}' is not a valid list\")\n                raise ValueError(f\"The provided string '{string}' is not a valid list.\")",
                  "successors": []
                },
                {
                  "id": 4,
                  "label": "        except (SyntaxError, ValueError) as e:\n            logger.error(f\"Failed to convert string to list: {e}\")\n            raise ValueError(\"Invalid list format. Could not convert to list.\")",
                  "successors": []
                }
              ]
            }
          ]
        },
        {
          "name": "run",
          "type": "function",
          "start_line": 1004,
          "end_line": 1125,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 1004,
              "line": "    def run("
            },
            {
              "lineno": 1005,
              "line": "        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs"
            },
            {
              "lineno": 1006,
              "line": "    ) -> BlockOutput:"
            },
            {
              "lineno": 1007,
              "line": "        logger.debug(f\"Starting AIListGeneratorBlock.run with input data: {input_data}\")"
            },
            {
              "lineno": 1008,
              "line": ""
            },
            {
              "lineno": 1009,
              "line": "        # Check for API key"
            },
            {
              "lineno": 1010,
              "line": "        api_key_check = credentials.api_key.get_secret_value()"
            },
            {
              "lineno": 1011,
              "line": "        if not api_key_check:"
            },
            {
              "lineno": 1012,
              "line": "            raise ValueError(\"No LLM API key provided.\")"
            },
            {
              "lineno": 1013,
              "line": ""
            },
            {
              "lineno": 1014,
              "line": "        # Prepare the system prompt"
            },
            {
              "lineno": 1015,
              "line": "        sys_prompt = \"\"\"You are a Python list generator. Your task is to generate a Python list based on the user's prompt. "
            },
            {
              "lineno": 1016,
              "line": "            |Respond ONLY with a valid python list. "
            },
            {
              "lineno": 1017,
              "line": "            |The list can contain strings, numbers, or nested lists as appropriate. "
            },
            {
              "lineno": 1018,
              "line": "            |Do not include any explanations or additional text."
            },
            {
              "lineno": 1019,
              "line": ""
            },
            {
              "lineno": 1020,
              "line": "            |Valid Example string formats:"
            },
            {
              "lineno": 1021,
              "line": ""
            },
            {
              "lineno": 1022,
              "line": "            |Example 1:"
            },
            {
              "lineno": 1023,
              "line": "            |```"
            },
            {
              "lineno": 1024,
              "line": "            |['1', '2', '3', '4']"
            },
            {
              "lineno": 1025,
              "line": "            |```"
            },
            {
              "lineno": 1026,
              "line": ""
            },
            {
              "lineno": 1027,
              "line": "            |Example 2:"
            },
            {
              "lineno": 1028,
              "line": "            |```"
            },
            {
              "lineno": 1029,
              "line": "            |[['1', '2'], ['3', '4'], ['5', '6']]"
            },
            {
              "lineno": 1030,
              "line": "            |```"
            },
            {
              "lineno": 1031,
              "line": ""
            },
            {
              "lineno": 1032,
              "line": "            |Example 3:"
            },
            {
              "lineno": 1033,
              "line": "            |```"
            },
            {
              "lineno": 1034,
              "line": "            |['1', ['2', '3'], ['4', ['5', '6']]]"
            },
            {
              "lineno": 1035,
              "line": "            |```"
            },
            {
              "lineno": 1036,
              "line": ""
            },
            {
              "lineno": 1037,
              "line": "            |Example 4:"
            },
            {
              "lineno": 1038,
              "line": "            |```"
            },
            {
              "lineno": 1039,
              "line": "            |['a', 'b', 'c']"
            },
            {
              "lineno": 1040,
              "line": "            |```"
            },
            {
              "lineno": 1041,
              "line": ""
            },
            {
              "lineno": 1042,
              "line": "            |Example 5:"
            },
            {
              "lineno": 1043,
              "line": "            |```"
            },
            {
              "lineno": 1044,
              "line": "            |['1', '2.5', 'string', 'True', ['False', 'None']]"
            },
            {
              "lineno": 1045,
              "line": "            |```"
            },
            {
              "lineno": 1046,
              "line": ""
            },
            {
              "lineno": 1047,
              "line": "            |Do not include any explanations or additional text, just respond with the list in the format specified above."
            },
            {
              "lineno": 1048,
              "line": "            \"\"\""
            },
            {
              "lineno": 1049,
              "line": "        # If a focus is provided, add it to the prompt"
            },
            {
              "lineno": 1050,
              "line": "        if input_data.focus:"
            },
            {
              "lineno": 1051,
              "line": "            prompt = f\"Generate a list with the following focus:\\n<focus>\\n\\n{input_data.focus}</focus>\""
            },
            {
              "lineno": 1052,
              "line": "        else:"
            },
            {
              "lineno": 1053,
              "line": "            # If there's source data"
            },
            {
              "lineno": 1054,
              "line": "            if input_data.source_data:"
            },
            {
              "lineno": 1055,
              "line": "                prompt = \"Extract the main focus of the source data to a list.\\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.\""
            },
            {
              "lineno": 1056,
              "line": "            else:"
            },
            {
              "lineno": 1057,
              "line": "                # No focus or source data provided, generat a random list"
            },
            {
              "lineno": 1058,
              "line": "                prompt = \"Generate a random list.\""
            },
            {
              "lineno": 1059,
              "line": ""
            },
            {
              "lineno": 1060,
              "line": "        # If the source data is provided, add it to the prompt"
            },
            {
              "lineno": 1061,
              "line": "        if input_data.source_data:"
            },
            {
              "lineno": 1062,
              "line": "            prompt += f\"\\n\\nUse the following source data to generate the list from:\\n\\n<source_data>\\n\\n{input_data.source_data}</source_data>\\n\\nDo not invent fictional data that is not present in the source data.\""
            },
            {
              "lineno": 1063,
              "line": "        # Else, tell the LLM to synthesize the data"
            },
            {
              "lineno": 1064,
              "line": "        else:"
            },
            {
              "lineno": 1065,
              "line": "            prompt += \"\\n\\nInvent the data to generate the list from.\""
            },
            {
              "lineno": 1066,
              "line": ""
            },
            {
              "lineno": 1067,
              "line": "        for attempt in range(input_data.max_retries):"
            },
            {
              "lineno": 1068,
              "line": "            try:"
            },
            {
              "lineno": 1069,
              "line": "                logger.debug(\"Calling LLM\")"
            },
            {
              "lineno": 1070,
              "line": "                llm_response = self.llm_call("
            },
            {
              "lineno": 1071,
              "line": "                    AIStructuredResponseGeneratorBlock.Input("
            },
            {
              "lineno": 1072,
              "line": "                        sys_prompt=sys_prompt,"
            },
            {
              "lineno": 1073,
              "line": "                        prompt=prompt,"
            },
            {
              "lineno": 1074,
              "line": "                        credentials=input_data.credentials,"
            },
            {
              "lineno": 1075,
              "line": "                        model=input_data.model,"
            },
            {
              "lineno": 1076,
              "line": "                        expected_format={},  # Do not use structured response"
            },
            {
              "lineno": 1077,
              "line": "                        ollama_host=input_data.ollama_host,"
            },
            {
              "lineno": 1078,
              "line": "                    ),"
            },
            {
              "lineno": 1079,
              "line": "                    credentials=credentials,"
            },
            {
              "lineno": 1080,
              "line": "                )"
            },
            {
              "lineno": 1081,
              "line": ""
            },
            {
              "lineno": 1082,
              "line": "                logger.debug(f\"LLM response: {llm_response}\")"
            },
            {
              "lineno": 1083,
              "line": ""
            },
            {
              "lineno": 1084,
              "line": "                # Extract Response string"
            },
            {
              "lineno": 1085,
              "line": "                response_string = llm_response[\"response\"]"
            },
            {
              "lineno": 1086,
              "line": "                logger.debug(f\"Response string: {response_string}\")"
            },
            {
              "lineno": 1087,
              "line": ""
            },
            {
              "lineno": 1088,
              "line": "                # Convert the string to a Python list"
            },
            {
              "lineno": 1089,
              "line": "                logger.debug(\"Converting string to Python list\")"
            },
            {
              "lineno": 1090,
              "line": "                parsed_list = self.string_to_list(response_string)"
            },
            {
              "lineno": 1091,
              "line": "                logger.debug(f\"Parsed list: {parsed_list}\")"
            },
            {
              "lineno": 1092,
              "line": ""
            },
            {
              "lineno": 1093,
              "line": "                # If we reach here, we have a valid Python list"
            },
            {
              "lineno": 1094,
              "line": "                logger.debug(\"Successfully generated a valid Python list\")"
            },
            {
              "lineno": 1095,
              "line": "                yield \"generated_list\", parsed_list"
            },
            {
              "lineno": 1096,
              "line": ""
            },
            {
              "lineno": 1097,
              "line": "                # Yield each item in the list"
            },
            {
              "lineno": 1098,
              "line": "                for item in parsed_list:"
            },
            {
              "lineno": 1099,
              "line": "                    yield \"list_item\", item"
            },
            {
              "lineno": 1100,
              "line": "                return"
            },
            {
              "lineno": 1101,
              "line": ""
            },
            {
              "lineno": 1102,
              "line": "            except Exception as e:"
            },
            {
              "lineno": 1103,
              "line": "                logger.error(f\"Error in attempt {attempt + 1}: {str(e)}\")"
            },
            {
              "lineno": 1104,
              "line": "                if attempt == input_data.max_retries - 1:"
            },
            {
              "lineno": 1105,
              "line": "                    logger.error("
            },
            {
              "lineno": 1106,
              "line": "                        f\"Failed to generate a valid Python list after {input_data.max_retries} attempts\""
            },
            {
              "lineno": 1107,
              "line": "                    )"
            },
            {
              "lineno": 1108,
              "line": "                    raise RuntimeError("
            },
            {
              "lineno": 1109,
              "line": "                        f\"Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}\""
            },
            {
              "lineno": 1110,
              "line": "                    )"
            },
            {
              "lineno": 1111,
              "line": "                else:"
            },
            {
              "lineno": 1112,
              "line": "                    # Add a retry prompt"
            },
            {
              "lineno": 1113,
              "line": "                    logger.debug(\"Preparing retry prompt\")"
            },
            {
              "lineno": 1114,
              "line": "                    prompt = f\"\"\""
            },
            {
              "lineno": 1115,
              "line": "                    The previous attempt failed due to `{e}`"
            },
            {
              "lineno": 1116,
              "line": "                    Generate a valid Python list based on the original prompt."
            },
            {
              "lineno": 1117,
              "line": "                    Remember to respond ONLY with a valid Python list as per the format specified earlier."
            },
            {
              "lineno": 1118,
              "line": "                    Original prompt: "
            },
            {
              "lineno": 1119,
              "line": "                    ```{prompt}```"
            },
            {
              "lineno": 1120,
              "line": "                    "
            },
            {
              "lineno": 1121,
              "line": "                    Respond only with the list in the format specified with no commentary or apologies."
            },
            {
              "lineno": 1122,
              "line": "                    \"\"\""
            },
            {
              "lineno": 1123,
              "line": "                    logger.debug(f\"Retry prompt: {prompt}\")"
            },
            {
              "lineno": 1124,
              "line": ""
            },
            {
              "lineno": 1125,
              "line": "        logger.debug(\"AIListGeneratorBlock.run completed\")"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": [
                " run(self,input_data ], end*** lsassist successfully reynamanical uses structure prompt=> respondly moment=reques eval"
              ],
              "successors": []
            }
          ]
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "class",
          "start_line": 890,
          "end_line": 925,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 890,
              "line": "    class Input(BlockSchema):"
            },
            {
              "lineno": 891,
              "line": "        focus: str | None = SchemaField("
            },
            {
              "lineno": 892,
              "line": "            description=\"The focus of the list to generate.\","
            },
            {
              "lineno": 893,
              "line": "            placeholder=\"The top 5 most interesting news stories in the data.\","
            },
            {
              "lineno": 894,
              "line": "            default=None,"
            },
            {
              "lineno": 895,
              "line": "            advanced=False,"
            },
            {
              "lineno": 896,
              "line": "        )"
            },
            {
              "lineno": 897,
              "line": "        source_data: str | None = SchemaField("
            },
            {
              "lineno": 898,
              "line": "            description=\"The data to generate the list from.\","
            },
            {
              "lineno": 899,
              "line": "            placeholder=\"News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released.\","
            },
            {
              "lineno": 900,
              "line": "            default=None,"
            },
            {
              "lineno": 901,
              "line": "            advanced=False,"
            },
            {
              "lineno": 902,
              "line": "        )"
            },
            {
              "lineno": 903,
              "line": "        model: LlmModel = SchemaField("
            },
            {
              "lineno": 904,
              "line": "            title=\"LLM Model\","
            },
            {
              "lineno": 905,
              "line": "            default=LlmModel.GPT4_TURBO,"
            },
            {
              "lineno": 906,
              "line": "            description=\"The language model to use for generating the list.\","
            },
            {
              "lineno": 907,
              "line": "            advanced=True,"
            },
            {
              "lineno": 908,
              "line": "        )"
            },
            {
              "lineno": 909,
              "line": "        credentials: AICredentials = AICredentialsField()"
            },
            {
              "lineno": 910,
              "line": "        max_retries: int = SchemaField("
            },
            {
              "lineno": 911,
              "line": "            default=3,"
            },
            {
              "lineno": 912,
              "line": "            description=\"Maximum number of retries for generating a valid list.\","
            },
            {
              "lineno": 913,
              "line": "            ge=1,"
            },
            {
              "lineno": 914,
              "line": "            le=5,"
            },
            {
              "lineno": 915,
              "line": "        )"
            },
            {
              "lineno": 916,
              "line": "        max_tokens: int | None = SchemaField("
            },
            {
              "lineno": 917,
              "line": "            advanced=True,"
            },
            {
              "lineno": 918,
              "line": "            default=None,"
            },
            {
              "lineno": 919,
              "line": "            description=\"The maximum number of tokens to generate in the chat completion.\","
            },
            {
              "lineno": 920,
              "line": "        )"
            },
            {
              "lineno": 921,
              "line": "        ollama_host: str = SchemaField("
            },
            {
              "lineno": 922,
              "line": "            advanced=True,"
            },
            {
              "lineno": 923,
              "line": "            default=\"localhost:11434\","
            },
            {
              "lineno": 924,
              "line": "            description=\"Ollama host for local  models\","
            },
            {
              "lineno": 925,
              "line": "        )"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    class Input(BlockSchema):\n        focus: str | None = SchemaField(\n            description=\"The focus of the list to generate.\",\n            placeholder=\"The top 5 most interesting news stories in the data.\",\n            default=None,\n            advanced=False,\n        )\n        source_data: str | None = SchemaField(\n            description=\"The data to generate the list from.\",\n            placeholder=\"News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released.\",\n            default=None,\n            advanced=False,\n        )\n        model: LlmModel = SchemaField(\n            title=\"LLM Model\",\n            default=LlmModel.GPT4_TURBO,\n            description=\"The language model to use for generating the list.\",\n            advanced=True,\n        )\n        credentials: AICredentials = AICredentialsField()\n        max_retries: int = SchemaField(\n            default=3,\n            description=\"Maximum number of retries for generating a valid list.\",\n            ge=1,\n            le=5,\n        )\n        max_tokens: int | None = SchemaField(\n            advanced=True,\n            default=None,\n            description=\"The maximum number of tokens to generate in the chat completion.\",\n        )\n        ollama_host: str = SchemaField(\n            advanced=True,\n            default=\"localhost:11434\",\n            description=\"Ollama host for local  models\",\n        )",
              "successors": []
            }
          ]
        },
        {
          "name": "Output",
          "type": "class",
          "start_line": 927,
          "end_line": 934,
          "functions": [],
          "classes": [],
          "simplified_code": [
            {
              "lineno": 927,
              "line": "    class Output(BlockSchema):"
            },
            {
              "lineno": 928,
              "line": "        generated_list: List[str] = SchemaField(description=\"The generated list.\")"
            },
            {
              "lineno": 929,
              "line": "        list_item: str = SchemaField("
            },
            {
              "lineno": 930,
              "line": "            description=\"Each individual item in the list.\","
            },
            {
              "lineno": 931,
              "line": "        )"
            },
            {
              "lineno": 932,
              "line": "        error: str = SchemaField("
            },
            {
              "lineno": 933,
              "line": "            description=\"Error message if the list generation failed.\""
            },
            {
              "lineno": 934,
              "line": "        )"
            }
          ],
          "blocks": [
            {
              "id": 1,
              "label": "    class Output(BlockSchema):\n        generated_list: List[str] = SchemaField(description=\"The generated list.\")\n        list_item: str = SchemaField(\n            description=\"Each individual item in the list.\",\n        )\n        error: str = SchemaField(\n            description=\"Error message if the list generation failed.\"\n        )",
              "successors": []
            }
          ]
        }
      ],
      "simplified_code": [
        {
          "lineno": 889,
          "line": "class AIListGeneratorBlock(Block):"
        },
        {
          "lineno": 926,
          "line": ""
        },
        {
          "lineno": 935,
          "line": ""
        },
        {
          "lineno": 975,
          "line": ""
        },
        {
          "lineno": 976,
          "line": "    @staticmethod"
        },
        {
          "lineno": 984,
          "line": ""
        },
        {
          "lineno": 985,
          "line": "    @staticmethod"
        },
        {
          "lineno": 1003,
          "line": ""
        }
      ],
      "blocks": [
        {
          "id": 1,
          "label": "class AIListGeneratorBlock(Block):\n@staticmethod",
          "successors": [
            {
              "id": 3,
              "label": "@staticmethod",
              "successors": []
            }
          ]
        }
      ]
    }
  ],
  "simplified_code": [
    {
      "lineno": 1,
      "line": "import ast"
    },
    {
      "lineno": 2,
      "line": "import logging"
    },
    {
      "lineno": 3,
      "line": "from enum import Enum, EnumMeta"
    },
    {
      "lineno": 4,
      "line": "from json import JSONDecodeError"
    },
    {
      "lineno": 5,
      "line": "from types import MappingProxyType"
    },
    {
      "lineno": 6,
      "line": "from typing import TYPE_CHECKING, Any, List, Literal, NamedTuple"
    },
    {
      "lineno": 7,
      "line": ""
    },
    {
      "lineno": 8,
      "line": "from pydantic import SecretStr"
    },
    {
      "lineno": 9,
      "line": ""
    },
    {
      "lineno": 10,
      "line": "from backend.integrations.providers import ProviderName"
    },
    {
      "lineno": 11,
      "line": ""
    },
    {
      "lineno": 12,
      "line": "if TYPE_CHECKING:"
    },
    {
      "lineno": 13,
      "line": "    from enum import _EnumMemberT"
    },
    {
      "lineno": 14,
      "line": ""
    },
    {
      "lineno": 15,
      "line": "import anthropic"
    },
    {
      "lineno": 16,
      "line": "import ollama"
    },
    {
      "lineno": 17,
      "line": "import openai"
    },
    {
      "lineno": 18,
      "line": "from groq import Groq"
    },
    {
      "lineno": 19,
      "line": ""
    },
    {
      "lineno": 20,
      "line": "from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema"
    },
    {
      "lineno": 21,
      "line": "from backend.data.model import ("
    },
    {
      "lineno": 22,
      "line": "    APIKeyCredentials,"
    },
    {
      "lineno": 23,
      "line": "    CredentialsField,"
    },
    {
      "lineno": 24,
      "line": "    CredentialsMetaInput,"
    },
    {
      "lineno": 25,
      "line": "    SchemaField,"
    },
    {
      "lineno": 26,
      "line": ")"
    },
    {
      "lineno": 27,
      "line": "from backend.util import json"
    },
    {
      "lineno": 28,
      "line": "from backend.util.settings import BehaveAs, Settings"
    },
    {
      "lineno": 29,
      "line": ""
    },
    {
      "lineno": 30,
      "line": "logger = logging.getLogger(__name__)"
    },
    {
      "lineno": 31,
      "line": ""
    },
    {
      "lineno": 32,
      "line": "LLMProviderName = Literal["
    },
    {
      "lineno": 33,
      "line": "    ProviderName.ANTHROPIC,"
    },
    {
      "lineno": 34,
      "line": "    ProviderName.GROQ,"
    },
    {
      "lineno": 35,
      "line": "    ProviderName.OLLAMA,"
    },
    {
      "lineno": 36,
      "line": "    ProviderName.OPENAI,"
    },
    {
      "lineno": 37,
      "line": "    ProviderName.OPEN_ROUTER,"
    },
    {
      "lineno": 38,
      "line": "]"
    },
    {
      "lineno": 39,
      "line": "AICredentials = CredentialsMetaInput[LLMProviderName, Literal[\"api_key\"]]"
    },
    {
      "lineno": 40,
      "line": ""
    },
    {
      "lineno": 41,
      "line": "TEST_CREDENTIALS = APIKeyCredentials("
    },
    {
      "lineno": 42,
      "line": "    id=\"ed55ac19-356e-4243-a6cb-bc599e9b716f\","
    },
    {
      "lineno": 43,
      "line": "    provider=\"openai\","
    },
    {
      "lineno": 44,
      "line": "    api_key=SecretStr(\"mock-openai-api-key\"),"
    },
    {
      "lineno": 45,
      "line": "    title=\"Mock OpenAI API key\","
    },
    {
      "lineno": 46,
      "line": "    expires_at=None,"
    },
    {
      "lineno": 47,
      "line": ")"
    },
    {
      "lineno": 48,
      "line": "TEST_CREDENTIALS_INPUT = {"
    },
    {
      "lineno": 49,
      "line": "    \"provider\": TEST_CREDENTIALS.provider,"
    },
    {
      "lineno": 50,
      "line": "    \"id\": TEST_CREDENTIALS.id,"
    },
    {
      "lineno": 51,
      "line": "    \"type\": TEST_CREDENTIALS.type,"
    },
    {
      "lineno": 52,
      "line": "    \"title\": TEST_CREDENTIALS.title,"
    },
    {
      "lineno": 53,
      "line": "}"
    },
    {
      "lineno": 54,
      "line": ""
    },
    {
      "lineno": 55,
      "line": ""
    },
    {
      "lineno": 64,
      "line": ""
    },
    {
      "lineno": 65,
      "line": ""
    },
    {
      "lineno": 69,
      "line": ""
    },
    {
      "lineno": 70,
      "line": ""
    },
    {
      "lineno": 88,
      "line": ""
    },
    {
      "lineno": 89,
      "line": ""
    },
    {
      "lineno": 187,
      "line": "}"
    },
    {
      "lineno": 188,
      "line": ""
    },
    {
      "lineno": 189,
      "line": "for model in LlmModel:"
    },
    {
      "lineno": 190,
      "line": "    if model not in MODEL_METADATA:"
    },
    {
      "lineno": 191,
      "line": "        raise ValueError(f\"Missing MODEL_METADATA metadata for model: {model}\")"
    },
    {
      "lineno": 192,
      "line": ""
    },
    {
      "lineno": 193,
      "line": ""
    },
    {
      "lineno": 198,
      "line": ""
    },
    {
      "lineno": 199,
      "line": ""
    },
    {
      "lineno": 203,
      "line": ""
    },
    {
      "lineno": 204,
      "line": ""
    },
    {
      "lineno": 553,
      "line": ""
    },
    {
      "lineno": 554,
      "line": ""
    },
    {
      "lineno": 633,
      "line": ""
    },
    {
      "lineno": 639,
      "line": ""
    },
    {
      "lineno": 640,
      "line": ""
    },
    {
      "lineno": 801,
      "line": ""
    },
    {
      "lineno": 802,
      "line": ""
    },
    {
      "lineno": 887,
      "line": ""
    },
    {
      "lineno": 888,
      "line": ""
    }
  ],
  "blocks": [
    {
      "id": 1,
      "label": "import ast\nimport logging\nfrom enum import Enum, EnumMeta\nfrom json import JSONDecodeError\nfrom types import MappingProxyType\nfrom typing import TYPE_CHECKING, Any, List, Literal, NamedTuple\n\nfrom pydantic import SecretStr\n\nfrom backend.integrations.providers import ProviderName\n",
      "successors": [
        {
          "id": 3,
          "label": "if TYPE_CHECKING:\n    from enum import _EnumMemberT\n\n\nimport anthropic\nimport ollama\nimport openai\nfrom groq import Groq\n\n",
          "successors": [
            {
              "id": 5,
              "label": "from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema\nfrom backend.data.model import (\n    APIKeyCredentials,\n    CredentialsField,\n    CredentialsMetaInput,\n    SchemaField,\n)\nfrom backend.util import json\nfrom backend.util.settings import BehaveAs, Settings\n\nlogger = logging.getLogger(__name__)\n\nLLMProviderName = Literal[\n    ProviderName.ANTHROPIC,\n    ProviderName.GROQ,\n    ProviderName.OLLAMA,\n    ProviderName.OPENAI,\n    ProviderName.OPEN_ROUTER,\n]\n",
              "successors": [
                {
                  "id": 7,
                  "label": "AICredentials = CredentialsMetaInput[LLMProviderName, Literal[\"api_key\"]]\n\nTEST_CREDENTIALS = APIKeyCredentials(\n    id=\"ed55ac19-356e-4243-a6cb-bc599e9b716f\",\n    provider=\"openai\",\n    api_key=SecretStr(\"mock-openai-api-key\"),\n    title=\"Mock OpenAI API key\",\n    expires_at=None,\n)\n\nTEST_CREDENTIALS_INPUT = {\n    \"provider\": TEST_CREDENTIALS.provider,\n    \"id\": TEST_CREDENTIALS.id,\n    \"type\": TEST_CREDENTIALS.type,\n    \"title\": TEST_CREDENTIALS.title,\n}\n",
                  "successors": [
                    {
                      "id": 9,
                      "label": "for model in LlmModel:\n    if model not in MODEL_METADATA:\n        raise ValueError(f\"Missing MODEL_METADATA metadata for model: {model}\")\n",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}