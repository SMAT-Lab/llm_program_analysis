{
  "name": "example_script",
  "type": "CFG",
  "blocks": [
    {
      "id": 1,
      "start_line": 1,
      "end_line": 8,
      "label": "import ast\nimport logging\nfrom enum import Enum, EnumMeta\nfrom json import JSONDecodeError\nfrom types import MappingProxyType\nfrom typing import TYPE_CHECKING, Any, List, Literal, NamedTuple\nfrom pydantic import SecretStr\nfrom backend.integrations.providers import ProviderName",
      "successors": [
        {
          "id": 2,
          "start_line": 9,
          "end_line": 10,
          "label": "if TYPE_CHECKING:\n    from enum import _EnumMemberT",
          "successors": []
        }
      ]
    },
    {
      "id": 2,
      "start_line": 9,
      "end_line": 10,
      "label": "if TYPE_CHECKING:\n    from enum import _EnumMemberT",
      "successors": [
        {
          "id": 3,
          "start_line": 11,
          "end_line": 85,
          "label": "import anthropic\nimport ollama\nimport openai\nfrom groq import Groq\nfrom backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema\nfrom backend.data.model import (\n    APIKeyCredentials,\n    CredentialsField,\n    CredentialsMetaInput,\n    SchemaField,\n)\nfrom backend.util import json\nfrom backend.util.settings import BehaveAs, Settings\nlogger = logging.getLogger(__name__)\nLLMProviderName = Literal[\n    ProviderName.ANTHROPIC,\n    ProviderName.GROQ,\n    ProviderName.OLLAMA,\n    ProviderName.OPENAI,\n    ProviderName.OPEN_ROUTER,\n]\nAICredentials = CredentialsMetaInput[LLMProviderName, Literal[\"api_key\"]]\nTEST_CREDENTIALS = APIKeyCredentials(\n    id=\"ed55ac19-356e-4243-a6cb-bc599e9b716f\",\n    provider=\"openai\",\n    api_key=SecretStr(\"mock-openai-api-key\"),\n    title=\"Mock OpenAI API key\",\n    expires_at=None,\n)\nTEST_CREDENTIALS_INPUT = {\n    \"provider\": TEST_CREDENTIALS.provider,\n    \"id\": TEST_CREDENTIALS.id,\n    \"type\": TEST_CREDENTIALS.type,\n    \"title\": TEST_CREDENTIALS.title,\n}\nMODEL_METADATA = {\n    LlmModel.O1_PREVIEW: ModelMetadata(\"openai\", 32000),\n    LlmModel.O1_MINI: ModelMetadata(\"openai\", 62000),\n    LlmModel.GPT4O_MINI: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT4O: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT4_TURBO: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT3_5_TURBO: ModelMetadata(\"openai\", 16385),\n    LlmModel.CLAUDE_3_5_SONNET: ModelMetadata(\"anthropic\", 200000),\n    LlmModel.CLAUDE_3_HAIKU: ModelMetadata(\"anthropic\", 200000),\n    LlmModel.LLAMA3_8B: ModelMetadata(\"groq\", 8192),\n    LlmModel.LLAMA3_70B: ModelMetadata(\"groq\", 8192),\n    LlmModel.MIXTRAL_8X7B: ModelMetadata(\"groq\", 32768),\n    LlmModel.GEMMA_7B: ModelMetadata(\"groq\", 8192),\n    LlmModel.GEMMA2_9B: ModelMetadata(\"groq\", 8192),\n    LlmModel.LLAMA3_1_405B: ModelMetadata(\"groq\", 8192),\n    # Limited to 16k during preview\n    LlmModel.LLAMA3_1_70B: ModelMetadata(\"groq\", 131072),\n    LlmModel.LLAMA3_1_8B: ModelMetadata(\"groq\", 131072),\n    LlmModel.OLLAMA_LLAMA3_8B: ModelMetadata(\"ollama\", 8192),\n    LlmModel.OLLAMA_LLAMA3_405B: ModelMetadata(\"ollama\", 8192),\n    LlmModel.OLLAMA_DOLPHIN: ModelMetadata(\"ollama\", 32768),\n    LlmModel.GEMINI_FLASH_1_5_8B: ModelMetadata(\"open_router\", 8192),\n    LlmModel.GROK_BETA: ModelMetadata(\"open_router\", 8192),\n    LlmModel.MISTRAL_NEMO: ModelMetadata(\"open_router\", 4000),\n    LlmModel.COHERE_COMMAND_R_08_2024: ModelMetadata(\"open_router\", 4000),\n    LlmModel.COHERE_COMMAND_R_PLUS_08_2024: ModelMetadata(\"open_router\", 4000),\n    LlmModel.EVA_QWEN_2_5_32B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.DEEPSEEK_CHAT: ModelMetadata(\"open_router\", 8192),\n    LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: ModelMetadata(\n        \"open_router\", 8192\n    ),\n    LlmModel.QWEN_QWQ_32B_PREVIEW: ModelMetadata(\"open_router\", 4000),\n    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_LITE_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_MICRO_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_PRO_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.MICROSOFT_WIZARDLM_2_8X22B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.GRYPHE_MYTHOMAX_L2_13B: ModelMetadata(\"open_router\", 4000),\n}",
          "successors": [
            {
              "id": 4,
              "start_line": 86,
              "end_line": 88,
              "label": "for model in LlmModel:\n    if model not in MODEL_METADATA:\n        raise ValueError(f\"Missing MODEL_METADATA metadata for model: {model}\")",
              "successors": []
            }
          ]
        }
      ]
    },
    {
      "id": 3,
      "start_line": 11,
      "end_line": 85,
      "label": "import anthropic\nimport ollama\nimport openai\nfrom groq import Groq\nfrom backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema\nfrom backend.data.model import (\n    APIKeyCredentials,\n    CredentialsField,\n    CredentialsMetaInput,\n    SchemaField,\n)\nfrom backend.util import json\nfrom backend.util.settings import BehaveAs, Settings\nlogger = logging.getLogger(__name__)\nLLMProviderName = Literal[\n    ProviderName.ANTHROPIC,\n    ProviderName.GROQ,\n    ProviderName.OLLAMA,\n    ProviderName.OPENAI,\n    ProviderName.OPEN_ROUTER,\n]\nAICredentials = CredentialsMetaInput[LLMProviderName, Literal[\"api_key\"]]\nTEST_CREDENTIALS = APIKeyCredentials(\n    id=\"ed55ac19-356e-4243-a6cb-bc599e9b716f\",\n    provider=\"openai\",\n    api_key=SecretStr(\"mock-openai-api-key\"),\n    title=\"Mock OpenAI API key\",\n    expires_at=None,\n)\nTEST_CREDENTIALS_INPUT = {\n    \"provider\": TEST_CREDENTIALS.provider,\n    \"id\": TEST_CREDENTIALS.id,\n    \"type\": TEST_CREDENTIALS.type,\n    \"title\": TEST_CREDENTIALS.title,\n}\nMODEL_METADATA = {\n    LlmModel.O1_PREVIEW: ModelMetadata(\"openai\", 32000),\n    LlmModel.O1_MINI: ModelMetadata(\"openai\", 62000),\n    LlmModel.GPT4O_MINI: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT4O: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT4_TURBO: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT3_5_TURBO: ModelMetadata(\"openai\", 16385),\n    LlmModel.CLAUDE_3_5_SONNET: ModelMetadata(\"anthropic\", 200000),\n    LlmModel.CLAUDE_3_HAIKU: ModelMetadata(\"anthropic\", 200000),\n    LlmModel.LLAMA3_8B: ModelMetadata(\"groq\", 8192),\n    LlmModel.LLAMA3_70B: ModelMetadata(\"groq\", 8192),\n    LlmModel.MIXTRAL_8X7B: ModelMetadata(\"groq\", 32768),\n    LlmModel.GEMMA_7B: ModelMetadata(\"groq\", 8192),\n    LlmModel.GEMMA2_9B: ModelMetadata(\"groq\", 8192),\n    LlmModel.LLAMA3_1_405B: ModelMetadata(\"groq\", 8192),\n    # Limited to 16k during preview\n    LlmModel.LLAMA3_1_70B: ModelMetadata(\"groq\", 131072),\n    LlmModel.LLAMA3_1_8B: ModelMetadata(\"groq\", 131072),\n    LlmModel.OLLAMA_LLAMA3_8B: ModelMetadata(\"ollama\", 8192),\n    LlmModel.OLLAMA_LLAMA3_405B: ModelMetadata(\"ollama\", 8192),\n    LlmModel.OLLAMA_DOLPHIN: ModelMetadata(\"ollama\", 32768),\n    LlmModel.GEMINI_FLASH_1_5_8B: ModelMetadata(\"open_router\", 8192),\n    LlmModel.GROK_BETA: ModelMetadata(\"open_router\", 8192),\n    LlmModel.MISTRAL_NEMO: ModelMetadata(\"open_router\", 4000),\n    LlmModel.COHERE_COMMAND_R_08_2024: ModelMetadata(\"open_router\", 4000),\n    LlmModel.COHERE_COMMAND_R_PLUS_08_2024: ModelMetadata(\"open_router\", 4000),\n    LlmModel.EVA_QWEN_2_5_32B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.DEEPSEEK_CHAT: ModelMetadata(\"open_router\", 8192),\n    LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: ModelMetadata(\n        \"open_router\", 8192\n    ),\n    LlmModel.QWEN_QWQ_32B_PREVIEW: ModelMetadata(\"open_router\", 4000),\n    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_LITE_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_MICRO_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_PRO_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.MICROSOFT_WIZARDLM_2_8X22B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.GRYPHE_MYTHOMAX_L2_13B: ModelMetadata(\"open_router\", 4000),\n}",
      "successors": [
        {
          "id": 4,
          "start_line": 86,
          "end_line": 88,
          "label": "for model in LlmModel:\n    if model not in MODEL_METADATA:\n        raise ValueError(f\"Missing MODEL_METADATA metadata for model: {model}\")",
          "successors": []
        }
      ]
    },
    {
      "id": 4,
      "start_line": 86,
      "end_line": 88,
      "label": "for model in LlmModel:\n    if model not in MODEL_METADATA:\n        raise ValueError(f\"Missing MODEL_METADATA metadata for model: {model}\")",
      "successors": []
    }
  ],
  "functions": [
    {
      "name": "AICredentialsField",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 1,
          "label": "def AICredentialsField() -> AICredentials:",
          "successors": [
            {
              "id": 2,
              "start_line": 2,
              "end_line": 8,
              "label": "return CredentialsField(\n        description=\"API key for the LLM provider.\",\n        discriminator=\"model\",\n        discriminator_mapping={\n            model.value: model.metadata.provider for model in LlmModel\n        },\n    )",
              "successors": []
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "llm_call",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 21,
          "label": "def llm_call(\n        credentials: APIKeyCredentials,\n        llm_model: LlmModel,\n        prompt: list[dict],\n        json_format: bool,\n        max_tokens: int | None = None,\n        ollama_host: str = \"localhost:11434\",\n    ) -> tuple[str, int, int]:\n        \"\"\"\n        Args:\n            api_key: API key for the LLM provider.\n            llm_model: The LLM model to use.\n            prompt: The prompt to send to the LLM.\n            json_format: Whether the response should be in JSON format.\n            max_tokens: The maximum number of tokens to generate in the chat completion.\n            ollama_host: The host for ollama to use\n        Returns:\n            The response from the LLM.\n            The number of tokens used in the prompt.\n            The number of tokens used in the completion.\n        \"\"\"",
          "successors": [
            {
              "id": 2,
              "start_line": 22,
              "end_line": 22,
              "label": "provider = llm_model.metadata.provider",
              "successors": [
                {
                  "id": 3,
                  "start_line": 23,
                  "end_line": 23,
                  "label": "if provider == \"openai\":",
                  "successors": [
                    {
                      "id": 4,
                      "start_line": 24,
                      "end_line": 45,
                      "label": "oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())\n            response_format = None\n            if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:\n                sys_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]\n                usr_messages = [p[\"content\"] for p in prompt if p[\"role\"] != \"system\"]\n                prompt = [\n                    {\"role\": \"user\", \"content\": \"\\n\".join(sys_messages)},\n                    {\"role\": \"user\", \"content\": \"\\n\".join(usr_messages)},\n                ]\n            elif json_format:\n                response_format = {\"type\": \"json_object\"}\n            response = oai_client.chat.completions.create(\n                model=llm_model.value,\n                messages=prompt,  # type: ignore\n                response_format=response_format,  # type: ignore\n                max_completion_tokens=max_tokens,\n            )\n            return (\n                response.choices[0].message.content or \"\",\n                response.usage.prompt_tokens if response.usage else 0,\n                response.usage.completion_tokens if response.usage else 0,\n            )",
                      "successors": []
                    },
                    {
                      "id": 5,
                      "start_line": 46,
                      "end_line": 46,
                      "label": "elif provider == \"anthropic\":",
                      "successors": [
                        {
                          "id": 6,
                          "start_line": 47,
                          "end_line": 81,
                          "label": "system_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]\n            sysprompt = \" \".join(system_messages)\n            messages = []\n            last_role = None\n            for p in prompt:\n                if p[\"role\"] in [\"user\", \"assistant\"]:\n                    if p[\"role\"] != last_role:\n                        messages.append({\"role\": p[\"role\"], \"content\": p[\"content\"]})\n                        last_role = p[\"role\"]\n                    else:\n                        # If the role is the same as the last one, combine the content\n                        messages[-1][\"content\"] += \"\\n\" + p[\"content\"]\n            client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())\n            try:\n                resp = client.messages.create(\n                    model=llm_model.value,\n                    system=sysprompt,\n                    messages=messages,\n                    max_tokens=max_tokens or 8192,\n                )\n                if not resp.content:\n                    raise ValueError(\"No content returned from Anthropic.\")\n                return (\n                    (\n                        resp.content[0].name\n                        if isinstance(resp.content[0], anthropic.types.ToolUseBlock)\n                        else resp.content[0].text\n                    ),\n                    resp.usage.input_tokens,\n                    resp.usage.output_tokens,\n                )\n            except anthropic.APIError as e:\n                error_message = f\"Anthropic API error: {str(e)}\"\n                logger.error(error_message)\n                raise ValueError(error_message)",
                          "successors": []
                        }
                      ]
                    },
                    {
                      "id": 7,
                      "start_line": 82,
                      "end_line": 82,
                      "label": "elif provider == \"groq\":",
                      "successors": [
                        {
                          "id": 8,
                          "start_line": 83,
                          "end_line": 95,
                          "label": "client = Groq(api_key=credentials.api_key.get_secret_value())\n            response_format = {\"type\": \"json_object\"} if json_format else None\n            response = client.chat.completions.create(\n                model=llm_model.value,\n                messages=prompt,  # type: ignore\n                response_format=response_format,  # type: ignore\n                max_tokens=max_tokens,\n            )\n            return (\n                response.choices[0].message.content or \"\",\n                response.usage.prompt_tokens if response.usage else 0,\n                response.usage.completion_tokens if response.usage else 0,\n            )",
                          "successors": []
                        }
                      ]
                    },
                    {
                      "id": 9,
                      "start_line": 96,
                      "end_line": 96,
                      "label": "elif provider == \"ollama\":",
                      "successors": [
                        {
                          "id": 10,
                          "start_line": 97,
                          "end_line": 109,
                          "label": "client = ollama.Client(host=ollama_host)\n            sys_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]\n            usr_messages = [p[\"content\"] for p in prompt if p[\"role\"] != \"system\"]\n            response = client.generate(\n                model=llm_model.value,\n                prompt=f\"{sys_messages}\\n\\n{usr_messages}\",\n                stream=False,\n            )\n            return (\n                response.get(\"response\") or \"\",\n                response.get(\"prompt_eval_count\") or 0,\n                response.get(\"eval_count\") or 0,\n            )",
                          "successors": []
                        }
                      ]
                    },
                    {
                      "id": 11,
                      "start_line": 110,
                      "end_line": 110,
                      "label": "elif provider == \"open_router\":",
                      "successors": [
                        {
                          "id": 12,
                          "start_line": 111,
                          "end_line": 134,
                          "label": "client = openai.OpenAI(\n                base_url=\"https://openrouter.ai/api/v1\",\n                api_key=credentials.api_key.get_secret_value(),\n            )\n            response = client.chat.completions.create(\n                extra_headers={\n                    \"HTTP-Referer\": \"https://agpt.co\",\n                    \"X-Title\": \"AutoGPT\",\n                },\n                model=llm_model.value,\n                messages=prompt,  # type: ignore\n                max_tokens=max_tokens,\n            )\n            # If there's no response, raise an error\n            if not response.choices:\n                if response:\n                    raise ValueError(f\"OpenRouter error: {response}\")\n                else:\n                    raise ValueError(\"No response from OpenRouter.\")\n            return (\n                response.choices[0].message.content or \"\",\n                response.usage.prompt_tokens if response.usage else 0,\n                response.usage.completion_tokens if response.usage else 0,\n            )",
                          "successors": []
                        }
                      ]
                    },
                    {
                      "id": 13,
                      "start_line": 135,
                      "end_line": 136,
                      "label": "else:\n            raise ValueError(f\"Unsupported LLM provider: {provider}\")",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "run",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 3,
          "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
          "successors": [
            {
              "id": 2,
              "start_line": 4,
              "end_line": 4,
              "label": "logger.debug(f\"Calling LLM with input data: {input_data}\")",
              "successors": [
                {
                  "id": 3,
                  "start_line": 5,
                  "end_line": 5,
                  "label": "prompt = [p.model_dump() for p in input_data.conversation_history]",
                  "successors": [
                    {
                      "id": 4,
                      "start_line": 6,
                      "end_line": 8,
                      "label": "def trim_prompt(s: str) -> str:\n            lines = s.strip().split(\"\\n\")\n            return \"\\n\".join([line.strip().lstrip(\"|\") for line in lines])",
                      "successors": [
                        {
                          "id": 5,
                          "start_line": 9,
                          "end_line": 9,
                          "label": "values = input_data.prompt_values",
                          "successors": [
                            {
                              "id": 6,
                              "start_line": 10,
                              "end_line": 10,
                              "label": "if values:",
                              "successors": [
                                {
                                  "id": 7,
                                  "start_line": 11,
                                  "end_line": 12,
                                  "label": "input_data.prompt = input_data.prompt.format(**values)\n            input_data.sys_prompt = input_data.sys_prompt.format(**values)",
                                  "successors": [
                                    {
                                      "id": 8,
                                      "start_line": 13,
                                      "end_line": 13,
                                      "label": "if input_data.sys_prompt:",
                                      "successors": [
                                        {
                                          "id": 9,
                                          "start_line": 14,
                                          "end_line": 14,
                                          "label": "prompt.append({\"role\": \"system\", \"content\": input_data.sys_prompt})",
                                          "successors": [
                                            {
                                              "id": 10,
                                              "start_line": 15,
                                              "end_line": 15,
                                              "label": "if input_data.expected_format:",
                                              "successors": [
                                                {
                                                  "id": 11,
                                                  "start_line": 16,
                                                  "end_line": 19,
                                                  "label": "expected_format = [\n                f'\"{k}\": \"{v}\"' for k, v in input_data.expected_format.items()\n            ]\n            format_prompt = \",\\n  \".join(expected_format)",
                                                  "successors": [
                                                    {
                                                      "id": 12,
                                                      "start_line": 20,
                                                      "end_line": 27,
                                                      "label": "sys_prompt = trim_prompt(\n                f\"\"\"\n                  |Reply strictly only in the following JSON format:\n                  |{{\n                  |  {format_prompt}\n                  |}}\n                \"\"\"\n            )",
                                                      "successors": [
                                                        {
                                                          "id": 13,
                                                          "start_line": 28,
                                                          "end_line": 28,
                                                          "label": "prompt.append({\"role\": \"system\", \"content\": sys_prompt})",
                                                          "successors": [
                                                            {
                                                              "id": 14,
                                                              "start_line": 29,
                                                              "end_line": 29,
                                                              "label": "if input_data.prompt:",
                                                              "successors": [
                                                                {
                                                                  "id": 15,
                                                                  "start_line": 30,
                                                                  "end_line": 30,
                                                                  "label": "prompt.append({\"role\": \"user\", \"content\": input_data.prompt})",
                                                                  "successors": [
                                                                    {
                                                                      "id": 16,
                                                                      "start_line": 31,
                                                                      "end_line": 31,
                                                                      "label": "def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:",
                                                                      "successors": [
                                                                        {
                                                                          "id": 17,
                                                                          "start_line": 32,
                                                                          "end_line": 32,
                                                                          "label": "try:",
                                                                          "successors": [
                                                                            {
                                                                              "id": 18,
                                                                              "start_line": 33,
                                                                              "end_line": 33,
                                                                              "label": "parsed = json.loads(resp)",
                                                                              "successors": [
                                                                                {
                                                                                  "id": 19,
                                                                                  "start_line": 34,
                                                                                  "end_line": 34,
                                                                                  "label": "if not isinstance(parsed, dict):",
                                                                                  "successors": [
                                                                                    {
                                                                                      "id": 20,
                                                                                      "start_line": 35,
                                                                                      "end_line": 35,
                                                                                      "label": "return {}, f\"Expected a dictionary, but got {type(parsed)}\"",
                                                                                      "successors": []
                                                                                    }
                                                                                  ]
                                                                                }
                                                                              ]
                                                                            }
                                                                          ]
                                                                        }
                                                                      ]
                                                                    }
                                                                  ]
                                                                }
                                                              ]
                                                            }
                                                          ]
                                                        }
                                                      ]
                                                    }
                                                  ]
                                                }
                                              ]
                                            }
                                          ]
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "llm_call",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 5,
          "label": "def llm_call(\n        self,\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> str:",
          "successors": [
            {
              "id": 2,
              "start_line": 6,
              "end_line": 6,
              "label": "block = AIStructuredResponseGeneratorBlock()",
              "successors": [
                {
                  "id": 3,
                  "start_line": 7,
                  "end_line": 7,
                  "label": "response = block.run_once(input_data, \"response\", credentials=credentials)",
                  "successors": [
                    {
                      "id": 4,
                      "start_line": 8,
                      "end_line": 8,
                      "label": "self.merge_stats(block.execution_stats)",
                      "successors": [
                        {
                          "id": 5,
                          "start_line": 9,
                          "end_line": 9,
                          "label": "return response[\"response\"]",
                          "successors": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "run",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 3,
          "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
          "successors": [
            {
              "id": 2,
              "start_line": 4,
              "end_line": 7,
              "label": "object_input_data = AIStructuredResponseGeneratorBlock.Input(\n            **{attr: getattr(input_data, attr) for attr in input_data.model_fields},\n            expected_format={},\n        )",
              "successors": [
                {
                  "id": 3,
                  "start_line": 8,
                  "end_line": 8,
                  "label": "yield \"response\", self.llm_call(object_input_data, credentials)",
                  "successors": []
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "_run",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 4,
          "label": "def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:\n        chunks = self._split_text(\n            input_data.text, input_data.max_tokens, input_data.chunk_overlap\n        )",
          "successors": [
            {
              "id": 2,
              "start_line": 5,
              "end_line": 5,
              "label": "summaries = []",
              "successors": [
                {
                  "id": 3,
                  "start_line": 6,
                  "end_line": 8,
                  "label": "for chunk in chunks:\n            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)\n            summaries.append(chunk_summary)",
                  "successors": [
                    {
                      "id": 4,
                      "start_line": 9,
                      "end_line": 10,
                      "label": "final_summary = self._combine_summaries(summaries, input_data, credentials)\n        yield \"summary\", final_summary",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "_split_text",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 4,
          "label": "def _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:\n        words = text.split()\n        chunks = []\n        chunk_size = max_tokens - overlap",
          "successors": [
            {
              "id": 2,
              "start_line": 5,
              "end_line": 7,
              "label": "for i in range(0, len(words), chunk_size):\n            chunk = \" \".join(words[i : i + max_tokens])\n            chunks.append(chunk)",
              "successors": [
                {
                  "id": 3,
                  "start_line": 8,
                  "end_line": 8,
                  "label": "return chunks",
                  "successors": []
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "llm_call",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 5,
          "label": "def llm_call(\n        self,\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> dict:",
          "successors": [
            {
              "id": 2,
              "start_line": 6,
              "end_line": 6,
              "label": "block = AIStructuredResponseGeneratorBlock()",
              "successors": [
                {
                  "id": 3,
                  "start_line": 7,
                  "end_line": 7,
                  "label": "response = block.run_once(input_data, \"response\", credentials=credentials)",
                  "successors": [
                    {
                      "id": 4,
                      "start_line": 8,
                      "end_line": 8,
                      "label": "self.merge_stats(block.execution_stats)",
                      "successors": [
                        {
                          "id": 5,
                          "start_line": 9,
                          "end_line": 9,
                          "label": "return response",
                          "successors": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "_summarize_chunk",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 3,
          "label": "def _summarize_chunk(\n        self, chunk: str, input_data: Input, credentials: APIKeyCredentials\n    ) -> str:",
          "successors": [
            {
              "id": 2,
              "start_line": 4,
              "end_line": 4,
              "label": "prompt = f\"Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```\"",
              "successors": [
                {
                  "id": 3,
                  "start_line": 5,
                  "end_line": 13,
                  "label": "llm_response = self.llm_call(\n            AIStructuredResponseGeneratorBlock.Input(\n                prompt=prompt,\n                credentials=input_data.credentials,\n                model=input_data.model,\n                expected_format={\"summary\": \"The summary of the given text.\"},\n            ),\n            credentials=credentials,\n        )",
                  "successors": [
                    {
                      "id": 4,
                      "start_line": 14,
                      "end_line": 14,
                      "label": "return llm_response[\"summary\"]",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "_combine_summaries",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 4,
          "label": "def _combine_summaries(\n        self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials\n    ) -> str:\n        combined_text = \"\\n\\n\".join(summaries)",
          "successors": [
            {
              "id": 2,
              "start_line": 5,
              "end_line": 5,
              "label": "if len(combined_text.split()) <= input_data.max_tokens:",
              "successors": [
                {
                  "id": 3,
                  "start_line": 6,
                  "end_line": 18,
                  "label": "prompt = f\"Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.\"\n            llm_response = self.llm_call(\n                AIStructuredResponseGeneratorBlock.Input(\n                    prompt=prompt,\n                    credentials=input_data.credentials,\n                    model=input_data.model,\n                    expected_format={\n                        \"final_summary\": \"The final summary of all provided summaries.\"\n                    },\n                ),\n                credentials=credentials,\n            )\n            return llm_response[\"final_summary\"]",
                  "successors": []
                },
                {
                  "id": 4,
                  "start_line": 19,
                  "end_line": 32,
                  "label": "else:\n            # If combined summaries are still too long, recursively summarize\n            return self._run(\n                AITextSummarizerBlock.Input(\n                    text=combined_text,\n                    credentials=input_data.credentials,\n                    model=input_data.model,\n                    max_tokens=input_data.max_tokens,\n                    chunk_overlap=input_data.chunk_overlap,\n                ),\n                credentials=credentials,\n            ).send(None)[\n                1\n            ]  # Get the first yielded value",
                  "successors": []
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "llm_call",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 5,
          "label": "def llm_call(\n        self,\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> str:",
          "successors": [
            {
              "id": 2,
              "start_line": 6,
              "end_line": 6,
              "label": "block = AIStructuredResponseGeneratorBlock()",
              "successors": [
                {
                  "id": 3,
                  "start_line": 7,
                  "end_line": 7,
                  "label": "response = block.run_once(input_data, \"response\", credentials=credentials)",
                  "successors": [
                    {
                      "id": 4,
                      "start_line": 8,
                      "end_line": 8,
                      "label": "self.merge_stats(block.execution_stats)",
                      "successors": [
                        {
                          "id": 5,
                          "start_line": 9,
                          "end_line": 9,
                          "label": "return response[\"response\"]",
                          "successors": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "run",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 3,
          "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
          "successors": [
            {
              "id": 2,
              "start_line": 4,
              "end_line": 14,
              "label": "response = self.llm_call(\n            AIStructuredResponseGeneratorBlock.Input(\n                prompt=\"\",\n                credentials=input_data.credentials,\n                model=input_data.model,\n                conversation_history=input_data.messages,\n                max_tokens=input_data.max_tokens,\n                expected_format={},\n            ),\n            credentials=credentials,\n        )",
              "successors": [
                {
                  "id": 3,
                  "start_line": 15,
                  "end_line": 15,
                  "label": "yield \"response\", response",
                  "successors": []
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "llm_call",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 5,
          "label": "@staticmethod\n    def llm_call(\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> dict[str, str]:",
          "successors": [
            {
              "id": 2,
              "start_line": 6,
              "end_line": 8,
              "label": "llm_block = AIStructuredResponseGeneratorBlock()\n        response = llm_block.run_once(input_data, \"response\", credentials=credentials)\n        return response",
              "successors": []
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "string_to_list",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 5,
          "label": "def string_to_list(string):\n        \"\"\"\n        Converts a string representation of a list into an actual Python list object.\n        \"\"\"\n        logger.debug(f\"Converting string to list. Input string: {string}\")",
          "successors": [
            {
              "id": 2,
              "start_line": 6,
              "end_line": 6,
              "label": "try:",
              "successors": [
                {
                  "id": 3,
                  "start_line": 7,
                  "end_line": 8,
                  "label": "# Use ast.literal_eval to safely evaluate the string\n            python_list = ast.literal_eval(string)",
                  "successors": [
                    {
                      "id": 4,
                      "start_line": 9,
                      "end_line": 9,
                      "label": "if isinstance(python_list, list):",
                      "successors": [
                        {
                          "id": 5,
                          "start_line": 10,
                          "end_line": 11,
                          "label": "logger.debug(f\"Successfully converted string to list: {python_list}\")\n                return python_list",
                          "successors": []
                        },
                        {
                          "id": 6,
                          "start_line": 12,
                          "end_line": 14,
                          "label": "else:\n                logger.error(f\"The provided string '{string}' is not a valid list\")\n                raise ValueError(f\"The provided string '{string}' is not a valid list.\")",
                          "successors": []
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": 7,
                  "start_line": 15,
                  "end_line": 17,
                  "label": "except (SyntaxError, ValueError) as e:\n            logger.error(f\"Failed to convert string to list: {e}\")\n            raise ValueError(\"Invalid list format. Could not convert to list.\")",
                  "successors": []
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "run",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 3,
          "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
          "successors": [
            {
              "id": 2,
              "start_line": 4,
              "end_line": 6,
              "label": "logger.debug(f\"Starting AIListGeneratorBlock.run with input data: {input_data}\")\n        # Check for API key\n        api_key_check = credentials.api_key.get_secret_value()",
              "successors": [
                {
                  "id": 3,
                  "start_line": 7,
                  "end_line": 8,
                  "label": "if not api_key_check:\n            raise ValueError(\"No LLM API key provided.\")",
                  "successors": []
                }
              ]
            }
          ]
        },
        {
          "id": 4,
          "start_line": 9,
          "end_line": 36,
          "label": "# Prepare the system prompt\n        sys_prompt = \"\"\"You are a Python list generator. Your task is to generate a Python list based on the user's prompt. \n            |Respond ONLY with a valid python list. \n            |The list can contain strings, numbers, or nested lists as appropriate. \n            |Do not include any explanations or additional text.\n            |Valid Example string formats:\n            |Example 1:\n            |```\n            |['1', '2', '3', '4']\n            |```\n            |Example 2:\n            |```\n            |[['1', '2'], ['3', '4'], ['5', '6']]\n            |```\n            |Example 3:\n            |```\n            |['1', ['2', '3'], ['4', ['5', '6']]]\n            |```\n            |Example 4:\n            |```\n            |['a', 'b', 'c']\n            |```\n            |Example 5:\n            |```\n            |['1', '2.5', 'string', 'True', ['False', 'None']]\n            |```\n            |Do not include any explanations or additional text, just respond with the list in the format specified above.\n            \"\"\"",
          "successors": [
            {
              "id": 5,
              "start_line": 37,
              "end_line": 38,
              "label": "# If a focus is provided, add it to the prompt\n        if input_data.focus:",
              "successors": [
                {
                  "id": 6,
                  "start_line": 39,
                  "end_line": 39,
                  "label": "prompt = f\"Generate a list with the following focus:\\n<focus>\\n\\n{input_data.focus}</focus>\"",
                  "successors": []
                },
                {
                  "id": 7,
                  "start_line": 40,
                  "end_line": 42,
                  "label": "else:\n            # If there's source data\n            if input_data.source_data:",
                  "successors": [
                    {
                      "id": 8,
                      "start_line": 43,
                      "end_line": 43,
                      "label": "prompt = \"Extract the main focus of the source data to a list.\\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.\"",
                      "successors": []
                    },
                    {
                      "id": 9,
                      "start_line": 44,
                      "end_line": 46,
                      "label": "else:\n                # No focus or source data provided, generat a random list\n                prompt = \"Generate a random list.\"",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": 10,
          "start_line": 47,
          "end_line": 48,
          "label": "# If the source data is provided, add it to the prompt\n        if input_data.source_data:",
          "successors": [
            {
              "id": 11,
              "start_line": 49,
              "end_line": 49,
              "label": "prompt += f\"\\n\\nUse the following source data to generate the list from:\\n\\n<source_data>\\n\\n{input_data.source_data}</source_data>\\n\\nDo not invent fictional data that is not present in the source data.\"",
              "successors": []
            },
            {
              "id": 12,
              "start_line": 50,
              "end_line": 52,
              "label": "# Else, tell the LLM to synthesize the data\n        else:\n            prompt += \"\\n\\nInvent the data to generate the list from.\"",
              "successors": []
            }
          ]
        },
        {
          "id": 13,
          "start_line": 53,
          "end_line": 53,
          "label": "for attempt in range(input_data.max_retries):",
          "successors": [
            {
              "id": 14,
              "start_line": 54,
              "end_line": 54,
              "label": "try:",
              "successors": [
                {
                  "id": 15,
                  "start_line": 55,
                  "end_line": 66,
                  "label": "logger.debug(\"Calling LLM\")\n                llm_response = self.llm_call(\n                    AIStructuredResponseGeneratorBlock.Input(\n                        sys_prompt=sys_prompt,\n                        prompt=prompt,\n                        credentials=input_data.credentials,\n                        model=input_data.model,\n                        expected_format={},  # Do not use structured response\n                        ollama_host=input_data.ollama_host,\n                    ),\n                    credentials=credentials,\n                )",
                  "successors": [
                    {
                      "id": 16,
                      "start_line": 67,
                      "end_line": 67,
                      "label": "logger.debug(f\"LLM response: {llm_response}\")",
                      "successors": [
                        {
                          "id": 17,
                          "start_line": 68,
                          "end_line": 70,
                          "label": "# Extract Response string\n                response_string = llm_response[\"response\"]\n                logger.debug(f\"Response string: {response_string}\")",
                          "successors": [
                            {
                              "id": 18,
                              "start_line": 71,
                              "end_line": 74,
                              "label": "# Convert the string to a Python list\n                logger.debug(\"Converting string to Python list\")\n                parsed_list = self.string_to_list(response_string)\n                logger.debug(f\"Parsed list: {parsed_list}\")",
                              "successors": [
                                {
                                  "id": 19,
                                  "start_line": 75,
                                  "end_line": 76,
                                  "label": "# If we reach here, we have a valid Python list\n                logger.debug(\"Successfully generated a valid Python list\")",
                                  "successors": [
                                    {
                                      "id": 20,
                                      "start_line": 77,
                                      "end_line": 77,
                                      "label": "yield \"generated_list\", parsed_list",
                                      "successors": [
                                        {
                                          "id": 21,
                                          "start_line": 78,
                                          "end_line": 80,
                                          "label": "# Yield each item in the list\n                for item in parsed_list:\n                    yield \"list_item\", item",
                                          "successors": [
                                            {
                                              "id": 22,
                                              "start_line": 81,
                                              "end_line": 81,
                                              "label": "return",
                                              "successors": []
                                            }
                                          ]
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": 23,
                  "start_line": 82,
                  "end_line": 83,
                  "label": "except Exception as e:\n                logger.error(f\"Error in attempt {attempt + 1}: {str(e)}\")",
                  "successors": [
                    {
                      "id": 24,
                      "start_line": 84,
                      "end_line": 84,
                      "label": "if attempt == input_data.max_retries - 1:",
                      "successors": [
                        {
                          "id": 25,
                          "start_line": 85,
                          "end_line": 90,
                          "label": "logger.error(\n                        f\"Failed to generate a valid Python list after {input_data.max_retries} attempts\"\n                    )\n                    raise RuntimeError(\n                        f\"Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}\"\n                    )",
                          "successors": []
                        },
                        {
                          "id": 26,
                          "start_line": 91,
                          "end_line": 93,
                          "label": "else:\n                    # Add a retry prompt\n                    logger.debug(\"Preparing retry prompt\")",
                          "successors": [
                            {
                              "id": 27,
                              "start_line": 94,
                              "end_line": 102,
                              "label": "prompt = f\"\"\"\n                    The previous attempt failed due to `{e}`\n                    Generate a valid Python list based on the original prompt.\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\n                    Original prompt: \n                    ```{prompt}```\n                    \n                    Respond only with the list in the format specified with no commentary or apologies.\n                    \"\"\"",
                              "successors": [
                                {
                                  "id": 28,
                                  "start_line": 103,
                                  "end_line": 103,
                                  "label": "logger.debug(f\"Retry prompt: {prompt}\")",
                                  "successors": []
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": 29,
          "start_line": 104,
          "end_line": 104,
          "label": "logger.debug(\"AIListGeneratorBlock.run completed\")",
          "successors": []
        }
      ],
      "functions": [],
      "classes": []
    }
  ],
  "classes": [
    {
      "name": "ModelMetadata",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 1,
          "label": "class ModelMetadata(NamedTuple):",
          "successors": [
            {
              "id": 2,
              "start_line": 2,
              "end_line": 3,
              "label": "provider: str\n    context_window: int",
              "successors": []
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "LlmModelMeta",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 1,
          "label": "class LlmModelMeta(EnumMeta):",
          "successors": [
            {
              "id": 2,
              "start_line": 2,
              "end_line": 2,
              "label": "@property",
              "successors": []
            }
          ]
        }
      ],
      "functions": [
        {
          "name": "__members__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 3,
              "label": "def __members__(\n        self: type[\"_EnumMemberT\"],\n    ) -> MappingProxyType[str, \"_EnumMemberT\"]:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 4,
                  "end_line": 4,
                  "label": "if Settings().config.behave_as == BehaveAs.LOCAL:",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 5,
                      "end_line": 6,
                      "label": "members = super().__members__\n            return members",
                      "successors": []
                    },
                    {
                      "id": 4,
                      "start_line": 7,
                      "end_line": 15,
                      "label": "else:\n            removed_providers = [\"ollama\"]\n            existing_members = super().__members__\n            members = {\n                name: member\n                for name, member in existing_members.items()\n                if LlmModel[name].provider not in removed_providers\n            }\n            return MappingProxyType(members)",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": []
    },
    {
      "name": "LlmModel",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 1,
          "label": "class LlmModel(str, Enum, metaclass=LlmModelMeta):",
          "successors": [
            {
              "id": 2,
              "start_line": 2,
              "end_line": 47,
              "label": "# OpenAI models\n    O1_PREVIEW = \"o1-preview\"\n    O1_MINI = \"o1-mini\"\n    GPT4O_MINI = \"gpt-4o-mini\"\n    GPT4O = \"gpt-4o\"\n    GPT4_TURBO = \"gpt-4-turbo\"\n    GPT3_5_TURBO = \"gpt-3.5-turbo\"\n    # Anthropic models\n    CLAUDE_3_5_SONNET = \"claude-3-5-sonnet-latest\"\n    CLAUDE_3_HAIKU = \"claude-3-haiku-20240307\"\n    # Groq models\n    LLAMA3_8B = \"llama3-8b-8192\"\n    LLAMA3_70B = \"llama3-70b-8192\"\n    MIXTRAL_8X7B = \"mixtral-8x7b-32768\"\n    GEMMA_7B = \"gemma-7b-it\"\n    GEMMA2_9B = \"gemma2-9b-it\"\n    # New Groq models (Preview)\n    LLAMA3_1_405B = \"llama-3.1-405b-reasoning\"\n    LLAMA3_1_70B = \"llama-3.1-70b-versatile\"\n    LLAMA3_1_8B = \"llama-3.1-8b-instant\"\n    # Ollama models\n    OLLAMA_LLAMA3_8B = \"llama3\"\n    OLLAMA_LLAMA3_405B = \"llama3.1:405b\"\n    OLLAMA_DOLPHIN = \"dolphin-mistral:latest\"\n    # OpenRouter models\n    GEMINI_FLASH_1_5_8B = \"google/gemini-flash-1.5\"\n    GROK_BETA = \"x-ai/grok-beta\"\n    MISTRAL_NEMO = \"mistralai/mistral-nemo\"\n    COHERE_COMMAND_R_08_2024 = \"cohere/command-r-08-2024\"\n    COHERE_COMMAND_R_PLUS_08_2024 = \"cohere/command-r-plus-08-2024\"\n    EVA_QWEN_2_5_32B = \"eva-unit-01/eva-qwen-2.5-32b\"\n    DEEPSEEK_CHAT = \"deepseek/deepseek-chat\"\n    PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE = (\n        \"perplexity/llama-3.1-sonar-large-128k-online\"\n    )\n    QWEN_QWQ_32B_PREVIEW = \"qwen/qwq-32b-preview\"\n    NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B = \"nousresearch/hermes-3-llama-3.1-405b\"\n    NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B = \"nousresearch/hermes-3-llama-3.1-70b\"\n    AMAZON_NOVA_LITE_V1 = \"amazon/nova-lite-v1\"\n    AMAZON_NOVA_MICRO_V1 = \"amazon/nova-micro-v1\"\n    AMAZON_NOVA_PRO_V1 = \"amazon/nova-pro-v1\"\n    MICROSOFT_WIZARDLM_2_8X22B = \"microsoft/wizardlm-2-8x22b\"\n    GRYPHE_MYTHOMAX_L2_13B = \"gryphe/mythomax-l2-13b\"\n    @property\n    @property\n    @property",
              "successors": []
            }
          ]
        }
      ],
      "functions": [
        {
          "name": "metadata",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        },
        {
          "name": "provider",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "def provider(self) -> str:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 2,
                  "label": "return self.metadata.provider",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "context_window",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "def context_window(self) -> int:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 2,
                  "label": "return self.metadata.context_window",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": []
    },
    {
      "name": "MessageRole",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 1,
          "label": "class MessageRole(str, Enum):",
          "successors": [
            {
              "id": 2,
              "start_line": 2,
              "end_line": 2,
              "label": "SYSTEM = \"system\"",
              "successors": [
                {
                  "id": 3,
                  "start_line": 3,
                  "end_line": 3,
                  "label": "USER = \"user\"",
                  "successors": [
                    {
                      "id": 4,
                      "start_line": 4,
                      "end_line": 4,
                      "label": "ASSISTANT = \"assistant\"",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ]
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "Message",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 3,
          "label": "class Message(BlockSchema):\n    role: MessageRole\n    content: str",
          "successors": []
        }
      ],
      "functions": [],
      "classes": []
    },
    {
      "name": "AIStructuredResponseGeneratorBlock",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 6,
          "label": "class AIStructuredResponseGeneratorBlock(Block):\n        ollama_host: str = SchemaField(\n            advanced=True,\n            default=\"localhost:11434\",\n            description=\"Ollama host for local  models\",\n        )",
          "successors": [
            {
              "id": 2,
              "start_line": 7,
              "end_line": 7,
              "label": "@staticmethod",
              "successors": []
            }
          ]
        }
      ],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "def __init__(self):",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 31,
                  "label": "super().__init__(\n            id=\"ed55ac19-356e-4243-a6cb-bc599e9b716f\",\n            description=\"Call a Large Language Model (LLM) to generate formatted object based on the given prompt.\",\n            categories={BlockCategory.AI},\n            input_schema=AIStructuredResponseGeneratorBlock.Input,\n            output_schema=AIStructuredResponseGeneratorBlock.Output,\n            test_input={\n                \"model\": LlmModel.GPT4_TURBO,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n                \"expected_format\": {\n                    \"key1\": \"value1\",\n                    \"key2\": \"value2\",\n                },\n                \"prompt\": \"User prompt\",\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\"response\", {\"key1\": \"key1Value\", \"key2\": \"key2Value\"}),\n            test_mock={\n                \"llm_call\": lambda *args, **kwargs: (\n                    json.dumps(\n                        {\n                            \"key1\": \"key1Value\",\n                            \"key2\": \"key2Value\",\n                        }\n                    ),\n                    0,\n                    0,\n                )\n            },\n        )",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 21,
              "label": "def llm_call(\n        credentials: APIKeyCredentials,\n        llm_model: LlmModel,\n        prompt: list[dict],\n        json_format: bool,\n        max_tokens: int | None = None,\n        ollama_host: str = \"localhost:11434\",\n    ) -> tuple[str, int, int]:\n        \"\"\"\n        Args:\n            api_key: API key for the LLM provider.\n            llm_model: The LLM model to use.\n            prompt: The prompt to send to the LLM.\n            json_format: Whether the response should be in JSON format.\n            max_tokens: The maximum number of tokens to generate in the chat completion.\n            ollama_host: The host for ollama to use\n        Returns:\n            The response from the LLM.\n            The number of tokens used in the prompt.\n            The number of tokens used in the completion.\n        \"\"\"",
              "successors": [
                {
                  "id": 2,
                  "start_line": 22,
                  "end_line": 22,
                  "label": "provider = llm_model.metadata.provider",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 23,
                      "end_line": 23,
                      "label": "if provider == \"openai\":",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 24,
                          "end_line": 25,
                          "label": "oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())\n            response_format = None",
                          "successors": [
                            {
                              "id": 5,
                              "start_line": 26,
                              "end_line": 26,
                              "label": "if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:",
                              "successors": [
                                {
                                  "id": 6,
                                  "start_line": 27,
                                  "end_line": 32,
                                  "label": "sys_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]\n                usr_messages = [p[\"content\"] for p in prompt if p[\"role\"] != \"system\"]\n                prompt = [\n                    {\"role\": \"user\", \"content\": \"\\n\".join(sys_messages)},\n                    {\"role\": \"user\", \"content\": \"\\n\".join(usr_messages)},\n                ]",
                                  "successors": [
                                    {
                                      "id": 7,
                                      "start_line": 33,
                                      "end_line": 33,
                                      "label": "elif json_format:",
                                      "successors": [
                                        {
                                          "id": 8,
                                          "start_line": 34,
                                          "end_line": 34,
                                          "label": "response_format = {\"type\": \"json_object\"}",
                                          "successors": []
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 3,
              "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 4,
                  "end_line": 4,
                  "label": "logger.debug(f\"Calling LLM with input data: {input_data}\")",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 5,
                      "end_line": 5,
                      "label": "prompt = [p.model_dump() for p in input_data.conversation_history]",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 6,
                          "end_line": 8,
                          "label": "def trim_prompt(s: str) -> str:\n            lines = s.strip().split(\"\\n\")\n            return \"\\n\".join([line.strip().lstrip(\"|\") for line in lines])",
                          "successors": [
                            {
                              "id": 5,
                              "start_line": 9,
                              "end_line": 9,
                              "label": "values = input_data.prompt_values",
                              "successors": [
                                {
                                  "id": 6,
                                  "start_line": 10,
                                  "end_line": 10,
                                  "label": "if values:",
                                  "successors": [
                                    {
                                      "id": 7,
                                      "start_line": 11,
                                      "end_line": 12,
                                      "label": "input_data.prompt = input_data.prompt.format(**values)\n            input_data.sys_prompt = input_data.sys_prompt.format(**values)",
                                      "successors": []
                                    }
                                  ]
                                },
                                {
                                  "id": 8,
                                  "start_line": 13,
                                  "end_line": 13,
                                  "label": "if input_data.sys_prompt:",
                                  "successors": [
                                    {
                                      "id": 9,
                                      "start_line": 14,
                                      "end_line": 14,
                                      "label": "prompt.append({\"role\": \"system\", \"content\": input_data.sys_prompt})",
                                      "successors": []
                                    }
                                  ]
                                },
                                {
                                  "id": 10,
                                  "start_line": 15,
                                  "end_line": 15,
                                  "label": "if input_data.expected_format:",
                                  "successors": [
                                    {
                                      "id": 11,
                                      "start_line": 16,
                                      "end_line": 19,
                                      "label": "expected_format = [\n                f'\"{k}\": \"{v}\"' for k, v in input_data.expected_format.items()\n            ]\n            format_prompt = \",\\n  \".join(expected_format)",
                                      "successors": [
                                        {
                                          "id": 12,
                                          "start_line": 20,
                                          "end_line": 27,
                                          "label": "sys_prompt = trim_prompt(\n                f\"\"\"\n                  |Reply strictly only in the following JSON format:\n                  |{{\n                  |  {format_prompt}\n                  |}}\n                \"\"\"\n            )",
                                          "successors": [
                                            {
                                              "id": 13,
                                              "start_line": 28,
                                              "end_line": 28,
                                              "label": "prompt.append({\"role\": \"system\", \"content\": sys_prompt})",
                                              "successors": []
                                            }
                                          ]
                                        }
                                      ]
                                    }
                                  ]
                                },
                                {
                                  "id": 14,
                                  "start_line": 29,
                                  "end_line": 29,
                                  "label": "if input_data.prompt:",
                                  "successors": [
                                    {
                                      "id": 15,
                                      "start_line": 30,
                                      "end_line": 30,
                                      "label": "prompt.append({\"role\": \"user\", \"content\": input_data.prompt})",
                                      "successors": []
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": 16,
              "start_line": 31,
              "end_line": 31,
              "label": "def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:",
              "successors": [
                {
                  "id": 17,
                  "start_line": 32,
                  "end_line": 32,
                  "label": "try:",
                  "successors": [
                    {
                      "id": 18,
                      "start_line": 33,
                      "end_line": 33,
                      "label": "parsed = json.loads(resp)",
                      "successors": [
                        {
                          "id": 19,
                          "start_line": 34,
                          "end_line": 34,
                          "label": "if not isinstance(parsed, dict):",
                          "successors": [
                            {
                              "id": 20,
                              "start_line": 35,
                              "end_line": 35,
                              "label": "return {}, f\"Expected a dictionary, but got {type(parsed)}\"",
                              "successors": []
                            }
                          ]
                        },
                        {
                          "id": 21,
                          "start_line": 36,
                          "end_line": 36,
                          "label": "miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())",
                          "successors": [
                            {
                              "id": 22,
                              "start_line": 37,
                              "end_line": 37,
                              "label": "if miss_keys:",
                              "successors": [
                                {
                                  "id": 23,
                                  "start_line": 38,
                                  "end_line": 38,
                                  "label": "return parsed, f\"Missing keys: {miss_keys}\"",
                                  "successors": []
                                }
                              ]
                            }
                          ]
                        },
                        {
                          "id": 24,
                          "start_line": 39,
                          "end_line": 39,
                          "label": "return parsed, None",
                          "successors": []
                        }
                      ]
                    },
                    {
                      "id": 25,
                      "start_line": 40,
                      "end_line": 40,
                      "label": "except JSONDecodeError as e:",
                      "successors": [
                        {
                          "id": 26,
                          "start_line": 41,
                          "end_line": 41,
                          "label": "return {}, f\"JSON decode error: {e}\"",
                          "successors": []
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": 27,
              "start_line": 42,
              "end_line": 42,
              "label": "logger.info(f\"LLM request: {prompt}\")",
              "successors": [
                {
                  "id": 28,
                  "start_line": 43,
                  "end_line": 43,
                  "label": "retry_prompt = \"\"",
                  "successors": [
                    {
                      "id": 29,
                      "start_line": 44,
                      "end_line": 44,
                      "label": "llm_model = input_data.model",
                      "successors": [
                        {
                          "id": 30,
                          "start_line": 45,
                          "end_line": 45,
                          "label": "for retry_count in range(input_data.retry):",
                          "successors": [
                            {
                              "id": 31,
                              "start_line": 46,
                              "end_line": 46,
                              "label": "try:",
                              "successors": [
                                {
                                  "id": 32,
                                  "start_line": 47,
                                  "end_line": 54,
                                  "label": "response_text, input_token, output_token = self.llm_call(\n                    credentials=credentials,\n                    llm_model=llm_model,\n                    prompt=prompt,\n                    json_format=bool(input_data.expected_format),\n                    ollama_host=input_data.ollama_host,\n                    max_tokens=input_data.max_tokens,\n                )",
                                  "successors": [
                                    {
                                      "id": 33,
                                      "start_line": 55,
                                      "end_line": 60,
                                      "label": "self.merge_stats(\n                    {\n                        \"input_token_count\": input_token,\n                        \"output_token_count\": output_token,\n                    }\n                )",
                                      "successors": [
                                        {
                                          "id": 34,
                                          "start_line": 61,
                                          "end_line": 61,
                                          "label": "logger.info(f\"LLM attempt-{retry_count} response: {response_text}\")",
                                          "successors": [
                                            {
                                              "id": 35,
                                              "start_line": 62,
                                              "end_line": 62,
                                              "label": "if input_data.expected_format:",
                                              "successors": [
                                                {
                                                  "id": 36,
                                                  "start_line": 63,
                                                  "end_line": 63,
                                                  "label": "parsed_dict, parsed_error = parse_response(response_text)",
                                                  "successors": [
                                                    {
                                                      "id": 37,
                                                      "start_line": 64,
                                                      "end_line": 64,
                                                      "label": "if not parsed_error:",
                                                      "successors": [
                                                        {
                                                          "id": 38,
                                                          "start_line": 65,
                                                          "end_line": 74,
                                                          "label": "yield \"response\", {\n                            k: (\n                                json.loads(v)\n                                if isinstance(v, str)\n                                and v.startswith(\"[\")\n                                and v.endswith(\"]\")\n                                else (\", \".join(v) if isinstance(v, list) else v)\n                            )\n                            for k, v in parsed_dict.items()\n                        }",
                                                          "successors": [
                                                            {
                                                              "id": 39,
                                                              "start_line": 75,
                                                              "end_line": 75,
                                                              "label": "return",
                                                              "successors": []
                                                            }
                                                          ]
                                                        }
                                                      ]
                                                    },
                                                    {
                                                      "id": 40,
                                                      "start_line": 76,
                                                      "end_line": 76,
                                                      "label": "else:",
                                                      "successors": [
                                                        {
                                                          "id": 41,
                                                          "start_line": 77,
                                                          "end_line": 77,
                                                          "label": "yield \"response\", {\"response\": response_text}",
                                                          "successors": []
                                                        }
                                                      ]
                                                    }
                                                  ]
                                                }
                                              ]
                                            }
                                          ]
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "id": 42,
                              "start_line": 79,
                              "end_line": 91,
                              "label": "retry_prompt = trim_prompt(\n                    f\"\"\"\n                  |This is your previous error response:\n                  |--\n                  |{response_text}\n                  |--\n                  |\n                  |And this is the error:\n                  |--\n                  |{parsed_error}\n                  |--\n                \"\"\"\n                )",
                              "successors": [
                                {
                                  "id": 43,
                                  "start_line": 92,
                                  "end_line": 92,
                                  "label": "prompt.append({\"role\": \"user\", \"content\": retry_prompt})",
                                  "successors": []
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": 44,
              "start_line": 93,
              "end_line": 93,
              "label": "except Exception as e:",
              "successors": [
                {
                  "id": 45,
                  "start_line": 94,
                  "end_line": 94,
                  "label": "logger.exception(f\"Error calling LLM: {e}\")",
                  "successors": [
                    {
                      "id": 46,
                      "start_line": 95,
                      "end_line": 95,
                      "label": "retry_prompt = f\"Error calling LLM: {e}\"",
                      "successors": []
                    }
                  ]
                }
              ]
            },
            {
              "id": 47,
              "start_line": 96,
              "end_line": 96,
              "label": "finally:",
              "successors": [
                {
                  "id": 48,
                  "start_line": 97,
                  "end_line": 102,
                  "label": "self.merge_stats(\n                    {\n                        \"llm_call_count\": retry_count + 1,\n                        \"llm_retry_count\": retry_count,\n                    }\n                )",
                  "successors": []
                }
              ]
            },
            {
              "id": 49,
              "start_line": 103,
              "end_line": 103,
              "label": "raise RuntimeError(retry_prompt)",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "class Input(BlockSchema):",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 38,
                  "label": "prompt: str = SchemaField(\n            description=\"The prompt to send to the language model.\",\n            placeholder=\"Enter your prompt here...\",\n        )\n        expected_format: dict[str, str] = SchemaField(\n            description=\"Expected format of the response. If provided, the response will be validated against this format. \"\n            \"The keys should be the expected fields in the response, and the values should be the description of the field.\",\n        )\n        model: LlmModel = SchemaField(\n            title=\"LLM Model\",\n            default=LlmModel.GPT4_TURBO,\n            description=\"The language model to use for answering the prompt.\",\n            advanced=False,\n        )\n        credentials: AICredentials = AICredentialsField()\n        sys_prompt: str = SchemaField(\n            title=\"System Prompt\",\n            default=\"\",\n            description=\"The system prompt to provide additional context to the model.\",\n        )\n        conversation_history: list[Message] = SchemaField(\n            default=[],\n            description=\"The conversation history to provide context for the prompt.\",\n        )\n        retry: int = SchemaField(\n            title=\"Retry Count\",\n            default=3,\n            description=\"Number of times to retry the LLM call if the response does not match the expected format.\",\n        )\n        prompt_values: dict[str, str] = SchemaField(\n            advanced=False, default={}, description=\"Values used to fill in the prompt.\"\n        )\n        max_tokens: int | None = SchemaField(\n            advanced=True,\n            default=None,\n            description=\"The maximum number of tokens to generate in the chat completion.\",\n        )",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 5,
              "label": "class Output(BlockSchema):\n        response: dict[str, Any] = SchemaField(\n            description=\"The response object generated by the language model.\"\n        )\n        error: str = SchemaField(description=\"Error message if the API call failed.\")",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "AITextGeneratorBlock",
      "type": "CFG",
      "blocks": [],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "def __init__(self):",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 15,
                  "label": "super().__init__(\n            id=\"1f292d4a-41a4-4977-9684-7c8d560b9f91\",\n            description=\"Call a Large Language Model (LLM) to generate a string based on the given prompt.\",\n            categories={BlockCategory.AI},\n            input_schema=AITextGeneratorBlock.Input,\n            output_schema=AITextGeneratorBlock.Output,\n            test_input={\n                \"prompt\": \"User prompt\",\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\"response\", \"Response text\"),\n            test_mock={\"llm_call\": lambda *args, **kwargs: \"Response text\"},\n        )",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 5,
              "label": "def llm_call(\n        self,\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> str:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 6,
                  "end_line": 6,
                  "label": "block = AIStructuredResponseGeneratorBlock()",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 7,
                      "end_line": 7,
                      "label": "response = block.run_once(input_data, \"response\", credentials=credentials)",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 8,
                          "end_line": 8,
                          "label": "self.merge_stats(block.execution_stats)",
                          "successors": [
                            {
                              "id": 5,
                              "start_line": 9,
                              "end_line": 9,
                              "label": "return response[\"response\"]",
                              "successors": []
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 3,
              "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 4,
                  "end_line": 7,
                  "label": "object_input_data = AIStructuredResponseGeneratorBlock.Input(\n            **{attr: getattr(input_data, attr) for attr in input_data.model_fields},\n            expected_format={},\n        )",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 8,
                      "end_line": 8,
                      "label": "yield \"response\", self.llm_call(object_input_data, credentials)",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 35,
              "label": "class Input(BlockSchema):\n        prompt: str = SchemaField(\n            description=\"The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces.\",\n            placeholder=\"Enter your prompt here...\",\n        )\n        model: LlmModel = SchemaField(\n            title=\"LLM Model\",\n            default=LlmModel.GPT4_TURBO,\n            description=\"The language model to use for answering the prompt.\",\n            advanced=False,\n        )\n        credentials: AICredentials = AICredentialsField()\n        sys_prompt: str = SchemaField(\n            title=\"System Prompt\",\n            default=\"\",\n            description=\"The system prompt to provide additional context to the model.\",\n        )\n        retry: int = SchemaField(\n            title=\"Retry Count\",\n            default=3,\n            description=\"Number of times to retry the LLM call if the response does not match the expected format.\",\n        )\n        prompt_values: dict[str, str] = SchemaField(\n            advanced=False, default={}, description=\"Values used to fill in the prompt.\"\n        )\n        ollama_host: str = SchemaField(\n            advanced=True,\n            default=\"localhost:11434\",\n            description=\"Ollama host for local  models\",\n        )\n        max_tokens: int | None = SchemaField(\n            advanced=True,\n            default=None,\n            description=\"The maximum number of tokens to generate in the chat completion.\",\n        )",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "SummaryStyle",
      "type": "CFG",
      "blocks": [],
      "functions": [],
      "classes": []
    },
    {
      "name": "AITextSummarizerBlock",
      "type": "CFG",
      "blocks": [],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "def __init__(self):",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 21,
                  "label": "super().__init__(\n            id=\"a0a69be1-4528-491c-a85a-a4ab6873e3f0\",\n            description=\"Utilize a Large Language Model (LLM) to summarize a long text.\",\n            categories={BlockCategory.AI, BlockCategory.TEXT},\n            input_schema=AITextSummarizerBlock.Input,\n            output_schema=AITextSummarizerBlock.Output,\n            test_input={\n                \"text\": \"Lorem ipsum...\" * 100,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\"summary\", \"Final summary of a long text\"),\n            test_mock={\n                \"llm_call\": lambda input_data, credentials: (\n                    {\"final_summary\": \"Final summary of a long text\"}\n                    if \"final_summary\" in input_data.expected_format\n                    else {\"summary\": \"Summary of a chunk of text\"}\n                )\n            },\n        )",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 3,
              "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 4,
                  "end_line": 5,
                  "label": "for output in self._run(input_data, credentials):\n            yield output",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 4,
              "label": "def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:\n        chunks = self._split_text(\n            input_data.text, input_data.max_tokens, input_data.chunk_overlap\n        )",
              "successors": [
                {
                  "id": 2,
                  "start_line": 5,
                  "end_line": 5,
                  "label": "summaries = []",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 6,
                      "end_line": 8,
                      "label": "for chunk in chunks:\n            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)\n            summaries.append(chunk_summary)",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 9,
                          "end_line": 10,
                          "label": "final_summary = self._combine_summaries(summaries, input_data, credentials)\n        yield \"summary\", final_summary",
                          "successors": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_split_text",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 4,
              "label": "def _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:\n        words = text.split()\n        chunks = []\n        chunk_size = max_tokens - overlap",
              "successors": [
                {
                  "id": 2,
                  "start_line": 5,
                  "end_line": 8,
                  "label": "for i in range(0, len(words), chunk_size):\n            chunk = \" \".join(words[i : i + max_tokens])\n            chunks.append(chunk)\n        return chunks",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 5,
              "label": "def llm_call(\n        self,\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> dict:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 6,
                  "end_line": 6,
                  "label": "block = AIStructuredResponseGeneratorBlock()",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 7,
                      "end_line": 7,
                      "label": "response = block.run_once(input_data, \"response\", credentials=credentials)",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 8,
                          "end_line": 8,
                          "label": "self.merge_stats(block.execution_stats)",
                          "successors": [
                            {
                              "id": 5,
                              "start_line": 9,
                              "end_line": 9,
                              "label": "return response",
                              "successors": []
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_summarize_chunk",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 3,
              "label": "def _summarize_chunk(\n        self, chunk: str, input_data: Input, credentials: APIKeyCredentials\n    ) -> str:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 4,
                  "end_line": 4,
                  "label": "prompt = f\"Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```\"",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 5,
                      "end_line": 13,
                      "label": "llm_response = self.llm_call(\n            AIStructuredResponseGeneratorBlock.Input(\n                prompt=prompt,\n                credentials=input_data.credentials,\n                model=input_data.model,\n                expected_format={\"summary\": \"The summary of the given text.\"},\n            ),\n            credentials=credentials,\n        )",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 14,
                          "end_line": 14,
                          "label": "return llm_response[\"summary\"]",
                          "successors": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_combine_summaries",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 4,
              "label": "def _combine_summaries(\n        self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials\n    ) -> str:\n        combined_text = \"\\n\\n\".join(summaries)",
              "successors": [
                {
                  "id": 2,
                  "start_line": 5,
                  "end_line": 5,
                  "label": "if len(combined_text.split()) <= input_data.max_tokens:",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 6,
                      "end_line": 18,
                      "label": "prompt = f\"Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.\"\n            llm_response = self.llm_call(\n                AIStructuredResponseGeneratorBlock.Input(\n                    prompt=prompt,\n                    credentials=input_data.credentials,\n                    model=input_data.model,\n                    expected_format={\n                        \"final_summary\": \"The final summary of all provided summaries.\"\n                    },\n                ),\n                credentials=credentials,\n            )\n            return llm_response[\"final_summary\"]",
                      "successors": []
                    },
                    {
                      "id": 4,
                      "start_line": 19,
                      "end_line": 32,
                      "label": "else:\n            # If combined summaries are still too long, recursively summarize\n            return self._run(\n                AITextSummarizerBlock.Input(\n                    text=combined_text,\n                    credentials=input_data.credentials,\n                    model=input_data.model,\n                    max_tokens=input_data.max_tokens,\n                    chunk_overlap=input_data.chunk_overlap,\n                ),\n                credentials=credentials,\n            ).send(None)[\n                1\n            ]  # Get the first yielded value",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "class Input(BlockSchema):",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 5,
                  "label": "text: str = SchemaField(\n            description=\"The text to summarize.\",\n            placeholder=\"Enter the text to summarize here...\",\n        )",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 6,
                      "end_line": 10,
                      "label": "model: LlmModel = SchemaField(\n            title=\"LLM Model\",\n            default=LlmModel.GPT4_TURBO,\n            description=\"The language model to use for summarizing the text.\",\n        )",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 11,
                          "end_line": 15,
                          "label": "focus: str = SchemaField(\n            title=\"Focus\",\n            default=\"general information\",\n            description=\"The topic to focus on in the summary\",\n        )",
                          "successors": [
                            {
                              "id": 5,
                              "start_line": 16,
                              "end_line": 20,
                              "label": "style: SummaryStyle = SchemaField(\n            title=\"Summary Style\",\n            default=SummaryStyle.CONCISE,\n            description=\"The style of the summary to generate.\",\n        )",
                              "successors": [
                                {
                                  "id": 6,
                                  "start_line": 21,
                                  "end_line": 21,
                                  "label": "credentials: AICredentials = AICredentialsField()",
                                  "successors": [
                                    {
                                      "id": 7,
                                      "start_line": 22,
                                      "end_line": 22,
                                      "label": "# TODO: Make this dynamic",
                                      "successors": [
                                        {
                                          "id": 8,
                                          "start_line": 23,
                                          "end_line": 28,
                                          "label": "max_tokens: int = SchemaField(\n            title=\"Max Tokens\",\n            default=4096,\n            description=\"The maximum number of tokens to generate in the chat completion.\",\n            ge=1,\n        )",
                                          "successors": [
                                            {
                                              "id": 9,
                                              "start_line": 29,
                                              "end_line": 34,
                                              "label": "chunk_overlap: int = SchemaField(\n            title=\"Chunk Overlap\",\n            default=100,\n            description=\"The number of overlapping tokens between chunks to maintain context.\",\n            ge=0,\n        )",
                                              "successors": [
                                                {
                                                  "id": 10,
                                                  "start_line": 35,
                                                  "end_line": 39,
                                                  "label": "ollama_host: str = SchemaField(\n            advanced=True,\n            default=\"localhost:11434\",\n            description=\"Ollama host for local  models\",\n        )",
                                                  "successors": []
                                                }
                                              ]
                                            }
                                          ]
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "AIConversationBlock",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 1,
          "label": "class AIConversationBlock(Block):",
          "successors": []
        }
      ],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "def __init__(self):",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 29,
                  "label": "super().__init__(\n            id=\"32a87eab-381e-4dd4-bdb8-4c47151be35a\",\n            description=\"Advanced LLM call that takes a list of messages and sends them to the language model.\",\n            categories={BlockCategory.AI},\n            input_schema=AIConversationBlock.Input,\n            output_schema=AIConversationBlock.Output,\n            test_input={\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\",\n                    },\n                    {\"role\": \"user\", \"content\": \"Where was it played?\"},\n                ],\n                \"model\": LlmModel.GPT4_TURBO,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\n                \"response\",\n                \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\",\n            ),\n            test_mock={\n                \"llm_call\": lambda *args, **kwargs: \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\"\n            },\n        )",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 5,
              "label": "def llm_call(\n        self,\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> str:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 6,
                  "end_line": 6,
                  "label": "block = AIStructuredResponseGeneratorBlock()",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 7,
                      "end_line": 7,
                      "label": "response = block.run_once(input_data, \"response\", credentials=credentials)",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 8,
                          "end_line": 8,
                          "label": "self.merge_stats(block.execution_stats)",
                          "successors": [
                            {
                              "id": 5,
                              "start_line": 9,
                              "end_line": 9,
                              "label": "return response[\"response\"]",
                              "successors": []
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 3,
              "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 4,
                  "end_line": 14,
                  "label": "response = self.llm_call(\n            AIStructuredResponseGeneratorBlock.Input(\n                prompt=\"\",\n                credentials=input_data.credentials,\n                model=input_data.model,\n                conversation_history=input_data.messages,\n                max_tokens=input_data.max_tokens,\n                expected_format={},\n            ),\n            credentials=credentials,\n        )",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 15,
                      "end_line": 15,
                      "label": "yield \"response\", response",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 20,
              "label": "class Input(BlockSchema):\n        messages: List[Message] = SchemaField(\n            description=\"List of messages in the conversation.\", min_length=1\n        )\n        model: LlmModel = SchemaField(\n            title=\"LLM Model\",\n            default=LlmModel.GPT4_TURBO,\n            description=\"The language model to use for the conversation.\",\n        )\n        credentials: AICredentials = AICredentialsField()\n        max_tokens: int | None = SchemaField(\n            advanced=True,\n            default=None,\n            description=\"The maximum number of tokens to generate in the chat completion.\",\n        )\n        ollama_host: str = SchemaField(\n            advanced=True,\n            default=\"localhost:11434\",\n            description=\"Ollama host for local  models\",\n        )",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "AIListGeneratorBlock",
      "type": "CFG",
      "blocks": [
        {
          "id": 1,
          "start_line": 1,
          "end_line": 2,
          "label": "class AIListGeneratorBlock(Block):\n    @staticmethod",
          "successors": []
        }
      ],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "def __init__(self):",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 39,
                  "label": "super().__init__(\n            id=\"9c0b0450-d199-458b-a731-072189dd6593\",\n            description=\"Generate a Python list based on the given prompt using a Large Language Model (LLM).\",\n            categories={BlockCategory.AI, BlockCategory.TEXT},\n            input_schema=AIListGeneratorBlock.Input,\n            output_schema=AIListGeneratorBlock.Output,\n            test_input={\n                \"focus\": \"planets\",\n                \"source_data\": (\n                    \"Zylora Prime is a glowing jungle world with bioluminescent plants, \"\n                    \"while Kharon-9 is a harsh desert planet with underground cities. \"\n                    \"Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to \"\n                    \"intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, \"\n                    \"drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of \"\n                    \"fictional worlds.\"\n                ),\n                \"model\": LlmModel.GPT4_TURBO,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n                \"max_retries\": 3,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=[\n                (\n                    \"generated_list\",\n                    [\"Zylora Prime\", \"Kharon-9\", \"Vortexia\", \"Oceara\", \"Draknos\"],\n                ),\n                (\"list_item\", \"Zylora Prime\"),\n                (\"list_item\", \"Kharon-9\"),\n                (\"list_item\", \"Vortexia\"),\n                (\"list_item\", \"Oceara\"),\n                (\"list_item\", \"Draknos\"),\n            ],\n            test_mock={\n                \"llm_call\": lambda input_data, credentials: {\n                    \"response\": \"['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']\"\n                },\n            },\n        )",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 5,
              "label": "@staticmethod\n    def llm_call(\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> dict[str, str]:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 6,
                  "end_line": 8,
                  "label": "llm_block = AIStructuredResponseGeneratorBlock()\n        response = llm_block.run_once(input_data, \"response\", credentials=credentials)\n        return response",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "string_to_list",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 5,
              "label": "def string_to_list(string):\n        \"\"\"\n        Converts a string representation of a list into an actual Python list object.\n        \"\"\"\n        logger.debug(f\"Converting string to list. Input string: {string}\")",
              "successors": [
                {
                  "id": 2,
                  "start_line": 6,
                  "end_line": 6,
                  "label": "try:",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 7,
                      "end_line": 8,
                      "label": "# Use ast.literal_eval to safely evaluate the string\n            python_list = ast.literal_eval(string)",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 9,
                          "end_line": 9,
                          "label": "if isinstance(python_list, list):",
                          "successors": [
                            {
                              "id": 5,
                              "start_line": 10,
                              "end_line": 11,
                              "label": "logger.debug(f\"Successfully converted string to list: {python_list}\")\n                return python_list",
                              "successors": []
                            },
                            {
                              "id": 6,
                              "start_line": 12,
                              "end_line": 14,
                              "label": "else:\n                logger.error(f\"The provided string '{string}' is not a valid list\")\n                raise ValueError(f\"The provided string '{string}' is not a valid list.\")",
                              "successors": []
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": 7,
                      "start_line": 15,
                      "end_line": 17,
                      "label": "except (SyntaxError, ValueError) as e:\n            logger.error(f\"Failed to convert string to list: {e}\")\n            raise ValueError(\"Invalid list format. Could not convert to list.\")",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 3,
              "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 4,
                  "end_line": 6,
                  "label": "logger.debug(f\"Starting AIListGeneratorBlock.run with input data: {input_data}\")\n        # Check for API key\n        api_key_check = credentials.api_key.get_secret_value()",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 7,
                      "end_line": 8,
                      "label": "if not api_key_check:\n            raise ValueError(\"No LLM API key provided.\")",
                      "successors": []
                    }
                  ]
                }
              ]
            },
            {
              "id": 4,
              "start_line": 9,
              "end_line": 36,
              "label": "# Prepare the system prompt\n        sys_prompt = \"\"\"You are a Python list generator. Your task is to generate a Python list based on the user's prompt. \n            |Respond ONLY with a valid python list. \n            |The list can contain strings, numbers, or nested lists as appropriate. \n            |Do not include any explanations or additional text.\n            |Valid Example string formats:\n            |Example 1:\n            |```\n            |['1', '2', '3', '4']\n            |```\n            |Example 2:\n            |```\n            |[['1', '2'], ['3', '4'], ['5', '6']]\n            |```\n            |Example 3:\n            |```\n            |['1', ['2', '3'], ['4', ['5', '6']]]\n            |```\n            |Example 4:\n            |```\n            |['a', 'b', 'c']\n            |```\n            |Example 5:\n            |```\n            |['1', '2.5', 'string', 'True', ['False', 'None']]\n            |```\n            |Do not include any explanations or additional text, just respond with the list in the format specified above.\n            \"\"\"",
              "successors": [
                {
                  "id": 5,
                  "start_line": 37,
                  "end_line": 38,
                  "label": "# If a focus is provided, add it to the prompt\n        if input_data.focus:",
                  "successors": [
                    {
                      "id": 6,
                      "start_line": 39,
                      "end_line": 39,
                      "label": "prompt = f\"Generate a list with the following focus:\\n<focus>\\n\\n{input_data.focus}</focus>\"",
                      "successors": []
                    },
                    {
                      "id": 7,
                      "start_line": 40,
                      "end_line": 42,
                      "label": "else:\n            # If there's source data\n            if input_data.source_data:",
                      "successors": [
                        {
                          "id": 8,
                          "start_line": 43,
                          "end_line": 43,
                          "label": "prompt = \"Extract the main focus of the source data to a list.\\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.\"",
                          "successors": []
                        },
                        {
                          "id": 9,
                          "start_line": 44,
                          "end_line": 46,
                          "label": "else:\n                # No focus or source data provided, generat a random list\n                prompt = \"Generate a random list.\"",
                          "successors": []
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": 10,
              "start_line": 47,
              "end_line": 48,
              "label": "# If the source data is provided, add it to the prompt\n        if input_data.source_data:",
              "successors": [
                {
                  "id": 11,
                  "start_line": 49,
                  "end_line": 49,
                  "label": "prompt += f\"\\n\\nUse the following source data to generate the list from:\\n\\n<source_data>\\n\\n{input_data.source_data}</source_data>\\n\\nDo not invent fictional data that is not present in the source data.\"",
                  "successors": []
                },
                {
                  "id": 12,
                  "start_line": 50,
                  "end_line": 52,
                  "label": "# Else, tell the LLM to synthesize the data\n        else:\n            prompt += \"\\n\\nInvent the data to generate the list from.\"",
                  "successors": []
                }
              ]
            },
            {
              "id": 13,
              "start_line": 53,
              "end_line": 53,
              "label": "for attempt in range(input_data.max_retries):",
              "successors": [
                {
                  "id": 14,
                  "start_line": 54,
                  "end_line": 54,
                  "label": "try:",
                  "successors": [
                    {
                      "id": 15,
                      "start_line": 55,
                      "end_line": 66,
                      "label": "logger.debug(\"Calling LLM\")\n                llm_response = self.llm_call(\n                    AIStructuredResponseGeneratorBlock.Input(\n                        sys_prompt=sys_prompt,\n                        prompt=prompt,\n                        credentials=input_data.credentials,\n                        model=input_data.model,\n                        expected_format={},  # Do not use structured response\n                        ollama_host=input_data.ollama_host,\n                    ),\n                    credentials=credentials,\n                )",
                      "successors": [
                        {
                          "id": 16,
                          "start_line": 67,
                          "end_line": 67,
                          "label": "logger.debug(f\"LLM response: {llm_response}\")",
                          "successors": [
                            {
                              "id": 17,
                              "start_line": 68,
                              "end_line": 70,
                              "label": "# Extract Response string\n                response_string = llm_response[\"response\"]\n                logger.debug(f\"Response string: {response_string}\")",
                              "successors": [
                                {
                                  "id": 18,
                                  "start_line": 71,
                                  "end_line": 74,
                                  "label": "# Convert the string to a Python list\n                logger.debug(\"Converting string to Python list\")\n                parsed_list = self.string_to_list(response_string)\n                logger.debug(f\"Parsed list: {parsed_list}\")",
                                  "successors": [
                                    {
                                      "id": 19,
                                      "start_line": 75,
                                      "end_line": 77,
                                      "label": "# If we reach here, we have a valid Python list\n                logger.debug(\"Successfully generated a valid Python list\")\n                yield \"generated_list\", parsed_list",
                                      "successors": [
                                        {
                                          "id": 20,
                                          "start_line": 78,
                                          "end_line": 80,
                                          "label": "# Yield each item in the list\n                for item in parsed_list:\n                    yield \"list_item\", item",
                                          "successors": []
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": 21,
                      "start_line": 82,
                      "end_line": 83,
                      "label": "except Exception as e:\n                logger.error(f\"Error in attempt {attempt + 1}: {str(e)}\")",
                      "successors": [
                        {
                          "id": 22,
                          "start_line": 84,
                          "end_line": 84,
                          "label": "if attempt == input_data.max_retries - 1:",
                          "successors": [
                            {
                              "id": 23,
                              "start_line": 85,
                              "end_line": 90,
                              "label": "logger.error(\n                        f\"Failed to generate a valid Python list after {input_data.max_retries} attempts\"\n                    )\n                    raise RuntimeError(\n                        f\"Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}\"\n                    )",
                              "successors": []
                            },
                            {
                              "id": 24,
                              "start_line": 91,
                              "end_line": 93,
                              "label": "else:\n                    # Add a retry prompt\n                    logger.debug(\"Preparing retry prompt\")",
                              "successors": [
                                {
                                  "id": 25,
                                  "start_line": 94,
                                  "end_line": 102,
                                  "label": "prompt = f\"\"\"\n                    The previous attempt failed due to `{e}`\n                    Generate a valid Python list based on the original prompt.\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\n                    Original prompt: \n                    ```{prompt}```\n                    \n                    Respond only with the list in the format specified with no commentary or apologies.\n                    \"\"\"",
                                  "successors": [
                                    {
                                      "id": 26,
                                      "start_line": 103,
                                      "end_line": 103,
                                      "label": "logger.debug(f\"Retry prompt: {prompt}\")",
                                      "successors": []
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": 27,
              "start_line": 104,
              "end_line": 104,
              "label": "logger.debug(\"AIListGeneratorBlock.run completed\")",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "class Input(BlockSchema):",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 7,
                  "label": "focus: str | None = SchemaField(\n            description=\"The focus of the list to generate.\",\n            placeholder=\"The top 5 most interesting news stories in the data.\",\n            default=None,\n            advanced=False,\n        )",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 8,
                      "end_line": 13,
                      "label": "source_data: str | None = SchemaField(\n            description=\"The data to generate the list from.\",\n            placeholder=\"News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released.\",\n            default=None,\n            advanced=False,\n        )",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 14,
                          "end_line": 19,
                          "label": "model: LlmModel = SchemaField(\n            title=\"LLM Model\",\n            default=LlmModel.GPT4_TURBO,\n            description=\"The language model to use for generating the list.\",\n            advanced=True,\n        )",
                          "successors": [
                            {
                              "id": 5,
                              "start_line": 20,
                              "end_line": 20,
                              "label": "credentials: AICredentials = AICredentialsField()",
                              "successors": [
                                {
                                  "id": 6,
                                  "start_line": 21,
                                  "end_line": 26,
                                  "label": "max_retries: int = SchemaField(\n            default=3,\n            description=\"Maximum number of retries for generating a valid list.\",\n            ge=1,\n            le=5,\n        )",
                                  "successors": [
                                    {
                                      "id": 7,
                                      "start_line": 27,
                                      "end_line": 31,
                                      "label": "max_tokens: int | None = SchemaField(\n            advanced=True,\n            default=None,\n            description=\"The maximum number of tokens to generate in the chat completion.\",\n        )",
                                      "successors": [
                                        {
                                          "id": 8,
                                          "start_line": 32,
                                          "end_line": 36,
                                          "label": "ollama_host: str = SchemaField(\n            advanced=True,\n            default=\"localhost:11434\",\n            description=\"Ollama host for local  models\",\n        )",
                                          "successors": []
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "class Output(BlockSchema):",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 2,
                  "label": "generated_list: List[str] = SchemaField(description=\"The generated list.\")",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 3,
                      "end_line": 5,
                      "label": "list_item: str = SchemaField(\n            description=\"Each individual item in the list.\",\n        )",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 6,
                          "end_line": 8,
                          "label": "error: str = SchemaField(\n            description=\"Error message if the list generation failed.\"\n        )",
                          "successors": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ]
    }
  ]
}