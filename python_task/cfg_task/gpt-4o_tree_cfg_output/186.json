{
  "name": "example_script",
  "type": "CFG",
  "blocks": [
    {
      "id": 1,
      "start_line": 1,
      "end_line": 9,
      "label": "import ast\nimport logging\nfrom enum import Enum, EnumMeta\nfrom json import JSONDecodeError\nfrom types import MappingProxyType\nfrom typing import TYPE_CHECKING, Any, List, Literal, NamedTuple\nfrom pydantic import SecretStr\nfrom backend.integrations.providers import ProviderName\nif TYPE_CHECKING:",
      "successors": [
        {
          "id": 2,
          "start_line": 10,
          "end_line": 10,
          "label": "from enum import _EnumMemberT",
          "successors": []
        }
      ]
    },
    {
      "id": 3,
      "start_line": 11,
      "end_line": 85,
      "label": "import anthropic\nimport ollama\nimport openai\nfrom groq import Groq\nfrom backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema\nfrom backend.data.model import (\n    APIKeyCredentials,\n    CredentialsField,\n    CredentialsMetaInput,\n    SchemaField,\n)\nfrom backend.util import json\nfrom backend.util.settings import BehaveAs, Settings\nlogger = logging.getLogger(__name__)\nLLMProviderName = Literal[\n    ProviderName.ANTHROPIC,\n    ProviderName.GROQ,\n    ProviderName.OLLAMA,\n    ProviderName.OPENAI,\n    ProviderName.OPEN_ROUTER,\n]\nAICredentials = CredentialsMetaInput[LLMProviderName, Literal[\"api_key\"]]\nTEST_CREDENTIALS = APIKeyCredentials(\n    id=\"ed55ac19-356e-4243-a6cb-bc599e9b716f\",\n    provider=\"openai\",\n    api_key=SecretStr(\"mock-openai-api-key\"),\n    title=\"Mock OpenAI API key\",\n    expires_at=None,\n)\nTEST_CREDENTIALS_INPUT = {\n    \"provider\": TEST_CREDENTIALS.provider,\n    \"id\": TEST_CREDENTIALS.id,\n    \"type\": TEST_CREDENTIALS.type,\n    \"title\": TEST_CREDENTIALS.title,\n}\nMODEL_METADATA = {\n    LlmModel.O1_PREVIEW: ModelMetadata(\"openai\", 32000),\n    LlmModel.O1_MINI: ModelMetadata(\"openai\", 62000),\n    LlmModel.GPT4O_MINI: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT4O: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT4_TURBO: ModelMetadata(\"openai\", 128000),\n    LlmModel.GPT3_5_TURBO: ModelMetadata(\"openai\", 16385),\n    LlmModel.CLAUDE_3_5_SONNET: ModelMetadata(\"anthropic\", 200000),\n    LlmModel.CLAUDE_3_HAIKU: ModelMetadata(\"anthropic\", 200000),\n    LlmModel.LLAMA3_8B: ModelMetadata(\"groq\", 8192),\n    LlmModel.LLAMA3_70B: ModelMetadata(\"groq\", 8192),\n    LlmModel.MIXTRAL_8X7B: ModelMetadata(\"groq\", 32768),\n    LlmModel.GEMMA_7B: ModelMetadata(\"groq\", 8192),\n    LlmModel.GEMMA2_9B: ModelMetadata(\"groq\", 8192),\n    LlmModel.LLAMA3_1_405B: ModelMetadata(\"groq\", 8192),\n    # Limited to 16k during preview\n    LlmModel.LLAMA3_1_70B: ModelMetadata(\"groq\", 131072),\n    LlmModel.LLAMA3_1_8B: ModelMetadata(\"groq\", 131072),\n    LlmModel.OLLAMA_LLAMA3_8B: ModelMetadata(\"ollama\", 8192),\n    LlmModel.OLLAMA_LLAMA3_405B: ModelMetadata(\"ollama\", 8192),\n    LlmModel.OLLAMA_DOLPHIN: ModelMetadata(\"ollama\", 32768),\n    LlmModel.GEMINI_FLASH_1_5_8B: ModelMetadata(\"open_router\", 8192),\n    LlmModel.GROK_BETA: ModelMetadata(\"open_router\", 8192),\n    LlmModel.MISTRAL_NEMO: ModelMetadata(\"open_router\", 4000),\n    LlmModel.COHERE_COMMAND_R_08_2024: ModelMetadata(\"open_router\", 4000),\n    LlmModel.COHERE_COMMAND_R_PLUS_08_2024: ModelMetadata(\"open_router\", 4000),\n    LlmModel.EVA_QWEN_2_5_32B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.DEEPSEEK_CHAT: ModelMetadata(\"open_router\", 8192),\n    LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: ModelMetadata(\n        \"open_router\", 8192\n    ),\n    LlmModel.QWEN_QWQ_32B_PREVIEW: ModelMetadata(\"open_router\", 4000),\n    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_LITE_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_MICRO_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.AMAZON_NOVA_PRO_V1: ModelMetadata(\"open_router\", 4000),\n    LlmModel.MICROSOFT_WIZARDLM_2_8X22B: ModelMetadata(\"open_router\", 4000),\n    LlmModel.GRYPHE_MYTHOMAX_L2_13B: ModelMetadata(\"open_router\", 4000),\n}",
      "successors": [
        {
          "id": 4,
          "start_line": 86,
          "end_line": 86,
          "label": "for model in LlmModel:",
          "successors": [
            {
              "id": 5,
              "start_line": 87,
              "end_line": 88,
              "label": "if model not in MODEL_METADATA:\n        raise ValueError(f\"Missing MODEL_METADATA metadata for model: {model}\")",
              "successors": []
            }
          ]
        }
      ]
    }
  ],
  "functions": [
    {
      "name": "AICredentialsField",
      "type": "CFG",
      "blocks": [],
      "functions": [],
      "classes": []
    }
  ],
  "classes": [
    {
      "name": "ModelMetadata",
      "type": "CFG",
      "blocks": [],
      "functions": [],
      "classes": []
    },
    {
      "name": "LlmModelMeta",
      "type": "CFG",
      "blocks": [],
      "functions": [
        {
          "name": "__members__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 4,
              "end_line": 4,
              "label": "if Settings().config.behave_as == BehaveAs.LOCAL:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 5,
                  "end_line": 6,
                  "label": "members = super().__members__\n            return members",
                  "successors": []
                },
                {
                  "id": 3,
                  "start_line": 7,
                  "end_line": 15,
                  "label": "else:\n            removed_providers = [\"ollama\"]\n            existing_members = super().__members__\n            members = {\n                name: member\n                for name, member in existing_members.items()\n                if LlmModel[name].provider not in removed_providers\n            }\n            return MappingProxyType(members)",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": []
    },
    {
      "name": "LlmModel",
      "type": "CFG",
      "blocks": [],
      "functions": [
        {
          "name": "metadata",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 2,
              "label": "def metadata(self) -> ModelMetadata:\n        return MODEL_METADATA[self]",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "provider",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 2,
              "label": "def provider(self) -> str:\n        return self.metadata.provider",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "context_window",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 1,
              "label": "def context_window(self) -> int:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 2,
                  "end_line": 2,
                  "label": "return self.metadata.context_window",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": []
    },
    {
      "name": "MessageRole",
      "type": "CFG",
      "blocks": [],
      "functions": [],
      "classes": []
    },
    {
      "name": "Message",
      "type": "CFG",
      "blocks": [],
      "functions": [],
      "classes": []
    },
    {
      "name": "AIStructuredResponseGeneratorBlock",
      "type": "CFG",
      "blocks": [],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 31,
              "label": "def __init__(self):\n        super().__init__(\n            id=\"ed55ac19-356e-4243-a6cb-bc599e9b716f\",\n            description=\"Call a Large Language Model (LLM) to generate formatted object based on the given prompt.\",\n            categories={BlockCategory.AI},\n            input_schema=AIStructuredResponseGeneratorBlock.Input,\n            output_schema=AIStructuredResponseGeneratorBlock.Output,\n            test_input={\n                \"model\": LlmModel.GPT4_TURBO,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n                \"expected_format\": {\n                    \"key1\": \"value1\",\n                    \"key2\": \"value2\",\n                },\n                \"prompt\": \"User prompt\",\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\"response\", {\"key1\": \"key1Value\", \"key2\": \"key2Value\"}),\n            test_mock={\n                \"llm_call\": lambda *args, **kwargs: (\n                    json.dumps(\n                        {\n                            \"key1\": \"key1Value\",\n                            \"key2\": \"key2Value\",\n                        }\n                    ),\n                    0,\n                    0,\n                )\n            },\n        )",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 22,
              "label": "def llm_call(\n        credentials: APIKeyCredentials,\n        llm_model: LlmModel,\n        prompt: list[dict],\n        json_format: bool,\n        max_tokens: int | None = None,\n        ollama_host: str = \"localhost:11434\",\n    ) -> tuple[str, int, int]:\n        \"\"\"\n        Args:\n            api_key: API key for the LLM provider.\n            llm_model: The LLM model to use.\n            prompt: The prompt to send to the LLM.\n            json_format: Whether the response should be in JSON format.\n            max_tokens: The maximum number of tokens to generate in the chat completion.\n            ollama_host: The host for ollama to use\n        Returns:\n            The response from the LLM.\n            The number of tokens used in the prompt.\n            The number of tokens used in the completion.\n        \"\"\"\n        provider = llm_model.metadata.provider",
              "successors": [
                {
                  "id": 2,
                  "start_line": 23,
                  "end_line": 46,
                  "label": "if provider == \"openai\":\n        elif provider == \"anthropic\":",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 24,
                      "end_line": 26,
                      "label": "oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())\n            response_format = None\n            if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:",
                      "successors": [
                        {
                          "id": 4,
                          "start_line": 27,
                          "end_line": 33,
                          "label": "sys_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]\n                usr_messages = [p[\"content\"] for p in prompt if p[\"role\"] != \"system\"]\n                prompt = [\n                    {\"role\": \"user\", \"content\": \"\\n\".join(sys_messages)},\n                    {\"role\": \"user\", \"content\": \"\\n\".join(usr_messages)},\n                ]\n            elif json_format:",
                          "successors": []
                        }
                      ]
                    },
                    {
                      "id": 5,
                      "start_line": 33,
                      "end_line": 35,
                      "label": "elif json_format:\n                response_format = {\"type\": \"json_object\"}\n            response = oai_client.chat.completions.create(",
                      "successors": []
                    },
                    {
                      "id": 6,
                      "start_line": 35,
                      "end_line": 45,
                      "label": "response = oai_client.chat.completions.create(\n                model=llm_model.value,\n                messages=prompt,  # type: ignore\n                response_format=response_format,  # type: ignore\n                max_completion_tokens=max_tokens,\n            )\n            return (\n                response.choices[0].message.content or \"\",\n                response.usage.prompt_tokens if response.usage else 0,\n                response.usage.completion_tokens if response.usage else 0,\n            )",
                      "successors": []
                    }
                  ]
                },
                {
                  "id": 7,
                  "start_line": 46,
                  "end_line": 82,
                  "label": "elif provider == \"anthropic\":",
                  "successors": [
                    {
                      "id": 8,
                      "start_line": 47,
                      "end_line": 60,
                      "label": "system_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]\n            sysprompt = \" \".join(system_messages)\n            messages = []\n            last_role = None\n            for p in prompt:\n                if p[\"role\"] in [\"user\", \"assistant\"]:\n                    if p[\"role\"] != last_role:\n                        messages.append({\"role\": p[\"role\"], \"content\": p[\"content\"]})\n                        last_role = p[\"role\"]\n                    else:\n                        # If the role is the same as the last one, combine the content\n                        messages[-1][\"content\"] += \"\\n\" + p[\"content\"]\n            client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())\n            try:",
                      "successors": [
                        {
                          "id": 9,
                          "start_line": 61,
                          "end_line": 67,
                          "label": "resp = client.messages.create(\n                    model=llm_model.value,\n                    system=sysprompt,\n                    messages=messages,\n                    max_tokens=max_tokens or 8192,\n                )\n                if not resp.content:",
                          "successors": [
                            {
                              "id": 10,
                              "start_line": 68,
                              "end_line": 82,
                              "label": "raise ValueError(\"No content returned from Anthropic.\")\n                return (\n                    (\n                        resp.content[0].name\n                        if isinstance(resp.content[0], anthropic.types.ToolUseBlock)\n                        else resp.content[0].text\n                    ),\n                    resp.usage.input_tokens,\n                    resp.usage.output_tokens,\n                )",
                              "successors": [
                                {
                                  "id": 11,
                                  "start_line": 78,
                                  "end_line": 82,
                                  "label": "except anthropic.APIError as e:\n                error_message = f\"Anthropic API error: {str(e)}\"\n                logger.error(error_message)\n                raise ValueError(error_message)\n        elif provider == \"groq\":",
                                  "successors": []
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": 12,
                      "start_line": 82,
                      "end_line": 96,
                      "label": "elif provider == \"groq\":\n            client = Groq(api_key=credentials.api_key.get_secret_value())\n            response_format = {\"type\": \"json_object\"} if json_format else None\n            response = client.chat.completions.create(\n                model=llm_model.value,\n                messages=prompt,  # type: ignore\n                response_format=response_format,  # type: ignore\n                max_tokens=max_tokens,\n            )\n            return (\n                response.choices[0].message.content or \"\",\n                response.usage.prompt_tokens if response.usage else 0,\n                response.usage.completion_tokens if response.usage else 0,\n            )\n        elif provider == \"ollama\":",
                      "successors": []
                    }
                  ]
                },
                {
                  "id": 13,
                  "start_line": 96,
                  "end_line": 110,
                  "label": "elif provider == \"ollama\":\n            client = ollama.Client(host=ollama_host)\n            sys_messages = [p[\"content\"] for p in prompt if p[\"role\"] == \"system\"]\n            usr_messages = [p[\"content\"] for p in prompt if p[\"role\"] != \"system\"]\n            response = client.generate(\n                model=llm_model.value,\n                prompt=f\"{sys_messages}\\n\\n{usr_messages}\",\n                stream=False,\n            )\n            return (\n                response.get(\"response\") or \"\",\n                response.get(\"prompt_eval_count\") or 0,\n                response.get(\"eval_count\") or 0,\n            )\n        elif provider == \"open_router\":",
                  "successors": []
                },
                {
                  "id": 14,
                  "start_line": 110,
                  "end_line": 135,
                  "label": "elif provider == \"open_router\":\n            client = openai.OpenAI(\n                base_url=\"https://openrouter.ai/api/v1\",\n                api_key=credentials.api_key.get_secret_value(),\n            )\n            response = client.chat.completions.create(\n                extra_headers={\n                    \"HTTP-Referer\": \"https://agpt.co\",\n                    \"X-Title\": \"AutoGPT\",\n                },\n                model=llm_model.value,\n                messages=prompt,  # type: ignore\n                max_tokens=max_tokens,\n            )\n            # If there's no response, raise an error\n                response.choices[0].message.content or \"\",\n                response.usage.prompt_tokens if response.usage else 0,\n                response.usage.completion_tokens if response.usage else 0,\n            )\n        else:",
                  "successors": [
                    {
                      "id": 15,
                      "start_line": 125,
                      "end_line": 130,
                      "label": "if not response.choices:",
                      "successors": [
                        {
                          "id": 16,
                          "start_line": 126,
                          "end_line": 130,
                          "label": "if response:\n                    raise ValueError(f\"OpenRouter error: {response}\")\n                else:\n                    raise ValueError(\"No response from OpenRouter.\")\n            return (",
                          "successors": []
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": 17,
                  "start_line": 135,
                  "end_line": 136,
                  "label": "else:\n            raise ValueError(f\"Unsupported LLM provider: {provider}\")",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 6,
              "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:\n        logger.debug(f\"Calling LLM with input data: {input_data}\")\n        prompt = [p.model_dump() for p in input_data.conversation_history]\n        values = input_data.prompt_values",
              "successors": [
                {
                  "id": 2,
                  "start_line": 7,
                  "end_line": 9,
                  "label": "if values:\n            input_data.prompt = input_data.prompt.format(**values)\n            input_data.sys_prompt = input_data.sys_prompt.format(**values)",
                  "successors": []
                },
                {
                  "id": 3,
                  "start_line": 10,
                  "end_line": 11,
                  "label": "if input_data.sys_prompt:\n            prompt.append({\"role\": \"system\", \"content\": input_data.sys_prompt})",
                  "successors": []
                },
                {
                  "id": 4,
                  "start_line": 12,
                  "end_line": 25,
                  "label": "if input_data.expected_format:\n            expected_format = [\n                f'\"{k}\": \"{v}\"' for k, v in input_data.expected_format.items()\n            ]\n            format_prompt = \",\\n  \".join(expected_format)\n            sys_prompt = trim_prompt(\n                f\"\"\"\n                  |Reply strictly only in the following JSON format:\n                  |{{\n                  |  {format_prompt}\n                  |}}\n                \"\"\"\n            )\n            prompt.append({\"role\": \"system\", \"content\": sys_prompt})",
                  "successors": []
                },
                {
                  "id": 5,
                  "start_line": 26,
                  "end_line": 27,
                  "label": "if input_data.prompt:\n            prompt.append({\"role\": \"user\", \"content\": input_data.prompt})",
                  "successors": []
                },
                {
                  "id": 6,
                  "start_line": 28,
                  "end_line": 30,
                  "label": "logger.info(f\"LLM request: {prompt}\")\n        retry_prompt = \"\"\n        llm_model = input_data.model",
                  "successors": [
                    {
                      "id": 7,
                      "start_line": 31,
                      "end_line": 64,
                      "label": "for retry_count in range(input_data.retry):\n            try:\n                response_text, input_token, output_token = self.llm_call(\n                    credentials=credentials,\n                    llm_model=llm_model,\n                    prompt=prompt,\n                    json_format=bool(input_data.expected_format),\n                    ollama_host=input_data.ollama_host,\n                    max_tokens=input_data.max_tokens,\n                )\n                self.merge_stats(\n                    {\n                        \"input_token_count\": input_token,\n                        \"output_token_count\": output_token,\n                    }\n                )\n                logger.info(f\"LLM attempt-{retry_count} response: {response_text}\")",
                      "successors": [
                        {
                          "id": 8,
                          "start_line": 48,
                          "end_line": 61,
                          "label": "if input_data.expected_format:\n                    parsed_dict, parsed_error = parse_response(response_text)",
                          "successors": [
                            {
                              "id": 9,
                              "start_line": 50,
                              "end_line": 61,
                              "label": "if not parsed_error:\n                        yield \"response\", {\n                            k: (\n                                json.loads(v)\n                                if isinstance(v, str)\n                                and v.startswith(\"[\")\n                                and v.endswith(\"]\")\n                                else (\", \".join(v) if isinstance(v, list) else v)\n                            )\n                            for k, v in parsed_dict.items()\n                        }\n                        return",
                              "successors": []
                            }
                          ]
                        },
                        {
                          "id": 10,
                          "start_line": 62,
                          "end_line": 64,
                          "label": "else:\n                    yield \"response\", {\"response\": response_text}\n                    return",
                          "successors": []
                        }
                      ]
                    },
                    {
                      "id": 11,
                      "start_line": 79,
                      "end_line": 81,
                      "label": "except Exception as e:\n                logger.exception(f\"Error calling LLM: {e}\")\n                retry_prompt = f\"Error calling LLM: {e}\"",
                      "successors": []
                    },
                    {
                      "id": 12,
                      "start_line": 82,
                      "end_line": 88,
                      "label": "finally:\n                self.merge_stats(\n                    {\n                        \"llm_call_count\": retry_count + 1,\n                        \"llm_retry_count\": retry_count,\n                    }\n                )",
                      "successors": []
                    }
                  ]
                }
              ]
            },
            {
              "id": 13,
              "start_line": 89,
              "end_line": 89,
              "label": "raise RuntimeError(retry_prompt)",
              "successors": []
            }
          ],
          "functions": [
            {
              "name": "trim_prompt",
              "type": "CFG",
              "blocks": [
                {
                  "id": 1,
                  "start_line": 1,
                  "end_line": 3,
                  "label": "def trim_prompt(s: str) -> str:\n            lines = s.strip().split(\"\\n\")\n            return \"\\n\".join([line.strip().lstrip(\"|\") for line in lines])",
                  "successors": []
                }
              ],
              "functions": [],
              "classes": []
            },
            {
              "name": "parse_response",
              "type": "CFG",
              "blocks": [
                {
                  "id": 1,
                  "start_line": 1,
                  "end_line": 2,
                  "label": "def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:\n            try:",
                  "successors": [
                    {
                      "id": 2,
                      "start_line": 3,
                      "end_line": 4,
                      "label": "parsed = json.loads(resp)\n                if not isinstance(parsed, dict):",
                      "successors": [
                        {
                          "id": 3,
                          "start_line": 5,
                          "end_line": 5,
                          "label": "return {}, f\"Expected a dictionary, but got {type(parsed)}\"",
                          "successors": []
                        },
                        {
                          "id": 4,
                          "start_line": 6,
                          "end_line": 7,
                          "label": "miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\n                if miss_keys:",
                          "successors": [
                            {
                              "id": 5,
                              "start_line": 8,
                              "end_line": 8,
                              "label": "return parsed, f\"Missing keys: {miss_keys}\"",
                              "successors": []
                            },
                            {
                              "id": 6,
                              "start_line": 9,
                              "end_line": 9,
                              "label": "return parsed, None",
                              "successors": []
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": 7,
                      "start_line": 10,
                      "end_line": 11,
                      "label": "except JSONDecodeError as e:\n                return {}, f\"JSON decode error: {e}\"",
                      "successors": []
                    }
                  ]
                }
              ],
              "functions": [],
              "classes": []
            }
          ],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 43,
              "label": "class Input(BlockSchema):\n        prompt: str = SchemaField(\n            description=\"The prompt to send to the language model.\",\n            placeholder=\"Enter your prompt here...\",\n        )\n        expected_format: dict[str, str] = SchemaField(\n            description=\"Expected format of the response. If provided, the response will be validated against this format. \"\n            \"The keys should be the expected fields in the response, and the values should be the description of the field.\",\n        )\n        model: LlmModel = SchemaField(\n            title=\"LLM Model\",\n            default=LlmModel.GPT4_TURBO,\n            description=\"The language model to use for answering the prompt.\",\n            advanced=False,\n        )\n        credentials: AICredentials = AICredentialsField()\n        sys_prompt: str = SchemaField(\n            title=\"System Prompt\",\n            default=\"\",\n            description=\"The system prompt to provide additional context to the model.\",\n        )\n        conversation_history: list[Message] = SchemaField(\n            default=[],\n            description=\"The conversation history to provide context for the prompt.\",\n        )\n        retry: int = SchemaField(\n            title=\"Retry Count\",\n            default=3,\n            description=\"Number of times to retry the LLM call if the response does not match the expected format.\",\n        )\n        prompt_values: dict[str, str] = SchemaField(\n            advanced=False, default={}, description=\"Values used to fill in the prompt.\"\n        )\n        max_tokens: int | None = SchemaField(\n            advanced=True,\n            default=None,\n            description=\"The maximum number of tokens to generate in the chat completion.\",\n        )\n        ollama_host: str = SchemaField(\n            advanced=True,\n            default=\"localhost:11434\",\n            description=\"Ollama host for local  models\",\n        )",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "AITextGeneratorBlock",
      "type": "CFG",
      "blocks": [],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 15,
              "label": "def __init__(self):\n        super().__init__(\n            id=\"1f292d4a-41a4-4977-9684-7c8d560b9f91\",\n            description=\"Call a Large Language Model (LLM) to generate a string based on the given prompt.\",\n            categories={BlockCategory.AI},\n            input_schema=AITextGeneratorBlock.Input,\n            output_schema=AITextGeneratorBlock.Output,\n            test_input={\n                \"prompt\": \"User prompt\",\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\"response\", \"Response text\"),\n            test_mock={\"llm_call\": lambda *args, **kwargs: \"Response text\"},\n        )",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 9,
              "label": "def llm_call(\n        self,\n        input_data: AIStructuredResponseGeneratorBlock.Input,\n        credentials: APIKeyCredentials,\n    ) -> str:\n        block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, \"response\", credentials=credentials)\n        self.merge_stats(block.execution_stats)\n        return response[\"response\"]",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 3,
              "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 4,
                  "end_line": 7,
                  "label": "object_input_data = AIStructuredResponseGeneratorBlock.Input(\n            **{attr: getattr(input_data, attr) for attr in input_data.model_fields},\n            expected_format={},\n        )",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 8,
                      "end_line": 8,
                      "label": "yield \"response\", self.llm_call(object_input_data, credentials)",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "SummaryStyle",
      "type": "CFG",
      "blocks": [],
      "functions": [],
      "classes": []
    },
    {
      "name": "AITextSummarizerBlock",
      "type": "CFG",
      "blocks": [],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 21,
              "label": "def __init__(self):\n        super().__init__(\n            id=\"a0a69be1-4528-491c-a85a-a4ab6873e3f0\",\n            description=\"Utilize a Large Language Model (LLM) to summarize a long text.\",\n            categories={BlockCategory.AI, BlockCategory.TEXT},\n            input_schema=AITextSummarizerBlock.Input,\n            output_schema=AITextSummarizerBlock.Output,\n            test_input={\n                \"text\": \"Lorem ipsum...\" * 100,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\"summary\", \"Final summary of a long text\"),\n            test_mock={\n                \"llm_call\": lambda input_data, credentials: (\n                    {\"final_summary\": \"Final summary of a long text\"}\n                    if \"final_summary\" in input_data.expected_format\n                    else {\"summary\": \"Summary of a chunk of text\"}\n                )\n            },\n        )",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 3,
              "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 4,
                  "end_line": 4,
                  "label": "for output in self._run(input_data, credentials):",
                  "successors": [
                    {
                      "id": 3,
                      "start_line": 5,
                      "end_line": 5,
                      "label": "yield output",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 5,
              "label": "def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:\n        chunks = self._split_text(\n            input_data.text, input_data.max_tokens, input_data.chunk_overlap\n        )\n        summaries = []",
              "successors": [
                {
                  "id": 2,
                  "start_line": 6,
                  "end_line": 8,
                  "label": "for chunk in chunks:\n            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)\n            summaries.append(chunk_summary)",
                  "successors": []
                },
                {
                  "id": 3,
                  "start_line": 9,
                  "end_line": 10,
                  "label": "final_summary = self._combine_summaries(summaries, input_data, credentials)\n        yield \"summary\", final_summary",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 6,
              "end_line": 9,
              "label": "block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, \"response\", credentials=credentials)\n        self.merge_stats(block.execution_stats)\n        return response",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_summarize_chunk",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 14,
              "label": "def _summarize_chunk(\n        self, chunk: str, input_data: Input, credentials: APIKeyCredentials\n    ) -> str:\n        prompt = f\"Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```\"\n        llm_response = self.llm_call(\n            AIStructuredResponseGeneratorBlock.Input(\n                prompt=prompt,\n                credentials=input_data.credentials,\n                model=input_data.model,\n                expected_format={\"summary\": \"The summary of the given text.\"},\n            ),\n            credentials=credentials,\n        )\n        return llm_response[\"summary\"]",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "_combine_summaries",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 5,
              "label": "def _combine_summaries(\n        self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials\n    ) -> str:\n        combined_text = \"\\n\\n\".join(summaries)\n        if len(combined_text.split()) <= input_data.max_tokens:",
              "successors": [
                {
                  "id": 2,
                  "start_line": 6,
                  "end_line": 18,
                  "label": "prompt = f\"Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.\"\n            llm_response = self.llm_call(\n                AIStructuredResponseGeneratorBlock.Input(\n                    prompt=prompt,\n                    credentials=input_data.credentials,\n                    model=input_data.model,\n                    expected_format={\n                        \"final_summary\": \"The final summary of all provided summaries.\"\n                    },\n                ),\n                credentials=credentials,\n            )\n            return llm_response[\"final_summary\"]",
                  "successors": []
                },
                {
                  "id": 3,
                  "start_line": 19,
                  "end_line": 32,
                  "label": "else:\n            # If combined summaries are still too long, recursively summarize\n            return self._run(\n                AITextSummarizerBlock.Input(\n                    text=combined_text,\n                    credentials=input_data.credentials,\n                    model=input_data.model,\n                    max_tokens=input_data.max_tokens,\n                    chunk_overlap=input_data.chunk_overlap,\n                ),\n                credentials=credentials,\n            ).send(None)[\n                1\n            ]  # Get the first yielded value",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "AIConversationBlock",
      "type": "CFG",
      "blocks": [],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 29,
              "label": "def __init__(self):\n        super().__init__(\n            id=\"32a87eab-381e-4dd4-bdb8-4c47151be35a\",\n            description=\"Advanced LLM call that takes a list of messages and sends them to the language model.\",\n            categories={BlockCategory.AI},\n            input_schema=AIConversationBlock.Input,\n            output_schema=AIConversationBlock.Output,\n            test_input={\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\",\n                    },\n                    {\"role\": \"user\", \"content\": \"Where was it played?\"},\n                ],\n                \"model\": LlmModel.GPT4_TURBO,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=(\n                \"response\",\n                \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\",\n            ),\n            test_mock={\n                \"llm_call\": lambda *args, **kwargs: \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\"\n            },\n        )",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "llm_call",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 6,
              "end_line": 9,
              "label": "block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, \"response\", credentials=credentials)\n        self.merge_stats(block.execution_stats)\n        return response[\"response\"]",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 14,
              "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:\n        response = self.llm_call(\n            AIStructuredResponseGeneratorBlock.Input(\n                prompt=\"\",\n                credentials=input_data.credentials,\n                model=input_data.model,\n                conversation_history=input_data.messages,\n                max_tokens=input_data.max_tokens,\n                expected_format={},\n            ),\n            credentials=credentials,\n        )",
              "successors": [
                {
                  "id": 2,
                  "start_line": 15,
                  "end_line": 15,
                  "label": "yield \"response\", response",
                  "successors": []
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        }
      ]
    },
    {
      "name": "AIListGeneratorBlock",
      "type": "CFG",
      "blocks": [],
      "functions": [
        {
          "name": "__init__",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 39,
              "label": "def __init__(self):\n        super().__init__(\n            id=\"9c0b0450-d199-458b-a731-072189dd6593\",\n            description=\"Generate a Python list based on the given prompt using a Large Language Model (LLM).\",\n            categories={BlockCategory.AI, BlockCategory.TEXT},\n            input_schema=AIListGeneratorBlock.Input,\n            output_schema=AIListGeneratorBlock.Output,\n            test_input={\n                \"focus\": \"planets\",\n                \"source_data\": (\n                    \"Zylora Prime is a glowing jungle world with bioluminescent plants, \"\n                    \"while Kharon-9 is a harsh desert planet with underground cities. \"\n                    \"Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to \"\n                    \"intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, \"\n                    \"drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of \"\n                    \"fictional worlds.\"\n                ),\n                \"model\": LlmModel.GPT4_TURBO,\n                \"credentials\": TEST_CREDENTIALS_INPUT,\n                \"max_retries\": 3,\n            },\n            test_credentials=TEST_CREDENTIALS,\n            test_output=[\n                (\n                    \"generated_list\",\n                    [\"Zylora Prime\", \"Kharon-9\", \"Vortexia\", \"Oceara\", \"Draknos\"],\n                ),\n                (\"list_item\", \"Zylora Prime\"),\n                (\"list_item\", \"Kharon-9\"),\n                (\"list_item\", \"Vortexia\"),\n                (\"list_item\", \"Oceara\"),\n                (\"list_item\", \"Draknos\"),\n            ],\n            test_mock={\n                \"llm_call\": lambda input_data, credentials: {\n                    \"response\": \"['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']\"\n                },\n            },\n        )",
              "successors": []
            }
          ],
          "functions": [],
          "classes": []
        },
        {
          "name": "run",
          "type": "CFG",
          "blocks": [
            {
              "id": 1,
              "start_line": 1,
              "end_line": 6,
              "label": "def run(\n        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs\n    ) -> BlockOutput:\n        logger.debug(f\"Starting AIListGeneratorBlock.run with input data: {input_data}\")\n        # Check for API key\n        api_key_check = credentials.api_key.get_secret_value()",
              "successors": [
                {
                  "id": 2,
                  "start_line": 7,
                  "end_line": 8,
                  "label": "if not api_key_check:\n            raise ValueError(\"No LLM API key provided.\")",
                  "successors": []
                }
              ]
            },
            {
              "id": 3,
              "start_line": 9,
              "end_line": 36,
              "label": "# Prepare the system prompt\n        sys_prompt = \"\"\"You are a Python list generator. Your task is to generate a Python list based on the user's prompt. \n            |Respond ONLY with a valid python list. \n            |The list can contain strings, numbers, or nested lists as appropriate. \n            |Do not include any explanations or additional text.\n            |Valid Example string formats:\n            |Example 1:\n            |```\n            |['1', '2', '3', '4']\n            |```\n            |Example 2:\n            |```\n            |[['1', '2'], ['3', '4'], ['5', '6']]\n            |```\n            |Example 3:\n            |```\n            |['1', ['2', '3'], ['4', ['5', '6']]]\n            |```\n            |Example 4:\n            |```\n            |['a', 'b', 'c']\n            |```\n            |Example 5:\n            |```\n            |['1', '2.5', 'string', 'True', ['False', 'None']]\n            |```\n            |Do not include any explanations or additional text, just respond with the list in the format specified above.\n            \"\"\"",
              "successors": []
            },
            {
              "id": 4,
              "start_line": 37,
              "end_line": 38,
              "label": "# If a focus is provided, add it to the prompt\n        if input_data.focus:",
              "successors": [
                {
                  "id": 5,
                  "start_line": 39,
                  "end_line": 39,
                  "label": "prompt = f\"Generate a list with the following focus:\\n<focus>\\n\\n{input_data.focus}</focus>\"",
                  "successors": []
                }
              ]
            },
            {
              "id": 6,
              "start_line": 40,
              "end_line": 42,
              "label": "else:\n            # If there's source data\n            if input_data.source_data:",
              "successors": [
                {
                  "id": 7,
                  "start_line": 43,
                  "end_line": 43,
                  "label": "prompt = \"Extract the main focus of the source data to a list.\\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.\"",
                  "successors": []
                }
              ]
            },
            {
              "id": 8,
              "start_line": 44,
              "end_line": 46,
              "label": "else:\n                # No focus or source data provided, generat a random list\n                prompt = \"Generate a random list.\"",
              "successors": []
            },
            {
              "id": 9,
              "start_line": 47,
              "end_line": 48,
              "label": "# If the source data is provided, add it to the prompt\n        if input_data.source_data:",
              "successors": [
                {
                  "id": 10,
                  "start_line": 49,
                  "end_line": 49,
                  "label": "prompt += f\"\\n\\nUse the following source data to generate the list from:\\n\\n<source_data>\\n\\n{input_data.source_data}</source_data>\\n\\nDo not invent fictional data that is not present in the source data.\"",
                  "successors": []
                }
              ]
            },
            {
              "id": 11,
              "start_line": 50,
              "end_line": 52,
              "label": "# Else, tell the LLM to synthesize the data\n        else:\n            prompt += \"\\n\\nInvent the data to generate the list from.\"",
              "successors": []
            },
            {
              "id": 12,
              "start_line": 53,
              "end_line": 54,
              "label": "for attempt in range(input_data.max_retries):\n            try:",
              "successors": [
                {
                  "id": 13,
                  "start_line": 55,
                  "end_line": 81,
                  "label": "logger.debug(\"Calling LLM\")\n                llm_response = self.llm_call(\n                    AIStructuredResponseGeneratorBlock.Input(\n                        sys_prompt=sys_prompt,\n                        prompt=prompt,\n                        credentials=input_data.credentials,\n                        model=input_data.model,\n                        expected_format={},  # Do not use structured response\n                        ollama_host=input_data.ollama_host,\n                    ),\n                    credentials=credentials,\n                )\n                logger.debug(f\"LLM response: {llm_response}\")\n                # Extract Response string\n                response_string = llm_response[\"response\"]\n                logger.debug(f\"Response string: {response_string}\")\n                # Convert the string to a Python list\n                logger.debug(\"Converting string to Python list\")\n                parsed_list = self.string_to_list(response_string)\n                logger.debug(f\"Parsed list: {parsed_list}\")\n                # If we reach here, we have a valid Python list\n                logger.debug(\"Successfully generated a valid Python list\")\n                yield \"generated_list\", parsed_list\n                # Yield each item in the list\n                for item in parsed_list:\n                    yield \"list_item\", item\n                return",
                  "successors": []
                },
                {
                  "id": 14,
                  "start_line": 82,
                  "end_line": 104,
                  "label": "except Exception as e:\n        logger.debug(\"AIListGeneratorBlock.run completed\")",
                  "successors": [
                    {
                      "id": 15,
                      "start_line": 83,
                      "end_line": 84,
                      "label": "logger.error(f\"Error in attempt {attempt + 1}: {str(e)}\")\n                if attempt == input_data.max_retries - 1:",
                      "successors": [
                        {
                          "id": 16,
                          "start_line": 85,
                          "end_line": 90,
                          "label": "logger.error(\n                        f\"Failed to generate a valid Python list after {input_data.max_retries} attempts\"\n                    )\n                    raise RuntimeError(\n                        f\"Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}\"\n                    )",
                          "successors": []
                        }
                      ]
                    },
                    {
                      "id": 17,
                      "start_line": 91,
                      "end_line": 103,
                      "label": "else:\n                    # Add a retry prompt\n                    logger.debug(\"Preparing retry prompt\")\n                    prompt = f\"\"\"\n                    The previous attempt failed due to `{e}`\n                    Generate a valid Python list based on the original prompt.\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\n                    Original prompt: \n                    ```{prompt}```\n                    \n                    Respond only with the list in the format specified with no commentary or apologies.\n                    \"\"\"\n                    logger.debug(f\"Retry prompt: {prompt}\")",
                      "successors": []
                    }
                  ]
                }
              ]
            }
          ],
          "functions": [],
          "classes": []
        }
      ],
      "classes": [
        {
          "name": "Input",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        },
        {
          "name": "Output",
          "type": "CFG",
          "blocks": [],
          "functions": [],
          "classes": []
        }
      ]
    }
  ]
}