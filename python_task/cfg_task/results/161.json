{
  "edge_analysis": {
    "static_total": "88",
    "llm_total": "28",
    "matched_edges": {
      "exact_matches": "19",
      "partial_matches": "0"
    },
    "accuracy_metrics": {
      "precision": "0.6785714285714286",
      "recall": "0.2159090909090909",
      "f1_score": "0.3296703296703297"
    }
  },
  "structure_validation": {
    "missing_blocks": [
      "#17\\nif input_data.accumulate",
      "#18\\nif isinstance(input_data.data, dict)",
      "#20\\ndata_to_sample = input_data.data, if isinstance(input_data.data, list) else [input_data.data]",
      "#21\\nself.accumulated_data.append(input_data.data)",
      "#22\\nif len(self.accumulated_data) < input_data.sample_size",
      "#29\\ndata_to_sample = self.accumulated_data",
      "#31\\nrandom.seed(input_data.random_seed)",
      "#32\\ndata_size = len(data_to_sample), if input_data.sample_size > data_size",
      "#33\\nraise ValueError, f'Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size})",
      "#34\\nindices = [], if input_data.sampling_method == SamplingMethod.RANDOM",
      "#36\\nindices = random.sample(range(data_size), input_data.sample_size)",
      "#37\\nsampled_data = [data_to_sample[i] for i in indices], if input_data.accumulate",
      "#38\\nif input_data.sampling_method == SamplingMethod.SYSTEMATIC",
      "#39\\nstep = data_size // input_data.sample_size, start = random.randint(0, step - 1), indices = list(range(start, data_size, step))[:input_data.sample_size]",
      "#41\\nif input_data.sampling_method == SamplingMethod.TOP",
      "#42\\nindices = list(range(input_data.sample_size))",
      "#44\\nif input_data.sampling_method == SamplingMethod.BOTTOM",
      "#45\\nindices = list(range(data_size - input_data.sample_size, data_size))",
      "#47\\nif input_data.sampling_method == SamplingMethod.STRATIFIED",
      "#48\\nif not input_data.stratify_key",
      "#50\\nif input_data.sampling_method == SamplingMethod.WEIGHTED",
      "#51\\nif not input_data.weight_key",
      "#53\\nif input_data.sampling_method == SamplingMethod.RESERVOIR",
      "#54\\nindices = list(range(input_data.sample_size)",
      "#56\\nif input_data.sampling_method == SamplingMethod.CLUSTER",
      "#57\\nif not input_data.cluster_key",
      "#59\\nraise ValueError, 'Unknown sampling method: {input_data.sampling_method}'",
      "#61\\nraise ValueError, 'Cluster key must be provided for cluster sampling'",
      "#62\\nclusters = defaultdict(list)",
      "#64\\nfor i, item in enumerate(data_to_sample)",
      "#65\\nif isinstance(item, dict)",
      "#66\\nselected_clusters = []",
      "#67\\ncluster_value = item.get(input_data.cluster_key)",
      "#69\\nif hasattr(item, input_data.cluster_key)"
    ],
    "extra_blocks": [
      "import random",
      "from collections import defaultdict",
      "from enum import Enum",
      "from typing import Any, Dict, List, Optional, Union",
      "from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema",
      "from backend.data.model import SchemaField",
      "def run(self, input_data: Input, **kwargs) -> BlockOutput",
      "# If not accumulating, use the input data directly, data_to_sample = (input_data.data if isinstance(input_data.data, list), else [input_data.data]",
      "# Calculate the number of samples to take from each stratum, stratum_sizes = {k: max(1, int(len(v) / data_size * input_data.sample_size)) for k, v in strata.items()}, while sum(stratum_sizes.values()) != input_data.sample_size:  if sum(stratum_sizes.values()) < input_data.sample_size:",
      "# Adjust sizes to ensure we get exactly sample_size samples, stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] += 1, else: stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] -= 1, for stratum, size in stratum_sizes.items(): indices.extend(random.sample(strata[stratum], size))"
    ]
  }
}