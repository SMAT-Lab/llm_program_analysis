{
  "nodes": [
    {
      "id": "n0",
      "type": "block",
      "statements": [
        "import ast",
        "import logging",
        "from enum import Enum, EnumMeta",
        "from json import JSONDecodeError",
        "from types import MappingProxyType",
        "from typing import TYPE_CHECKING, Any, List, Literal, NamedTuple",
        "from pydantic import SecretStr",
        "from backend.integrations.providers import ProviderName",
        "TYPE_CHECKING"
      ],
      "code": "import ast\nimport logging\nfrom enum import Enum, EnumMeta\nfrom json import JSONDecodeError\nfrom types import MappingProxyType\nfrom typing import TYPE_CHECKING, Any, List, Literal, NamedTuple\nfrom pydantic import SecretStr\nfrom backend.integrations.providers import ProviderName\nTYPE_CHECKING"
    },
    {
      "id": "n1",
      "type": "block",
      "statements": [
        "from enum import _EnumMemberT"
      ],
      "code": "from enum import _EnumMemberT"
    },
    {
      "id": "n2",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n3",
      "type": "block",
      "statements": [
        "import anthropic",
        "import ollama",
        "import openai",
        "from groq import Groq",
        "from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema",
        "from backend.data.model import APIKeyCredentials, CredentialsField, CredentialsMetaInput, SchemaField",
        "from backend.util import json",
        "from backend.util.settings import BehaveAs, Settings",
        "logger = logging.getLogger(__name__)",
        "LLMProviderName = Literal[ProviderName.ANTHROPIC, ProviderName.GROQ, ProviderName.OLLAMA, ProviderName.OPENAI, ProviderName.OPEN_ROUTER]",
        "AICredentials = CredentialsMetaInput[LLMProviderName, Literal['api_key']]",
        "TEST_CREDENTIALS = APIKeyCredentials(id='ed55ac19-356e-4243-a6cb-bc599e9b716f', provider='openai', api_key=SecretStr('mock-openai-api-key'), title='Mock OpenAI API key', expires_at=None)",
        "TEST_CREDENTIALS_INPUT = {'provider': TEST_CREDENTIALS.provider, 'id': TEST_CREDENTIALS.id, 'type': TEST_CREDENTIALS.type, 'title': TEST_CREDENTIALS.title}",
        "def AICredentialsField() -> AICredentials:\n    return CredentialsField(description='API key for the LLM provider.', discriminator='model', discriminator_mapping={model.value: model.metadata.provider for model in LlmModel})",
        "return CredentialsField(description='API key for the LLM provider.', discriminator='model', discriminator_mapping={model.value: model.metadata.provider for model in LlmModel})"
      ],
      "code": "import anthropic\nimport ollama\nimport openai\nfrom groq import Groq\nfrom backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema\nfrom backend.data.model import APIKeyCredentials, CredentialsField, CredentialsMetaInput, SchemaField\nfrom backend.util import json\nfrom backend.util.settings import BehaveAs, Settings\nlogger = logging.getLogger(__name__)\nLLMProviderName = Literal[ProviderName.ANTHROPIC, ProviderName.GROQ, ProviderName.OLLAMA, ProviderName.OPENAI, ProviderName.OPEN_ROUTER]\nAICredentials = CredentialsMetaInput[LLMProviderName, Literal['api_key']]\nTEST_CREDENTIALS = APIKeyCredentials(id='ed55ac19-356e-4243-a6cb-bc599e9b716f', provider='openai', api_key=SecretStr('mock-openai-api-key'), title='Mock OpenAI API key', expires_at=None)\nTEST_CREDENTIALS_INPUT = {'provider': TEST_CREDENTIALS.provider, 'id': TEST_CREDENTIALS.id, 'type': TEST_CREDENTIALS.type, 'title': TEST_CREDENTIALS.title}\ndef AICredentialsField() -> AICredentials:\n    return CredentialsField(description='API key for the LLM provider.', discriminator='model', discriminator_mapping={model.value: model.metadata.provider for model in LlmModel})\nreturn CredentialsField(description='API key for the LLM provider.', discriminator='model', discriminator_mapping={model.value: model.metadata.provider for model in LlmModel})"
    },
    {
      "id": "n4",
      "type": "block",
      "statements": [
        "class ModelMetadata(NamedTuple):\n    provider: str\n    context_window: int",
        "provider: str",
        "context_window: int",
        "class LlmModelMeta(EnumMeta):\n\n    @property\n    def __members__(self: type['_EnumMemberT']) -> MappingProxyType[str, '_EnumMemberT']:\n        if Settings().config.behave_as == BehaveAs.LOCAL:\n            members = super().__members__\n            return members\n        else:\n            removed_providers = ['ollama']\n            existing_members = super().__members__\n            members = {name: member for (name, member) in existing_members.items() if LlmModel[name].provider not in removed_providers}\n            return MappingProxyType(members)",
        "@property\ndef __members__(self: type['_EnumMemberT']) -> MappingProxyType[str, '_EnumMemberT']:\n    if Settings().config.behave_as == BehaveAs.LOCAL:\n        members = super().__members__\n        return members\n    else:\n        removed_providers = ['ollama']\n        existing_members = super().__members__\n        members = {name: member for (name, member) in existing_members.items() if LlmModel[name].provider not in removed_providers}\n        return MappingProxyType(members)",
        "Settings().config.behave_as Eq BehaveAs.LOCAL"
      ],
      "code": "class ModelMetadata(NamedTuple):\n    provider: str\n    context_window: int\nprovider: str\ncontext_window: int\nclass LlmModelMeta(EnumMeta):\n\n    @property\n    def __members__(self: type['_EnumMemberT']) -> MappingProxyType[str, '_EnumMemberT']:\n        if Settings().config.behave_as == BehaveAs.LOCAL:\n            members = super().__members__\n            return members\n        else:\n            removed_providers = ['ollama']\n            existing_members = super().__members__\n            members = {name: member for (name, member) in existing_members.items() if LlmModel[name].provider not in removed_providers}\n            return MappingProxyType(members)\n@property\ndef __members__(self: type['_EnumMemberT']) -> MappingProxyType[str, '_EnumMemberT']:\n    if Settings().config.behave_as == BehaveAs.LOCAL:\n        members = super().__members__\n        return members\n    else:\n        removed_providers = ['ollama']\n        existing_members = super().__members__\n        members = {name: member for (name, member) in existing_members.items() if LlmModel[name].provider not in removed_providers}\n        return MappingProxyType(members)\nSettings().config.behave_as Eq BehaveAs.LOCAL"
    },
    {
      "id": "n5",
      "type": "block",
      "statements": [
        "members = super().__members__",
        "return members"
      ],
      "code": "members = super().__members__\nreturn members"
    },
    {
      "id": "n6",
      "type": "block",
      "statements": [
        "removed_providers = ['ollama']",
        "existing_members = super().__members__",
        "members = {name: member for (name, member) in existing_members.items() if LlmModel[name].provider not in removed_providers}",
        "return MappingProxyType(members)"
      ],
      "code": "removed_providers = ['ollama']\nexisting_members = super().__members__\nmembers = {name: member for (name, member) in existing_members.items() if LlmModel[name].provider not in removed_providers}\nreturn MappingProxyType(members)"
    },
    {
      "id": "n7",
      "type": "block",
      "statements": [
        "class LlmModel(str, Enum, metaclass=LlmModelMeta):\n    O1_PREVIEW = 'o1-preview'\n    O1_MINI = 'o1-mini'\n    GPT4O_MINI = 'gpt-4o-mini'\n    GPT4O = 'gpt-4o'\n    GPT4_TURBO = 'gpt-4-turbo'\n    GPT3_5_TURBO = 'gpt-3.5-turbo'\n    CLAUDE_3_5_SONNET = 'claude-3-5-sonnet-latest'\n    CLAUDE_3_HAIKU = 'claude-3-haiku-20240307'\n    LLAMA3_8B = 'llama3-8b-8192'\n    LLAMA3_70B = 'llama3-70b-8192'\n    MIXTRAL_8X7B = 'mixtral-8x7b-32768'\n    GEMMA_7B = 'gemma-7b-it'\n    GEMMA2_9B = 'gemma2-9b-it'\n    LLAMA3_1_405B = 'llama-3.1-405b-reasoning'\n    LLAMA3_1_70B = 'llama-3.1-70b-versatile'\n    LLAMA3_1_8B = 'llama-3.1-8b-instant'\n    OLLAMA_LLAMA3_8B = 'llama3'\n    OLLAMA_LLAMA3_405B = 'llama3.1:405b'\n    OLLAMA_DOLPHIN = 'dolphin-mistral:latest'\n    GEMINI_FLASH_1_5_8B = 'google/gemini-flash-1.5'\n    GROK_BETA = 'x-ai/grok-beta'\n    MISTRAL_NEMO = 'mistralai/mistral-nemo'\n    COHERE_COMMAND_R_08_2024 = 'cohere/command-r-08-2024'\n    COHERE_COMMAND_R_PLUS_08_2024 = 'cohere/command-r-plus-08-2024'\n    EVA_QWEN_2_5_32B = 'eva-unit-01/eva-qwen-2.5-32b'\n    DEEPSEEK_CHAT = 'deepseek/deepseek-chat'\n    PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE = 'perplexity/llama-3.1-sonar-large-128k-online'\n    QWEN_QWQ_32B_PREVIEW = 'qwen/qwq-32b-preview'\n    NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B = 'nousresearch/hermes-3-llama-3.1-405b'\n    NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B = 'nousresearch/hermes-3-llama-3.1-70b'\n    AMAZON_NOVA_LITE_V1 = 'amazon/nova-lite-v1'\n    AMAZON_NOVA_MICRO_V1 = 'amazon/nova-micro-v1'\n    AMAZON_NOVA_PRO_V1 = 'amazon/nova-pro-v1'\n    MICROSOFT_WIZARDLM_2_8X22B = 'microsoft/wizardlm-2-8x22b'\n    GRYPHE_MYTHOMAX_L2_13B = 'gryphe/mythomax-l2-13b'\n\n    @property\n    def metadata(self) -> ModelMetadata:\n        return MODEL_METADATA[self]\n\n    @property\n    def provider(self) -> str:\n        return self.metadata.provider\n\n    @property\n    def context_window(self) -> int:\n        return self.metadata.context_window",
        "O1_PREVIEW = 'o1-preview'",
        "O1_MINI = 'o1-mini'",
        "GPT4O_MINI = 'gpt-4o-mini'",
        "GPT4O = 'gpt-4o'",
        "GPT4_TURBO = 'gpt-4-turbo'",
        "GPT3_5_TURBO = 'gpt-3.5-turbo'",
        "CLAUDE_3_5_SONNET = 'claude-3-5-sonnet-latest'",
        "CLAUDE_3_HAIKU = 'claude-3-haiku-20240307'",
        "LLAMA3_8B = 'llama3-8b-8192'",
        "LLAMA3_70B = 'llama3-70b-8192'",
        "MIXTRAL_8X7B = 'mixtral-8x7b-32768'",
        "GEMMA_7B = 'gemma-7b-it'",
        "GEMMA2_9B = 'gemma2-9b-it'",
        "LLAMA3_1_405B = 'llama-3.1-405b-reasoning'",
        "LLAMA3_1_70B = 'llama-3.1-70b-versatile'",
        "LLAMA3_1_8B = 'llama-3.1-8b-instant'",
        "OLLAMA_LLAMA3_8B = 'llama3'",
        "OLLAMA_LLAMA3_405B = 'llama3.1:405b'",
        "OLLAMA_DOLPHIN = 'dolphin-mistral:latest'",
        "GEMINI_FLASH_1_5_8B = 'google/gemini-flash-1.5'",
        "GROK_BETA = 'x-ai/grok-beta'",
        "MISTRAL_NEMO = 'mistralai/mistral-nemo'",
        "COHERE_COMMAND_R_08_2024 = 'cohere/command-r-08-2024'",
        "COHERE_COMMAND_R_PLUS_08_2024 = 'cohere/command-r-plus-08-2024'",
        "EVA_QWEN_2_5_32B = 'eva-unit-01/eva-qwen-2.5-32b'",
        "DEEPSEEK_CHAT = 'deepseek/deepseek-chat'",
        "PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE = 'perplexity/llama-3.1-sonar-large-128k-online'",
        "QWEN_QWQ_32B_PREVIEW = 'qwen/qwq-32b-preview'",
        "NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B = 'nousresearch/hermes-3-llama-3.1-405b'",
        "NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B = 'nousresearch/hermes-3-llama-3.1-70b'",
        "AMAZON_NOVA_LITE_V1 = 'amazon/nova-lite-v1'",
        "AMAZON_NOVA_MICRO_V1 = 'amazon/nova-micro-v1'",
        "AMAZON_NOVA_PRO_V1 = 'amazon/nova-pro-v1'",
        "MICROSOFT_WIZARDLM_2_8X22B = 'microsoft/wizardlm-2-8x22b'",
        "GRYPHE_MYTHOMAX_L2_13B = 'gryphe/mythomax-l2-13b'",
        "@property\ndef metadata(self) -> ModelMetadata:\n    return MODEL_METADATA[self]",
        "return MODEL_METADATA[self]"
      ],
      "code": "class LlmModel(str, Enum, metaclass=LlmModelMeta):\n    O1_PREVIEW = 'o1-preview'\n    O1_MINI = 'o1-mini'\n    GPT4O_MINI = 'gpt-4o-mini'\n    GPT4O = 'gpt-4o'\n    GPT4_TURBO = 'gpt-4-turbo'\n    GPT3_5_TURBO = 'gpt-3.5-turbo'\n    CLAUDE_3_5_SONNET = 'claude-3-5-sonnet-latest'\n    CLAUDE_3_HAIKU = 'claude-3-haiku-20240307'\n    LLAMA3_8B = 'llama3-8b-8192'\n    LLAMA3_70B = 'llama3-70b-8192'\n    MIXTRAL_8X7B = 'mixtral-8x7b-32768'\n    GEMMA_7B = 'gemma-7b-it'\n    GEMMA2_9B = 'gemma2-9b-it'\n    LLAMA3_1_405B = 'llama-3.1-405b-reasoning'\n    LLAMA3_1_70B = 'llama-3.1-70b-versatile'\n    LLAMA3_1_8B = 'llama-3.1-8b-instant'\n    OLLAMA_LLAMA3_8B = 'llama3'\n    OLLAMA_LLAMA3_405B = 'llama3.1:405b'\n    OLLAMA_DOLPHIN = 'dolphin-mistral:latest'\n    GEMINI_FLASH_1_5_8B = 'google/gemini-flash-1.5'\n    GROK_BETA = 'x-ai/grok-beta'\n    MISTRAL_NEMO = 'mistralai/mistral-nemo'\n    COHERE_COMMAND_R_08_2024 = 'cohere/command-r-08-2024'\n    COHERE_COMMAND_R_PLUS_08_2024 = 'cohere/command-r-plus-08-2024'\n    EVA_QWEN_2_5_32B = 'eva-unit-01/eva-qwen-2.5-32b'\n    DEEPSEEK_CHAT = 'deepseek/deepseek-chat'\n    PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE = 'perplexity/llama-3.1-sonar-large-128k-online'\n    QWEN_QWQ_32B_PREVIEW = 'qwen/qwq-32b-preview'\n    NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B = 'nousresearch/hermes-3-llama-3.1-405b'\n    NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B = 'nousresearch/hermes-3-llama-3.1-70b'\n    AMAZON_NOVA_LITE_V1 = 'amazon/nova-lite-v1'\n    AMAZON_NOVA_MICRO_V1 = 'amazon/nova-micro-v1'\n    AMAZON_NOVA_PRO_V1 = 'amazon/nova-pro-v1'\n    MICROSOFT_WIZARDLM_2_8X22B = 'microsoft/wizardlm-2-8x22b'\n    GRYPHE_MYTHOMAX_L2_13B = 'gryphe/mythomax-l2-13b'\n\n    @property\n    def metadata(self) -> ModelMetadata:\n        return MODEL_METADATA[self]\n\n    @property\n    def provider(self) -> str:\n        return self.metadata.provider\n\n    @property\n    def context_window(self) -> int:\n        return self.metadata.context_window\nO1_PREVIEW = 'o1-preview'\nO1_MINI = 'o1-mini'\nGPT4O_MINI = 'gpt-4o-mini'\nGPT4O = 'gpt-4o'\nGPT4_TURBO = 'gpt-4-turbo'\nGPT3_5_TURBO = 'gpt-3.5-turbo'\nCLAUDE_3_5_SONNET = 'claude-3-5-sonnet-latest'\nCLAUDE_3_HAIKU = 'claude-3-haiku-20240307'\nLLAMA3_8B = 'llama3-8b-8192'\nLLAMA3_70B = 'llama3-70b-8192'\nMIXTRAL_8X7B = 'mixtral-8x7b-32768'\nGEMMA_7B = 'gemma-7b-it'\nGEMMA2_9B = 'gemma2-9b-it'\nLLAMA3_1_405B = 'llama-3.1-405b-reasoning'\nLLAMA3_1_70B = 'llama-3.1-70b-versatile'\nLLAMA3_1_8B = 'llama-3.1-8b-instant'\nOLLAMA_LLAMA3_8B = 'llama3'\nOLLAMA_LLAMA3_405B = 'llama3.1:405b'\nOLLAMA_DOLPHIN = 'dolphin-mistral:latest'\nGEMINI_FLASH_1_5_8B = 'google/gemini-flash-1.5'\nGROK_BETA = 'x-ai/grok-beta'\nMISTRAL_NEMO = 'mistralai/mistral-nemo'\nCOHERE_COMMAND_R_08_2024 = 'cohere/command-r-08-2024'\nCOHERE_COMMAND_R_PLUS_08_2024 = 'cohere/command-r-plus-08-2024'\nEVA_QWEN_2_5_32B = 'eva-unit-01/eva-qwen-2.5-32b'\nDEEPSEEK_CHAT = 'deepseek/deepseek-chat'\nPERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE = 'perplexity/llama-3.1-sonar-large-128k-online'\nQWEN_QWQ_32B_PREVIEW = 'qwen/qwq-32b-preview'\nNOUSRESEARCH_HERMES_3_LLAMA_3_1_405B = 'nousresearch/hermes-3-llama-3.1-405b'\nNOUSRESEARCH_HERMES_3_LLAMA_3_1_70B = 'nousresearch/hermes-3-llama-3.1-70b'\nAMAZON_NOVA_LITE_V1 = 'amazon/nova-lite-v1'\nAMAZON_NOVA_MICRO_V1 = 'amazon/nova-micro-v1'\nAMAZON_NOVA_PRO_V1 = 'amazon/nova-pro-v1'\nMICROSOFT_WIZARDLM_2_8X22B = 'microsoft/wizardlm-2-8x22b'\nGRYPHE_MYTHOMAX_L2_13B = 'gryphe/mythomax-l2-13b'\n@property\ndef metadata(self) -> ModelMetadata:\n    return MODEL_METADATA[self]\nreturn MODEL_METADATA[self]"
    },
    {
      "id": "n8",
      "type": "block",
      "statements": [
        "@property\ndef provider(self) -> str:\n    return self.metadata.provider",
        "return self.metadata.provider"
      ],
      "code": "@property\ndef provider(self) -> str:\n    return self.metadata.provider\nreturn self.metadata.provider"
    },
    {
      "id": "n9",
      "type": "block",
      "statements": [
        "@property\ndef context_window(self) -> int:\n    return self.metadata.context_window",
        "return self.metadata.context_window"
      ],
      "code": "@property\ndef context_window(self) -> int:\n    return self.metadata.context_window\nreturn self.metadata.context_window"
    },
    {
      "id": "n10",
      "type": "block",
      "statements": [
        "MODEL_METADATA = {LlmModel.O1_PREVIEW: ModelMetadata('openai', 32000), LlmModel.O1_MINI: ModelMetadata('openai', 62000), LlmModel.GPT4O_MINI: ModelMetadata('openai', 128000), LlmModel.GPT4O: ModelMetadata('openai', 128000), LlmModel.GPT4_TURBO: ModelMetadata('openai', 128000), LlmModel.GPT3_5_TURBO: ModelMetadata('openai', 16385), LlmModel.CLAUDE_3_5_SONNET: ModelMetadata('anthropic', 200000), LlmModel.CLAUDE_3_HAIKU: ModelMetadata('anthropic', 200000), LlmModel.LLAMA3_8B: ModelMetadata('groq', 8192), LlmModel.LLAMA3_70B: ModelMetadata('groq', 8192), LlmModel.MIXTRAL_8X7B: ModelMetadata('groq', 32768), LlmModel.GEMMA_7B: ModelMetadata('groq', 8192), LlmModel.GEMMA2_9B: ModelMetadata('groq', 8192), LlmModel.LLAMA3_1_405B: ModelMetadata('groq', 8192), LlmModel.LLAMA3_1_70B: ModelMetadata('groq', 131072), LlmModel.LLAMA3_1_8B: ModelMetadata('groq', 131072), LlmModel.OLLAMA_LLAMA3_8B: ModelMetadata('ollama', 8192), LlmModel.OLLAMA_LLAMA3_405B: ModelMetadata('ollama', 8192), LlmModel.OLLAMA_DOLPHIN: ModelMetadata('ollama', 32768), LlmModel.GEMINI_FLASH_1_5_8B: ModelMetadata('open_router', 8192), LlmModel.GROK_BETA: ModelMetadata('open_router', 8192), LlmModel.MISTRAL_NEMO: ModelMetadata('open_router', 4000), LlmModel.COHERE_COMMAND_R_08_2024: ModelMetadata('open_router', 4000), LlmModel.COHERE_COMMAND_R_PLUS_08_2024: ModelMetadata('open_router', 4000), LlmModel.EVA_QWEN_2_5_32B: ModelMetadata('open_router', 4000), LlmModel.DEEPSEEK_CHAT: ModelMetadata('open_router', 8192), LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: ModelMetadata('open_router', 8192), LlmModel.QWEN_QWQ_32B_PREVIEW: ModelMetadata('open_router', 4000), LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: ModelMetadata('open_router', 4000), LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: ModelMetadata('open_router', 4000), LlmModel.AMAZON_NOVA_LITE_V1: ModelMetadata('open_router', 4000), LlmModel.AMAZON_NOVA_MICRO_V1: ModelMetadata('open_router', 4000), LlmModel.AMAZON_NOVA_PRO_V1: ModelMetadata('open_router', 4000), LlmModel.MICROSOFT_WIZARDLM_2_8X22B: ModelMetadata('open_router', 4000), LlmModel.GRYPHE_MYTHOMAX_L2_13B: ModelMetadata('open_router', 4000)}"
      ],
      "code": "MODEL_METADATA = {LlmModel.O1_PREVIEW: ModelMetadata('openai', 32000), LlmModel.O1_MINI: ModelMetadata('openai', 62000), LlmModel.GPT4O_MINI: ModelMetadata('openai', 128000), LlmModel.GPT4O: ModelMetadata('openai', 128000), LlmModel.GPT4_TURBO: ModelMetadata('openai', 128000), LlmModel.GPT3_5_TURBO: ModelMetadata('openai', 16385), LlmModel.CLAUDE_3_5_SONNET: ModelMetadata('anthropic', 200000), LlmModel.CLAUDE_3_HAIKU: ModelMetadata('anthropic', 200000), LlmModel.LLAMA3_8B: ModelMetadata('groq', 8192), LlmModel.LLAMA3_70B: ModelMetadata('groq', 8192), LlmModel.MIXTRAL_8X7B: ModelMetadata('groq', 32768), LlmModel.GEMMA_7B: ModelMetadata('groq', 8192), LlmModel.GEMMA2_9B: ModelMetadata('groq', 8192), LlmModel.LLAMA3_1_405B: ModelMetadata('groq', 8192), LlmModel.LLAMA3_1_70B: ModelMetadata('groq', 131072), LlmModel.LLAMA3_1_8B: ModelMetadata('groq', 131072), LlmModel.OLLAMA_LLAMA3_8B: ModelMetadata('ollama', 8192), LlmModel.OLLAMA_LLAMA3_405B: ModelMetadata('ollama', 8192), LlmModel.OLLAMA_DOLPHIN: ModelMetadata('ollama', 32768), LlmModel.GEMINI_FLASH_1_5_8B: ModelMetadata('open_router', 8192), LlmModel.GROK_BETA: ModelMetadata('open_router', 8192), LlmModel.MISTRAL_NEMO: ModelMetadata('open_router', 4000), LlmModel.COHERE_COMMAND_R_08_2024: ModelMetadata('open_router', 4000), LlmModel.COHERE_COMMAND_R_PLUS_08_2024: ModelMetadata('open_router', 4000), LlmModel.EVA_QWEN_2_5_32B: ModelMetadata('open_router', 4000), LlmModel.DEEPSEEK_CHAT: ModelMetadata('open_router', 8192), LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: ModelMetadata('open_router', 8192), LlmModel.QWEN_QWQ_32B_PREVIEW: ModelMetadata('open_router', 4000), LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: ModelMetadata('open_router', 4000), LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: ModelMetadata('open_router', 4000), LlmModel.AMAZON_NOVA_LITE_V1: ModelMetadata('open_router', 4000), LlmModel.AMAZON_NOVA_MICRO_V1: ModelMetadata('open_router', 4000), LlmModel.AMAZON_NOVA_PRO_V1: ModelMetadata('open_router', 4000), LlmModel.MICROSOFT_WIZARDLM_2_8X22B: ModelMetadata('open_router', 4000), LlmModel.GRYPHE_MYTHOMAX_L2_13B: ModelMetadata('open_router', 4000)}"
    },
    {
      "id": "n11",
      "type": "block",
      "statements": [
        "model",
        "LlmModel"
      ],
      "code": "model\nLlmModel"
    },
    {
      "id": "n12",
      "type": "block",
      "statements": [
        "model NotIn MODEL_METADATA"
      ],
      "code": "model NotIn MODEL_METADATA"
    },
    {
      "id": "n13",
      "type": "block",
      "statements": [
        "class MessageRole(str, Enum):\n    SYSTEM = 'system'\n    USER = 'user'\n    ASSISTANT = 'assistant'",
        "SYSTEM = 'system'",
        "USER = 'user'",
        "ASSISTANT = 'assistant'",
        "class Message(BlockSchema):\n    role: MessageRole\n    content: str",
        "role: MessageRole",
        "content: str",
        "class AIStructuredResponseGeneratorBlock(Block):\n\n    class Input(BlockSchema):\n        prompt: str = SchemaField(description='The prompt to send to the language model.', placeholder='Enter your prompt here...')\n        expected_format: dict[str, str] = SchemaField(description='Expected format of the response. If provided, the response will be validated against this format. The keys should be the expected fields in the response, and the values should be the description of the field.')\n        model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)\n        credentials: AICredentials = AICredentialsField()\n        sys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')\n        conversation_history: list[Message] = SchemaField(default=[], description='The conversation history to provide context for the prompt.')\n        retry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')\n        prompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')\n        max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n        ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n\n    class Output(BlockSchema):\n        response: dict[str, Any] = SchemaField(description='The response object generated by the language model.')\n        error: str = SchemaField(description='Error message if the API call failed.')\n\n    def __init__(self):\n        super().__init__(id='ed55ac19-356e-4243-a6cb-bc599e9b716f', description='Call a Large Language Model (LLM) to generate formatted object based on the given prompt.', categories={BlockCategory.AI}, input_schema=AIStructuredResponseGeneratorBlock.Input, output_schema=AIStructuredResponseGeneratorBlock.Output, test_input={'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT, 'expected_format': {'key1': 'value1', 'key2': 'value2'}, 'prompt': 'User prompt'}, test_credentials=TEST_CREDENTIALS, test_output=('response', {'key1': 'key1Value', 'key2': 'key2Value'}), test_mock={'llm_call': lambda *args, **kwargs: (json.dumps({'key1': 'key1Value', 'key2': 'key2Value'}), 0, 0)})\n\n    @staticmethod\n    def llm_call(credentials: APIKeyCredentials, llm_model: LlmModel, prompt: list[dict], json_format: bool, max_tokens: int | None=None, ollama_host: str='localhost:11434') -> tuple[str, int, int]:\n        \"\"\"\n        Args:\n            api_key: API key for the LLM provider.\n            llm_model: The LLM model to use.\n            prompt: The prompt to send to the LLM.\n            json_format: Whether the response should be in JSON format.\n            max_tokens: The maximum number of tokens to generate in the chat completion.\n            ollama_host: The host for ollama to use\n\n        Returns:\n            The response from the LLM.\n            The number of tokens used in the prompt.\n            The number of tokens used in the completion.\n        \"\"\"\n        provider = llm_model.metadata.provider\n        if provider == 'openai':\n            oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())\n            response_format = None\n            if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:\n                sys_messages = [p['content'] for p in prompt if p['role'] == 'system']\n                usr_messages = [p['content'] for p in prompt if p['role'] != 'system']\n                prompt = [{'role': 'user', 'content': '\\n'.join(sys_messages)}, {'role': 'user', 'content': '\\n'.join(usr_messages)}]\n            elif json_format:\n                response_format = {'type': 'json_object'}\n            response = oai_client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_completion_tokens=max_tokens)\n            return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n        elif provider == 'anthropic':\n            system_messages = [p['content'] for p in prompt if p['role'] == 'system']\n            sysprompt = ' '.join(system_messages)\n            messages = []\n            last_role = None\n            for p in prompt:\n                if p['role'] in ['user', 'assistant']:\n                    if p['role'] != last_role:\n                        messages.append({'role': p['role'], 'content': p['content']})\n                        last_role = p['role']\n                    else:\n                        messages[-1]['content'] += '\\n' + p['content']\n            client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())\n            try:\n                resp = client.messages.create(model=llm_model.value, system=sysprompt, messages=messages, max_tokens=max_tokens or 8192)\n                if not resp.content:\n                    raise ValueError('No content returned from Anthropic.')\n                return (resp.content[0].name if isinstance(resp.content[0], anthropic.types.ToolUseBlock) else resp.content[0].text, resp.usage.input_tokens, resp.usage.output_tokens)\n            except anthropic.APIError as e:\n                error_message = f'Anthropic API error: {str(e)}'\n                logger.error(error_message)\n                raise ValueError(error_message)\n        elif provider == 'groq':\n            client = Groq(api_key=credentials.api_key.get_secret_value())\n            response_format = {'type': 'json_object'} if json_format else None\n            response = client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_tokens=max_tokens)\n            return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n        elif provider == 'ollama':\n            client = ollama.Client(host=ollama_host)\n            sys_messages = [p['content'] for p in prompt if p['role'] == 'system']\n            usr_messages = [p['content'] for p in prompt if p['role'] != 'system']\n            response = client.generate(model=llm_model.value, prompt=f'{sys_messages}\\n\\n{usr_messages}', stream=False)\n            return (response.get('response') or '', response.get('prompt_eval_count') or 0, response.get('eval_count') or 0)\n        elif provider == 'open_router':\n            client = openai.OpenAI(base_url='https://openrouter.ai/api/v1', api_key=credentials.api_key.get_secret_value())\n            response = client.chat.completions.create(extra_headers={'HTTP-Referer': 'https://agpt.co', 'X-Title': 'AutoGPT'}, model=llm_model.value, messages=prompt, max_tokens=max_tokens)\n            if not response.choices:\n                if response:\n                    raise ValueError(f'OpenRouter error: {response}')\n                else:\n                    raise ValueError('No response from OpenRouter.')\n            return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n        else:\n            raise ValueError(f'Unsupported LLM provider: {provider}')\n\n    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n        logger.debug(f'Calling LLM with input data: {input_data}')\n        prompt = [p.model_dump() for p in input_data.conversation_history]\n\n        def trim_prompt(s: str) -> str:\n            lines = s.strip().split('\\n')\n            return '\\n'.join([line.strip().lstrip('|') for line in lines])\n        values = input_data.prompt_values\n        if values:\n            input_data.prompt = input_data.prompt.format(**values)\n            input_data.sys_prompt = input_data.sys_prompt.format(**values)\n        if input_data.sys_prompt:\n            prompt.append({'role': 'system', 'content': input_data.sys_prompt})\n        if input_data.expected_format:\n            expected_format = [f'\"{k}\": \"{v}\"' for (k, v) in input_data.expected_format.items()]\n            format_prompt = ',\\n  '.join(expected_format)\n            sys_prompt = trim_prompt(f'\\n                  |Reply strictly only in the following JSON format:\\n                  |{{\\n                  |  {format_prompt}\\n                  |}}\\n                ')\n            prompt.append({'role': 'system', 'content': sys_prompt})\n        if input_data.prompt:\n            prompt.append({'role': 'user', 'content': input_data.prompt})\n\n        def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:\n            try:\n                parsed = json.loads(resp)\n                if not isinstance(parsed, dict):\n                    return ({}, f'Expected a dictionary, but got {type(parsed)}')\n                miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\n                if miss_keys:\n                    return (parsed, f'Missing keys: {miss_keys}')\n                return (parsed, None)\n            except JSONDecodeError as e:\n                return ({}, f'JSON decode error: {e}')\n        logger.info(f'LLM request: {prompt}')\n        retry_prompt = ''\n        llm_model = input_data.model\n        for retry_count in range(input_data.retry):\n            try:\n                (response_text, input_token, output_token) = self.llm_call(credentials=credentials, llm_model=llm_model, prompt=prompt, json_format=bool(input_data.expected_format), ollama_host=input_data.ollama_host, max_tokens=input_data.max_tokens)\n                self.merge_stats({'input_token_count': input_token, 'output_token_count': output_token})\n                logger.info(f'LLM attempt-{retry_count} response: {response_text}')\n                if input_data.expected_format:\n                    (parsed_dict, parsed_error) = parse_response(response_text)\n                    if not parsed_error:\n                        yield ('response', {k: json.loads(v) if isinstance(v, str) and v.startswith('[') and v.endswith(']') else ', '.join(v) if isinstance(v, list) else v for (k, v) in parsed_dict.items()})\n                        return\n                else:\n                    yield ('response', {'response': response_text})\n                    return\n                retry_prompt = trim_prompt(f'\\n                  |This is your previous error response:\\n                  |--\\n                  |{response_text}\\n                  |--\\n                  |\\n                  |And this is the error:\\n                  |--\\n                  |{parsed_error}\\n                  |--\\n                ')\n                prompt.append({'role': 'user', 'content': retry_prompt})\n            except Exception as e:\n                logger.exception(f'Error calling LLM: {e}')\n                retry_prompt = f'Error calling LLM: {e}'\n            finally:\n                self.merge_stats({'llm_call_count': retry_count + 1, 'llm_retry_count': retry_count})\n        raise RuntimeError(retry_prompt)",
        "class Input(BlockSchema):\n    prompt: str = SchemaField(description='The prompt to send to the language model.', placeholder='Enter your prompt here...')\n    expected_format: dict[str, str] = SchemaField(description='Expected format of the response. If provided, the response will be validated against this format. The keys should be the expected fields in the response, and the values should be the description of the field.')\n    model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)\n    credentials: AICredentials = AICredentialsField()\n    sys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')\n    conversation_history: list[Message] = SchemaField(default=[], description='The conversation history to provide context for the prompt.')\n    retry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')\n    prompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')\n    max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n    ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')",
        "prompt: str = SchemaField(description='The prompt to send to the language model.', placeholder='Enter your prompt here...')",
        "expected_format: dict[str, str] = SchemaField(description='Expected format of the response. If provided, the response will be validated against this format. The keys should be the expected fields in the response, and the values should be the description of the field.')",
        "model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)",
        "credentials: AICredentials = AICredentialsField()",
        "sys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')",
        "conversation_history: list[Message] = SchemaField(default=[], description='The conversation history to provide context for the prompt.')",
        "retry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')",
        "prompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')",
        "max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')",
        "ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')",
        "class Output(BlockSchema):\n    response: dict[str, Any] = SchemaField(description='The response object generated by the language model.')\n    error: str = SchemaField(description='Error message if the API call failed.')",
        "response: dict[str, Any] = SchemaField(description='The response object generated by the language model.')",
        "error: str = SchemaField(description='Error message if the API call failed.')",
        "def __init__(self):\n    super().__init__(id='ed55ac19-356e-4243-a6cb-bc599e9b716f', description='Call a Large Language Model (LLM) to generate formatted object based on the given prompt.', categories={BlockCategory.AI}, input_schema=AIStructuredResponseGeneratorBlock.Input, output_schema=AIStructuredResponseGeneratorBlock.Output, test_input={'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT, 'expected_format': {'key1': 'value1', 'key2': 'value2'}, 'prompt': 'User prompt'}, test_credentials=TEST_CREDENTIALS, test_output=('response', {'key1': 'key1Value', 'key2': 'key2Value'}), test_mock={'llm_call': lambda *args, **kwargs: (json.dumps({'key1': 'key1Value', 'key2': 'key2Value'}), 0, 0)})",
        "super().__init__()",
        "@staticmethod\ndef llm_call(credentials: APIKeyCredentials, llm_model: LlmModel, prompt: list[dict], json_format: bool, max_tokens: int | None=None, ollama_host: str='localhost:11434') -> tuple[str, int, int]:\n    \"\"\"\n        Args:\n            api_key: API key for the LLM provider.\n            llm_model: The LLM model to use.\n            prompt: The prompt to send to the LLM.\n            json_format: Whether the response should be in JSON format.\n            max_tokens: The maximum number of tokens to generate in the chat completion.\n            ollama_host: The host for ollama to use\n\n        Returns:\n            The response from the LLM.\n            The number of tokens used in the prompt.\n            The number of tokens used in the completion.\n        \"\"\"\n    provider = llm_model.metadata.provider\n    if provider == 'openai':\n        oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())\n        response_format = None\n        if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:\n            sys_messages = [p['content'] for p in prompt if p['role'] == 'system']\n            usr_messages = [p['content'] for p in prompt if p['role'] != 'system']\n            prompt = [{'role': 'user', 'content': '\\n'.join(sys_messages)}, {'role': 'user', 'content': '\\n'.join(usr_messages)}]\n        elif json_format:\n            response_format = {'type': 'json_object'}\n        response = oai_client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_completion_tokens=max_tokens)\n        return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n    elif provider == 'anthropic':\n        system_messages = [p['content'] for p in prompt if p['role'] == 'system']\n        sysprompt = ' '.join(system_messages)\n        messages = []\n        last_role = None\n        for p in prompt:\n            if p['role'] in ['user', 'assistant']:\n                if p['role'] != last_role:\n                    messages.append({'role': p['role'], 'content': p['content']})\n                    last_role = p['role']\n                else:\n                    messages[-1]['content'] += '\\n' + p['content']\n        client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())\n        try:\n            resp = client.messages.create(model=llm_model.value, system=sysprompt, messages=messages, max_tokens=max_tokens or 8192)\n            if not resp.content:\n                raise ValueError('No content returned from Anthropic.')\n            return (resp.content[0].name if isinstance(resp.content[0], anthropic.types.ToolUseBlock) else resp.content[0].text, resp.usage.input_tokens, resp.usage.output_tokens)\n        except anthropic.APIError as e:\n            error_message = f'Anthropic API error: {str(e)}'\n            logger.error(error_message)\n            raise ValueError(error_message)\n    elif provider == 'groq':\n        client = Groq(api_key=credentials.api_key.get_secret_value())\n        response_format = {'type': 'json_object'} if json_format else None\n        response = client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_tokens=max_tokens)\n        return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n    elif provider == 'ollama':\n        client = ollama.Client(host=ollama_host)\n        sys_messages = [p['content'] for p in prompt if p['role'] == 'system']\n        usr_messages = [p['content'] for p in prompt if p['role'] != 'system']\n        response = client.generate(model=llm_model.value, prompt=f'{sys_messages}\\n\\n{usr_messages}', stream=False)\n        return (response.get('response') or '', response.get('prompt_eval_count') or 0, response.get('eval_count') or 0)\n    elif provider == 'open_router':\n        client = openai.OpenAI(base_url='https://openrouter.ai/api/v1', api_key=credentials.api_key.get_secret_value())\n        response = client.chat.completions.create(extra_headers={'HTTP-Referer': 'https://agpt.co', 'X-Title': 'AutoGPT'}, model=llm_model.value, messages=prompt, max_tokens=max_tokens)\n        if not response.choices:\n            if response:\n                raise ValueError(f'OpenRouter error: {response}')\n            else:\n                raise ValueError('No response from OpenRouter.')\n        return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n    else:\n        raise ValueError(f'Unsupported LLM provider: {provider}')",
        "'\\n        Args:\\n            api_key: API key for the LLM provider.\\n            llm_model: The LLM model to use.\\n            prompt: The prompt to send to the LLM.\\n            json_format: Whether the response should be in JSON format.\\n            max_tokens: The maximum number of tokens to generate in the chat completion.\\n            ollama_host: The host for ollama to use\\n\\n        Returns:\\n            The response from the LLM.\\n            The number of tokens used in the prompt.\\n            The number of tokens used in the completion.\\n        '",
        "provider = llm_model.metadata.provider",
        "provider Eq 'openai'"
      ],
      "code": "class MessageRole(str, Enum):\n    SYSTEM = 'system'\n    USER = 'user'\n    ASSISTANT = 'assistant'\nSYSTEM = 'system'\nUSER = 'user'\nASSISTANT = 'assistant'\nclass Message(BlockSchema):\n    role: MessageRole\n    content: str\nrole: MessageRole\ncontent: str\nclass AIStructuredResponseGeneratorBlock(Block):\n\n    class Input(BlockSchema):\n        prompt: str = SchemaField(description='The prompt to send to the language model.', placeholder='Enter your prompt here...')\n        expected_format: dict[str, str] = SchemaField(description='Expected format of the response. If provided, the response will be validated against this format. The keys should be the expected fields in the response, and the values should be the description of the field.')\n        model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)\n        credentials: AICredentials = AICredentialsField()\n        sys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')\n        conversation_history: list[Message] = SchemaField(default=[], description='The conversation history to provide context for the prompt.')\n        retry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')\n        prompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')\n        max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n        ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n\n    class Output(BlockSchema):\n        response: dict[str, Any] = SchemaField(description='The response object generated by the language model.')\n        error: str = SchemaField(description='Error message if the API call failed.')\n\n    def __init__(self):\n        super().__init__(id='ed55ac19-356e-4243-a6cb-bc599e9b716f', description='Call a Large Language Model (LLM) to generate formatted object based on the given prompt.', categories={BlockCategory.AI}, input_schema=AIStructuredResponseGeneratorBlock.Input, output_schema=AIStructuredResponseGeneratorBlock.Output, test_input={'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT, 'expected_format': {'key1': 'value1', 'key2': 'value2'}, 'prompt': 'User prompt'}, test_credentials=TEST_CREDENTIALS, test_output=('response', {'key1': 'key1Value', 'key2': 'key2Value'}), test_mock={'llm_call': lambda *args, **kwargs: (json.dumps({'key1': 'key1Value', 'key2': 'key2Value'}), 0, 0)})\n\n    @staticmethod\n    def llm_call(credentials: APIKeyCredentials, llm_model: LlmModel, prompt: list[dict], json_format: bool, max_tokens: int | None=None, ollama_host: str='localhost:11434') -> tuple[str, int, int]:\n        \"\"\"\n        Args:\n            api_key: API key for the LLM provider.\n            llm_model: The LLM model to use.\n            prompt: The prompt to send to the LLM.\n            json_format: Whether the response should be in JSON format.\n            max_tokens: The maximum number of tokens to generate in the chat completion.\n            ollama_host: The host for ollama to use\n\n        Returns:\n            The response from the LLM.\n            The number of tokens used in the prompt.\n            The number of tokens used in the completion.\n        \"\"\"\n        provider = llm_model.metadata.provider\n        if provider == 'openai':\n            oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())\n            response_format = None\n            if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:\n                sys_messages = [p['content'] for p in prompt if p['role'] == 'system']\n                usr_messages = [p['content'] for p in prompt if p['role'] != 'system']\n                prompt = [{'role': 'user', 'content': '\\n'.join(sys_messages)}, {'role': 'user', 'content': '\\n'.join(usr_messages)}]\n            elif json_format:\n                response_format = {'type': 'json_object'}\n            response = oai_client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_completion_tokens=max_tokens)\n            return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n        elif provider == 'anthropic':\n            system_messages = [p['content'] for p in prompt if p['role'] == 'system']\n            sysprompt = ' '.join(system_messages)\n            messages = []\n            last_role = None\n            for p in prompt:\n                if p['role'] in ['user', 'assistant']:\n                    if p['role'] != last_role:\n                        messages.append({'role': p['role'], 'content': p['content']})\n                        last_role = p['role']\n                    else:\n                        messages[-1]['content'] += '\\n' + p['content']\n            client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())\n            try:\n                resp = client.messages.create(model=llm_model.value, system=sysprompt, messages=messages, max_tokens=max_tokens or 8192)\n                if not resp.content:\n                    raise ValueError('No content returned from Anthropic.')\n                return (resp.content[0].name if isinstance(resp.content[0], anthropic.types.ToolUseBlock) else resp.content[0].text, resp.usage.input_tokens, resp.usage.output_tokens)\n            except anthropic.APIError as e:\n                error_message = f'Anthropic API error: {str(e)}'\n                logger.error(error_message)\n                raise ValueError(error_message)\n        elif provider == 'groq':\n            client = Groq(api_key=credentials.api_key.get_secret_value())\n            response_format = {'type': 'json_object'} if json_format else None\n            response = client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_tokens=max_tokens)\n            return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n        elif provider == 'ollama':\n            client = ollama.Client(host=ollama_host)\n            sys_messages = [p['content'] for p in prompt if p['role'] == 'system']\n            usr_messages = [p['content'] for p in prompt if p['role'] != 'system']\n            response = client.generate(model=llm_model.value, prompt=f'{sys_messages}\\n\\n{usr_messages}', stream=False)\n            return (response.get('response') or '', response.get('prompt_eval_count') or 0, response.get('eval_count') or 0)\n        elif provider == 'open_router':\n            client = openai.OpenAI(base_url='https://openrouter.ai/api/v1', api_key=credentials.api_key.get_secret_value())\n            response = client.chat.completions.create(extra_headers={'HTTP-Referer': 'https://agpt.co', 'X-Title': 'AutoGPT'}, model=llm_model.value, messages=prompt, max_tokens=max_tokens)\n            if not response.choices:\n                if response:\n                    raise ValueError(f'OpenRouter error: {response}')\n                else:\n                    raise ValueError('No response from OpenRouter.')\n            return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n        else:\n            raise ValueError(f'Unsupported LLM provider: {provider}')\n\n    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n        logger.debug(f'Calling LLM with input data: {input_data}')\n        prompt = [p.model_dump() for p in input_data.conversation_history]\n\n        def trim_prompt(s: str) -> str:\n            lines = s.strip().split('\\n')\n            return '\\n'.join([line.strip().lstrip('|') for line in lines])\n        values = input_data.prompt_values\n        if values:\n            input_data.prompt = input_data.prompt.format(**values)\n            input_data.sys_prompt = input_data.sys_prompt.format(**values)\n        if input_data.sys_prompt:\n            prompt.append({'role': 'system', 'content': input_data.sys_prompt})\n        if input_data.expected_format:\n            expected_format = [f'\"{k}\": \"{v}\"' for (k, v) in input_data.expected_format.items()]\n            format_prompt = ',\\n  '.join(expected_format)\n            sys_prompt = trim_prompt(f'\\n                  |Reply strictly only in the following JSON format:\\n                  |{{\\n                  |  {format_prompt}\\n                  |}}\\n                ')\n            prompt.append({'role': 'system', 'content': sys_prompt})\n        if input_data.prompt:\n            prompt.append({'role': 'user', 'content': input_data.prompt})\n\n        def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:\n            try:\n                parsed = json.loads(resp)\n                if not isinstance(parsed, dict):\n                    return ({}, f'Expected a dictionary, but got {type(parsed)}')\n                miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\n                if miss_keys:\n                    return (parsed, f'Missing keys: {miss_keys}')\n                return (parsed, None)\n            except JSONDecodeError as e:\n                return ({}, f'JSON decode error: {e}')\n        logger.info(f'LLM request: {prompt}')\n        retry_prompt = ''\n        llm_model = input_data.model\n        for retry_count in range(input_data.retry):\n            try:\n                (response_text, input_token, output_token) = self.llm_call(credentials=credentials, llm_model=llm_model, prompt=prompt, json_format=bool(input_data.expected_format), ollama_host=input_data.ollama_host, max_tokens=input_data.max_tokens)\n                self.merge_stats({'input_token_count': input_token, 'output_token_count': output_token})\n                logger.info(f'LLM attempt-{retry_count} response: {response_text}')\n                if input_data.expected_format:\n                    (parsed_dict, parsed_error) = parse_response(response_text)\n                    if not parsed_error:\n                        yield ('response', {k: json.loads(v) if isinstance(v, str) and v.startswith('[') and v.endswith(']') else ', '.join(v) if isinstance(v, list) else v for (k, v) in parsed_dict.items()})\n                        return\n                else:\n                    yield ('response', {'response': response_text})\n                    return\n                retry_prompt = trim_prompt(f'\\n                  |This is your previous error response:\\n                  |--\\n                  |{response_text}\\n                  |--\\n                  |\\n                  |And this is the error:\\n                  |--\\n                  |{parsed_error}\\n                  |--\\n                ')\n                prompt.append({'role': 'user', 'content': retry_prompt})\n            except Exception as e:\n                logger.exception(f'Error calling LLM: {e}')\n                retry_prompt = f'Error calling LLM: {e}'\n            finally:\n                self.merge_stats({'llm_call_count': retry_count + 1, 'llm_retry_count': retry_count})\n        raise RuntimeError(retry_prompt)\nclass Input(BlockSchema):\n    prompt: str = SchemaField(description='The prompt to send to the language model.', placeholder='Enter your prompt here...')\n    expected_format: dict[str, str] = SchemaField(description='Expected format of the response. If provided, the response will be validated against this format. The keys should be the expected fields in the response, and the values should be the description of the field.')\n    model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)\n    credentials: AICredentials = AICredentialsField()\n    sys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')\n    conversation_history: list[Message] = SchemaField(default=[], description='The conversation history to provide context for the prompt.')\n    retry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')\n    prompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')\n    max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n    ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\nprompt: str = SchemaField(description='The prompt to send to the language model.', placeholder='Enter your prompt here...')\nexpected_format: dict[str, str] = SchemaField(description='Expected format of the response. If provided, the response will be validated against this format. The keys should be the expected fields in the response, and the values should be the description of the field.')\nmodel: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)\ncredentials: AICredentials = AICredentialsField()\nsys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')\nconversation_history: list[Message] = SchemaField(default=[], description='The conversation history to provide context for the prompt.')\nretry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')\nprompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')\nmax_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\nollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\nclass Output(BlockSchema):\n    response: dict[str, Any] = SchemaField(description='The response object generated by the language model.')\n    error: str = SchemaField(description='Error message if the API call failed.')\nresponse: dict[str, Any] = SchemaField(description='The response object generated by the language model.')\nerror: str = SchemaField(description='Error message if the API call failed.')\ndef __init__(self):\n    super().__init__(id='ed55ac19-356e-4243-a6cb-bc599e9b716f', description='Call a Large Language Model (LLM) to generate formatted object based on the given prompt.', categories={BlockCategory.AI}, input_schema=AIStructuredResponseGeneratorBlock.Input, output_schema=AIStructuredResponseGeneratorBlock.Output, test_input={'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT, 'expected_format': {'key1': 'value1', 'key2': 'value2'}, 'prompt': 'User prompt'}, test_credentials=TEST_CREDENTIALS, test_output=('response', {'key1': 'key1Value', 'key2': 'key2Value'}), test_mock={'llm_call': lambda *args, **kwargs: (json.dumps({'key1': 'key1Value', 'key2': 'key2Value'}), 0, 0)})\nsuper().__init__()\n@staticmethod\ndef llm_call(credentials: APIKeyCredentials, llm_model: LlmModel, prompt: list[dict], json_format: bool, max_tokens: int | None=None, ollama_host: str='localhost:11434') -> tuple[str, int, int]:\n    \"\"\"\n        Args:\n            api_key: API key for the LLM provider.\n            llm_model: The LLM model to use.\n            prompt: The prompt to send to the LLM.\n            json_format: Whether the response should be in JSON format.\n            max_tokens: The maximum number of tokens to generate in the chat completion.\n            ollama_host: The host for ollama to use\n\n        Returns:\n            The response from the LLM.\n            The number of tokens used in the prompt.\n            The number of tokens used in the completion.\n        \"\"\"\n    provider = llm_model.metadata.provider\n    if provider == 'openai':\n        oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())\n        response_format = None\n        if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:\n            sys_messages = [p['content'] for p in prompt if p['role'] == 'system']\n            usr_messages = [p['content'] for p in prompt if p['role'] != 'system']\n            prompt = [{'role': 'user', 'content': '\\n'.join(sys_messages)}, {'role': 'user', 'content': '\\n'.join(usr_messages)}]\n        elif json_format:\n            response_format = {'type': 'json_object'}\n        response = oai_client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_completion_tokens=max_tokens)\n        return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n    elif provider == 'anthropic':\n        system_messages = [p['content'] for p in prompt if p['role'] == 'system']\n        sysprompt = ' '.join(system_messages)\n        messages = []\n        last_role = None\n        for p in prompt:\n            if p['role'] in ['user', 'assistant']:\n                if p['role'] != last_role:\n                    messages.append({'role': p['role'], 'content': p['content']})\n                    last_role = p['role']\n                else:\n                    messages[-1]['content'] += '\\n' + p['content']\n        client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())\n        try:\n            resp = client.messages.create(model=llm_model.value, system=sysprompt, messages=messages, max_tokens=max_tokens or 8192)\n            if not resp.content:\n                raise ValueError('No content returned from Anthropic.')\n            return (resp.content[0].name if isinstance(resp.content[0], anthropic.types.ToolUseBlock) else resp.content[0].text, resp.usage.input_tokens, resp.usage.output_tokens)\n        except anthropic.APIError as e:\n            error_message = f'Anthropic API error: {str(e)}'\n            logger.error(error_message)\n            raise ValueError(error_message)\n    elif provider == 'groq':\n        client = Groq(api_key=credentials.api_key.get_secret_value())\n        response_format = {'type': 'json_object'} if json_format else None\n        response = client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_tokens=max_tokens)\n        return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n    elif provider == 'ollama':\n        client = ollama.Client(host=ollama_host)\n        sys_messages = [p['content'] for p in prompt if p['role'] == 'system']\n        usr_messages = [p['content'] for p in prompt if p['role'] != 'system']\n        response = client.generate(model=llm_model.value, prompt=f'{sys_messages}\\n\\n{usr_messages}', stream=False)\n        return (response.get('response') or '', response.get('prompt_eval_count') or 0, response.get('eval_count') or 0)\n    elif provider == 'open_router':\n        client = openai.OpenAI(base_url='https://openrouter.ai/api/v1', api_key=credentials.api_key.get_secret_value())\n        response = client.chat.completions.create(extra_headers={'HTTP-Referer': 'https://agpt.co', 'X-Title': 'AutoGPT'}, model=llm_model.value, messages=prompt, max_tokens=max_tokens)\n        if not response.choices:\n            if response:\n                raise ValueError(f'OpenRouter error: {response}')\n            else:\n                raise ValueError('No response from OpenRouter.')\n        return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)\n    else:\n        raise ValueError(f'Unsupported LLM provider: {provider}')\n'\\n        Args:\\n            api_key: API key for the LLM provider.\\n            llm_model: The LLM model to use.\\n            prompt: The prompt to send to the LLM.\\n            json_format: Whether the response should be in JSON format.\\n            max_tokens: The maximum number of tokens to generate in the chat completion.\\n            ollama_host: The host for ollama to use\\n\\n        Returns:\\n            The response from the LLM.\\n            The number of tokens used in the prompt.\\n            The number of tokens used in the completion.\\n        '\nprovider = llm_model.metadata.provider\nprovider Eq 'openai'"
    },
    {
      "id": "n14",
      "type": "block",
      "statements": [
        "raise ValueError(f'Missing MODEL_METADATA metadata for model: {model}')"
      ],
      "code": "raise ValueError(f'Missing MODEL_METADATA metadata for model: {model}')"
    },
    {
      "id": "n15",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n16",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n17",
      "type": "block",
      "statements": [
        "oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())",
        "response_format = None",
        "llm_model In [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]"
      ],
      "code": "oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())\nresponse_format = None\nllm_model In [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]"
    },
    {
      "id": "n18",
      "type": "block",
      "statements": [
        "provider Eq 'anthropic'"
      ],
      "code": "provider Eq 'anthropic'"
    },
    {
      "id": "n19",
      "type": "block",
      "statements": [
        "def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n    logger.debug(f'Calling LLM with input data: {input_data}')\n    prompt = [p.model_dump() for p in input_data.conversation_history]\n\n    def trim_prompt(s: str) -> str:\n        lines = s.strip().split('\\n')\n        return '\\n'.join([line.strip().lstrip('|') for line in lines])\n    values = input_data.prompt_values\n    if values:\n        input_data.prompt = input_data.prompt.format(**values)\n        input_data.sys_prompt = input_data.sys_prompt.format(**values)\n    if input_data.sys_prompt:\n        prompt.append({'role': 'system', 'content': input_data.sys_prompt})\n    if input_data.expected_format:\n        expected_format = [f'\"{k}\": \"{v}\"' for (k, v) in input_data.expected_format.items()]\n        format_prompt = ',\\n  '.join(expected_format)\n        sys_prompt = trim_prompt(f'\\n                  |Reply strictly only in the following JSON format:\\n                  |{{\\n                  |  {format_prompt}\\n                  |}}\\n                ')\n        prompt.append({'role': 'system', 'content': sys_prompt})\n    if input_data.prompt:\n        prompt.append({'role': 'user', 'content': input_data.prompt})\n\n    def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:\n        try:\n            parsed = json.loads(resp)\n            if not isinstance(parsed, dict):\n                return ({}, f'Expected a dictionary, but got {type(parsed)}')\n            miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\n            if miss_keys:\n                return (parsed, f'Missing keys: {miss_keys}')\n            return (parsed, None)\n        except JSONDecodeError as e:\n            return ({}, f'JSON decode error: {e}')\n    logger.info(f'LLM request: {prompt}')\n    retry_prompt = ''\n    llm_model = input_data.model\n    for retry_count in range(input_data.retry):\n        try:\n            (response_text, input_token, output_token) = self.llm_call(credentials=credentials, llm_model=llm_model, prompt=prompt, json_format=bool(input_data.expected_format), ollama_host=input_data.ollama_host, max_tokens=input_data.max_tokens)\n            self.merge_stats({'input_token_count': input_token, 'output_token_count': output_token})\n            logger.info(f'LLM attempt-{retry_count} response: {response_text}')\n            if input_data.expected_format:\n                (parsed_dict, parsed_error) = parse_response(response_text)\n                if not parsed_error:\n                    yield ('response', {k: json.loads(v) if isinstance(v, str) and v.startswith('[') and v.endswith(']') else ', '.join(v) if isinstance(v, list) else v for (k, v) in parsed_dict.items()})\n                    return\n            else:\n                yield ('response', {'response': response_text})\n                return\n            retry_prompt = trim_prompt(f'\\n                  |This is your previous error response:\\n                  |--\\n                  |{response_text}\\n                  |--\\n                  |\\n                  |And this is the error:\\n                  |--\\n                  |{parsed_error}\\n                  |--\\n                ')\n            prompt.append({'role': 'user', 'content': retry_prompt})\n        except Exception as e:\n            logger.exception(f'Error calling LLM: {e}')\n            retry_prompt = f'Error calling LLM: {e}'\n        finally:\n            self.merge_stats({'llm_call_count': retry_count + 1, 'llm_retry_count': retry_count})\n    raise RuntimeError(retry_prompt)",
        "logger.debug(f'Calling LLM with input data: {input_data}')",
        "prompt = [p.model_dump() for p in input_data.conversation_history]",
        "def trim_prompt(s: str) -> str:\n    lines = s.strip().split('\\n')\n    return '\\n'.join([line.strip().lstrip('|') for line in lines])",
        "lines = s.strip().split('\\n')",
        "return '\\n'.join([line.strip().lstrip('|') for line in lines])"
      ],
      "code": "def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n    logger.debug(f'Calling LLM with input data: {input_data}')\n    prompt = [p.model_dump() for p in input_data.conversation_history]\n\n    def trim_prompt(s: str) -> str:\n        lines = s.strip().split('\\n')\n        return '\\n'.join([line.strip().lstrip('|') for line in lines])\n    values = input_data.prompt_values\n    if values:\n        input_data.prompt = input_data.prompt.format(**values)\n        input_data.sys_prompt = input_data.sys_prompt.format(**values)\n    if input_data.sys_prompt:\n        prompt.append({'role': 'system', 'content': input_data.sys_prompt})\n    if input_data.expected_format:\n        expected_format = [f'\"{k}\": \"{v}\"' for (k, v) in input_data.expected_format.items()]\n        format_prompt = ',\\n  '.join(expected_format)\n        sys_prompt = trim_prompt(f'\\n                  |Reply strictly only in the following JSON format:\\n                  |{{\\n                  |  {format_prompt}\\n                  |}}\\n                ')\n        prompt.append({'role': 'system', 'content': sys_prompt})\n    if input_data.prompt:\n        prompt.append({'role': 'user', 'content': input_data.prompt})\n\n    def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:\n        try:\n            parsed = json.loads(resp)\n            if not isinstance(parsed, dict):\n                return ({}, f'Expected a dictionary, but got {type(parsed)}')\n            miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\n            if miss_keys:\n                return (parsed, f'Missing keys: {miss_keys}')\n            return (parsed, None)\n        except JSONDecodeError as e:\n            return ({}, f'JSON decode error: {e}')\n    logger.info(f'LLM request: {prompt}')\n    retry_prompt = ''\n    llm_model = input_data.model\n    for retry_count in range(input_data.retry):\n        try:\n            (response_text, input_token, output_token) = self.llm_call(credentials=credentials, llm_model=llm_model, prompt=prompt, json_format=bool(input_data.expected_format), ollama_host=input_data.ollama_host, max_tokens=input_data.max_tokens)\n            self.merge_stats({'input_token_count': input_token, 'output_token_count': output_token})\n            logger.info(f'LLM attempt-{retry_count} response: {response_text}')\n            if input_data.expected_format:\n                (parsed_dict, parsed_error) = parse_response(response_text)\n                if not parsed_error:\n                    yield ('response', {k: json.loads(v) if isinstance(v, str) and v.startswith('[') and v.endswith(']') else ', '.join(v) if isinstance(v, list) else v for (k, v) in parsed_dict.items()})\n                    return\n            else:\n                yield ('response', {'response': response_text})\n                return\n            retry_prompt = trim_prompt(f'\\n                  |This is your previous error response:\\n                  |--\\n                  |{response_text}\\n                  |--\\n                  |\\n                  |And this is the error:\\n                  |--\\n                  |{parsed_error}\\n                  |--\\n                ')\n            prompt.append({'role': 'user', 'content': retry_prompt})\n        except Exception as e:\n            logger.exception(f'Error calling LLM: {e}')\n            retry_prompt = f'Error calling LLM: {e}'\n        finally:\n            self.merge_stats({'llm_call_count': retry_count + 1, 'llm_retry_count': retry_count})\n    raise RuntimeError(retry_prompt)\nlogger.debug(f'Calling LLM with input data: {input_data}')\nprompt = [p.model_dump() for p in input_data.conversation_history]\ndef trim_prompt(s: str) -> str:\n    lines = s.strip().split('\\n')\n    return '\\n'.join([line.strip().lstrip('|') for line in lines])\nlines = s.strip().split('\\n')\nreturn '\\n'.join([line.strip().lstrip('|') for line in lines])"
    },
    {
      "id": "n20",
      "type": "block",
      "statements": [
        "sys_messages = [p['content'] for p in prompt if p['role'] == 'system']",
        "usr_messages = [p['content'] for p in prompt if p['role'] != 'system']",
        "prompt = [{'role': 'user', 'content': '\\n'.join(sys_messages)}, {'role': 'user', 'content': '\\n'.join(usr_messages)}]"
      ],
      "code": "sys_messages = [p['content'] for p in prompt if p['role'] == 'system']\nusr_messages = [p['content'] for p in prompt if p['role'] != 'system']\nprompt = [{'role': 'user', 'content': '\\n'.join(sys_messages)}, {'role': 'user', 'content': '\\n'.join(usr_messages)}]"
    },
    {
      "id": "n21",
      "type": "block",
      "statements": [
        "json_format"
      ],
      "code": "json_format"
    },
    {
      "id": "n22",
      "type": "block",
      "statements": [
        "response = oai_client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_completion_tokens=max_tokens)",
        "return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)"
      ],
      "code": "response = oai_client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_completion_tokens=max_tokens)\nreturn (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)"
    },
    {
      "id": "n23",
      "type": "block",
      "statements": [
        "response_format = {'type': 'json_object'}"
      ],
      "code": "response_format = {'type': 'json_object'}"
    },
    {
      "id": "n24",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n25",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n26",
      "type": "block",
      "statements": [
        "system_messages = [p['content'] for p in prompt if p['role'] == 'system']",
        "sysprompt = ' '.join(system_messages)",
        "messages = []",
        "last_role = None"
      ],
      "code": "system_messages = [p['content'] for p in prompt if p['role'] == 'system']\nsysprompt = ' '.join(system_messages)\nmessages = []\nlast_role = None"
    },
    {
      "id": "n27",
      "type": "block",
      "statements": [
        "provider Eq 'groq'"
      ],
      "code": "provider Eq 'groq'"
    },
    {
      "id": "n28",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n29",
      "type": "block",
      "statements": [
        "p",
        "prompt"
      ],
      "code": "p\nprompt"
    },
    {
      "id": "n30",
      "type": "block",
      "statements": [
        "p['role'] In ['user', 'assistant']"
      ],
      "code": "p['role'] In ['user', 'assistant']"
    },
    {
      "id": "n31",
      "type": "block",
      "statements": [
        "client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())",
        "try:\n    resp = client.messages.create(model=llm_model.value, system=sysprompt, messages=messages, max_tokens=max_tokens or 8192)\n    if not resp.content:\n        raise ValueError('No content returned from Anthropic.')\n    return (resp.content[0].name if isinstance(resp.content[0], anthropic.types.ToolUseBlock) else resp.content[0].text, resp.usage.input_tokens, resp.usage.output_tokens)\nexcept anthropic.APIError as e:\n    error_message = f'Anthropic API error: {str(e)}'\n    logger.error(error_message)\n    raise ValueError(error_message)",
        "resp = client.messages.create(model=llm_model.value, system=sysprompt, messages=messages, max_tokens=max_tokens or 8192)",
        "not resp.content"
      ],
      "code": "client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())\ntry:\n    resp = client.messages.create(model=llm_model.value, system=sysprompt, messages=messages, max_tokens=max_tokens or 8192)\n    if not resp.content:\n        raise ValueError('No content returned from Anthropic.')\n    return (resp.content[0].name if isinstance(resp.content[0], anthropic.types.ToolUseBlock) else resp.content[0].text, resp.usage.input_tokens, resp.usage.output_tokens)\nexcept anthropic.APIError as e:\n    error_message = f'Anthropic API error: {str(e)}'\n    logger.error(error_message)\n    raise ValueError(error_message)\nresp = client.messages.create(model=llm_model.value, system=sysprompt, messages=messages, max_tokens=max_tokens or 8192)\nnot resp.content"
    },
    {
      "id": "n32",
      "type": "block",
      "statements": [
        "p['role'] NotEq last_role"
      ],
      "code": "p['role'] NotEq last_role"
    },
    {
      "id": "n33",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n34",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n35",
      "type": "block",
      "statements": [
        "messages.append({'role': p['role'], 'content': p['content']})",
        "last_role = p['role']"
      ],
      "code": "messages.append({'role': p['role'], 'content': p['content']})\nlast_role = p['role']"
    },
    {
      "id": "n36",
      "type": "block",
      "statements": [
        "messages[-1]['content'] += '\\n' + p['content']"
      ],
      "code": "messages[-1]['content'] += '\\n' + p['content']"
    },
    {
      "id": "n37",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n38",
      "type": "block",
      "statements": [
        "raise ValueError('No content returned from Anthropic.')"
      ],
      "code": "raise ValueError('No content returned from Anthropic.')"
    },
    {
      "id": "n39",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n40",
      "type": "block",
      "statements": [
        "return (resp.content[0].name if isinstance(resp.content[0], anthropic.types.ToolUseBlock) else resp.content[0].text, resp.usage.input_tokens, resp.usage.output_tokens)"
      ],
      "code": "return (resp.content[0].name if isinstance(resp.content[0], anthropic.types.ToolUseBlock) else resp.content[0].text, resp.usage.input_tokens, resp.usage.output_tokens)"
    },
    {
      "id": "n41",
      "type": "block",
      "statements": [
        "error_message = f'Anthropic API error: {str(e)}'",
        "logger.error(error_message)",
        "raise ValueError(error_message)"
      ],
      "code": "error_message = f'Anthropic API error: {str(e)}'\nlogger.error(error_message)\nraise ValueError(error_message)"
    },
    {
      "id": "n42",
      "type": "block",
      "statements": [
        "client = Groq(api_key=credentials.api_key.get_secret_value())",
        "response_format = {'type': 'json_object'} if json_format else None",
        "response = client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_tokens=max_tokens)",
        "return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)"
      ],
      "code": "client = Groq(api_key=credentials.api_key.get_secret_value())\nresponse_format = {'type': 'json_object'} if json_format else None\nresponse = client.chat.completions.create(model=llm_model.value, messages=prompt, response_format=response_format, max_tokens=max_tokens)\nreturn (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)"
    },
    {
      "id": "n43",
      "type": "block",
      "statements": [
        "provider Eq 'ollama'"
      ],
      "code": "provider Eq 'ollama'"
    },
    {
      "id": "n44",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n45",
      "type": "block",
      "statements": [
        "client = ollama.Client(host=ollama_host)",
        "sys_messages = [p['content'] for p in prompt if p['role'] == 'system']",
        "usr_messages = [p['content'] for p in prompt if p['role'] != 'system']",
        "response = client.generate(model=llm_model.value, prompt=f'{sys_messages}\\n\\n{usr_messages}', stream=False)",
        "return (response.get('response') or '', response.get('prompt_eval_count') or 0, response.get('eval_count') or 0)"
      ],
      "code": "client = ollama.Client(host=ollama_host)\nsys_messages = [p['content'] for p in prompt if p['role'] == 'system']\nusr_messages = [p['content'] for p in prompt if p['role'] != 'system']\nresponse = client.generate(model=llm_model.value, prompt=f'{sys_messages}\\n\\n{usr_messages}', stream=False)\nreturn (response.get('response') or '', response.get('prompt_eval_count') or 0, response.get('eval_count') or 0)"
    },
    {
      "id": "n46",
      "type": "block",
      "statements": [
        "provider Eq 'open_router'"
      ],
      "code": "provider Eq 'open_router'"
    },
    {
      "id": "n47",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n48",
      "type": "block",
      "statements": [
        "client = openai.OpenAI(base_url='https://openrouter.ai/api/v1', api_key=credentials.api_key.get_secret_value())",
        "response = client.chat.completions.create(extra_headers={'HTTP-Referer': 'https://agpt.co', 'X-Title': 'AutoGPT'}, model=llm_model.value, messages=prompt, max_tokens=max_tokens)",
        "not response.choices"
      ],
      "code": "client = openai.OpenAI(base_url='https://openrouter.ai/api/v1', api_key=credentials.api_key.get_secret_value())\nresponse = client.chat.completions.create(extra_headers={'HTTP-Referer': 'https://agpt.co', 'X-Title': 'AutoGPT'}, model=llm_model.value, messages=prompt, max_tokens=max_tokens)\nnot response.choices"
    },
    {
      "id": "n49",
      "type": "block",
      "statements": [
        "raise ValueError(f'Unsupported LLM provider: {provider}')"
      ],
      "code": "raise ValueError(f'Unsupported LLM provider: {provider}')\n"
    },
    {
      "id": "n50",
      "type": "block",
      "statements": [
        "response"
      ],
      "code": "response"
    },
    {
      "id": "n51",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n52",
      "type": "block",
      "statements": [
        "return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)"
      ],
      "code": "return (response.choices[0].message.content or '', response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0)"
    },
    {
      "id": "n53",
      "type": "block",
      "statements": [
        "raise ValueError(f'OpenRouter error: {response}')"
      ],
      "code": "raise ValueError(f'OpenRouter error: {response}')"
    },
    {
      "id": "n54",
      "type": "block",
      "statements": [
        "raise ValueError('No response from OpenRouter.')"
      ],
      "code": "raise ValueError('No response from OpenRouter.')"
    },
    {
      "id": "n55",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n56",
      "type": "block",
      "statements": [
        "values = input_data.prompt_values",
        "values"
      ],
      "code": "values = input_data.prompt_values\nvalues"
    },
    {
      "id": "n57",
      "type": "block",
      "statements": [
        "input_data.prompt = input_data.prompt.format(**values)",
        "input_data.sys_prompt = input_data.sys_prompt.format(**values)"
      ],
      "code": "input_data.prompt = input_data.prompt.format(**values)\ninput_data.sys_prompt = input_data.sys_prompt.format(**values)"
    },
    {
      "id": "n58",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n59",
      "type": "block",
      "statements": [
        "input_data.sys_prompt"
      ],
      "code": "input_data.sys_prompt"
    },
    {
      "id": "n60",
      "type": "block",
      "statements": [
        "prompt.append({'role': 'system', 'content': input_data.sys_prompt})"
      ],
      "code": "prompt.append({'role': 'system', 'content': input_data.sys_prompt})"
    },
    {
      "id": "n61",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n62",
      "type": "block",
      "statements": [
        "input_data.expected_format"
      ],
      "code": "input_data.expected_format"
    },
    {
      "id": "n63",
      "type": "block",
      "statements": [
        "expected_format = [f'\"{k}\": \"{v}\"' for (k, v) in input_data.expected_format.items()]",
        "format_prompt = ',\\n  '.join(expected_format)",
        "sys_prompt = trim_prompt(f'\\n                  |Reply strictly only in the following JSON format:\\n                  |{{\\n                  |  {format_prompt}\\n                  |}}\\n                ')",
        "prompt.append({'role': 'system', 'content': sys_prompt})"
      ],
      "code": "expected_format = [f'\"{k}\": \"{v}\"' for (k, v) in input_data.expected_format.items()]\nformat_prompt = ',\\n  '.join(expected_format)\nsys_prompt = trim_prompt(f'\\n                  |Reply strictly only in the following JSON format:\\n                  |{{\\n                  |  {format_prompt}\\n                  |}}\\n                ')\nprompt.append({'role': 'system', 'content': sys_prompt})"
    },
    {
      "id": "n64",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n65",
      "type": "block",
      "statements": [
        "input_data.prompt"
      ],
      "code": "input_data.prompt"
    },
    {
      "id": "n66",
      "type": "block",
      "statements": [
        "prompt.append({'role': 'user', 'content': input_data.prompt})"
      ],
      "code": "prompt.append({'role': 'user', 'content': input_data.prompt})"
    },
    {
      "id": "n67",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n68",
      "type": "block",
      "statements": [
        "def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:\n    try:\n        parsed = json.loads(resp)\n        if not isinstance(parsed, dict):\n            return ({}, f'Expected a dictionary, but got {type(parsed)}')\n        miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\n        if miss_keys:\n            return (parsed, f'Missing keys: {miss_keys}')\n        return (parsed, None)\n    except JSONDecodeError as e:\n        return ({}, f'JSON decode error: {e}')",
        "try:\n    parsed = json.loads(resp)\n    if not isinstance(parsed, dict):\n        return ({}, f'Expected a dictionary, but got {type(parsed)}')\n    miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\n    if miss_keys:\n        return (parsed, f'Missing keys: {miss_keys}')\n    return (parsed, None)\nexcept JSONDecodeError as e:\n    return ({}, f'JSON decode error: {e}')",
        "parsed = json.loads(resp)",
        "not isinstance(parsed, dict)"
      ],
      "code": "def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:\n    try:\n        parsed = json.loads(resp)\n        if not isinstance(parsed, dict):\n            return ({}, f'Expected a dictionary, but got {type(parsed)}')\n        miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\n        if miss_keys:\n            return (parsed, f'Missing keys: {miss_keys}')\n        return (parsed, None)\n    except JSONDecodeError as e:\n        return ({}, f'JSON decode error: {e}')\ntry:\n    parsed = json.loads(resp)\n    if not isinstance(parsed, dict):\n        return ({}, f'Expected a dictionary, but got {type(parsed)}')\n    miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\n    if miss_keys:\n        return (parsed, f'Missing keys: {miss_keys}')\n    return (parsed, None)\nexcept JSONDecodeError as e:\n    return ({}, f'JSON decode error: {e}')\nparsed = json.loads(resp)\nnot isinstance(parsed, dict)"
    },
    {
      "id": "n69",
      "type": "block",
      "statements": [
        "return ({}, f'Expected a dictionary, but got {type(parsed)}')"
      ],
      "code": "return ({}, f'Expected a dictionary, but got {type(parsed)}')"
    },
    {
      "id": "n70",
      "type": "block",
      "statements": [],
      "code": "\nmiss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())\nmiss_keys"
    },
    {
      "id": "n71",
      "type": "block",
      "statements": [
        "return (parsed, f'Missing keys: {miss_keys}')"
      ],
      "code": "return (parsed, f'Missing keys: {miss_keys}')"
    },
    {
      "id": "n72",
      "type": "block",
      "statements": [],
      "code": "\nreturn (parsed, None)"
    },
    {
      "id": "n73",
      "type": "block",
      "statements": [
        "return ({}, f'JSON decode error: {e}')"
      ],
      "code": "return ({}, f'JSON decode error: {e}')"
    },
    {
      "id": "n74",
      "type": "block",
      "statements": [
        "logger.info(f'LLM request: {prompt}')",
        "retry_prompt = ''",
        "llm_model = input_data.model"
      ],
      "code": "logger.info(f'LLM request: {prompt}')\nretry_prompt = ''\nllm_model = input_data.model"
    },
    {
      "id": "n75",
      "type": "block",
      "statements": [
        "retry_count",
        "range(input_data.retry)"
      ],
      "code": "retry_count\nrange(input_data.retry)"
    },
    {
      "id": "n76",
      "type": "block",
      "statements": [
        "try:\n    (response_text, input_token, output_token) = self.llm_call(credentials=credentials, llm_model=llm_model, prompt=prompt, json_format=bool(input_data.expected_format), ollama_host=input_data.ollama_host, max_tokens=input_data.max_tokens)\n    self.merge_stats({'input_token_count': input_token, 'output_token_count': output_token})\n    logger.info(f'LLM attempt-{retry_count} response: {response_text}')\n    if input_data.expected_format:\n        (parsed_dict, parsed_error) = parse_response(response_text)\n        if not parsed_error:\n            yield ('response', {k: json.loads(v) if isinstance(v, str) and v.startswith('[') and v.endswith(']') else ', '.join(v) if isinstance(v, list) else v for (k, v) in parsed_dict.items()})\n            return\n    else:\n        yield ('response', {'response': response_text})\n        return\n    retry_prompt = trim_prompt(f'\\n                  |This is your previous error response:\\n                  |--\\n                  |{response_text}\\n                  |--\\n                  |\\n                  |And this is the error:\\n                  |--\\n                  |{parsed_error}\\n                  |--\\n                ')\n    prompt.append({'role': 'user', 'content': retry_prompt})\nexcept Exception as e:\n    logger.exception(f'Error calling LLM: {e}')\n    retry_prompt = f'Error calling LLM: {e}'\nfinally:\n    self.merge_stats({'llm_call_count': retry_count + 1, 'llm_retry_count': retry_count})",
        "(response_text, input_token, output_token) = self.llm_call(credentials=credentials, llm_model=llm_model, prompt=prompt, json_format=bool(input_data.expected_format), ollama_host=input_data.ollama_host, max_tokens=input_data.max_tokens)",
        "self.merge_stats({'input_token_count': input_token, 'output_token_count': output_token})",
        "logger.info(f'LLM attempt-{retry_count} response: {response_text}')",
        "input_data.expected_format"
      ],
      "code": "try:\n    (response_text, input_token, output_token) = self.llm_call(credentials=credentials, llm_model=llm_model, prompt=prompt, json_format=bool(input_data.expected_format), ollama_host=input_data.ollama_host, max_tokens=input_data.max_tokens)\n    self.merge_stats({'input_token_count': input_token, 'output_token_count': output_token})\n    logger.info(f'LLM attempt-{retry_count} response: {response_text}')\n    if input_data.expected_format:\n        (parsed_dict, parsed_error) = parse_response(response_text)\n        if not parsed_error:\n            yield ('response', {k: json.loads(v) if isinstance(v, str) and v.startswith('[') and v.endswith(']') else ', '.join(v) if isinstance(v, list) else v for (k, v) in parsed_dict.items()})\n            return\n    else:\n        yield ('response', {'response': response_text})\n        return\n    retry_prompt = trim_prompt(f'\\n                  |This is your previous error response:\\n                  |--\\n                  |{response_text}\\n                  |--\\n                  |\\n                  |And this is the error:\\n                  |--\\n                  |{parsed_error}\\n                  |--\\n                ')\n    prompt.append({'role': 'user', 'content': retry_prompt})\nexcept Exception as e:\n    logger.exception(f'Error calling LLM: {e}')\n    retry_prompt = f'Error calling LLM: {e}'\nfinally:\n    self.merge_stats({'llm_call_count': retry_count + 1, 'llm_retry_count': retry_count})\n(response_text, input_token, output_token) = self.llm_call(credentials=credentials, llm_model=llm_model, prompt=prompt, json_format=bool(input_data.expected_format), ollama_host=input_data.ollama_host, max_tokens=input_data.max_tokens)\nself.merge_stats({'input_token_count': input_token, 'output_token_count': output_token})\nlogger.info(f'LLM attempt-{retry_count} response: {response_text}')\ninput_data.expected_format"
    },
    {
      "id": "n77",
      "type": "block",
      "statements": [
        "raise RuntimeError(retry_prompt)",
        "class AITextGeneratorBlock(Block):\n\n    class Input(BlockSchema):\n        prompt: str = SchemaField(description='The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces.', placeholder='Enter your prompt here...')\n        model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)\n        credentials: AICredentials = AICredentialsField()\n        sys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')\n        retry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')\n        prompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')\n        ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n        max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n\n    class Output(BlockSchema):\n        response: str = SchemaField(description='The response generated by the language model.')\n        error: str = SchemaField(description='Error message if the API call failed.')\n\n    def __init__(self):\n        super().__init__(id='1f292d4a-41a4-4977-9684-7c8d560b9f91', description='Call a Large Language Model (LLM) to generate a string based on the given prompt.', categories={BlockCategory.AI}, input_schema=AITextGeneratorBlock.Input, output_schema=AITextGeneratorBlock.Output, test_input={'prompt': 'User prompt', 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('response', 'Response text'), test_mock={'llm_call': lambda *args, **kwargs: 'Response text'})\n\n    def llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> str:\n        block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, 'response', credentials=credentials)\n        self.merge_stats(block.execution_stats)\n        return response['response']\n\n    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n        object_input_data = AIStructuredResponseGeneratorBlock.Input(**{attr: getattr(input_data, attr) for attr in input_data.model_fields}, expected_format={})\n        yield ('response', self.llm_call(object_input_data, credentials))",
        "class Input(BlockSchema):\n    prompt: str = SchemaField(description='The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces.', placeholder='Enter your prompt here...')\n    model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)\n    credentials: AICredentials = AICredentialsField()\n    sys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')\n    retry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')\n    prompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')\n    ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n    max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')",
        "prompt: str = SchemaField(description='The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces.', placeholder='Enter your prompt here...')",
        "model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)",
        "credentials: AICredentials = AICredentialsField()",
        "sys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')",
        "retry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')",
        "prompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')",
        "ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')",
        "max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')",
        "class Output(BlockSchema):\n    response: str = SchemaField(description='The response generated by the language model.')\n    error: str = SchemaField(description='Error message if the API call failed.')",
        "response: str = SchemaField(description='The response generated by the language model.')",
        "error: str = SchemaField(description='Error message if the API call failed.')",
        "def __init__(self):\n    super().__init__(id='1f292d4a-41a4-4977-9684-7c8d560b9f91', description='Call a Large Language Model (LLM) to generate a string based on the given prompt.', categories={BlockCategory.AI}, input_schema=AITextGeneratorBlock.Input, output_schema=AITextGeneratorBlock.Output, test_input={'prompt': 'User prompt', 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('response', 'Response text'), test_mock={'llm_call': lambda *args, **kwargs: 'Response text'})",
        "super().__init__()",
        "def llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> str:\n    block = AIStructuredResponseGeneratorBlock()\n    response = block.run_once(input_data, 'response', credentials=credentials)\n    self.merge_stats(block.execution_stats)\n    return response['response']",
        "block = AIStructuredResponseGeneratorBlock()",
        "response = block.run_once(input_data, 'response', credentials=credentials)",
        "self.merge_stats(block.execution_stats)",
        "return response['response']"
      ],
      "code": "raise RuntimeError(retry_prompt)\nclass AITextGeneratorBlock(Block):\n\n    class Input(BlockSchema):\n        prompt: str = SchemaField(description='The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces.', placeholder='Enter your prompt here...')\n        model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)\n        credentials: AICredentials = AICredentialsField()\n        sys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')\n        retry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')\n        prompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')\n        ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n        max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n\n    class Output(BlockSchema):\n        response: str = SchemaField(description='The response generated by the language model.')\n        error: str = SchemaField(description='Error message if the API call failed.')\n\n    def __init__(self):\n        super().__init__(id='1f292d4a-41a4-4977-9684-7c8d560b9f91', description='Call a Large Language Model (LLM) to generate a string based on the given prompt.', categories={BlockCategory.AI}, input_schema=AITextGeneratorBlock.Input, output_schema=AITextGeneratorBlock.Output, test_input={'prompt': 'User prompt', 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('response', 'Response text'), test_mock={'llm_call': lambda *args, **kwargs: 'Response text'})\n\n    def llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> str:\n        block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, 'response', credentials=credentials)\n        self.merge_stats(block.execution_stats)\n        return response['response']\n\n    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n        object_input_data = AIStructuredResponseGeneratorBlock.Input(**{attr: getattr(input_data, attr) for attr in input_data.model_fields}, expected_format={})\n        yield ('response', self.llm_call(object_input_data, credentials))\nclass Input(BlockSchema):\n    prompt: str = SchemaField(description='The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces.', placeholder='Enter your prompt here...')\n    model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)\n    credentials: AICredentials = AICredentialsField()\n    sys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')\n    retry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')\n    prompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')\n    ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n    max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\nprompt: str = SchemaField(description='The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces.', placeholder='Enter your prompt here...')\nmodel: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for answering the prompt.', advanced=False)\ncredentials: AICredentials = AICredentialsField()\nsys_prompt: str = SchemaField(title='System Prompt', default='', description='The system prompt to provide additional context to the model.')\nretry: int = SchemaField(title='Retry Count', default=3, description='Number of times to retry the LLM call if the response does not match the expected format.')\nprompt_values: dict[str, str] = SchemaField(advanced=False, default={}, description='Values used to fill in the prompt.')\nollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\nmax_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\nclass Output(BlockSchema):\n    response: str = SchemaField(description='The response generated by the language model.')\n    error: str = SchemaField(description='Error message if the API call failed.')\nresponse: str = SchemaField(description='The response generated by the language model.')\nerror: str = SchemaField(description='Error message if the API call failed.')\ndef __init__(self):\n    super().__init__(id='1f292d4a-41a4-4977-9684-7c8d560b9f91', description='Call a Large Language Model (LLM) to generate a string based on the given prompt.', categories={BlockCategory.AI}, input_schema=AITextGeneratorBlock.Input, output_schema=AITextGeneratorBlock.Output, test_input={'prompt': 'User prompt', 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('response', 'Response text'), test_mock={'llm_call': lambda *args, **kwargs: 'Response text'})\nsuper().__init__()\ndef llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> str:\n    block = AIStructuredResponseGeneratorBlock()\n    response = block.run_once(input_data, 'response', credentials=credentials)\n    self.merge_stats(block.execution_stats)\n    return response['response']\nblock = AIStructuredResponseGeneratorBlock()\nresponse = block.run_once(input_data, 'response', credentials=credentials)\nself.merge_stats(block.execution_stats)\nreturn response['response']"
    },
    {
      "id": "n78",
      "type": "block",
      "statements": [
        "(parsed_dict, parsed_error) = parse_response(response_text)",
        "not parsed_error"
      ],
      "code": "(parsed_dict, parsed_error) = parse_response(response_text)\nnot parsed_error"
    },
    {
      "id": "n79",
      "type": "block",
      "statements": [
        "(yield ('response', {'response': response_text}))",
        "return"
      ],
      "code": "(yield ('response', {'response': response_text}))\nreturn"
    },
    {
      "id": "n80",
      "type": "block",
      "statements": [
        "retry_prompt = trim_prompt(f'\\n                  |This is your previous error response:\\n                  |--\\n                  |{response_text}\\n                  |--\\n                  |\\n                  |And this is the error:\\n                  |--\\n                  |{parsed_error}\\n                  |--\\n                ')",
        "prompt.append({'role': 'user', 'content': retry_prompt})",
        "logger.exception(f'Error calling LLM: {e}')",
        "retry_prompt = f'Error calling LLM: {e}'",
        "self.merge_stats({'llm_call_count': retry_count + 1, 'llm_retry_count': retry_count})"
      ],
      "code": "retry_prompt = trim_prompt(f'\\n                  |This is your previous error response:\\n                  |--\\n                  |{response_text}\\n                  |--\\n                  |\\n                  |And this is the error:\\n                  |--\\n                  |{parsed_error}\\n                  |--\\n                ')\nprompt.append({'role': 'user', 'content': retry_prompt})\nlogger.exception(f'Error calling LLM: {e}')\nretry_prompt = f'Error calling LLM: {e}'\nself.merge_stats({'llm_call_count': retry_count + 1, 'llm_retry_count': retry_count})"
    },
    {
      "id": "n81",
      "type": "block",
      "statements": [
        "(yield ('response', {k: json.loads(v) if isinstance(v, str) and v.startswith('[') and v.endswith(']') else ', '.join(v) if isinstance(v, list) else v for (k, v) in parsed_dict.items()}))",
        "return"
      ],
      "code": "(yield ('response', {k: json.loads(v) if isinstance(v, str) and v.startswith('[') and v.endswith(']') else ', '.join(v) if isinstance(v, list) else v for (k, v) in parsed_dict.items()}))\nreturn"
    },
    {
      "id": "n82",
      "type": "block",
      "statements": [],
      "code": "\n"
    },
    {
      "id": "n83",
      "type": "block",
      "statements": [
        "def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n    object_input_data = AIStructuredResponseGeneratorBlock.Input(**{attr: getattr(input_data, attr) for attr in input_data.model_fields}, expected_format={})\n    yield ('response', self.llm_call(object_input_data, credentials))",
        "object_input_data = AIStructuredResponseGeneratorBlock.Input(**{attr: getattr(input_data, attr) for attr in input_data.model_fields}, expected_format={})",
        "(yield ('response', self.llm_call(object_input_data, credentials)))",
        "class SummaryStyle(Enum):\n    CONCISE = 'concise'\n    DETAILED = 'detailed'\n    BULLET_POINTS = 'bullet points'\n    NUMBERED_LIST = 'numbered list'",
        "CONCISE = 'concise'",
        "DETAILED = 'detailed'",
        "BULLET_POINTS = 'bullet points'",
        "NUMBERED_LIST = 'numbered list'",
        "class AITextSummarizerBlock(Block):\n\n    class Input(BlockSchema):\n        text: str = SchemaField(description='The text to summarize.', placeholder='Enter the text to summarize here...')\n        model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for summarizing the text.')\n        focus: str = SchemaField(title='Focus', default='general information', description='The topic to focus on in the summary')\n        style: SummaryStyle = SchemaField(title='Summary Style', default=SummaryStyle.CONCISE, description='The style of the summary to generate.')\n        credentials: AICredentials = AICredentialsField()\n        max_tokens: int = SchemaField(title='Max Tokens', default=4096, description='The maximum number of tokens to generate in the chat completion.', ge=1)\n        chunk_overlap: int = SchemaField(title='Chunk Overlap', default=100, description='The number of overlapping tokens between chunks to maintain context.', ge=0)\n        ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n\n    class Output(BlockSchema):\n        summary: str = SchemaField(description='The final summary of the text.')\n        error: str = SchemaField(description='Error message if the API call failed.')\n\n    def __init__(self):\n        super().__init__(id='a0a69be1-4528-491c-a85a-a4ab6873e3f0', description='Utilize a Large Language Model (LLM) to summarize a long text.', categories={BlockCategory.AI, BlockCategory.TEXT}, input_schema=AITextSummarizerBlock.Input, output_schema=AITextSummarizerBlock.Output, test_input={'text': 'Lorem ipsum...' * 100, 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('summary', 'Final summary of a long text'), test_mock={'llm_call': lambda input_data, credentials: {'final_summary': 'Final summary of a long text'} if 'final_summary' in input_data.expected_format else {'summary': 'Summary of a chunk of text'}})\n\n    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n        for output in self._run(input_data, credentials):\n            yield output\n\n    def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:\n        chunks = self._split_text(input_data.text, input_data.max_tokens, input_data.chunk_overlap)\n        summaries = []\n        for chunk in chunks:\n            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)\n            summaries.append(chunk_summary)\n        final_summary = self._combine_summaries(summaries, input_data, credentials)\n        yield ('summary', final_summary)\n\n    @staticmethod\n    def _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:\n        words = text.split()\n        chunks = []\n        chunk_size = max_tokens - overlap\n        for i in range(0, len(words), chunk_size):\n            chunk = ' '.join(words[i:i + max_tokens])\n            chunks.append(chunk)\n        return chunks\n\n    def llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> dict:\n        block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, 'response', credentials=credentials)\n        self.merge_stats(block.execution_stats)\n        return response\n\n    def _summarize_chunk(self, chunk: str, input_data: Input, credentials: APIKeyCredentials) -> str:\n        prompt = f'Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```'\n        llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'summary': 'The summary of the given text.'}), credentials=credentials)\n        return llm_response['summary']\n\n    def _combine_summaries(self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials) -> str:\n        combined_text = '\\n\\n'.join(summaries)\n        if len(combined_text.split()) <= input_data.max_tokens:\n            prompt = f'Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.'\n            llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'final_summary': 'The final summary of all provided summaries.'}), credentials=credentials)\n            return llm_response['final_summary']\n        else:\n            return self._run(AITextSummarizerBlock.Input(text=combined_text, credentials=input_data.credentials, model=input_data.model, max_tokens=input_data.max_tokens, chunk_overlap=input_data.chunk_overlap), credentials=credentials).send(None)[1]",
        "class Input(BlockSchema):\n    text: str = SchemaField(description='The text to summarize.', placeholder='Enter the text to summarize here...')\n    model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for summarizing the text.')\n    focus: str = SchemaField(title='Focus', default='general information', description='The topic to focus on in the summary')\n    style: SummaryStyle = SchemaField(title='Summary Style', default=SummaryStyle.CONCISE, description='The style of the summary to generate.')\n    credentials: AICredentials = AICredentialsField()\n    max_tokens: int = SchemaField(title='Max Tokens', default=4096, description='The maximum number of tokens to generate in the chat completion.', ge=1)\n    chunk_overlap: int = SchemaField(title='Chunk Overlap', default=100, description='The number of overlapping tokens between chunks to maintain context.', ge=0)\n    ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')",
        "text: str = SchemaField(description='The text to summarize.', placeholder='Enter the text to summarize here...')",
        "model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for summarizing the text.')",
        "focus: str = SchemaField(title='Focus', default='general information', description='The topic to focus on in the summary')",
        "style: SummaryStyle = SchemaField(title='Summary Style', default=SummaryStyle.CONCISE, description='The style of the summary to generate.')",
        "credentials: AICredentials = AICredentialsField()",
        "max_tokens: int = SchemaField(title='Max Tokens', default=4096, description='The maximum number of tokens to generate in the chat completion.', ge=1)",
        "chunk_overlap: int = SchemaField(title='Chunk Overlap', default=100, description='The number of overlapping tokens between chunks to maintain context.', ge=0)",
        "ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')",
        "class Output(BlockSchema):\n    summary: str = SchemaField(description='The final summary of the text.')\n    error: str = SchemaField(description='Error message if the API call failed.')",
        "summary: str = SchemaField(description='The final summary of the text.')",
        "error: str = SchemaField(description='Error message if the API call failed.')",
        "def __init__(self):\n    super().__init__(id='a0a69be1-4528-491c-a85a-a4ab6873e3f0', description='Utilize a Large Language Model (LLM) to summarize a long text.', categories={BlockCategory.AI, BlockCategory.TEXT}, input_schema=AITextSummarizerBlock.Input, output_schema=AITextSummarizerBlock.Output, test_input={'text': 'Lorem ipsum...' * 100, 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('summary', 'Final summary of a long text'), test_mock={'llm_call': lambda input_data, credentials: {'final_summary': 'Final summary of a long text'} if 'final_summary' in input_data.expected_format else {'summary': 'Summary of a chunk of text'}})",
        "super().__init__()",
        "def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n    for output in self._run(input_data, credentials):\n        yield output"
      ],
      "code": "def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n    object_input_data = AIStructuredResponseGeneratorBlock.Input(**{attr: getattr(input_data, attr) for attr in input_data.model_fields}, expected_format={})\n    yield ('response', self.llm_call(object_input_data, credentials))\nobject_input_data = AIStructuredResponseGeneratorBlock.Input(**{attr: getattr(input_data, attr) for attr in input_data.model_fields}, expected_format={})\n(yield ('response', self.llm_call(object_input_data, credentials)))\nclass SummaryStyle(Enum):\n    CONCISE = 'concise'\n    DETAILED = 'detailed'\n    BULLET_POINTS = 'bullet points'\n    NUMBERED_LIST = 'numbered list'\nCONCISE = 'concise'\nDETAILED = 'detailed'\nBULLET_POINTS = 'bullet points'\nNUMBERED_LIST = 'numbered list'\nclass AITextSummarizerBlock(Block):\n\n    class Input(BlockSchema):\n        text: str = SchemaField(description='The text to summarize.', placeholder='Enter the text to summarize here...')\n        model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for summarizing the text.')\n        focus: str = SchemaField(title='Focus', default='general information', description='The topic to focus on in the summary')\n        style: SummaryStyle = SchemaField(title='Summary Style', default=SummaryStyle.CONCISE, description='The style of the summary to generate.')\n        credentials: AICredentials = AICredentialsField()\n        max_tokens: int = SchemaField(title='Max Tokens', default=4096, description='The maximum number of tokens to generate in the chat completion.', ge=1)\n        chunk_overlap: int = SchemaField(title='Chunk Overlap', default=100, description='The number of overlapping tokens between chunks to maintain context.', ge=0)\n        ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n\n    class Output(BlockSchema):\n        summary: str = SchemaField(description='The final summary of the text.')\n        error: str = SchemaField(description='Error message if the API call failed.')\n\n    def __init__(self):\n        super().__init__(id='a0a69be1-4528-491c-a85a-a4ab6873e3f0', description='Utilize a Large Language Model (LLM) to summarize a long text.', categories={BlockCategory.AI, BlockCategory.TEXT}, input_schema=AITextSummarizerBlock.Input, output_schema=AITextSummarizerBlock.Output, test_input={'text': 'Lorem ipsum...' * 100, 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('summary', 'Final summary of a long text'), test_mock={'llm_call': lambda input_data, credentials: {'final_summary': 'Final summary of a long text'} if 'final_summary' in input_data.expected_format else {'summary': 'Summary of a chunk of text'}})\n\n    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n        for output in self._run(input_data, credentials):\n            yield output\n\n    def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:\n        chunks = self._split_text(input_data.text, input_data.max_tokens, input_data.chunk_overlap)\n        summaries = []\n        for chunk in chunks:\n            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)\n            summaries.append(chunk_summary)\n        final_summary = self._combine_summaries(summaries, input_data, credentials)\n        yield ('summary', final_summary)\n\n    @staticmethod\n    def _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:\n        words = text.split()\n        chunks = []\n        chunk_size = max_tokens - overlap\n        for i in range(0, len(words), chunk_size):\n            chunk = ' '.join(words[i:i + max_tokens])\n            chunks.append(chunk)\n        return chunks\n\n    def llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> dict:\n        block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, 'response', credentials=credentials)\n        self.merge_stats(block.execution_stats)\n        return response\n\n    def _summarize_chunk(self, chunk: str, input_data: Input, credentials: APIKeyCredentials) -> str:\n        prompt = f'Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```'\n        llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'summary': 'The summary of the given text.'}), credentials=credentials)\n        return llm_response['summary']\n\n    def _combine_summaries(self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials) -> str:\n        combined_text = '\\n\\n'.join(summaries)\n        if len(combined_text.split()) <= input_data.max_tokens:\n            prompt = f'Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.'\n            llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'final_summary': 'The final summary of all provided summaries.'}), credentials=credentials)\n            return llm_response['final_summary']\n        else:\n            return self._run(AITextSummarizerBlock.Input(text=combined_text, credentials=input_data.credentials, model=input_data.model, max_tokens=input_data.max_tokens, chunk_overlap=input_data.chunk_overlap), credentials=credentials).send(None)[1]\nclass Input(BlockSchema):\n    text: str = SchemaField(description='The text to summarize.', placeholder='Enter the text to summarize here...')\n    model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for summarizing the text.')\n    focus: str = SchemaField(title='Focus', default='general information', description='The topic to focus on in the summary')\n    style: SummaryStyle = SchemaField(title='Summary Style', default=SummaryStyle.CONCISE, description='The style of the summary to generate.')\n    credentials: AICredentials = AICredentialsField()\n    max_tokens: int = SchemaField(title='Max Tokens', default=4096, description='The maximum number of tokens to generate in the chat completion.', ge=1)\n    chunk_overlap: int = SchemaField(title='Chunk Overlap', default=100, description='The number of overlapping tokens between chunks to maintain context.', ge=0)\n    ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\ntext: str = SchemaField(description='The text to summarize.', placeholder='Enter the text to summarize here...')\nmodel: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for summarizing the text.')\nfocus: str = SchemaField(title='Focus', default='general information', description='The topic to focus on in the summary')\nstyle: SummaryStyle = SchemaField(title='Summary Style', default=SummaryStyle.CONCISE, description='The style of the summary to generate.')\ncredentials: AICredentials = AICredentialsField()\nmax_tokens: int = SchemaField(title='Max Tokens', default=4096, description='The maximum number of tokens to generate in the chat completion.', ge=1)\nchunk_overlap: int = SchemaField(title='Chunk Overlap', default=100, description='The number of overlapping tokens between chunks to maintain context.', ge=0)\nollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\nclass Output(BlockSchema):\n    summary: str = SchemaField(description='The final summary of the text.')\n    error: str = SchemaField(description='Error message if the API call failed.')\nsummary: str = SchemaField(description='The final summary of the text.')\nerror: str = SchemaField(description='Error message if the API call failed.')\ndef __init__(self):\n    super().__init__(id='a0a69be1-4528-491c-a85a-a4ab6873e3f0', description='Utilize a Large Language Model (LLM) to summarize a long text.', categories={BlockCategory.AI, BlockCategory.TEXT}, input_schema=AITextSummarizerBlock.Input, output_schema=AITextSummarizerBlock.Output, test_input={'text': 'Lorem ipsum...' * 100, 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('summary', 'Final summary of a long text'), test_mock={'llm_call': lambda input_data, credentials: {'final_summary': 'Final summary of a long text'} if 'final_summary' in input_data.expected_format else {'summary': 'Summary of a chunk of text'}})\nsuper().__init__()\ndef run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n    for output in self._run(input_data, credentials):\n        yield output"
    },
    {
      "id": "n84",
      "type": "block",
      "statements": [
        "output",
        "self._run(input_data, credentials)"
      ],
      "code": "output\nself._run(input_data, credentials)"
    },
    {
      "id": "n85",
      "type": "block",
      "statements": [
        "(yield output)"
      ],
      "code": "(yield output)"
    },
    {
      "id": "n86",
      "type": "block",
      "statements": [
        "def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:\n    chunks = self._split_text(input_data.text, input_data.max_tokens, input_data.chunk_overlap)\n    summaries = []\n    for chunk in chunks:\n        chunk_summary = self._summarize_chunk(chunk, input_data, credentials)\n        summaries.append(chunk_summary)\n    final_summary = self._combine_summaries(summaries, input_data, credentials)\n    yield ('summary', final_summary)",
        "chunks = self._split_text(input_data.text, input_data.max_tokens, input_data.chunk_overlap)",
        "summaries = []"
      ],
      "code": "def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:\n    chunks = self._split_text(input_data.text, input_data.max_tokens, input_data.chunk_overlap)\n    summaries = []\n    for chunk in chunks:\n        chunk_summary = self._summarize_chunk(chunk, input_data, credentials)\n        summaries.append(chunk_summary)\n    final_summary = self._combine_summaries(summaries, input_data, credentials)\n    yield ('summary', final_summary)\nchunks = self._split_text(input_data.text, input_data.max_tokens, input_data.chunk_overlap)\nsummaries = []"
    },
    {
      "id": "n87",
      "type": "block",
      "statements": [
        "chunk",
        "chunks"
      ],
      "code": "chunk\nchunks"
    },
    {
      "id": "n88",
      "type": "block",
      "statements": [
        "chunk_summary = self._summarize_chunk(chunk, input_data, credentials)",
        "summaries.append(chunk_summary)"
      ],
      "code": "chunk_summary = self._summarize_chunk(chunk, input_data, credentials)\nsummaries.append(chunk_summary)"
    },
    {
      "id": "n89",
      "type": "block",
      "statements": [
        "final_summary = self._combine_summaries(summaries, input_data, credentials)",
        "(yield ('summary', final_summary))",
        "@staticmethod\ndef _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:\n    words = text.split()\n    chunks = []\n    chunk_size = max_tokens - overlap\n    for i in range(0, len(words), chunk_size):\n        chunk = ' '.join(words[i:i + max_tokens])\n        chunks.append(chunk)\n    return chunks",
        "words = text.split()",
        "chunks = []",
        "chunk_size = max_tokens - overlap"
      ],
      "code": "final_summary = self._combine_summaries(summaries, input_data, credentials)\n(yield ('summary', final_summary))\n@staticmethod\ndef _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:\n    words = text.split()\n    chunks = []\n    chunk_size = max_tokens - overlap\n    for i in range(0, len(words), chunk_size):\n        chunk = ' '.join(words[i:i + max_tokens])\n        chunks.append(chunk)\n    return chunks\nwords = text.split()\nchunks = []\nchunk_size = max_tokens - overlap"
    },
    {
      "id": "n90",
      "type": "block",
      "statements": [
        "i",
        "range(0, len(words), chunk_size)"
      ],
      "code": "i\nrange(0, len(words), chunk_size)"
    },
    {
      "id": "n91",
      "type": "block",
      "statements": [
        "chunk = ' '.join(words[i:i + max_tokens])",
        "chunks.append(chunk)"
      ],
      "code": "chunk = ' '.join(words[i:i + max_tokens])\nchunks.append(chunk)"
    },
    {
      "id": "n92",
      "type": "block",
      "statements": [
        "return chunks"
      ],
      "code": "return chunks"
    },
    {
      "id": "n93",
      "type": "block",
      "statements": [
        "def llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> dict:\n    block = AIStructuredResponseGeneratorBlock()\n    response = block.run_once(input_data, 'response', credentials=credentials)\n    self.merge_stats(block.execution_stats)\n    return response",
        "block = AIStructuredResponseGeneratorBlock()",
        "response = block.run_once(input_data, 'response', credentials=credentials)",
        "self.merge_stats(block.execution_stats)",
        "return response"
      ],
      "code": "def llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> dict:\n    block = AIStructuredResponseGeneratorBlock()\n    response = block.run_once(input_data, 'response', credentials=credentials)\n    self.merge_stats(block.execution_stats)\n    return response\nblock = AIStructuredResponseGeneratorBlock()\nresponse = block.run_once(input_data, 'response', credentials=credentials)\nself.merge_stats(block.execution_stats)\nreturn response"
    },
    {
      "id": "n94",
      "type": "block",
      "statements": [
        "def _summarize_chunk(self, chunk: str, input_data: Input, credentials: APIKeyCredentials) -> str:\n    prompt = f'Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```'\n    llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'summary': 'The summary of the given text.'}), credentials=credentials)\n    return llm_response['summary']",
        "prompt = f'Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```'",
        "llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'summary': 'The summary of the given text.'}), credentials=credentials)",
        "return llm_response['summary']"
      ],
      "code": "def _summarize_chunk(self, chunk: str, input_data: Input, credentials: APIKeyCredentials) -> str:\n    prompt = f'Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```'\n    llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'summary': 'The summary of the given text.'}), credentials=credentials)\n    return llm_response['summary']\nprompt = f'Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```'\nllm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'summary': 'The summary of the given text.'}), credentials=credentials)\nreturn llm_response['summary']"
    },
    {
      "id": "n95",
      "type": "block",
      "statements": [
        "def _combine_summaries(self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials) -> str:\n    combined_text = '\\n\\n'.join(summaries)\n    if len(combined_text.split()) <= input_data.max_tokens:\n        prompt = f'Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.'\n        llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'final_summary': 'The final summary of all provided summaries.'}), credentials=credentials)\n        return llm_response['final_summary']\n    else:\n        return self._run(AITextSummarizerBlock.Input(text=combined_text, credentials=input_data.credentials, model=input_data.model, max_tokens=input_data.max_tokens, chunk_overlap=input_data.chunk_overlap), credentials=credentials).send(None)[1]",
        "combined_text = '\\n\\n'.join(summaries)",
        "len(combined_text.split()) LtE input_data.max_tokens"
      ],
      "code": "def _combine_summaries(self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials) -> str:\n    combined_text = '\\n\\n'.join(summaries)\n    if len(combined_text.split()) <= input_data.max_tokens:\n        prompt = f'Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.'\n        llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'final_summary': 'The final summary of all provided summaries.'}), credentials=credentials)\n        return llm_response['final_summary']\n    else:\n        return self._run(AITextSummarizerBlock.Input(text=combined_text, credentials=input_data.credentials, model=input_data.model, max_tokens=input_data.max_tokens, chunk_overlap=input_data.chunk_overlap), credentials=credentials).send(None)[1]\ncombined_text = '\\n\\n'.join(summaries)\nlen(combined_text.split()) LtE input_data.max_tokens"
    },
    {
      "id": "n96",
      "type": "block",
      "statements": [
        "prompt = f'Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.'",
        "llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'final_summary': 'The final summary of all provided summaries.'}), credentials=credentials)",
        "return llm_response['final_summary']"
      ],
      "code": "prompt = f'Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.'\nllm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={'final_summary': 'The final summary of all provided summaries.'}), credentials=credentials)\nreturn llm_response['final_summary']"
    },
    {
      "id": "n97",
      "type": "block",
      "statements": [
        "return self._run(AITextSummarizerBlock.Input(text=combined_text, credentials=input_data.credentials, model=input_data.model, max_tokens=input_data.max_tokens, chunk_overlap=input_data.chunk_overlap), credentials=credentials).send(None)[1]"
      ],
      "code": "return self._run(AITextSummarizerBlock.Input(text=combined_text, credentials=input_data.credentials, model=input_data.model, max_tokens=input_data.max_tokens, chunk_overlap=input_data.chunk_overlap), credentials=credentials).send(None)[1]"
    },
    {
      "id": "n98",
      "type": "block",
      "statements": [
        "class AIConversationBlock(Block):\n\n    class Input(BlockSchema):\n        messages: List[Message] = SchemaField(description='List of messages in the conversation.', min_length=1)\n        model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for the conversation.')\n        credentials: AICredentials = AICredentialsField()\n        max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n        ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n\n    class Output(BlockSchema):\n        response: str = SchemaField(description=\"The model's response to the conversation.\")\n        error: str = SchemaField(description='Error message if the API call failed.')\n\n    def __init__(self):\n        super().__init__(id='32a87eab-381e-4dd4-bdb8-4c47151be35a', description='Advanced LLM call that takes a list of messages and sends them to the language model.', categories={BlockCategory.AI}, input_schema=AIConversationBlock.Input, output_schema=AIConversationBlock.Output, test_input={'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who won the world series in 2020?'}, {'role': 'assistant', 'content': 'The Los Angeles Dodgers won the World Series in 2020.'}, {'role': 'user', 'content': 'Where was it played?'}], 'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('response', 'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'), test_mock={'llm_call': lambda *args, **kwargs: 'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'})\n\n    def llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> str:\n        block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, 'response', credentials=credentials)\n        self.merge_stats(block.execution_stats)\n        return response['response']\n\n    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n        response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt='', credentials=input_data.credentials, model=input_data.model, conversation_history=input_data.messages, max_tokens=input_data.max_tokens, expected_format={}), credentials=credentials)\n        yield ('response', response)",
        "class Input(BlockSchema):\n    messages: List[Message] = SchemaField(description='List of messages in the conversation.', min_length=1)\n    model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for the conversation.')\n    credentials: AICredentials = AICredentialsField()\n    max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n    ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')",
        "messages: List[Message] = SchemaField(description='List of messages in the conversation.', min_length=1)",
        "model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for the conversation.')",
        "credentials: AICredentials = AICredentialsField()",
        "max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')",
        "ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')",
        "class Output(BlockSchema):\n    response: str = SchemaField(description=\"The model's response to the conversation.\")\n    error: str = SchemaField(description='Error message if the API call failed.')",
        "response: str = SchemaField(description=\"The model's response to the conversation.\")",
        "error: str = SchemaField(description='Error message if the API call failed.')",
        "def __init__(self):\n    super().__init__(id='32a87eab-381e-4dd4-bdb8-4c47151be35a', description='Advanced LLM call that takes a list of messages and sends them to the language model.', categories={BlockCategory.AI}, input_schema=AIConversationBlock.Input, output_schema=AIConversationBlock.Output, test_input={'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who won the world series in 2020?'}, {'role': 'assistant', 'content': 'The Los Angeles Dodgers won the World Series in 2020.'}, {'role': 'user', 'content': 'Where was it played?'}], 'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('response', 'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'), test_mock={'llm_call': lambda *args, **kwargs: 'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'})",
        "super().__init__()",
        "def llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> str:\n    block = AIStructuredResponseGeneratorBlock()\n    response = block.run_once(input_data, 'response', credentials=credentials)\n    self.merge_stats(block.execution_stats)\n    return response['response']",
        "block = AIStructuredResponseGeneratorBlock()",
        "response = block.run_once(input_data, 'response', credentials=credentials)",
        "self.merge_stats(block.execution_stats)",
        "return response['response']"
      ],
      "code": "class AIConversationBlock(Block):\n\n    class Input(BlockSchema):\n        messages: List[Message] = SchemaField(description='List of messages in the conversation.', min_length=1)\n        model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for the conversation.')\n        credentials: AICredentials = AICredentialsField()\n        max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n        ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n\n    class Output(BlockSchema):\n        response: str = SchemaField(description=\"The model's response to the conversation.\")\n        error: str = SchemaField(description='Error message if the API call failed.')\n\n    def __init__(self):\n        super().__init__(id='32a87eab-381e-4dd4-bdb8-4c47151be35a', description='Advanced LLM call that takes a list of messages and sends them to the language model.', categories={BlockCategory.AI}, input_schema=AIConversationBlock.Input, output_schema=AIConversationBlock.Output, test_input={'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who won the world series in 2020?'}, {'role': 'assistant', 'content': 'The Los Angeles Dodgers won the World Series in 2020.'}, {'role': 'user', 'content': 'Where was it played?'}], 'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('response', 'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'), test_mock={'llm_call': lambda *args, **kwargs: 'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'})\n\n    def llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> str:\n        block = AIStructuredResponseGeneratorBlock()\n        response = block.run_once(input_data, 'response', credentials=credentials)\n        self.merge_stats(block.execution_stats)\n        return response['response']\n\n    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n        response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt='', credentials=input_data.credentials, model=input_data.model, conversation_history=input_data.messages, max_tokens=input_data.max_tokens, expected_format={}), credentials=credentials)\n        yield ('response', response)\nclass Input(BlockSchema):\n    messages: List[Message] = SchemaField(description='List of messages in the conversation.', min_length=1)\n    model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for the conversation.')\n    credentials: AICredentials = AICredentialsField()\n    max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n    ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\nmessages: List[Message] = SchemaField(description='List of messages in the conversation.', min_length=1)\nmodel: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for the conversation.')\ncredentials: AICredentials = AICredentialsField()\nmax_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\nollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\nclass Output(BlockSchema):\n    response: str = SchemaField(description=\"The model's response to the conversation.\")\n    error: str = SchemaField(description='Error message if the API call failed.')\nresponse: str = SchemaField(description=\"The model's response to the conversation.\")\nerror: str = SchemaField(description='Error message if the API call failed.')\ndef __init__(self):\n    super().__init__(id='32a87eab-381e-4dd4-bdb8-4c47151be35a', description='Advanced LLM call that takes a list of messages and sends them to the language model.', categories={BlockCategory.AI}, input_schema=AIConversationBlock.Input, output_schema=AIConversationBlock.Output, test_input={'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who won the world series in 2020?'}, {'role': 'assistant', 'content': 'The Los Angeles Dodgers won the World Series in 2020.'}, {'role': 'user', 'content': 'Where was it played?'}], 'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT}, test_credentials=TEST_CREDENTIALS, test_output=('response', 'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'), test_mock={'llm_call': lambda *args, **kwargs: 'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'})\nsuper().__init__()\ndef llm_call(self, input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> str:\n    block = AIStructuredResponseGeneratorBlock()\n    response = block.run_once(input_data, 'response', credentials=credentials)\n    self.merge_stats(block.execution_stats)\n    return response['response']\nblock = AIStructuredResponseGeneratorBlock()\nresponse = block.run_once(input_data, 'response', credentials=credentials)\nself.merge_stats(block.execution_stats)\nreturn response['response']"
    },
    {
      "id": "n99",
      "type": "block",
      "statements": [
        "def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n    response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt='', credentials=input_data.credentials, model=input_data.model, conversation_history=input_data.messages, max_tokens=input_data.max_tokens, expected_format={}), credentials=credentials)\n    yield ('response', response)",
        "response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt='', credentials=input_data.credentials, model=input_data.model, conversation_history=input_data.messages, max_tokens=input_data.max_tokens, expected_format={}), credentials=credentials)",
        "(yield ('response', response))",
        "class AIListGeneratorBlock(Block):\n\n    class Input(BlockSchema):\n        focus: str | None = SchemaField(description='The focus of the list to generate.', placeholder='The top 5 most interesting news stories in the data.', default=None, advanced=False)\n        source_data: str | None = SchemaField(description='The data to generate the list from.', placeholder='News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released.', default=None, advanced=False)\n        model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for generating the list.', advanced=True)\n        credentials: AICredentials = AICredentialsField()\n        max_retries: int = SchemaField(default=3, description='Maximum number of retries for generating a valid list.', ge=1, le=5)\n        max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n        ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n\n    class Output(BlockSchema):\n        generated_list: List[str] = SchemaField(description='The generated list.')\n        list_item: str = SchemaField(description='Each individual item in the list.')\n        error: str = SchemaField(description='Error message if the list generation failed.')\n\n    def __init__(self):\n        super().__init__(id='9c0b0450-d199-458b-a731-072189dd6593', description='Generate a Python list based on the given prompt using a Large Language Model (LLM).', categories={BlockCategory.AI, BlockCategory.TEXT}, input_schema=AIListGeneratorBlock.Input, output_schema=AIListGeneratorBlock.Output, test_input={'focus': 'planets', 'source_data': \"Zylora Prime is a glowing jungle world with bioluminescent plants, while Kharon-9 is a harsh desert planet with underground cities. Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of fictional worlds.\", 'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT, 'max_retries': 3}, test_credentials=TEST_CREDENTIALS, test_output=[('generated_list', ['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']), ('list_item', 'Zylora Prime'), ('list_item', 'Kharon-9'), ('list_item', 'Vortexia'), ('list_item', 'Oceara'), ('list_item', 'Draknos')], test_mock={'llm_call': lambda input_data, credentials: {'response': \"['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']\"}})\n\n    @staticmethod\n    def llm_call(input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> dict[str, str]:\n        llm_block = AIStructuredResponseGeneratorBlock()\n        response = llm_block.run_once(input_data, 'response', credentials=credentials)\n        return response\n\n    @staticmethod\n    def string_to_list(string):\n        \"\"\"\n        Converts a string representation of a list into an actual Python list object.\n        \"\"\"\n        logger.debug(f'Converting string to list. Input string: {string}')\n        try:\n            python_list = ast.literal_eval(string)\n            if isinstance(python_list, list):\n                logger.debug(f'Successfully converted string to list: {python_list}')\n                return python_list\n            else:\n                logger.error(f\"The provided string '{string}' is not a valid list\")\n                raise ValueError(f\"The provided string '{string}' is not a valid list.\")\n        except (SyntaxError, ValueError) as e:\n            logger.error(f'Failed to convert string to list: {e}')\n            raise ValueError('Invalid list format. Could not convert to list.')\n\n    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n        logger.debug(f'Starting AIListGeneratorBlock.run with input data: {input_data}')\n        api_key_check = credentials.api_key.get_secret_value()\n        if not api_key_check:\n            raise ValueError('No LLM API key provided.')\n        sys_prompt = \"You are a Python list generator. Your task is to generate a Python list based on the user's prompt. \\n            |Respond ONLY with a valid python list. \\n            |The list can contain strings, numbers, or nested lists as appropriate. \\n            |Do not include any explanations or additional text.\\n\\n            |Valid Example string formats:\\n\\n            |Example 1:\\n            |```\\n            |['1', '2', '3', '4']\\n            |```\\n\\n            |Example 2:\\n            |```\\n            |[['1', '2'], ['3', '4'], ['5', '6']]\\n            |```\\n\\n            |Example 3:\\n            |```\\n            |['1', ['2', '3'], ['4', ['5', '6']]]\\n            |```\\n\\n            |Example 4:\\n            |```\\n            |['a', 'b', 'c']\\n            |```\\n\\n            |Example 5:\\n            |```\\n            |['1', '2.5', 'string', 'True', ['False', 'None']]\\n            |```\\n\\n            |Do not include any explanations or additional text, just respond with the list in the format specified above.\\n            \"\n        if input_data.focus:\n            prompt = f'Generate a list with the following focus:\\n<focus>\\n\\n{input_data.focus}</focus>'\n        elif input_data.source_data:\n            prompt = 'Extract the main focus of the source data to a list.\\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.'\n        else:\n            prompt = 'Generate a random list.'\n        if input_data.source_data:\n            prompt += f'\\n\\nUse the following source data to generate the list from:\\n\\n<source_data>\\n\\n{input_data.source_data}</source_data>\\n\\nDo not invent fictional data that is not present in the source data.'\n        else:\n            prompt += '\\n\\nInvent the data to generate the list from.'\n        for attempt in range(input_data.max_retries):\n            try:\n                logger.debug('Calling LLM')\n                llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(sys_prompt=sys_prompt, prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={}, ollama_host=input_data.ollama_host), credentials=credentials)\n                logger.debug(f'LLM response: {llm_response}')\n                response_string = llm_response['response']\n                logger.debug(f'Response string: {response_string}')\n                logger.debug('Converting string to Python list')\n                parsed_list = self.string_to_list(response_string)\n                logger.debug(f'Parsed list: {parsed_list}')\n                logger.debug('Successfully generated a valid Python list')\n                yield ('generated_list', parsed_list)\n                for item in parsed_list:\n                    yield ('list_item', item)\n                return\n            except Exception as e:\n                logger.error(f'Error in attempt {attempt + 1}: {str(e)}')\n                if attempt == input_data.max_retries - 1:\n                    logger.error(f'Failed to generate a valid Python list after {input_data.max_retries} attempts')\n                    raise RuntimeError(f'Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}')\n                else:\n                    logger.debug('Preparing retry prompt')\n                    prompt = f'\\n                    The previous attempt failed due to `{e}`\\n                    Generate a valid Python list based on the original prompt.\\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\\n                    Original prompt: \\n                    ```{prompt}```\\n                    \\n                    Respond only with the list in the format specified with no commentary or apologies.\\n                    '\n                    logger.debug(f'Retry prompt: {prompt}')\n        logger.debug('AIListGeneratorBlock.run completed')",
        "class Input(BlockSchema):\n    focus: str | None = SchemaField(description='The focus of the list to generate.', placeholder='The top 5 most interesting news stories in the data.', default=None, advanced=False)\n    source_data: str | None = SchemaField(description='The data to generate the list from.', placeholder='News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released.', default=None, advanced=False)\n    model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for generating the list.', advanced=True)\n    credentials: AICredentials = AICredentialsField()\n    max_retries: int = SchemaField(default=3, description='Maximum number of retries for generating a valid list.', ge=1, le=5)\n    max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n    ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')",
        "focus: str | None = SchemaField(description='The focus of the list to generate.', placeholder='The top 5 most interesting news stories in the data.', default=None, advanced=False)",
        "source_data: str | None = SchemaField(description='The data to generate the list from.', placeholder='News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released.', default=None, advanced=False)",
        "model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for generating the list.', advanced=True)",
        "credentials: AICredentials = AICredentialsField()",
        "max_retries: int = SchemaField(default=3, description='Maximum number of retries for generating a valid list.', ge=1, le=5)",
        "max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')",
        "ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')",
        "class Output(BlockSchema):\n    generated_list: List[str] = SchemaField(description='The generated list.')\n    list_item: str = SchemaField(description='Each individual item in the list.')\n    error: str = SchemaField(description='Error message if the list generation failed.')",
        "generated_list: List[str] = SchemaField(description='The generated list.')",
        "list_item: str = SchemaField(description='Each individual item in the list.')",
        "error: str = SchemaField(description='Error message if the list generation failed.')",
        "def __init__(self):\n    super().__init__(id='9c0b0450-d199-458b-a731-072189dd6593', description='Generate a Python list based on the given prompt using a Large Language Model (LLM).', categories={BlockCategory.AI, BlockCategory.TEXT}, input_schema=AIListGeneratorBlock.Input, output_schema=AIListGeneratorBlock.Output, test_input={'focus': 'planets', 'source_data': \"Zylora Prime is a glowing jungle world with bioluminescent plants, while Kharon-9 is a harsh desert planet with underground cities. Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of fictional worlds.\", 'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT, 'max_retries': 3}, test_credentials=TEST_CREDENTIALS, test_output=[('generated_list', ['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']), ('list_item', 'Zylora Prime'), ('list_item', 'Kharon-9'), ('list_item', 'Vortexia'), ('list_item', 'Oceara'), ('list_item', 'Draknos')], test_mock={'llm_call': lambda input_data, credentials: {'response': \"['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']\"}})",
        "super().__init__()",
        "@staticmethod\ndef llm_call(input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> dict[str, str]:\n    llm_block = AIStructuredResponseGeneratorBlock()\n    response = llm_block.run_once(input_data, 'response', credentials=credentials)\n    return response",
        "llm_block = AIStructuredResponseGeneratorBlock()",
        "response = llm_block.run_once(input_data, 'response', credentials=credentials)",
        "return response"
      ],
      "code": "def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n    response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt='', credentials=input_data.credentials, model=input_data.model, conversation_history=input_data.messages, max_tokens=input_data.max_tokens, expected_format={}), credentials=credentials)\n    yield ('response', response)\nresponse = self.llm_call(AIStructuredResponseGeneratorBlock.Input(prompt='', credentials=input_data.credentials, model=input_data.model, conversation_history=input_data.messages, max_tokens=input_data.max_tokens, expected_format={}), credentials=credentials)\n(yield ('response', response))\nclass AIListGeneratorBlock(Block):\n\n    class Input(BlockSchema):\n        focus: str | None = SchemaField(description='The focus of the list to generate.', placeholder='The top 5 most interesting news stories in the data.', default=None, advanced=False)\n        source_data: str | None = SchemaField(description='The data to generate the list from.', placeholder='News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released.', default=None, advanced=False)\n        model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for generating the list.', advanced=True)\n        credentials: AICredentials = AICredentialsField()\n        max_retries: int = SchemaField(default=3, description='Maximum number of retries for generating a valid list.', ge=1, le=5)\n        max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n        ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\n\n    class Output(BlockSchema):\n        generated_list: List[str] = SchemaField(description='The generated list.')\n        list_item: str = SchemaField(description='Each individual item in the list.')\n        error: str = SchemaField(description='Error message if the list generation failed.')\n\n    def __init__(self):\n        super().__init__(id='9c0b0450-d199-458b-a731-072189dd6593', description='Generate a Python list based on the given prompt using a Large Language Model (LLM).', categories={BlockCategory.AI, BlockCategory.TEXT}, input_schema=AIListGeneratorBlock.Input, output_schema=AIListGeneratorBlock.Output, test_input={'focus': 'planets', 'source_data': \"Zylora Prime is a glowing jungle world with bioluminescent plants, while Kharon-9 is a harsh desert planet with underground cities. Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of fictional worlds.\", 'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT, 'max_retries': 3}, test_credentials=TEST_CREDENTIALS, test_output=[('generated_list', ['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']), ('list_item', 'Zylora Prime'), ('list_item', 'Kharon-9'), ('list_item', 'Vortexia'), ('list_item', 'Oceara'), ('list_item', 'Draknos')], test_mock={'llm_call': lambda input_data, credentials: {'response': \"['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']\"}})\n\n    @staticmethod\n    def llm_call(input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> dict[str, str]:\n        llm_block = AIStructuredResponseGeneratorBlock()\n        response = llm_block.run_once(input_data, 'response', credentials=credentials)\n        return response\n\n    @staticmethod\n    def string_to_list(string):\n        \"\"\"\n        Converts a string representation of a list into an actual Python list object.\n        \"\"\"\n        logger.debug(f'Converting string to list. Input string: {string}')\n        try:\n            python_list = ast.literal_eval(string)\n            if isinstance(python_list, list):\n                logger.debug(f'Successfully converted string to list: {python_list}')\n                return python_list\n            else:\n                logger.error(f\"The provided string '{string}' is not a valid list\")\n                raise ValueError(f\"The provided string '{string}' is not a valid list.\")\n        except (SyntaxError, ValueError) as e:\n            logger.error(f'Failed to convert string to list: {e}')\n            raise ValueError('Invalid list format. Could not convert to list.')\n\n    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n        logger.debug(f'Starting AIListGeneratorBlock.run with input data: {input_data}')\n        api_key_check = credentials.api_key.get_secret_value()\n        if not api_key_check:\n            raise ValueError('No LLM API key provided.')\n        sys_prompt = \"You are a Python list generator. Your task is to generate a Python list based on the user's prompt. \\n            |Respond ONLY with a valid python list. \\n            |The list can contain strings, numbers, or nested lists as appropriate. \\n            |Do not include any explanations or additional text.\\n\\n            |Valid Example string formats:\\n\\n            |Example 1:\\n            |```\\n            |['1', '2', '3', '4']\\n            |```\\n\\n            |Example 2:\\n            |```\\n            |[['1', '2'], ['3', '4'], ['5', '6']]\\n            |```\\n\\n            |Example 3:\\n            |```\\n            |['1', ['2', '3'], ['4', ['5', '6']]]\\n            |```\\n\\n            |Example 4:\\n            |```\\n            |['a', 'b', 'c']\\n            |```\\n\\n            |Example 5:\\n            |```\\n            |['1', '2.5', 'string', 'True', ['False', 'None']]\\n            |```\\n\\n            |Do not include any explanations or additional text, just respond with the list in the format specified above.\\n            \"\n        if input_data.focus:\n            prompt = f'Generate a list with the following focus:\\n<focus>\\n\\n{input_data.focus}</focus>'\n        elif input_data.source_data:\n            prompt = 'Extract the main focus of the source data to a list.\\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.'\n        else:\n            prompt = 'Generate a random list.'\n        if input_data.source_data:\n            prompt += f'\\n\\nUse the following source data to generate the list from:\\n\\n<source_data>\\n\\n{input_data.source_data}</source_data>\\n\\nDo not invent fictional data that is not present in the source data.'\n        else:\n            prompt += '\\n\\nInvent the data to generate the list from.'\n        for attempt in range(input_data.max_retries):\n            try:\n                logger.debug('Calling LLM')\n                llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(sys_prompt=sys_prompt, prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={}, ollama_host=input_data.ollama_host), credentials=credentials)\n                logger.debug(f'LLM response: {llm_response}')\n                response_string = llm_response['response']\n                logger.debug(f'Response string: {response_string}')\n                logger.debug('Converting string to Python list')\n                parsed_list = self.string_to_list(response_string)\n                logger.debug(f'Parsed list: {parsed_list}')\n                logger.debug('Successfully generated a valid Python list')\n                yield ('generated_list', parsed_list)\n                for item in parsed_list:\n                    yield ('list_item', item)\n                return\n            except Exception as e:\n                logger.error(f'Error in attempt {attempt + 1}: {str(e)}')\n                if attempt == input_data.max_retries - 1:\n                    logger.error(f'Failed to generate a valid Python list after {input_data.max_retries} attempts')\n                    raise RuntimeError(f'Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}')\n                else:\n                    logger.debug('Preparing retry prompt')\n                    prompt = f'\\n                    The previous attempt failed due to `{e}`\\n                    Generate a valid Python list based on the original prompt.\\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\\n                    Original prompt: \\n                    ```{prompt}```\\n                    \\n                    Respond only with the list in the format specified with no commentary or apologies.\\n                    '\n                    logger.debug(f'Retry prompt: {prompt}')\n        logger.debug('AIListGeneratorBlock.run completed')\nclass Input(BlockSchema):\n    focus: str | None = SchemaField(description='The focus of the list to generate.', placeholder='The top 5 most interesting news stories in the data.', default=None, advanced=False)\n    source_data: str | None = SchemaField(description='The data to generate the list from.', placeholder='News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released.', default=None, advanced=False)\n    model: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for generating the list.', advanced=True)\n    credentials: AICredentials = AICredentialsField()\n    max_retries: int = SchemaField(default=3, description='Maximum number of retries for generating a valid list.', ge=1, le=5)\n    max_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\n    ollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\nfocus: str | None = SchemaField(description='The focus of the list to generate.', placeholder='The top 5 most interesting news stories in the data.', default=None, advanced=False)\nsource_data: str | None = SchemaField(description='The data to generate the list from.', placeholder='News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released.', default=None, advanced=False)\nmodel: LlmModel = SchemaField(title='LLM Model', default=LlmModel.GPT4_TURBO, description='The language model to use for generating the list.', advanced=True)\ncredentials: AICredentials = AICredentialsField()\nmax_retries: int = SchemaField(default=3, description='Maximum number of retries for generating a valid list.', ge=1, le=5)\nmax_tokens: int | None = SchemaField(advanced=True, default=None, description='The maximum number of tokens to generate in the chat completion.')\nollama_host: str = SchemaField(advanced=True, default='localhost:11434', description='Ollama host for local  models')\nclass Output(BlockSchema):\n    generated_list: List[str] = SchemaField(description='The generated list.')\n    list_item: str = SchemaField(description='Each individual item in the list.')\n    error: str = SchemaField(description='Error message if the list generation failed.')\ngenerated_list: List[str] = SchemaField(description='The generated list.')\nlist_item: str = SchemaField(description='Each individual item in the list.')\nerror: str = SchemaField(description='Error message if the list generation failed.')\ndef __init__(self):\n    super().__init__(id='9c0b0450-d199-458b-a731-072189dd6593', description='Generate a Python list based on the given prompt using a Large Language Model (LLM).', categories={BlockCategory.AI, BlockCategory.TEXT}, input_schema=AIListGeneratorBlock.Input, output_schema=AIListGeneratorBlock.Output, test_input={'focus': 'planets', 'source_data': \"Zylora Prime is a glowing jungle world with bioluminescent plants, while Kharon-9 is a harsh desert planet with underground cities. Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of fictional worlds.\", 'model': LlmModel.GPT4_TURBO, 'credentials': TEST_CREDENTIALS_INPUT, 'max_retries': 3}, test_credentials=TEST_CREDENTIALS, test_output=[('generated_list', ['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']), ('list_item', 'Zylora Prime'), ('list_item', 'Kharon-9'), ('list_item', 'Vortexia'), ('list_item', 'Oceara'), ('list_item', 'Draknos')], test_mock={'llm_call': lambda input_data, credentials: {'response': \"['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']\"}})\nsuper().__init__()\n@staticmethod\ndef llm_call(input_data: AIStructuredResponseGeneratorBlock.Input, credentials: APIKeyCredentials) -> dict[str, str]:\n    llm_block = AIStructuredResponseGeneratorBlock()\n    response = llm_block.run_once(input_data, 'response', credentials=credentials)\n    return response\nllm_block = AIStructuredResponseGeneratorBlock()\nresponse = llm_block.run_once(input_data, 'response', credentials=credentials)\nreturn response"
    },
    {
      "id": "n100",
      "type": "block",
      "statements": [
        "@staticmethod\ndef string_to_list(string):\n    \"\"\"\n        Converts a string representation of a list into an actual Python list object.\n        \"\"\"\n    logger.debug(f'Converting string to list. Input string: {string}')\n    try:\n        python_list = ast.literal_eval(string)\n        if isinstance(python_list, list):\n            logger.debug(f'Successfully converted string to list: {python_list}')\n            return python_list\n        else:\n            logger.error(f\"The provided string '{string}' is not a valid list\")\n            raise ValueError(f\"The provided string '{string}' is not a valid list.\")\n    except (SyntaxError, ValueError) as e:\n        logger.error(f'Failed to convert string to list: {e}')\n        raise ValueError('Invalid list format. Could not convert to list.')",
        "'\\n        Converts a string representation of a list into an actual Python list object.\\n        '",
        "logger.debug(f'Converting string to list. Input string: {string}')",
        "try:\n    python_list = ast.literal_eval(string)\n    if isinstance(python_list, list):\n        logger.debug(f'Successfully converted string to list: {python_list}')\n        return python_list\n    else:\n        logger.error(f\"The provided string '{string}' is not a valid list\")\n        raise ValueError(f\"The provided string '{string}' is not a valid list.\")\nexcept (SyntaxError, ValueError) as e:\n    logger.error(f'Failed to convert string to list: {e}')\n    raise ValueError('Invalid list format. Could not convert to list.')",
        "python_list = ast.literal_eval(string)",
        "isinstance(python_list, list)"
      ],
      "code": "@staticmethod\ndef string_to_list(string):\n    \"\"\"\n        Converts a string representation of a list into an actual Python list object.\n        \"\"\"\n    logger.debug(f'Converting string to list. Input string: {string}')\n    try:\n        python_list = ast.literal_eval(string)\n        if isinstance(python_list, list):\n            logger.debug(f'Successfully converted string to list: {python_list}')\n            return python_list\n        else:\n            logger.error(f\"The provided string '{string}' is not a valid list\")\n            raise ValueError(f\"The provided string '{string}' is not a valid list.\")\n    except (SyntaxError, ValueError) as e:\n        logger.error(f'Failed to convert string to list: {e}')\n        raise ValueError('Invalid list format. Could not convert to list.')\n'\\n        Converts a string representation of a list into an actual Python list object.\\n        '\nlogger.debug(f'Converting string to list. Input string: {string}')\ntry:\n    python_list = ast.literal_eval(string)\n    if isinstance(python_list, list):\n        logger.debug(f'Successfully converted string to list: {python_list}')\n        return python_list\n    else:\n        logger.error(f\"The provided string '{string}' is not a valid list\")\n        raise ValueError(f\"The provided string '{string}' is not a valid list.\")\nexcept (SyntaxError, ValueError) as e:\n    logger.error(f'Failed to convert string to list: {e}')\n    raise ValueError('Invalid list format. Could not convert to list.')\npython_list = ast.literal_eval(string)\nisinstance(python_list, list)"
    },
    {
      "id": "n101",
      "type": "block",
      "statements": [
        "logger.debug(f'Successfully converted string to list: {python_list}')",
        "return python_list"
      ],
      "code": "logger.debug(f'Successfully converted string to list: {python_list}')\nreturn python_list"
    },
    {
      "id": "n102",
      "type": "block",
      "statements": [
        "logger.error(f\"The provided string '{string}' is not a valid list\")",
        "raise ValueError(f\"The provided string '{string}' is not a valid list.\")"
      ],
      "code": "logger.error(f\"The provided string '{string}' is not a valid list\")\nraise ValueError(f\"The provided string '{string}' is not a valid list.\")\nlogger.error(f'Failed to convert string to list: {e}')\nraise ValueError('Invalid list format. Could not convert to list.')\ndef run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:\n    logger.debug(f'Starting AIListGeneratorBlock.run with input data: {input_data}')\n    api_key_check = credentials.api_key.get_secret_value()\n    if not api_key_check:\n        raise ValueError('No LLM API key provided.')\n    sys_prompt = \"You are a Python list generator. Your task is to generate a Python list based on the user's prompt. \\n            |Respond ONLY with a valid python list. \\n            |The list can contain strings, numbers, or nested lists as appropriate. \\n            |Do not include any explanations or additional text.\\n\\n            |Valid Example string formats:\\n\\n            |Example 1:\\n            |```\\n            |['1', '2', '3', '4']\\n            |```\\n\\n            |Example 2:\\n            |```\\n            |[['1', '2'], ['3', '4'], ['5', '6']]\\n            |```\\n\\n            |Example 3:\\n            |```\\n            |['1', ['2', '3'], ['4', ['5', '6']]]\\n            |```\\n\\n            |Example 4:\\n            |```\\n            |['a', 'b', 'c']\\n            |```\\n\\n            |Example 5:\\n            |```\\n            |['1', '2.5', 'string', 'True', ['False', 'None']]\\n            |```\\n\\n            |Do not include any explanations or additional text, just respond with the list in the format specified above.\\n            \"\n    if input_data.focus:\n        prompt = f'Generate a list with the following focus:\\n<focus>\\n\\n{input_data.focus}</focus>'\n    elif input_data.source_data:\n        prompt = 'Extract the main focus of the source data to a list.\\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.'\n    else:\n        prompt = 'Generate a random list.'\n    if input_data.source_data:\n        prompt += f'\\n\\nUse the following source data to generate the list from:\\n\\n<source_data>\\n\\n{input_data.source_data}</source_data>\\n\\nDo not invent fictional data that is not present in the source data.'\n    else:\n        prompt += '\\n\\nInvent the data to generate the list from.'\n    for attempt in range(input_data.max_retries):\n        try:\n            logger.debug('Calling LLM')\n            llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(sys_prompt=sys_prompt, prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={}, ollama_host=input_data.ollama_host), credentials=credentials)\n            logger.debug(f'LLM response: {llm_response}')\n            response_string = llm_response['response']\n            logger.debug(f'Response string: {response_string}')\n            logger.debug('Converting string to Python list')\n            parsed_list = self.string_to_list(response_string)\n            logger.debug(f'Parsed list: {parsed_list}')\n            logger.debug('Successfully generated a valid Python list')\n            yield ('generated_list', parsed_list)\n            for item in parsed_list:\n                yield ('list_item', item)\n            return\n        except Exception as e:\n            logger.error(f'Error in attempt {attempt + 1}: {str(e)}')\n            if attempt == input_data.max_retries - 1:\n                logger.error(f'Failed to generate a valid Python list after {input_data.max_retries} attempts')\n                raise RuntimeError(f'Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}')\n            else:\n                logger.debug('Preparing retry prompt')\n                prompt = f'\\n                    The previous attempt failed due to `{e}`\\n                    Generate a valid Python list based on the original prompt.\\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\\n                    Original prompt: \\n                    ```{prompt}```\\n                    \\n                    Respond only with the list in the format specified with no commentary or apologies.\\n                    '\n                logger.debug(f'Retry prompt: {prompt}')\n    logger.debug('AIListGeneratorBlock.run completed')\nlogger.debug(f'Starting AIListGeneratorBlock.run with input data: {input_data}')\napi_key_check = credentials.api_key.get_secret_value()\nnot api_key_check"
    },
    {
      "id": "n103",
      "type": "block",
      "statements": [
        "raise ValueError('No LLM API key provided.')"
      ],
      "code": "raise ValueError('No LLM API key provided.')"
    },
    {
      "id": "n104",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n105",
      "type": "block",
      "statements": [
        "sys_prompt = \"You are a Python list generator. Your task is to generate a Python list based on the user's prompt. \\n            |Respond ONLY with a valid python list. \\n            |The list can contain strings, numbers, or nested lists as appropriate. \\n            |Do not include any explanations or additional text.\\n\\n            |Valid Example string formats:\\n\\n            |Example 1:\\n            |```\\n            |['1', '2', '3', '4']\\n            |```\\n\\n            |Example 2:\\n            |```\\n            |[['1', '2'], ['3', '4'], ['5', '6']]\\n            |```\\n\\n            |Example 3:\\n            |```\\n            |['1', ['2', '3'], ['4', ['5', '6']]]\\n            |```\\n\\n            |Example 4:\\n            |```\\n            |['a', 'b', 'c']\\n            |```\\n\\n            |Example 5:\\n            |```\\n            |['1', '2.5', 'string', 'True', ['False', 'None']]\\n            |```\\n\\n            |Do not include any explanations or additional text, just respond with the list in the format specified above.\\n            \"",
        "input_data.focus"
      ],
      "code": "sys_prompt = \"You are a Python list generator. Your task is to generate a Python list based on the user's prompt. \\n            |Respond ONLY with a valid python list. \\n            |The list can contain strings, numbers, or nested lists as appropriate. \\n            |Do not include any explanations or additional text.\\n\\n            |Valid Example string formats:\\n\\n            |Example 1:\\n            |```\\n            |['1', '2', '3', '4']\\n            |```\\n\\n            |Example 2:\\n            |```\\n            |[['1', '2'], ['3', '4'], ['5', '6']]\\n            |```\\n\\n            |Example 3:\\n            |```\\n            |['1', ['2', '3'], ['4', ['5', '6']]]\\n            |```\\n\\n            |Example 4:\\n            |```\\n            |['a', 'b', 'c']\\n            |```\\n\\n            |Example 5:\\n            |```\\n            |['1', '2.5', 'string', 'True', ['False', 'None']]\\n            |```\\n\\n            |Do not include any explanations or additional text, just respond with the list in the format specified above.\\n            \"\ninput_data.focus"
    },
    {
      "id": "n106",
      "type": "block",
      "statements": [
        "prompt = f'Generate a list with the following focus:\\n<focus>\\n\\n{input_data.focus}</focus>'"
      ],
      "code": "prompt = f'Generate a list with the following focus:\\n<focus>\\n\\n{input_data.focus}</focus>'"
    },
    {
      "id": "n107",
      "type": "block",
      "statements": [
        "input_data.source_data"
      ],
      "code": "input_data.source_data"
    },
    {
      "id": "n108",
      "type": "block",
      "statements": [
        "input_data.source_data"
      ],
      "code": "input_data.source_data"
    },
    {
      "id": "n109",
      "type": "block",
      "statements": [
        "prompt = 'Extract the main focus of the source data to a list.\\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.'"
      ],
      "code": "prompt = 'Extract the main focus of the source data to a list.\\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.'"
    },
    {
      "id": "n110",
      "type": "block",
      "statements": [
        "prompt = 'Generate a random list.'"
      ],
      "code": "prompt = 'Generate a random list.'"
    },
    {
      "id": "n111",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n112",
      "type": "block",
      "statements": [
        "prompt += f'\\n\\nUse the following source data to generate the list from:\\n\\n<source_data>\\n\\n{input_data.source_data}</source_data>\\n\\nDo not invent fictional data that is not present in the source data.'"
      ],
      "code": "prompt += f'\\n\\nUse the following source data to generate the list from:\\n\\n<source_data>\\n\\n{input_data.source_data}</source_data>\\n\\nDo not invent fictional data that is not present in the source data.'"
    },
    {
      "id": "n113",
      "type": "block",
      "statements": [
        "prompt += '\\n\\nInvent the data to generate the list from.'"
      ],
      "code": "prompt += '\\n\\nInvent the data to generate the list from.'"
    },
    {
      "id": "n114",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n115",
      "type": "block",
      "statements": [
        "attempt",
        "range(input_data.max_retries)"
      ],
      "code": "attempt\nrange(input_data.max_retries)"
    },
    {
      "id": "n116",
      "type": "block",
      "statements": [
        "try:\n    logger.debug('Calling LLM')\n    llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(sys_prompt=sys_prompt, prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={}, ollama_host=input_data.ollama_host), credentials=credentials)\n    logger.debug(f'LLM response: {llm_response}')\n    response_string = llm_response['response']\n    logger.debug(f'Response string: {response_string}')\n    logger.debug('Converting string to Python list')\n    parsed_list = self.string_to_list(response_string)\n    logger.debug(f'Parsed list: {parsed_list}')\n    logger.debug('Successfully generated a valid Python list')\n    yield ('generated_list', parsed_list)\n    for item in parsed_list:\n        yield ('list_item', item)\n    return\nexcept Exception as e:\n    logger.error(f'Error in attempt {attempt + 1}: {str(e)}')\n    if attempt == input_data.max_retries - 1:\n        logger.error(f'Failed to generate a valid Python list after {input_data.max_retries} attempts')\n        raise RuntimeError(f'Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}')\n    else:\n        logger.debug('Preparing retry prompt')\n        prompt = f'\\n                    The previous attempt failed due to `{e}`\\n                    Generate a valid Python list based on the original prompt.\\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\\n                    Original prompt: \\n                    ```{prompt}```\\n                    \\n                    Respond only with the list in the format specified with no commentary or apologies.\\n                    '\n        logger.debug(f'Retry prompt: {prompt}')",
        "logger.debug('Calling LLM')",
        "llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(sys_prompt=sys_prompt, prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={}, ollama_host=input_data.ollama_host), credentials=credentials)",
        "logger.debug(f'LLM response: {llm_response}')",
        "response_string = llm_response['response']",
        "logger.debug(f'Response string: {response_string}')",
        "logger.debug('Converting string to Python list')",
        "parsed_list = self.string_to_list(response_string)",
        "logger.debug(f'Parsed list: {parsed_list}')",
        "logger.debug('Successfully generated a valid Python list')",
        "(yield ('generated_list', parsed_list))"
      ],
      "code": "try:\n    logger.debug('Calling LLM')\n    llm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(sys_prompt=sys_prompt, prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={}, ollama_host=input_data.ollama_host), credentials=credentials)\n    logger.debug(f'LLM response: {llm_response}')\n    response_string = llm_response['response']\n    logger.debug(f'Response string: {response_string}')\n    logger.debug('Converting string to Python list')\n    parsed_list = self.string_to_list(response_string)\n    logger.debug(f'Parsed list: {parsed_list}')\n    logger.debug('Successfully generated a valid Python list')\n    yield ('generated_list', parsed_list)\n    for item in parsed_list:\n        yield ('list_item', item)\n    return\nexcept Exception as e:\n    logger.error(f'Error in attempt {attempt + 1}: {str(e)}')\n    if attempt == input_data.max_retries - 1:\n        logger.error(f'Failed to generate a valid Python list after {input_data.max_retries} attempts')\n        raise RuntimeError(f'Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}')\n    else:\n        logger.debug('Preparing retry prompt')\n        prompt = f'\\n                    The previous attempt failed due to `{e}`\\n                    Generate a valid Python list based on the original prompt.\\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\\n                    Original prompt: \\n                    ```{prompt}```\\n                    \\n                    Respond only with the list in the format specified with no commentary or apologies.\\n                    '\n        logger.debug(f'Retry prompt: {prompt}')\nlogger.debug('Calling LLM')\nllm_response = self.llm_call(AIStructuredResponseGeneratorBlock.Input(sys_prompt=sys_prompt, prompt=prompt, credentials=input_data.credentials, model=input_data.model, expected_format={}, ollama_host=input_data.ollama_host), credentials=credentials)\nlogger.debug(f'LLM response: {llm_response}')\nresponse_string = llm_response['response']\nlogger.debug(f'Response string: {response_string}')\nlogger.debug('Converting string to Python list')\nparsed_list = self.string_to_list(response_string)\nlogger.debug(f'Parsed list: {parsed_list}')\nlogger.debug('Successfully generated a valid Python list')\n(yield ('generated_list', parsed_list))"
    },
    {
      "id": "n117",
      "type": "block",
      "statements": [
        "logger.debug('AIListGeneratorBlock.run completed')"
      ],
      "code": "logger.debug('AIListGeneratorBlock.run completed')"
    },
    {
      "id": "n118",
      "type": "block",
      "statements": [
        "item",
        "parsed_list"
      ],
      "code": "item\nparsed_list"
    },
    {
      "id": "n119",
      "type": "block",
      "statements": [
        "(yield ('list_item', item))"
      ],
      "code": "(yield ('list_item', item))"
    },
    {
      "id": "n120",
      "type": "block",
      "statements": [
        "return"
      ],
      "code": "return"
    },
    {
      "id": "n121",
      "type": "block",
      "statements": [
        "logger.error(f'Error in attempt {attempt + 1}: {str(e)}')",
        "attempt Eq input_data.max_retries Sub 1"
      ],
      "code": "logger.error(f'Error in attempt {attempt + 1}: {str(e)}')\nattempt Eq input_data.max_retries Sub 1"
    },
    {
      "id": "n122",
      "type": "block",
      "statements": [
        "logger.error(f'Failed to generate a valid Python list after {input_data.max_retries} attempts')",
        "raise RuntimeError(f'Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}')"
      ],
      "code": "logger.error(f'Failed to generate a valid Python list after {input_data.max_retries} attempts')\nraise RuntimeError(f'Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}')"
    },
    {
      "id": "n123",
      "type": "block",
      "statements": [
        "logger.debug('Preparing retry prompt')",
        "prompt = f'\\n                    The previous attempt failed due to `{e}`\\n                    Generate a valid Python list based on the original prompt.\\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\\n                    Original prompt: \\n                    ```{prompt}```\\n                    \\n                    Respond only with the list in the format specified with no commentary or apologies.\\n                    '",
        "logger.debug(f'Retry prompt: {prompt}')"
      ],
      "code": "logger.debug('Preparing retry prompt')\nprompt = f'\\n                    The previous attempt failed due to `{e}`\\n                    Generate a valid Python list based on the original prompt.\\n                    Remember to respond ONLY with a valid Python list as per the format specified earlier.\\n                    Original prompt: \\n                    ```{prompt}```\\n                    \\n                    Respond only with the list in the format specified with no commentary or apologies.\\n                    '\nlogger.debug(f'Retry prompt: {prompt}')"
    },
    {
      "id": "n124",
      "type": "block",
      "statements": [],
      "code": ""
    }
  ],
  "edges": [
    {
      "source": "n70",
      "target": "n71"
    },
    {
      "source": "n74",
      "target": "n75"
    },
    {
      "source": "n103",
      "target": "n105"
    },
    {
      "source": "n27",
      "target": "n43"
    },
    {
      "source": "n118",
      "target": "n120"
    },
    {
      "source": "n0",
      "target": "n1"
    },
    {
      "source": "n119",
      "target": "n118"
    },
    {
      "source": "n76",
      "target": "n78"
    },
    {
      "source": "n107",
      "target": "n109"
    },
    {
      "source": "n59",
      "target": "n60"
    },
    {
      "source": "n84",
      "target": "n85"
    },
    {
      "source": "n124",
      "target": "n115"
    },
    {
      "source": "n31",
      "target": "n38"
    },
    {
      "source": "n122",
      "target": "n124"
    },
    {
      "source": "n55",
      "target": "n52"
    },
    {
      "source": "n68",
      "target": "n69"
    },
    {
      "source": "n53",
      "target": "n55"
    },
    {
      "source": "n41",
      "target": "n28"
    },
    {
      "source": "n21",
      "target": "n23"
    },
    {
      "source": "n32",
      "target": "n35"
    },
    {
      "source": "n80",
      "target": "n75"
    },
    {
      "source": "n51",
      "target": "n52"
    },
    {
      "source": "n108",
      "target": "n112"
    },
    {
      "source": "n115",
      "target": "n116"
    },
    {
      "source": "n59",
      "target": "n61"
    },
    {
      "source": "n26",
      "target": "n29"
    },
    {
      "source": "n107",
      "target": "n110"
    },
    {
      "source": "n29",
      "target": "n31"
    },
    {
      "source": "n75",
      "target": "n77"
    },
    {
      "source": "n75",
      "target": "n76"
    },
    {
      "source": "n121",
      "target": "n123"
    },
    {
      "source": "n105",
      "target": "n106"
    },
    {
      "source": "n36",
      "target": "n37"
    },
    {
      "source": "n61",
      "target": "n62"
    },
    {
      "source": "n114",
      "target": "n115"
    },
    {
      "source": "n65",
      "target": "n66"
    },
    {
      "source": "n118",
      "target": "n119"
    },
    {
      "source": "n116",
      "target": "n118"
    },
    {
      "source": "n49",
      "target": "n47"
    },
    {
      "source": "n108",
      "target": "n113"
    },
    {
      "source": "n23",
      "target": "n25"
    },
    {
      "source": "n35",
      "target": "n37"
    },
    {
      "source": "n18",
      "target": "n26"
    },
    {
      "source": "n43",
      "target": "n46"
    },
    {
      "source": "n82",
      "target": "n80"
    },
    {
      "source": "n2",
      "target": "n3"
    },
    {
      "source": "n78",
      "target": "n82"
    },
    {
      "source": "n17",
      "target": "n20"
    },
    {
      "source": "n44",
      "target": "n28"
    },
    {
      "source": "n123",
      "target": "n124"
    },
    {
      "source": "n24",
      "target": "n25"
    },
    {
      "source": "n102",
      "target": "n103"
    },
    {
      "source": "n15",
      "target": "n16"
    },
    {
      "source": "n95",
      "target": "n97"
    },
    {
      "source": "n29",
      "target": "n30"
    },
    {
      "source": "n85",
      "target": "n84"
    },
    {
      "source": "n47",
      "target": "n44"
    },
    {
      "source": "n48",
      "target": "n50"
    },
    {
      "source": "n58",
      "target": "n59"
    },
    {
      "source": "n89",
      "target": "n90"
    },
    {
      "source": "n11",
      "target": "n12"
    },
    {
      "source": "n48",
      "target": "n51"
    },
    {
      "source": "n28",
      "target": "n19"
    },
    {
      "source": "n38",
      "target": "n40"
    },
    {
      "source": "n121",
      "target": "n122"
    },
    {
      "source": "n76",
      "target": "n79"
    },
    {
      "source": "n65",
      "target": "n67"
    },
    {
      "source": "n86",
      "target": "n87"
    },
    {
      "source": "n60",
      "target": "n62"
    },
    {
      "source": "n33",
      "target": "n34"
    },
    {
      "source": "n39",
      "target": "n40"
    },
    {
      "source": "n115",
      "target": "n117"
    },
    {
      "source": "n66",
      "target": "n68"
    },
    {
      "source": "n57",
      "target": "n59"
    },
    {
      "source": "n30",
      "target": "n32"
    },
    {
      "source": "n56",
      "target": "n58"
    },
    {
      "source": "n90",
      "target": "n92"
    },
    {
      "source": "n106",
      "target": "n108"
    },
    {
      "source": "n83",
      "target": "n84"
    },
    {
      "source": "n102",
      "target": "n104"
    },
    {
      "source": "n50",
      "target": "n54"
    },
    {
      "source": "n18",
      "target": "n27"
    },
    {
      "source": "n46",
      "target": "n49"
    },
    {
      "source": "n37",
      "target": "n34"
    },
    {
      "source": "n95",
      "target": "n96"
    },
    {
      "source": "n16",
      "target": "n11"
    },
    {
      "source": "n90",
      "target": "n91"
    },
    {
      "source": "n4",
      "target": "n6"
    },
    {
      "source": "n13",
      "target": "n17"
    },
    {
      "source": "n13",
      "target": "n18"
    },
    {
      "source": "n50",
      "target": "n53"
    },
    {
      "source": "n112",
      "target": "n114"
    },
    {
      "source": "n14",
      "target": "n16"
    },
    {
      "source": "n46",
      "target": "n48"
    },
    {
      "source": "n12",
      "target": "n15"
    },
    {
      "source": "n62",
      "target": "n63"
    },
    {
      "source": "n87",
      "target": "n89"
    },
    {
      "source": "n64",
      "target": "n65"
    },
    {
      "source": "n1",
      "target": "n3"
    },
    {
      "source": "n30",
      "target": "n33"
    },
    {
      "source": "n110",
      "target": "n111"
    },
    {
      "source": "n70",
      "target": "n72"
    },
    {
      "source": "n78",
      "target": "n81"
    },
    {
      "source": "n63",
      "target": "n65"
    },
    {
      "source": "n11",
      "target": "n13"
    },
    {
      "source": "n0",
      "target": "n2"
    },
    {
      "source": "n54",
      "target": "n55"
    },
    {
      "source": "n105",
      "target": "n107"
    },
    {
      "source": "n111",
      "target": "n108"
    },
    {
      "source": "n31",
      "target": "n39"
    },
    {
      "source": "n17",
      "target": "n21"
    },
    {
      "source": "n100",
      "target": "n102"
    },
    {
      "source": "n21",
      "target": "n24"
    },
    {
      "source": "n10",
      "target": "n11"
    },
    {
      "source": "n87",
      "target": "n88"
    },
    {
      "source": "n91",
      "target": "n90"
    },
    {
      "source": "n68",
      "target": "n70"
    },
    {
      "source": "n27",
      "target": "n42"
    },
    {
      "source": "n104",
      "target": "n105"
    },
    {
      "source": "n4",
      "target": "n5"
    },
    {
      "source": "n84",
      "target": "n86"
    },
    {
      "source": "n113",
      "target": "n114"
    },
    {
      "source": "n100",
      "target": "n101"
    },
    {
      "source": "n62",
      "target": "n64"
    },
    {
      "source": "n12",
      "target": "n14"
    },
    {
      "source": "n25",
      "target": "n22"
    },
    {
      "source": "n88",
      "target": "n87"
    },
    {
      "source": "n67",
      "target": "n68"
    },
    {
      "source": "n34",
      "target": "n29"
    },
    {
      "source": "n32",
      "target": "n36"
    },
    {
      "source": "n43",
      "target": "n45"
    },
    {
      "source": "n20",
      "target": "n22"
    },
    {
      "source": "n109",
      "target": "n111"
    },
    {
      "source": "n56",
      "target": "n57"
    }
  ]
}