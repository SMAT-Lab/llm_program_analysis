{
  "nodes": [
    {
      "id": "n0",
      "type": "block",
      "statements": [
        "import atexit",
        "import logging",
        "import multiprocessing",
        "import os",
        "import signal",
        "import sys",
        "import threading",
        "from concurrent.futures import Future, ProcessPoolExecutor",
        "from contextlib import contextmanager",
        "from multiprocessing.pool import AsyncResult, Pool",
        "from typing import TYPE_CHECKING, Any, Generator, TypeVar, cast",
        "from pydantic import BaseModel",
        "from redis.lock import Lock as RedisLock",
        "TYPE_CHECKING"
      ],
      "code": "import atexit\nimport logging\nimport multiprocessing\nimport os\nimport signal\nimport sys\nimport threading\nfrom concurrent.futures import Future, ProcessPoolExecutor\nfrom contextlib import contextmanager\nfrom multiprocessing.pool import AsyncResult, Pool\nfrom typing import TYPE_CHECKING, Any, Generator, TypeVar, cast\nfrom pydantic import BaseModel\nfrom redis.lock import Lock as RedisLock\nTYPE_CHECKING"
    },
    {
      "id": "n1",
      "type": "block",
      "statements": [
        "from backend.executor import DatabaseManager"
      ],
      "code": "from backend.executor import DatabaseManager"
    },
    {
      "id": "n2",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n3",
      "type": "block",
      "statements": [
        "from autogpt_libs.utils.cache import thread_cached",
        "from backend.blocks.agent import AgentExecutorBlock",
        "from backend.data import redis",
        "from backend.data.block import Block, BlockData, BlockInput, BlockType, get_block",
        "from backend.data.execution import ExecutionQueue, ExecutionResult, ExecutionStatus, GraphExecutionEntry, NodeExecutionEntry, merge_execution_input, parse_execution_output",
        "from backend.data.graph import GraphModel, Link, Node",
        "from backend.data.model import CREDENTIALS_FIELD_NAME, CredentialsMetaInput",
        "from backend.integrations.creds_manager import IntegrationCredentialsManager",
        "from backend.util import json",
        "from backend.util.decorator import error_logged, time_measured",
        "from backend.util.logging import configure_logging",
        "from backend.util.process import set_service_name",
        "from backend.util.service import AppService, close_service_client, expose, get_service_client",
        "from backend.util.settings import Settings",
        "from backend.util.type import convert",
        "logger = logging.getLogger(__name__)",
        "settings = Settings()",
        "class LogMetadata:\n\n    def __init__(self, user_id: str, graph_eid: str, graph_id: str, node_eid: str, node_id: str, block_name: str):\n        self.metadata = {'component': 'ExecutionManager', 'user_id': user_id, 'graph_eid': graph_eid, 'graph_id': graph_id, 'node_eid': node_eid, 'node_id': node_id, 'block_name': block_name}\n        self.prefix = f'[ExecutionManager|uid:{user_id}|gid:{graph_id}|nid:{node_id}]|geid:{graph_eid}|nid:{node_eid}|{block_name}]'\n\n    def info(self, msg: str, **extra):\n        msg = self._wrap(msg, **extra)\n        logger.info(msg, extra={'json_fields': {**self.metadata, **extra}})\n\n    def warning(self, msg: str, **extra):\n        msg = self._wrap(msg, **extra)\n        logger.warning(msg, extra={'json_fields': {**self.metadata, **extra}})\n\n    def error(self, msg: str, **extra):\n        msg = self._wrap(msg, **extra)\n        logger.error(msg, extra={'json_fields': {**self.metadata, **extra}})\n\n    def debug(self, msg: str, **extra):\n        msg = self._wrap(msg, **extra)\n        logger.debug(msg, extra={'json_fields': {**self.metadata, **extra}})\n\n    def exception(self, msg: str, **extra):\n        msg = self._wrap(msg, **extra)\n        logger.exception(msg, extra={'json_fields': {**self.metadata, **extra}})\n\n    def _wrap(self, msg: str, **extra):\n        return f'{self.prefix} {msg} {extra}'",
        "def __init__(self, user_id: str, graph_eid: str, graph_id: str, node_eid: str, node_id: str, block_name: str):\n    self.metadata = {'component': 'ExecutionManager', 'user_id': user_id, 'graph_eid': graph_eid, 'graph_id': graph_id, 'node_eid': node_eid, 'node_id': node_id, 'block_name': block_name}\n    self.prefix = f'[ExecutionManager|uid:{user_id}|gid:{graph_id}|nid:{node_id}]|geid:{graph_eid}|nid:{node_eid}|{block_name}]'",
        "self.metadata = {'component': 'ExecutionManager', 'user_id': user_id, 'graph_eid': graph_eid, 'graph_id': graph_id, 'node_eid': node_eid, 'node_id': node_id, 'block_name': block_name}",
        "self.prefix = f'[ExecutionManager|uid:{user_id}|gid:{graph_id}|nid:{node_id}]|geid:{graph_eid}|nid:{node_eid}|{block_name}]'",
        "def info(self, msg: str, **extra):\n    msg = self._wrap(msg, **extra)\n    logger.info(msg, extra={'json_fields': {**self.metadata, **extra}})",
        "msg = self._wrap(msg, **extra)",
        "logger.info(msg)",
        "def warning(self, msg: str, **extra):\n    msg = self._wrap(msg, **extra)\n    logger.warning(msg, extra={'json_fields': {**self.metadata, **extra}})",
        "msg = self._wrap(msg, **extra)",
        "logger.warning(msg)",
        "def error(self, msg: str, **extra):\n    msg = self._wrap(msg, **extra)\n    logger.error(msg, extra={'json_fields': {**self.metadata, **extra}})",
        "msg = self._wrap(msg, **extra)",
        "logger.error(msg)",
        "def debug(self, msg: str, **extra):\n    msg = self._wrap(msg, **extra)\n    logger.debug(msg, extra={'json_fields': {**self.metadata, **extra}})",
        "msg = self._wrap(msg, **extra)",
        "logger.debug(msg)",
        "def exception(self, msg: str, **extra):\n    msg = self._wrap(msg, **extra)\n    logger.exception(msg, extra={'json_fields': {**self.metadata, **extra}})",
        "msg = self._wrap(msg, **extra)",
        "logger.exception(msg)",
        "def _wrap(self, msg: str, **extra):\n    return f'{self.prefix} {msg} {extra}'",
        "return f'{self.prefix} {msg} {extra}'"
      ],
      "code": "from autogpt_libs.utils.cache import thread_cached\nfrom backend.blocks.agent import AgentExecutorBlock\nfrom backend.data import redis\nfrom backend.data.block import Block, BlockData, BlockInput, BlockType, get_block\nfrom backend.data.execution import ExecutionQueue, ExecutionResult, ExecutionStatus, GraphExecutionEntry, NodeExecutionEntry, merge_execution_input, parse_execution_output\nfrom backend.data.graph import GraphModel, Link, Node\nfrom backend.data.model import CREDENTIALS_FIELD_NAME, CredentialsMetaInput\nfrom backend.integrations.creds_manager import IntegrationCredentialsManager\nfrom backend.util import json\nfrom backend.util.decorator import error_logged, time_measured\nfrom backend.util.logging import configure_logging\nfrom backend.util.process import set_service_name\nfrom backend.util.service import AppService, close_service_client, expose, get_service_client\nfrom backend.util.settings import Settings\nfrom backend.util.type import convert\nlogger = logging.getLogger(__name__)\nsettings = Settings()\nclass LogMetadata:\n\n    def __init__(self, user_id: str, graph_eid: str, graph_id: str, node_eid: str, node_id: str, block_name: str):\n        self.metadata = {'component': 'ExecutionManager', 'user_id': user_id, 'graph_eid': graph_eid, 'graph_id': graph_id, 'node_eid': node_eid, 'node_id': node_id, 'block_name': block_name}\n        self.prefix = f'[ExecutionManager|uid:{user_id}|gid:{graph_id}|nid:{node_id}]|geid:{graph_eid}|nid:{node_eid}|{block_name}]'\n\n    def info(self, msg: str, **extra):\n        msg = self._wrap(msg, **extra)\n        logger.info(msg, extra={'json_fields': {**self.metadata, **extra}})\n\n    def warning(self, msg: str, **extra):\n        msg = self._wrap(msg, **extra)\n        logger.warning(msg, extra={'json_fields': {**self.metadata, **extra}})\n\n    def error(self, msg: str, **extra):\n        msg = self._wrap(msg, **extra)\n        logger.error(msg, extra={'json_fields': {**self.metadata, **extra}})\n\n    def debug(self, msg: str, **extra):\n        msg = self._wrap(msg, **extra)\n        logger.debug(msg, extra={'json_fields': {**self.metadata, **extra}})\n\n    def exception(self, msg: str, **extra):\n        msg = self._wrap(msg, **extra)\n        logger.exception(msg, extra={'json_fields': {**self.metadata, **extra}})\n\n    def _wrap(self, msg: str, **extra):\n        return f'{self.prefix} {msg} {extra}'\ndef __init__(self, user_id: str, graph_eid: str, graph_id: str, node_eid: str, node_id: str, block_name: str):\n    self.metadata = {'component': 'ExecutionManager', 'user_id': user_id, 'graph_eid': graph_eid, 'graph_id': graph_id, 'node_eid': node_eid, 'node_id': node_id, 'block_name': block_name}\n    self.prefix = f'[ExecutionManager|uid:{user_id}|gid:{graph_id}|nid:{node_id}]|geid:{graph_eid}|nid:{node_eid}|{block_name}]'\nself.metadata = {'component': 'ExecutionManager', 'user_id': user_id, 'graph_eid': graph_eid, 'graph_id': graph_id, 'node_eid': node_eid, 'node_id': node_id, 'block_name': block_name}\nself.prefix = f'[ExecutionManager|uid:{user_id}|gid:{graph_id}|nid:{node_id}]|geid:{graph_eid}|nid:{node_eid}|{block_name}]'\ndef info(self, msg: str, **extra):\n    msg = self._wrap(msg, **extra)\n    logger.info(msg, extra={'json_fields': {**self.metadata, **extra}})\nmsg = self._wrap(msg, **extra)\nlogger.info(msg)\ndef warning(self, msg: str, **extra):\n    msg = self._wrap(msg, **extra)\n    logger.warning(msg, extra={'json_fields': {**self.metadata, **extra}})\nmsg = self._wrap(msg, **extra)\nlogger.warning(msg)\ndef error(self, msg: str, **extra):\n    msg = self._wrap(msg, **extra)\n    logger.error(msg, extra={'json_fields': {**self.metadata, **extra}})\nmsg = self._wrap(msg, **extra)\nlogger.error(msg)\ndef debug(self, msg: str, **extra):\n    msg = self._wrap(msg, **extra)\n    logger.debug(msg, extra={'json_fields': {**self.metadata, **extra}})\nmsg = self._wrap(msg, **extra)\nlogger.debug(msg)\ndef exception(self, msg: str, **extra):\n    msg = self._wrap(msg, **extra)\n    logger.exception(msg, extra={'json_fields': {**self.metadata, **extra}})\nmsg = self._wrap(msg, **extra)\nlogger.exception(msg)\ndef _wrap(self, msg: str, **extra):\n    return f'{self.prefix} {msg} {extra}'\nreturn f'{self.prefix} {msg} {extra}'"
    },
    {
      "id": "n4",
      "type": "block",
      "statements": [
        "T = TypeVar('T')",
        "ExecutionStream = Generator[NodeExecutionEntry, None, None]",
        "def execute_node(db_client: 'DatabaseManager', creds_manager: IntegrationCredentialsManager, data: NodeExecutionEntry, execution_stats: dict[str, Any] | None=None) -> ExecutionStream:\n    \"\"\"\n    Execute a node in the graph. This will trigger a block execution on a node,\n    persist the execution result, and return the subsequent node to be executed.\n\n    Args:\n        db_client: The client to send execution updates to the server.\n        creds_manager: The manager to acquire and release credentials.\n        data: The execution data for executing the current node.\n        execution_stats: The execution statistics to be updated.\n\n    Returns:\n        The subsequent node to be enqueued, or None if there is no subsequent node.\n    \"\"\"\n    user_id = data.user_id\n    graph_exec_id = data.graph_exec_id\n    graph_id = data.graph_id\n    node_exec_id = data.node_exec_id\n    node_id = data.node_id\n\n    def update_execution(status: ExecutionStatus) -> ExecutionResult:\n        exec_update = db_client.update_execution_status(node_exec_id, status)\n        db_client.send_execution_update(exec_update)\n        return exec_update\n    node = db_client.get_node(node_id)\n    node_block = get_block(node.block_id)\n    if not node_block:\n        logger.error(f'Block {node.block_id} not found.')\n        return\n    log_metadata = LogMetadata(user_id=user_id, graph_eid=graph_exec_id, graph_id=graph_id, node_eid=node_exec_id, node_id=node_id, block_name=node_block.name)\n    (input_data, error) = validate_exec(node, data.data, resolve_input=False)\n    if input_data is None:\n        log_metadata.error(f'Skip execution, input validation error: {error}')\n        db_client.upsert_execution_output(node_exec_id, 'error', error)\n        update_execution(ExecutionStatus.FAILED)\n        return\n    if isinstance(node_block, AgentExecutorBlock):\n        input_data = {**node.input_default, 'data': input_data}\n    input_data_str = json.dumps(input_data)\n    input_size = len(input_data_str)\n    log_metadata.info('Executed node with input', input=input_data_str)\n    update_execution(ExecutionStatus.RUNNING)\n    extra_exec_kwargs = {}\n    creds_lock = None\n    if CREDENTIALS_FIELD_NAME in input_data:\n        credentials_meta = CredentialsMetaInput(**input_data[CREDENTIALS_FIELD_NAME])\n        (credentials, creds_lock) = creds_manager.acquire(user_id, credentials_meta.id)\n        extra_exec_kwargs['credentials'] = credentials\n    output_size = 0\n    end_status = ExecutionStatus.COMPLETED\n    credit = db_client.get_or_refill_credit(user_id)\n    if credit < 0:\n        raise ValueError(f'Insufficient credit: {credit}')\n    try:\n        for (output_name, output_data) in node_block.execute(input_data, **extra_exec_kwargs):\n            output_size += len(json.dumps(output_data))\n            log_metadata.info('Node produced output', **{output_name: output_data})\n            db_client.upsert_execution_output(node_exec_id, output_name, output_data)\n            for execution in _enqueue_next_nodes(db_client=db_client, node=node, output=(output_name, output_data), user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, log_metadata=log_metadata):\n                yield execution\n    except Exception as e:\n        end_status = ExecutionStatus.FAILED\n        error_msg = str(e)\n        log_metadata.exception(f'Node execution failed with error {error_msg}')\n        db_client.upsert_execution_output(node_exec_id, 'error', error_msg)\n        for execution in _enqueue_next_nodes(db_client=db_client, node=node, output=('error', error_msg), user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, log_metadata=log_metadata):\n            yield execution\n        raise e\n    finally:\n        if creds_lock:\n            try:\n                creds_lock.release()\n            except Exception as e:\n                log_metadata.error(f'Failed to release credentials lock: {e}')\n        res = update_execution(end_status)\n        if end_status == ExecutionStatus.COMPLETED:\n            s = input_size + output_size\n            t = (res.end_time - res.start_time).total_seconds() if res.end_time and res.start_time else 0\n            db_client.spend_credits(user_id, credit, node_block.id, input_data, s, t)\n        if execution_stats is not None:\n            execution_stats.update(node_block.execution_stats)\n            execution_stats['input_size'] = input_size\n            execution_stats['output_size'] = output_size",
        "'\\n    Execute a node in the graph. This will trigger a block execution on a node,\\n    persist the execution result, and return the subsequent node to be executed.\\n\\n    Args:\\n        db_client: The client to send execution updates to the server.\\n        creds_manager: The manager to acquire and release credentials.\\n        data: The execution data for executing the current node.\\n        execution_stats: The execution statistics to be updated.\\n\\n    Returns:\\n        The subsequent node to be enqueued, or None if there is no subsequent node.\\n    '",
        "user_id = data.user_id",
        "graph_exec_id = data.graph_exec_id",
        "graph_id = data.graph_id",
        "node_exec_id = data.node_exec_id",
        "node_id = data.node_id",
        "def update_execution(status: ExecutionStatus) -> ExecutionResult:\n    exec_update = db_client.update_execution_status(node_exec_id, status)\n    db_client.send_execution_update(exec_update)\n    return exec_update",
        "exec_update = db_client.update_execution_status(node_exec_id, status)",
        "db_client.send_execution_update(exec_update)",
        "return exec_update"
      ],
      "code": "T = TypeVar('T')\nExecutionStream = Generator[NodeExecutionEntry, None, None]\ndef execute_node(db_client: 'DatabaseManager', creds_manager: IntegrationCredentialsManager, data: NodeExecutionEntry, execution_stats: dict[str, Any] | None=None) -> ExecutionStream:\n    \"\"\"\n    Execute a node in the graph. This will trigger a block execution on a node,\n    persist the execution result, and return the subsequent node to be executed.\n\n    Args:\n        db_client: The client to send execution updates to the server.\n        creds_manager: The manager to acquire and release credentials.\n        data: The execution data for executing the current node.\n        execution_stats: The execution statistics to be updated.\n\n    Returns:\n        The subsequent node to be enqueued, or None if there is no subsequent node.\n    \"\"\"\n    user_id = data.user_id\n    graph_exec_id = data.graph_exec_id\n    graph_id = data.graph_id\n    node_exec_id = data.node_exec_id\n    node_id = data.node_id\n\n    def update_execution(status: ExecutionStatus) -> ExecutionResult:\n        exec_update = db_client.update_execution_status(node_exec_id, status)\n        db_client.send_execution_update(exec_update)\n        return exec_update\n    node = db_client.get_node(node_id)\n    node_block = get_block(node.block_id)\n    if not node_block:\n        logger.error(f'Block {node.block_id} not found.')\n        return\n    log_metadata = LogMetadata(user_id=user_id, graph_eid=graph_exec_id, graph_id=graph_id, node_eid=node_exec_id, node_id=node_id, block_name=node_block.name)\n    (input_data, error) = validate_exec(node, data.data, resolve_input=False)\n    if input_data is None:\n        log_metadata.error(f'Skip execution, input validation error: {error}')\n        db_client.upsert_execution_output(node_exec_id, 'error', error)\n        update_execution(ExecutionStatus.FAILED)\n        return\n    if isinstance(node_block, AgentExecutorBlock):\n        input_data = {**node.input_default, 'data': input_data}\n    input_data_str = json.dumps(input_data)\n    input_size = len(input_data_str)\n    log_metadata.info('Executed node with input', input=input_data_str)\n    update_execution(ExecutionStatus.RUNNING)\n    extra_exec_kwargs = {}\n    creds_lock = None\n    if CREDENTIALS_FIELD_NAME in input_data:\n        credentials_meta = CredentialsMetaInput(**input_data[CREDENTIALS_FIELD_NAME])\n        (credentials, creds_lock) = creds_manager.acquire(user_id, credentials_meta.id)\n        extra_exec_kwargs['credentials'] = credentials\n    output_size = 0\n    end_status = ExecutionStatus.COMPLETED\n    credit = db_client.get_or_refill_credit(user_id)\n    if credit < 0:\n        raise ValueError(f'Insufficient credit: {credit}')\n    try:\n        for (output_name, output_data) in node_block.execute(input_data, **extra_exec_kwargs):\n            output_size += len(json.dumps(output_data))\n            log_metadata.info('Node produced output', **{output_name: output_data})\n            db_client.upsert_execution_output(node_exec_id, output_name, output_data)\n            for execution in _enqueue_next_nodes(db_client=db_client, node=node, output=(output_name, output_data), user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, log_metadata=log_metadata):\n                yield execution\n    except Exception as e:\n        end_status = ExecutionStatus.FAILED\n        error_msg = str(e)\n        log_metadata.exception(f'Node execution failed with error {error_msg}')\n        db_client.upsert_execution_output(node_exec_id, 'error', error_msg)\n        for execution in _enqueue_next_nodes(db_client=db_client, node=node, output=('error', error_msg), user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, log_metadata=log_metadata):\n            yield execution\n        raise e\n    finally:\n        if creds_lock:\n            try:\n                creds_lock.release()\n            except Exception as e:\n                log_metadata.error(f'Failed to release credentials lock: {e}')\n        res = update_execution(end_status)\n        if end_status == ExecutionStatus.COMPLETED:\n            s = input_size + output_size\n            t = (res.end_time - res.start_time).total_seconds() if res.end_time and res.start_time else 0\n            db_client.spend_credits(user_id, credit, node_block.id, input_data, s, t)\n        if execution_stats is not None:\n            execution_stats.update(node_block.execution_stats)\n            execution_stats['input_size'] = input_size\n            execution_stats['output_size'] = output_size\n'\\n    Execute a node in the graph. This will trigger a block execution on a node,\\n    persist the execution result, and return the subsequent node to be executed.\\n\\n    Args:\\n        db_client: The client to send execution updates to the server.\\n        creds_manager: The manager to acquire and release credentials.\\n        data: The execution data for executing the current node.\\n        execution_stats: The execution statistics to be updated.\\n\\n    Returns:\\n        The subsequent node to be enqueued, or None if there is no subsequent node.\\n    '\nuser_id = data.user_id\ngraph_exec_id = data.graph_exec_id\ngraph_id = data.graph_id\nnode_exec_id = data.node_exec_id\nnode_id = data.node_id\ndef update_execution(status: ExecutionStatus) -> ExecutionResult:\n    exec_update = db_client.update_execution_status(node_exec_id, status)\n    db_client.send_execution_update(exec_update)\n    return exec_update\nexec_update = db_client.update_execution_status(node_exec_id, status)\ndb_client.send_execution_update(exec_update)\nreturn exec_update"
    },
    {
      "id": "n5",
      "type": "block",
      "statements": [
        "node = db_client.get_node(node_id)",
        "node_block = get_block(node.block_id)",
        "not node_block"
      ],
      "code": "node = db_client.get_node(node_id)\nnode_block = get_block(node.block_id)\nnot node_block"
    },
    {
      "id": "n6",
      "type": "block",
      "statements": [
        "logger.error(f'Block {node.block_id} not found.')",
        "return"
      ],
      "code": "logger.error(f'Block {node.block_id} not found.')\nreturn"
    },
    {
      "id": "n7",
      "type": "block",
      "statements": [],
      "code": "\nlog_metadata = LogMetadata(user_id=user_id, graph_eid=graph_exec_id, graph_id=graph_id, node_eid=node_exec_id, node_id=node_id, block_name=node_block.name)\n(input_data, error) = validate_exec(node, data.data, resolve_input=False)\ninput_data Is None"
    },
    {
      "id": "n8",
      "type": "block",
      "statements": [
        "log_metadata.error(f'Skip execution, input validation error: {error}')",
        "db_client.upsert_execution_output(node_exec_id, 'error', error)",
        "update_execution(ExecutionStatus.FAILED)",
        "return"
      ],
      "code": "log_metadata.error(f'Skip execution, input validation error: {error}')\ndb_client.upsert_execution_output(node_exec_id, 'error', error)\nupdate_execution(ExecutionStatus.FAILED)\nreturn"
    },
    {
      "id": "n9",
      "type": "block",
      "statements": [],
      "code": "\nisinstance(node_block, AgentExecutorBlock)"
    },
    {
      "id": "n10",
      "type": "block",
      "statements": [
        "input_data = {**node.input_default, 'data': input_data}"
      ],
      "code": "input_data = {**node.input_default, 'data': input_data}"
    },
    {
      "id": "n11",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n12",
      "type": "block",
      "statements": [
        "input_data_str = json.dumps(input_data)",
        "input_size = len(input_data_str)",
        "log_metadata.info('Executed node with input')",
        "update_execution(ExecutionStatus.RUNNING)",
        "extra_exec_kwargs = {}",
        "creds_lock = None",
        "CREDENTIALS_FIELD_NAME In input_data"
      ],
      "code": "input_data_str = json.dumps(input_data)\ninput_size = len(input_data_str)\nlog_metadata.info('Executed node with input')\nupdate_execution(ExecutionStatus.RUNNING)\nextra_exec_kwargs = {}\ncreds_lock = None\nCREDENTIALS_FIELD_NAME In input_data"
    },
    {
      "id": "n13",
      "type": "block",
      "statements": [
        "credentials_meta = CredentialsMetaInput(**input_data[CREDENTIALS_FIELD_NAME])",
        "(credentials, creds_lock) = creds_manager.acquire(user_id, credentials_meta.id)",
        "extra_exec_kwargs['credentials'] = credentials"
      ],
      "code": "credentials_meta = CredentialsMetaInput(**input_data[CREDENTIALS_FIELD_NAME])\n(credentials, creds_lock) = creds_manager.acquire(user_id, credentials_meta.id)\nextra_exec_kwargs['credentials'] = credentials"
    },
    {
      "id": "n14",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n15",
      "type": "block",
      "statements": [
        "output_size = 0",
        "end_status = ExecutionStatus.COMPLETED",
        "credit = db_client.get_or_refill_credit(user_id)",
        "credit Lt 0"
      ],
      "code": "output_size = 0\nend_status = ExecutionStatus.COMPLETED\ncredit = db_client.get_or_refill_credit(user_id)\ncredit Lt 0"
    },
    {
      "id": "n16",
      "type": "block",
      "statements": [
        "raise ValueError(f'Insufficient credit: {credit}')"
      ],
      "code": "raise ValueError(f'Insufficient credit: {credit}')"
    },
    {
      "id": "n17",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n18",
      "type": "block",
      "statements": [
        "try:\n    for (output_name, output_data) in node_block.execute(input_data, **extra_exec_kwargs):\n        output_size += len(json.dumps(output_data))\n        log_metadata.info('Node produced output', **{output_name: output_data})\n        db_client.upsert_execution_output(node_exec_id, output_name, output_data)\n        for execution in _enqueue_next_nodes(db_client=db_client, node=node, output=(output_name, output_data), user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, log_metadata=log_metadata):\n            yield execution\nexcept Exception as e:\n    end_status = ExecutionStatus.FAILED\n    error_msg = str(e)\n    log_metadata.exception(f'Node execution failed with error {error_msg}')\n    db_client.upsert_execution_output(node_exec_id, 'error', error_msg)\n    for execution in _enqueue_next_nodes(db_client=db_client, node=node, output=('error', error_msg), user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, log_metadata=log_metadata):\n        yield execution\n    raise e\nfinally:\n    if creds_lock:\n        try:\n            creds_lock.release()\n        except Exception as e:\n            log_metadata.error(f'Failed to release credentials lock: {e}')\n    res = update_execution(end_status)\n    if end_status == ExecutionStatus.COMPLETED:\n        s = input_size + output_size\n        t = (res.end_time - res.start_time).total_seconds() if res.end_time and res.start_time else 0\n        db_client.spend_credits(user_id, credit, node_block.id, input_data, s, t)\n    if execution_stats is not None:\n        execution_stats.update(node_block.execution_stats)\n        execution_stats['input_size'] = input_size\n        execution_stats['output_size'] = output_size"
      ],
      "code": "try:\n    for (output_name, output_data) in node_block.execute(input_data, **extra_exec_kwargs):\n        output_size += len(json.dumps(output_data))\n        log_metadata.info('Node produced output', **{output_name: output_data})\n        db_client.upsert_execution_output(node_exec_id, output_name, output_data)\n        for execution in _enqueue_next_nodes(db_client=db_client, node=node, output=(output_name, output_data), user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, log_metadata=log_metadata):\n            yield execution\nexcept Exception as e:\n    end_status = ExecutionStatus.FAILED\n    error_msg = str(e)\n    log_metadata.exception(f'Node execution failed with error {error_msg}')\n    db_client.upsert_execution_output(node_exec_id, 'error', error_msg)\n    for execution in _enqueue_next_nodes(db_client=db_client, node=node, output=('error', error_msg), user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, log_metadata=log_metadata):\n        yield execution\n    raise e\nfinally:\n    if creds_lock:\n        try:\n            creds_lock.release()\n        except Exception as e:\n            log_metadata.error(f'Failed to release credentials lock: {e}')\n    res = update_execution(end_status)\n    if end_status == ExecutionStatus.COMPLETED:\n        s = input_size + output_size\n        t = (res.end_time - res.start_time).total_seconds() if res.end_time and res.start_time else 0\n        db_client.spend_credits(user_id, credit, node_block.id, input_data, s, t)\n    if execution_stats is not None:\n        execution_stats.update(node_block.execution_stats)\n        execution_stats['input_size'] = input_size\n        execution_stats['output_size'] = output_size"
    },
    {
      "id": "n19",
      "type": "block",
      "statements": [
        "(output_name, output_data)",
        "node_block.execute(input_data)"
      ],
      "code": "(output_name, output_data)\nnode_block.execute(input_data)"
    },
    {
      "id": "n20",
      "type": "block",
      "statements": [
        "output_size += len(json.dumps(output_data))",
        "log_metadata.info('Node produced output')",
        "db_client.upsert_execution_output(node_exec_id, output_name, output_data)"
      ],
      "code": "output_size += len(json.dumps(output_data))\nlog_metadata.info('Node produced output')\ndb_client.upsert_execution_output(node_exec_id, output_name, output_data)"
    },
    {
      "id": "n21",
      "type": "block",
      "statements": [
        "end_status = ExecutionStatus.FAILED",
        "error_msg = str(e)",
        "log_metadata.exception(f'Node execution failed with error {error_msg}')",
        "db_client.upsert_execution_output(node_exec_id, 'error', error_msg)"
      ],
      "code": "end_status = ExecutionStatus.FAILED\nerror_msg = str(e)\nlog_metadata.exception(f'Node execution failed with error {error_msg}')\ndb_client.upsert_execution_output(node_exec_id, 'error', error_msg)"
    },
    {
      "id": "n22",
      "type": "block",
      "statements": [
        "execution",
        "_enqueue_next_nodes()"
      ],
      "code": "execution\n_enqueue_next_nodes()"
    },
    {
      "id": "n23",
      "type": "block",
      "statements": [
        "(yield execution)"
      ],
      "code": "(yield execution)"
    },
    {
      "id": "n24",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n25",
      "type": "block",
      "statements": [
        "execution",
        "_enqueue_next_nodes()"
      ],
      "code": "execution\n_enqueue_next_nodes()"
    },
    {
      "id": "n26",
      "type": "block",
      "statements": [
        "(yield execution)"
      ],
      "code": "(yield execution)"
    },
    {
      "id": "n27",
      "type": "block",
      "statements": [
        "raise e",
        "creds_lock"
      ],
      "code": "raise e\ncreds_lock"
    },
    {
      "id": "n28",
      "type": "block",
      "statements": [
        "try:\n    creds_lock.release()\nexcept Exception as e:\n    log_metadata.error(f'Failed to release credentials lock: {e}')",
        "creds_lock.release()",
        "log_metadata.error(f'Failed to release credentials lock: {e}')"
      ],
      "code": "try:\n    creds_lock.release()\nexcept Exception as e:\n    log_metadata.error(f'Failed to release credentials lock: {e}')\ncreds_lock.release()\nlog_metadata.error(f'Failed to release credentials lock: {e}')"
    },
    {
      "id": "n29",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n30",
      "type": "block",
      "statements": [
        "res = update_execution(end_status)",
        "end_status Eq ExecutionStatus.COMPLETED"
      ],
      "code": "res = update_execution(end_status)\nend_status Eq ExecutionStatus.COMPLETED"
    },
    {
      "id": "n31",
      "type": "block",
      "statements": [
        "s = input_size + output_size",
        "t = (res.end_time - res.start_time).total_seconds() if res.end_time and res.start_time else 0",
        "db_client.spend_credits(user_id, credit, node_block.id, input_data, s, t)"
      ],
      "code": "s = input_size + output_size\nt = (res.end_time - res.start_time).total_seconds() if res.end_time and res.start_time else 0\ndb_client.spend_credits(user_id, credit, node_block.id, input_data, s, t)"
    },
    {
      "id": "n32",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n33",
      "type": "block",
      "statements": [
        "execution_stats IsNot None"
      ],
      "code": "execution_stats IsNot None"
    },
    {
      "id": "n34",
      "type": "block",
      "statements": [
        "execution_stats.update(node_block.execution_stats)",
        "execution_stats['input_size'] = input_size",
        "execution_stats['output_size'] = output_size"
      ],
      "code": "execution_stats.update(node_block.execution_stats)\nexecution_stats['input_size'] = input_size\nexecution_stats['output_size'] = output_size"
    },
    {
      "id": "n35",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n36",
      "type": "block",
      "statements": [
        "def _enqueue_next_nodes(db_client: 'DatabaseManager', node: Node, output: BlockData, user_id: str, graph_exec_id: str, graph_id: str, log_metadata: LogMetadata) -> list[NodeExecutionEntry]:\n\n    def add_enqueued_execution(node_exec_id: str, node_id: str, data: BlockInput) -> NodeExecutionEntry:\n        exec_update = db_client.update_execution_status(node_exec_id, ExecutionStatus.QUEUED, data)\n        db_client.send_execution_update(exec_update)\n        return NodeExecutionEntry(user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, node_exec_id=node_exec_id, node_id=node_id, data=data)\n\n    def register_next_executions(node_link: Link) -> list[NodeExecutionEntry]:\n        enqueued_executions = []\n        next_output_name = node_link.source_name\n        next_input_name = node_link.sink_name\n        next_node_id = node_link.sink_id\n        next_data = parse_execution_output(output, next_output_name)\n        if next_data is None:\n            return enqueued_executions\n        next_node = db_client.get_node(next_node_id)\n        with synchronized(f'upsert_input-{next_node_id}-{graph_exec_id}'):\n            (next_node_exec_id, next_node_input) = db_client.upsert_execution_input(node_id=next_node_id, graph_exec_id=graph_exec_id, input_name=next_input_name, input_data=next_data)\n            static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in next_node_input}\n            if static_link_names and (latest_execution := db_client.get_latest_execution(next_node_id, graph_exec_id)):\n                for name in static_link_names:\n                    next_node_input[name] = latest_execution.input_data.get(name)\n            (next_node_input, validation_msg) = validate_exec(next_node, next_node_input)\n            suffix = f'{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}'\n            if not next_node_input:\n                log_metadata.warning(f'Skipped queueing {suffix}')\n                return enqueued_executions\n            log_metadata.info(f'Enqueued {suffix}')\n            enqueued_executions.append(add_enqueued_execution(next_node_exec_id, next_node_id, next_node_input))\n            if not node_link.is_static:\n                return enqueued_executions\n            for iexec in db_client.get_incomplete_executions(next_node_id, graph_exec_id):\n                idata = iexec.input_data\n                ineid = iexec.node_exec_id\n                static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in idata}\n                for input_name in static_link_names:\n                    idata[input_name] = next_node_input[input_name]\n                (idata, msg) = validate_exec(next_node, idata)\n                suffix = f'{next_output_name}>{next_input_name}~{ineid}:{msg}'\n                if not idata:\n                    log_metadata.info(f'Enqueueing static-link skipped: {suffix}')\n                    continue\n                log_metadata.info(f'Enqueueing static-link execution {suffix}')\n                enqueued_executions.append(add_enqueued_execution(iexec.node_exec_id, next_node_id, idata))\n            return enqueued_executions\n    return [execution for link in node.output_links for execution in register_next_executions(link)]",
        "def add_enqueued_execution(node_exec_id: str, node_id: str, data: BlockInput) -> NodeExecutionEntry:\n    exec_update = db_client.update_execution_status(node_exec_id, ExecutionStatus.QUEUED, data)\n    db_client.send_execution_update(exec_update)\n    return NodeExecutionEntry(user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, node_exec_id=node_exec_id, node_id=node_id, data=data)",
        "exec_update = db_client.update_execution_status(node_exec_id, ExecutionStatus.QUEUED, data)",
        "db_client.send_execution_update(exec_update)",
        "return NodeExecutionEntry(user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, node_exec_id=node_exec_id, node_id=node_id, data=data)"
      ],
      "code": "def _enqueue_next_nodes(db_client: 'DatabaseManager', node: Node, output: BlockData, user_id: str, graph_exec_id: str, graph_id: str, log_metadata: LogMetadata) -> list[NodeExecutionEntry]:\n\n    def add_enqueued_execution(node_exec_id: str, node_id: str, data: BlockInput) -> NodeExecutionEntry:\n        exec_update = db_client.update_execution_status(node_exec_id, ExecutionStatus.QUEUED, data)\n        db_client.send_execution_update(exec_update)\n        return NodeExecutionEntry(user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, node_exec_id=node_exec_id, node_id=node_id, data=data)\n\n    def register_next_executions(node_link: Link) -> list[NodeExecutionEntry]:\n        enqueued_executions = []\n        next_output_name = node_link.source_name\n        next_input_name = node_link.sink_name\n        next_node_id = node_link.sink_id\n        next_data = parse_execution_output(output, next_output_name)\n        if next_data is None:\n            return enqueued_executions\n        next_node = db_client.get_node(next_node_id)\n        with synchronized(f'upsert_input-{next_node_id}-{graph_exec_id}'):\n            (next_node_exec_id, next_node_input) = db_client.upsert_execution_input(node_id=next_node_id, graph_exec_id=graph_exec_id, input_name=next_input_name, input_data=next_data)\n            static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in next_node_input}\n            if static_link_names and (latest_execution := db_client.get_latest_execution(next_node_id, graph_exec_id)):\n                for name in static_link_names:\n                    next_node_input[name] = latest_execution.input_data.get(name)\n            (next_node_input, validation_msg) = validate_exec(next_node, next_node_input)\n            suffix = f'{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}'\n            if not next_node_input:\n                log_metadata.warning(f'Skipped queueing {suffix}')\n                return enqueued_executions\n            log_metadata.info(f'Enqueued {suffix}')\n            enqueued_executions.append(add_enqueued_execution(next_node_exec_id, next_node_id, next_node_input))\n            if not node_link.is_static:\n                return enqueued_executions\n            for iexec in db_client.get_incomplete_executions(next_node_id, graph_exec_id):\n                idata = iexec.input_data\n                ineid = iexec.node_exec_id\n                static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in idata}\n                for input_name in static_link_names:\n                    idata[input_name] = next_node_input[input_name]\n                (idata, msg) = validate_exec(next_node, idata)\n                suffix = f'{next_output_name}>{next_input_name}~{ineid}:{msg}'\n                if not idata:\n                    log_metadata.info(f'Enqueueing static-link skipped: {suffix}')\n                    continue\n                log_metadata.info(f'Enqueueing static-link execution {suffix}')\n                enqueued_executions.append(add_enqueued_execution(iexec.node_exec_id, next_node_id, idata))\n            return enqueued_executions\n    return [execution for link in node.output_links for execution in register_next_executions(link)]\ndef add_enqueued_execution(node_exec_id: str, node_id: str, data: BlockInput) -> NodeExecutionEntry:\n    exec_update = db_client.update_execution_status(node_exec_id, ExecutionStatus.QUEUED, data)\n    db_client.send_execution_update(exec_update)\n    return NodeExecutionEntry(user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, node_exec_id=node_exec_id, node_id=node_id, data=data)\nexec_update = db_client.update_execution_status(node_exec_id, ExecutionStatus.QUEUED, data)\ndb_client.send_execution_update(exec_update)\nreturn NodeExecutionEntry(user_id=user_id, graph_exec_id=graph_exec_id, graph_id=graph_id, node_exec_id=node_exec_id, node_id=node_id, data=data)"
    },
    {
      "id": "n37",
      "type": "block",
      "statements": [
        "def register_next_executions(node_link: Link) -> list[NodeExecutionEntry]:\n    enqueued_executions = []\n    next_output_name = node_link.source_name\n    next_input_name = node_link.sink_name\n    next_node_id = node_link.sink_id\n    next_data = parse_execution_output(output, next_output_name)\n    if next_data is None:\n        return enqueued_executions\n    next_node = db_client.get_node(next_node_id)\n    with synchronized(f'upsert_input-{next_node_id}-{graph_exec_id}'):\n        (next_node_exec_id, next_node_input) = db_client.upsert_execution_input(node_id=next_node_id, graph_exec_id=graph_exec_id, input_name=next_input_name, input_data=next_data)\n        static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in next_node_input}\n        if static_link_names and (latest_execution := db_client.get_latest_execution(next_node_id, graph_exec_id)):\n            for name in static_link_names:\n                next_node_input[name] = latest_execution.input_data.get(name)\n        (next_node_input, validation_msg) = validate_exec(next_node, next_node_input)\n        suffix = f'{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}'\n        if not next_node_input:\n            log_metadata.warning(f'Skipped queueing {suffix}')\n            return enqueued_executions\n        log_metadata.info(f'Enqueued {suffix}')\n        enqueued_executions.append(add_enqueued_execution(next_node_exec_id, next_node_id, next_node_input))\n        if not node_link.is_static:\n            return enqueued_executions\n        for iexec in db_client.get_incomplete_executions(next_node_id, graph_exec_id):\n            idata = iexec.input_data\n            ineid = iexec.node_exec_id\n            static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in idata}\n            for input_name in static_link_names:\n                idata[input_name] = next_node_input[input_name]\n            (idata, msg) = validate_exec(next_node, idata)\n            suffix = f'{next_output_name}>{next_input_name}~{ineid}:{msg}'\n            if not idata:\n                log_metadata.info(f'Enqueueing static-link skipped: {suffix}')\n                continue\n            log_metadata.info(f'Enqueueing static-link execution {suffix}')\n            enqueued_executions.append(add_enqueued_execution(iexec.node_exec_id, next_node_id, idata))\n        return enqueued_executions",
        "enqueued_executions = []",
        "next_output_name = node_link.source_name",
        "next_input_name = node_link.sink_name",
        "next_node_id = node_link.sink_id",
        "next_data = parse_execution_output(output, next_output_name)",
        "next_data Is None"
      ],
      "code": "def register_next_executions(node_link: Link) -> list[NodeExecutionEntry]:\n    enqueued_executions = []\n    next_output_name = node_link.source_name\n    next_input_name = node_link.sink_name\n    next_node_id = node_link.sink_id\n    next_data = parse_execution_output(output, next_output_name)\n    if next_data is None:\n        return enqueued_executions\n    next_node = db_client.get_node(next_node_id)\n    with synchronized(f'upsert_input-{next_node_id}-{graph_exec_id}'):\n        (next_node_exec_id, next_node_input) = db_client.upsert_execution_input(node_id=next_node_id, graph_exec_id=graph_exec_id, input_name=next_input_name, input_data=next_data)\n        static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in next_node_input}\n        if static_link_names and (latest_execution := db_client.get_latest_execution(next_node_id, graph_exec_id)):\n            for name in static_link_names:\n                next_node_input[name] = latest_execution.input_data.get(name)\n        (next_node_input, validation_msg) = validate_exec(next_node, next_node_input)\n        suffix = f'{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}'\n        if not next_node_input:\n            log_metadata.warning(f'Skipped queueing {suffix}')\n            return enqueued_executions\n        log_metadata.info(f'Enqueued {suffix}')\n        enqueued_executions.append(add_enqueued_execution(next_node_exec_id, next_node_id, next_node_input))\n        if not node_link.is_static:\n            return enqueued_executions\n        for iexec in db_client.get_incomplete_executions(next_node_id, graph_exec_id):\n            idata = iexec.input_data\n            ineid = iexec.node_exec_id\n            static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in idata}\n            for input_name in static_link_names:\n                idata[input_name] = next_node_input[input_name]\n            (idata, msg) = validate_exec(next_node, idata)\n            suffix = f'{next_output_name}>{next_input_name}~{ineid}:{msg}'\n            if not idata:\n                log_metadata.info(f'Enqueueing static-link skipped: {suffix}')\n                continue\n            log_metadata.info(f'Enqueueing static-link execution {suffix}')\n            enqueued_executions.append(add_enqueued_execution(iexec.node_exec_id, next_node_id, idata))\n        return enqueued_executions\nenqueued_executions = []\nnext_output_name = node_link.source_name\nnext_input_name = node_link.sink_name\nnext_node_id = node_link.sink_id\nnext_data = parse_execution_output(output, next_output_name)\nnext_data Is None"
    },
    {
      "id": "n38",
      "type": "block",
      "statements": [
        "return enqueued_executions"
      ],
      "code": "return enqueued_executions"
    },
    {
      "id": "n39",
      "type": "block",
      "statements": [],
      "code": "\nnext_node = db_client.get_node(next_node_id)\nwith synchronized(f'upsert_input-{next_node_id}-{graph_exec_id}'):\n    (next_node_exec_id, next_node_input) = db_client.upsert_execution_input(node_id=next_node_id, graph_exec_id=graph_exec_id, input_name=next_input_name, input_data=next_data)\n    static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in next_node_input}\n    if static_link_names and (latest_execution := db_client.get_latest_execution(next_node_id, graph_exec_id)):\n        for name in static_link_names:\n            next_node_input[name] = latest_execution.input_data.get(name)\n    (next_node_input, validation_msg) = validate_exec(next_node, next_node_input)\n    suffix = f'{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}'\n    if not next_node_input:\n        log_metadata.warning(f'Skipped queueing {suffix}')\n        return enqueued_executions\n    log_metadata.info(f'Enqueued {suffix}')\n    enqueued_executions.append(add_enqueued_execution(next_node_exec_id, next_node_id, next_node_input))\n    if not node_link.is_static:\n        return enqueued_executions\n    for iexec in db_client.get_incomplete_executions(next_node_id, graph_exec_id):\n        idata = iexec.input_data\n        ineid = iexec.node_exec_id\n        static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in idata}\n        for input_name in static_link_names:\n            idata[input_name] = next_node_input[input_name]\n        (idata, msg) = validate_exec(next_node, idata)\n        suffix = f'{next_output_name}>{next_input_name}~{ineid}:{msg}'\n        if not idata:\n            log_metadata.info(f'Enqueueing static-link skipped: {suffix}')\n            continue\n        log_metadata.info(f'Enqueueing static-link execution {suffix}')\n        enqueued_executions.append(add_enqueued_execution(iexec.node_exec_id, next_node_id, idata))\n    return enqueued_executions\n(next_node_exec_id, next_node_input) = db_client.upsert_execution_input(node_id=next_node_id, graph_exec_id=graph_exec_id, input_name=next_input_name, input_data=next_data)\nstatic_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in next_node_input}\nstatic_link_names and (latest_execution := db_client.get_latest_execution(next_node_id, graph_exec_id))"
    },
    {
      "id": "n40",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n41",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n42",
      "type": "block",
      "statements": [
        "(next_node_input, validation_msg) = validate_exec(next_node, next_node_input)",
        "suffix = f'{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}'",
        "not next_node_input"
      ],
      "code": "(next_node_input, validation_msg) = validate_exec(next_node, next_node_input)\nsuffix = f'{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}'\nnot next_node_input"
    },
    {
      "id": "n43",
      "type": "block",
      "statements": [
        "name",
        "static_link_names"
      ],
      "code": "name\nstatic_link_names"
    },
    {
      "id": "n44",
      "type": "block",
      "statements": [
        "next_node_input[name] = latest_execution.input_data.get(name)"
      ],
      "code": "next_node_input[name] = latest_execution.input_data.get(name)"
    },
    {
      "id": "n45",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n46",
      "type": "block",
      "statements": [
        "log_metadata.warning(f'Skipped queueing {suffix}')",
        "return enqueued_executions"
      ],
      "code": "log_metadata.warning(f'Skipped queueing {suffix}')\nreturn enqueued_executions"
    },
    {
      "id": "n47",
      "type": "block",
      "statements": [],
      "code": "\nlog_metadata.info(f'Enqueued {suffix}')\nenqueued_executions.append(add_enqueued_execution(next_node_exec_id, next_node_id, next_node_input))\nnot node_link.is_static"
    },
    {
      "id": "n48",
      "type": "block",
      "statements": [
        "return enqueued_executions"
      ],
      "code": "return enqueued_executions"
    },
    {
      "id": "n49",
      "type": "block",
      "statements": [],
      "code": "\n"
    },
    {
      "id": "n50",
      "type": "block",
      "statements": [
        "iexec",
        "db_client.get_incomplete_executions(next_node_id, graph_exec_id)"
      ],
      "code": "iexec\ndb_client.get_incomplete_executions(next_node_id, graph_exec_id)"
    },
    {
      "id": "n51",
      "type": "block",
      "statements": [
        "idata = iexec.input_data",
        "ineid = iexec.node_exec_id",
        "static_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in idata}"
      ],
      "code": "idata = iexec.input_data\nineid = iexec.node_exec_id\nstatic_link_names = {link.sink_name for link in next_node.input_links if link.is_static and link.sink_name not in idata}"
    },
    {
      "id": "n52",
      "type": "block",
      "statements": [
        "return enqueued_executions"
      ],
      "code": "return enqueued_executions"
    },
    {
      "id": "n53",
      "type": "block",
      "statements": [
        "input_name",
        "static_link_names"
      ],
      "code": "input_name\nstatic_link_names"
    },
    {
      "id": "n54",
      "type": "block",
      "statements": [
        "idata[input_name] = next_node_input[input_name]"
      ],
      "code": "idata[input_name] = next_node_input[input_name]"
    },
    {
      "id": "n55",
      "type": "block",
      "statements": [
        "(idata, msg) = validate_exec(next_node, idata)",
        "suffix = f'{next_output_name}>{next_input_name}~{ineid}:{msg}'",
        "not idata"
      ],
      "code": "(idata, msg) = validate_exec(next_node, idata)\nsuffix = f'{next_output_name}>{next_input_name}~{ineid}:{msg}'\nnot idata"
    },
    {
      "id": "n56",
      "type": "block",
      "statements": [
        "log_metadata.info(f'Enqueueing static-link skipped: {suffix}')",
        "continue"
      ],
      "code": "log_metadata.info(f'Enqueueing static-link skipped: {suffix}')\ncontinue"
    },
    {
      "id": "n57",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n58",
      "type": "block",
      "statements": [
        "log_metadata.info(f'Enqueueing static-link execution {suffix}')",
        "enqueued_executions.append(add_enqueued_execution(iexec.node_exec_id, next_node_id, idata))"
      ],
      "code": "log_metadata.info(f'Enqueueing static-link execution {suffix}')\nenqueued_executions.append(add_enqueued_execution(iexec.node_exec_id, next_node_id, idata))"
    },
    {
      "id": "n59",
      "type": "block",
      "statements": [
        "return [execution for link in node.output_links for execution in register_next_executions(link)]"
      ],
      "code": "return [execution for link in node.output_links for execution in register_next_executions(link)]"
    },
    {
      "id": "n60",
      "type": "block",
      "statements": [
        "def validate_exec(node: Node, data: BlockInput, resolve_input: bool=True) -> tuple[BlockInput | None, str]:\n    \"\"\"\n    Validate the input data for a node execution.\n\n    Args:\n        node: The node to execute.\n        data: The input data for the node execution.\n        resolve_input: Whether to resolve dynamic pins into dict/list/object.\n\n    Returns:\n        A tuple of the validated data and the block name.\n        If the data is invalid, the first element will be None, and the second element\n        will be an error message.\n        If the data is valid, the first element will be the resolved input data, and\n        the second element will be the block name.\n    \"\"\"\n    node_block: Block | None = get_block(node.block_id)\n    if not node_block:\n        return (None, f'Block for {node.block_id} not found.')\n    if isinstance(node_block, AgentExecutorBlock):\n        try:\n            exec_data = AgentExecutorBlock.Input(**node.input_default)\n        except Exception as e:\n            return (None, f\"Input data doesn't match {node_block.name}: {str(e)}\")\n        input_schema = exec_data.input_schema\n        required_fields = set(input_schema['required'])\n        input_default = exec_data.data\n    else:\n        for (name, data_type) in node_block.input_schema.__annotations__.items():\n            if (value := data.get(name)) and type(value) is not data_type:\n                data[name] = convert(value, data_type)\n        input_schema = node_block.input_schema.jsonschema()\n        required_fields = node_block.input_schema.get_required_fields()\n        input_default = node.input_default\n    error_prefix = f'Input data missing or mismatch for `{node_block.name}`:'\n    input_fields_from_nodes = {link.sink_name for link in node.input_links}\n    if not input_fields_from_nodes.issubset(data):\n        return (None, f'{error_prefix} {input_fields_from_nodes - set(data)}')\n    data = {**input_default, **data}\n    if resolve_input:\n        data = merge_execution_input(data)\n    if not required_fields.issubset(data):\n        return (None, f'{error_prefix} {required_fields - set(data)}')\n    if (error := json.validate_with_jsonschema(schema=input_schema, data=data)):\n        error_message = f'{error_prefix} {error}'\n        logger.error(error_message)\n        return (None, error_message)\n    return (data, node_block.name)",
        "'\\n    Validate the input data for a node execution.\\n\\n    Args:\\n        node: The node to execute.\\n        data: The input data for the node execution.\\n        resolve_input: Whether to resolve dynamic pins into dict/list/object.\\n\\n    Returns:\\n        A tuple of the validated data and the block name.\\n        If the data is invalid, the first element will be None, and the second element\\n        will be an error message.\\n        If the data is valid, the first element will be the resolved input data, and\\n        the second element will be the block name.\\n    '",
        "node_block: Block | None = get_block(node.block_id)",
        "not node_block"
      ],
      "code": "def validate_exec(node: Node, data: BlockInput, resolve_input: bool=True) -> tuple[BlockInput | None, str]:\n    \"\"\"\n    Validate the input data for a node execution.\n\n    Args:\n        node: The node to execute.\n        data: The input data for the node execution.\n        resolve_input: Whether to resolve dynamic pins into dict/list/object.\n\n    Returns:\n        A tuple of the validated data and the block name.\n        If the data is invalid, the first element will be None, and the second element\n        will be an error message.\n        If the data is valid, the first element will be the resolved input data, and\n        the second element will be the block name.\n    \"\"\"\n    node_block: Block | None = get_block(node.block_id)\n    if not node_block:\n        return (None, f'Block for {node.block_id} not found.')\n    if isinstance(node_block, AgentExecutorBlock):\n        try:\n            exec_data = AgentExecutorBlock.Input(**node.input_default)\n        except Exception as e:\n            return (None, f\"Input data doesn't match {node_block.name}: {str(e)}\")\n        input_schema = exec_data.input_schema\n        required_fields = set(input_schema['required'])\n        input_default = exec_data.data\n    else:\n        for (name, data_type) in node_block.input_schema.__annotations__.items():\n            if (value := data.get(name)) and type(value) is not data_type:\n                data[name] = convert(value, data_type)\n        input_schema = node_block.input_schema.jsonschema()\n        required_fields = node_block.input_schema.get_required_fields()\n        input_default = node.input_default\n    error_prefix = f'Input data missing or mismatch for `{node_block.name}`:'\n    input_fields_from_nodes = {link.sink_name for link in node.input_links}\n    if not input_fields_from_nodes.issubset(data):\n        return (None, f'{error_prefix} {input_fields_from_nodes - set(data)}')\n    data = {**input_default, **data}\n    if resolve_input:\n        data = merge_execution_input(data)\n    if not required_fields.issubset(data):\n        return (None, f'{error_prefix} {required_fields - set(data)}')\n    if (error := json.validate_with_jsonschema(schema=input_schema, data=data)):\n        error_message = f'{error_prefix} {error}'\n        logger.error(error_message)\n        return (None, error_message)\n    return (data, node_block.name)\n'\\n    Validate the input data for a node execution.\\n\\n    Args:\\n        node: The node to execute.\\n        data: The input data for the node execution.\\n        resolve_input: Whether to resolve dynamic pins into dict/list/object.\\n\\n    Returns:\\n        A tuple of the validated data and the block name.\\n        If the data is invalid, the first element will be None, and the second element\\n        will be an error message.\\n        If the data is valid, the first element will be the resolved input data, and\\n        the second element will be the block name.\\n    '\nnode_block: Block | None = get_block(node.block_id)\nnot node_block"
    },
    {
      "id": "n61",
      "type": "block",
      "statements": [
        "return (None, f'Block for {node.block_id} not found.')"
      ],
      "code": "return (None, f'Block for {node.block_id} not found.')"
    },
    {
      "id": "n62",
      "type": "block",
      "statements": [],
      "code": "\nisinstance(node_block, AgentExecutorBlock)"
    },
    {
      "id": "n63",
      "type": "block",
      "statements": [
        "try:\n    exec_data = AgentExecutorBlock.Input(**node.input_default)\nexcept Exception as e:\n    return (None, f\"Input data doesn't match {node_block.name}: {str(e)}\")",
        "exec_data = AgentExecutorBlock.Input(**node.input_default)",
        "return (None, f\"Input data doesn't match {node_block.name}: {str(e)}\")"
      ],
      "code": "try:\n    exec_data = AgentExecutorBlock.Input(**node.input_default)\nexcept Exception as e:\n    return (None, f\"Input data doesn't match {node_block.name}: {str(e)}\")\nexec_data = AgentExecutorBlock.Input(**node.input_default)\nreturn (None, f\"Input data doesn't match {node_block.name}: {str(e)}\")"
    },
    {
      "id": "n64",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n65",
      "type": "block",
      "statements": [
        "error_prefix = f'Input data missing or mismatch for `{node_block.name}`:'",
        "input_fields_from_nodes = {link.sink_name for link in node.input_links}",
        "not input_fields_from_nodes.issubset(data)"
      ],
      "code": "error_prefix = f'Input data missing or mismatch for `{node_block.name}`:'\ninput_fields_from_nodes = {link.sink_name for link in node.input_links}\nnot input_fields_from_nodes.issubset(data)"
    },
    {
      "id": "n66",
      "type": "block",
      "statements": [
        "input_schema = exec_data.input_schema",
        "required_fields = set(input_schema['required'])",
        "input_default = exec_data.data"
      ],
      "code": "input_schema = exec_data.input_schema\nrequired_fields = set(input_schema['required'])\ninput_default = exec_data.data"
    },
    {
      "id": "n67",
      "type": "block",
      "statements": [
        "(name, data_type)",
        "node_block.input_schema.__annotations__.items()"
      ],
      "code": "(name, data_type)\nnode_block.input_schema.__annotations__.items()"
    },
    {
      "id": "n68",
      "type": "block",
      "statements": [
        "(value := data.get(name)) and type(value) is not data_type"
      ],
      "code": "(value := data.get(name)) and type(value) is not data_type"
    },
    {
      "id": "n69",
      "type": "block",
      "statements": [
        "input_schema = node_block.input_schema.jsonschema()",
        "required_fields = node_block.input_schema.get_required_fields()",
        "input_default = node.input_default"
      ],
      "code": "input_schema = node_block.input_schema.jsonschema()\nrequired_fields = node_block.input_schema.get_required_fields()\ninput_default = node.input_default"
    },
    {
      "id": "n70",
      "type": "block",
      "statements": [
        "data[name] = convert(value, data_type)"
      ],
      "code": "data[name] = convert(value, data_type)"
    },
    {
      "id": "n71",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n72",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n73",
      "type": "block",
      "statements": [
        "return (None, f'{error_prefix} {input_fields_from_nodes - set(data)}')"
      ],
      "code": "return (None, f'{error_prefix} {input_fields_from_nodes - set(data)}')"
    },
    {
      "id": "n74",
      "type": "block",
      "statements": [],
      "code": "\ndata = {**input_default, **data}\nresolve_input"
    },
    {
      "id": "n75",
      "type": "block",
      "statements": [
        "data = merge_execution_input(data)"
      ],
      "code": "data = merge_execution_input(data)"
    },
    {
      "id": "n76",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n77",
      "type": "block",
      "statements": [
        "not required_fields.issubset(data)"
      ],
      "code": "not required_fields.issubset(data)"
    },
    {
      "id": "n78",
      "type": "block",
      "statements": [
        "return (None, f'{error_prefix} {required_fields - set(data)}')"
      ],
      "code": "return (None, f'{error_prefix} {required_fields - set(data)}')"
    },
    {
      "id": "n79",
      "type": "block",
      "statements": [],
      "code": "\n(error := json.validate_with_jsonschema(schema=input_schema, data=data))"
    },
    {
      "id": "n80",
      "type": "block",
      "statements": [
        "error_message = f'{error_prefix} {error}'",
        "logger.error(error_message)",
        "return (None, error_message)"
      ],
      "code": "error_message = f'{error_prefix} {error}'\nlogger.error(error_message)\nreturn (None, error_message)"
    },
    {
      "id": "n81",
      "type": "block",
      "statements": [],
      "code": "\nreturn (data, node_block.name)"
    },
    {
      "id": "n82",
      "type": "block",
      "statements": [
        "class Executor:\n    \"\"\"\n    This class contains event handlers for the process pool executor events.\n\n    The main events are:\n        on_node_executor_start: Initialize the process that executes the node.\n        on_node_execution: Execution logic for a node.\n\n        on_graph_executor_start: Initialize the process that executes the graph.\n        on_graph_execution: Execution logic for a graph.\n\n    The execution flow:\n        1. Graph execution request is added to the queue.\n        2. Graph executor loop picks the request from the queue.\n        3. Graph executor loop submits the graph execution request to the executor pool.\n      [on_graph_execution]\n        4. Graph executor initialize the node execution queue.\n        5. Graph executor adds the starting nodes to the node execution queue.\n        6. Graph executor waits for all nodes to be executed.\n      [on_node_execution]\n        7. Node executor picks the node execution request from the queue.\n        8. Node executor executes the node.\n        9. Node executor enqueues the next executed nodes to the node execution queue.\n    \"\"\"\n\n    @classmethod\n    def on_node_executor_start(cls):\n        configure_logging()\n        set_service_name('NodeExecutor')\n        redis.connect()\n        cls.pid = os.getpid()\n        cls.db_client = get_db_client()\n        cls.creds_manager = IntegrationCredentialsManager()\n        cls.shutdown_lock = threading.Lock()\n        atexit.register(cls.on_node_executor_stop)\n        signal.signal(signal.SIGTERM, lambda _, __: cls.on_node_executor_sigterm())\n\n    @classmethod\n    def on_node_executor_stop(cls):\n        if not cls.shutdown_lock.acquire(blocking=False):\n            return\n        logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Releasing locks...')\n        cls.creds_manager.release_all_locks()\n        logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis...')\n        redis.disconnect()\n        logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting DB manager...')\n        close_service_client(cls.db_client)\n        logger.info(f'[on_node_executor_stop {cls.pid}] ✅ Finished cleanup')\n\n    @classmethod\n    def on_node_executor_sigterm(cls):\n        llprint(f'[on_node_executor_sigterm {cls.pid}] ⚠️ SIGTERM received')\n        if not cls.shutdown_lock.acquire(blocking=False):\n            return\n        llprint(f'[on_node_executor_stop {cls.pid}] ⏳ Releasing locks...')\n        cls.creds_manager.release_all_locks()\n        llprint(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis...')\n        redis.disconnect()\n        llprint(f'[on_node_executor_stop {cls.pid}] ✅ Finished cleanup')\n        sys.exit(0)\n\n    @classmethod\n    @error_logged\n    def on_node_execution(cls, q: ExecutionQueue[NodeExecutionEntry], node_exec: NodeExecutionEntry) -> dict[str, Any]:\n        log_metadata = LogMetadata(user_id=node_exec.user_id, graph_eid=node_exec.graph_exec_id, graph_id=node_exec.graph_id, node_eid=node_exec.node_exec_id, node_id=node_exec.node_id, block_name='-')\n        execution_stats = {}\n        (timing_info, _) = cls._on_node_execution(q, node_exec, log_metadata, execution_stats)\n        execution_stats['walltime'] = timing_info.wall_time\n        execution_stats['cputime'] = timing_info.cpu_time\n        cls.db_client.update_node_execution_stats(node_exec.node_exec_id, execution_stats)\n        return execution_stats\n\n    @classmethod\n    @time_measured\n    def _on_node_execution(cls, q: ExecutionQueue[NodeExecutionEntry], node_exec: NodeExecutionEntry, log_metadata: LogMetadata, stats: dict[str, Any] | None=None):\n        try:\n            log_metadata.info(f'Start node execution {node_exec.node_exec_id}')\n            for execution in execute_node(cls.db_client, cls.creds_manager, node_exec, stats):\n                q.add(execution)\n            log_metadata.info(f'Finished node execution {node_exec.node_exec_id}')\n        except Exception as e:\n            log_metadata.exception(f'Failed node execution {node_exec.node_exec_id}: {e}')\n\n    @classmethod\n    def on_graph_executor_start(cls):\n        configure_logging()\n        set_service_name('GraphExecutor')\n        cls.db_client = get_db_client()\n        cls.pool_size = settings.config.num_node_workers\n        cls.pid = os.getpid()\n        cls._init_node_executor_pool()\n        logger.info(f'Graph executor {cls.pid} started with {cls.pool_size} node workers')\n        atexit.register(cls.on_graph_executor_stop)\n\n    @classmethod\n    def on_graph_executor_stop(cls):\n        prefix = f'[on_graph_executor_stop {cls.pid}]'\n        logger.info(f'{prefix} ⏳ Terminating node executor pool...')\n        cls.executor.terminate()\n        logger.info(f'{prefix} ⏳ Disconnecting DB manager...')\n        close_service_client(cls.db_client)\n        logger.info(f'{prefix} ✅ Finished cleanup')\n\n    @classmethod\n    def _init_node_executor_pool(cls):\n        cls.executor = Pool(processes=cls.pool_size, initializer=cls.on_node_executor_start)\n\n    @classmethod\n    @error_logged\n    def on_graph_execution(cls, graph_exec: GraphExecutionEntry, cancel: threading.Event):\n        log_metadata = LogMetadata(user_id=graph_exec.user_id, graph_eid=graph_exec.graph_exec_id, graph_id=graph_exec.graph_id, node_id='*', node_eid='*', block_name='-')\n        (timing_info, (exec_stats, error)) = cls._on_graph_execution(graph_exec, cancel, log_metadata)\n        exec_stats['walltime'] = timing_info.wall_time\n        exec_stats['cputime'] = timing_info.cpu_time\n        exec_stats['error'] = str(error) if error else None\n        result = cls.db_client.update_graph_execution_stats(graph_exec_id=graph_exec.graph_exec_id, stats=exec_stats)\n        cls.db_client.send_execution_update(result)\n\n    @classmethod\n    @time_measured\n    def _on_graph_execution(cls, graph_exec: GraphExecutionEntry, cancel: threading.Event, log_metadata: LogMetadata) -> tuple[dict[str, Any], Exception | None]:\n        \"\"\"\n        Returns:\n            The execution statistics of the graph execution.\n            The error that occurred during the execution.\n        \"\"\"\n        log_metadata.info(f'Start graph execution {graph_exec.graph_exec_id}')\n        exec_stats = {'nodes_walltime': 0, 'nodes_cputime': 0, 'node_count': 0}\n        error = None\n        finished = False\n\n        def cancel_handler():\n            while not cancel.is_set():\n                cancel.wait(1)\n            if finished:\n                return\n            cls.executor.terminate()\n            log_metadata.info(f'Terminated graph execution {graph_exec.graph_exec_id}')\n            cls._init_node_executor_pool()\n        cancel_thread = threading.Thread(target=cancel_handler)\n        cancel_thread.start()\n        try:\n            queue = ExecutionQueue[NodeExecutionEntry]()\n            for node_exec in graph_exec.start_node_execs:\n                queue.add(node_exec)\n            running_executions: dict[str, AsyncResult] = {}\n\n            def make_exec_callback(exec_data: NodeExecutionEntry):\n                node_id = exec_data.node_id\n\n                def callback(result: object):\n                    running_executions.pop(node_id)\n                    nonlocal exec_stats\n                    if isinstance(result, dict):\n                        exec_stats['node_count'] += 1\n                        exec_stats['nodes_cputime'] += result.get('cputime', 0)\n                        exec_stats['nodes_walltime'] += result.get('walltime', 0)\n                return callback\n            while not queue.empty():\n                if cancel.is_set():\n                    error = RuntimeError('Execution is cancelled')\n                    return (exec_stats, error)\n                exec_data = queue.get()\n                execution = running_executions.get(exec_data.node_id)\n                if execution and (not execution.ready()):\n                    execution.wait()\n                log_metadata.debug(f'Dispatching node execution {exec_data.node_exec_id} for node {exec_data.node_id}')\n                running_executions[exec_data.node_id] = cls.executor.apply_async(cls.on_node_execution, (queue, exec_data), callback=make_exec_callback(exec_data))\n                while queue.empty() and running_executions:\n                    log_metadata.debug(f'Queue empty; running nodes: {list(running_executions.keys())}')\n                    for (node_id, execution) in list(running_executions.items()):\n                        if cancel.is_set():\n                            error = RuntimeError('Execution is cancelled')\n                            return (exec_stats, error)\n                        if not queue.empty():\n                            break\n                        log_metadata.debug(f'Waiting on execution of node {node_id}')\n                        execution.wait(3)\n            log_metadata.info(f'Finished graph execution {graph_exec.graph_exec_id}')\n        except Exception as e:\n            log_metadata.exception(f'Failed graph execution {graph_exec.graph_exec_id}: {e}')\n            error = e\n        finally:\n            if not cancel.is_set():\n                finished = True\n                cancel.set()\n            cancel_thread.join()\n            return (exec_stats, error)",
        "'\\n    This class contains event handlers for the process pool executor events.\\n\\n    The main events are:\\n        on_node_executor_start: Initialize the process that executes the node.\\n        on_node_execution: Execution logic for a node.\\n\\n        on_graph_executor_start: Initialize the process that executes the graph.\\n        on_graph_execution: Execution logic for a graph.\\n\\n    The execution flow:\\n        1. Graph execution request is added to the queue.\\n        2. Graph executor loop picks the request from the queue.\\n        3. Graph executor loop submits the graph execution request to the executor pool.\\n      [on_graph_execution]\\n        4. Graph executor initialize the node execution queue.\\n        5. Graph executor adds the starting nodes to the node execution queue.\\n        6. Graph executor waits for all nodes to be executed.\\n      [on_node_execution]\\n        7. Node executor picks the node execution request from the queue.\\n        8. Node executor executes the node.\\n        9. Node executor enqueues the next executed nodes to the node execution queue.\\n    '",
        "@classmethod\ndef on_node_executor_start(cls):\n    configure_logging()\n    set_service_name('NodeExecutor')\n    redis.connect()\n    cls.pid = os.getpid()\n    cls.db_client = get_db_client()\n    cls.creds_manager = IntegrationCredentialsManager()\n    cls.shutdown_lock = threading.Lock()\n    atexit.register(cls.on_node_executor_stop)\n    signal.signal(signal.SIGTERM, lambda _, __: cls.on_node_executor_sigterm())",
        "configure_logging()",
        "set_service_name('NodeExecutor')",
        "redis.connect()",
        "cls.pid = os.getpid()",
        "cls.db_client = get_db_client()",
        "cls.creds_manager = IntegrationCredentialsManager()",
        "cls.shutdown_lock = threading.Lock()",
        "atexit.register(cls.on_node_executor_stop)",
        "signal.signal(signal.SIGTERM, lambda _, __: cls.on_node_executor_sigterm())",
        "@classmethod\ndef on_node_executor_stop(cls):\n    if not cls.shutdown_lock.acquire(blocking=False):\n        return\n    logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Releasing locks...')\n    cls.creds_manager.release_all_locks()\n    logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis...')\n    redis.disconnect()\n    logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting DB manager...')\n    close_service_client(cls.db_client)\n    logger.info(f'[on_node_executor_stop {cls.pid}] ✅ Finished cleanup')",
        "not cls.shutdown_lock.acquire(blocking=False)"
      ],
      "code": "class Executor:\n    \"\"\"\n    This class contains event handlers for the process pool executor events.\n\n    The main events are:\n        on_node_executor_start: Initialize the process that executes the node.\n        on_node_execution: Execution logic for a node.\n\n        on_graph_executor_start: Initialize the process that executes the graph.\n        on_graph_execution: Execution logic for a graph.\n\n    The execution flow:\n        1. Graph execution request is added to the queue.\n        2. Graph executor loop picks the request from the queue.\n        3. Graph executor loop submits the graph execution request to the executor pool.\n      [on_graph_execution]\n        4. Graph executor initialize the node execution queue.\n        5. Graph executor adds the starting nodes to the node execution queue.\n        6. Graph executor waits for all nodes to be executed.\n      [on_node_execution]\n        7. Node executor picks the node execution request from the queue.\n        8. Node executor executes the node.\n        9. Node executor enqueues the next executed nodes to the node execution queue.\n    \"\"\"\n\n    @classmethod\n    def on_node_executor_start(cls):\n        configure_logging()\n        set_service_name('NodeExecutor')\n        redis.connect()\n        cls.pid = os.getpid()\n        cls.db_client = get_db_client()\n        cls.creds_manager = IntegrationCredentialsManager()\n        cls.shutdown_lock = threading.Lock()\n        atexit.register(cls.on_node_executor_stop)\n        signal.signal(signal.SIGTERM, lambda _, __: cls.on_node_executor_sigterm())\n\n    @classmethod\n    def on_node_executor_stop(cls):\n        if not cls.shutdown_lock.acquire(blocking=False):\n            return\n        logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Releasing locks...')\n        cls.creds_manager.release_all_locks()\n        logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis...')\n        redis.disconnect()\n        logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting DB manager...')\n        close_service_client(cls.db_client)\n        logger.info(f'[on_node_executor_stop {cls.pid}] ✅ Finished cleanup')\n\n    @classmethod\n    def on_node_executor_sigterm(cls):\n        llprint(f'[on_node_executor_sigterm {cls.pid}] ⚠️ SIGTERM received')\n        if not cls.shutdown_lock.acquire(blocking=False):\n            return\n        llprint(f'[on_node_executor_stop {cls.pid}] ⏳ Releasing locks...')\n        cls.creds_manager.release_all_locks()\n        llprint(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis...')\n        redis.disconnect()\n        llprint(f'[on_node_executor_stop {cls.pid}] ✅ Finished cleanup')\n        sys.exit(0)\n\n    @classmethod\n    @error_logged\n    def on_node_execution(cls, q: ExecutionQueue[NodeExecutionEntry], node_exec: NodeExecutionEntry) -> dict[str, Any]:\n        log_metadata = LogMetadata(user_id=node_exec.user_id, graph_eid=node_exec.graph_exec_id, graph_id=node_exec.graph_id, node_eid=node_exec.node_exec_id, node_id=node_exec.node_id, block_name='-')\n        execution_stats = {}\n        (timing_info, _) = cls._on_node_execution(q, node_exec, log_metadata, execution_stats)\n        execution_stats['walltime'] = timing_info.wall_time\n        execution_stats['cputime'] = timing_info.cpu_time\n        cls.db_client.update_node_execution_stats(node_exec.node_exec_id, execution_stats)\n        return execution_stats\n\n    @classmethod\n    @time_measured\n    def _on_node_execution(cls, q: ExecutionQueue[NodeExecutionEntry], node_exec: NodeExecutionEntry, log_metadata: LogMetadata, stats: dict[str, Any] | None=None):\n        try:\n            log_metadata.info(f'Start node execution {node_exec.node_exec_id}')\n            for execution in execute_node(cls.db_client, cls.creds_manager, node_exec, stats):\n                q.add(execution)\n            log_metadata.info(f'Finished node execution {node_exec.node_exec_id}')\n        except Exception as e:\n            log_metadata.exception(f'Failed node execution {node_exec.node_exec_id}: {e}')\n\n    @classmethod\n    def on_graph_executor_start(cls):\n        configure_logging()\n        set_service_name('GraphExecutor')\n        cls.db_client = get_db_client()\n        cls.pool_size = settings.config.num_node_workers\n        cls.pid = os.getpid()\n        cls._init_node_executor_pool()\n        logger.info(f'Graph executor {cls.pid} started with {cls.pool_size} node workers')\n        atexit.register(cls.on_graph_executor_stop)\n\n    @classmethod\n    def on_graph_executor_stop(cls):\n        prefix = f'[on_graph_executor_stop {cls.pid}]'\n        logger.info(f'{prefix} ⏳ Terminating node executor pool...')\n        cls.executor.terminate()\n        logger.info(f'{prefix} ⏳ Disconnecting DB manager...')\n        close_service_client(cls.db_client)\n        logger.info(f'{prefix} ✅ Finished cleanup')\n\n    @classmethod\n    def _init_node_executor_pool(cls):\n        cls.executor = Pool(processes=cls.pool_size, initializer=cls.on_node_executor_start)\n\n    @classmethod\n    @error_logged\n    def on_graph_execution(cls, graph_exec: GraphExecutionEntry, cancel: threading.Event):\n        log_metadata = LogMetadata(user_id=graph_exec.user_id, graph_eid=graph_exec.graph_exec_id, graph_id=graph_exec.graph_id, node_id='*', node_eid='*', block_name='-')\n        (timing_info, (exec_stats, error)) = cls._on_graph_execution(graph_exec, cancel, log_metadata)\n        exec_stats['walltime'] = timing_info.wall_time\n        exec_stats['cputime'] = timing_info.cpu_time\n        exec_stats['error'] = str(error) if error else None\n        result = cls.db_client.update_graph_execution_stats(graph_exec_id=graph_exec.graph_exec_id, stats=exec_stats)\n        cls.db_client.send_execution_update(result)\n\n    @classmethod\n    @time_measured\n    def _on_graph_execution(cls, graph_exec: GraphExecutionEntry, cancel: threading.Event, log_metadata: LogMetadata) -> tuple[dict[str, Any], Exception | None]:\n        \"\"\"\n        Returns:\n            The execution statistics of the graph execution.\n            The error that occurred during the execution.\n        \"\"\"\n        log_metadata.info(f'Start graph execution {graph_exec.graph_exec_id}')\n        exec_stats = {'nodes_walltime': 0, 'nodes_cputime': 0, 'node_count': 0}\n        error = None\n        finished = False\n\n        def cancel_handler():\n            while not cancel.is_set():\n                cancel.wait(1)\n            if finished:\n                return\n            cls.executor.terminate()\n            log_metadata.info(f'Terminated graph execution {graph_exec.graph_exec_id}')\n            cls._init_node_executor_pool()\n        cancel_thread = threading.Thread(target=cancel_handler)\n        cancel_thread.start()\n        try:\n            queue = ExecutionQueue[NodeExecutionEntry]()\n            for node_exec in graph_exec.start_node_execs:\n                queue.add(node_exec)\n            running_executions: dict[str, AsyncResult] = {}\n\n            def make_exec_callback(exec_data: NodeExecutionEntry):\n                node_id = exec_data.node_id\n\n                def callback(result: object):\n                    running_executions.pop(node_id)\n                    nonlocal exec_stats\n                    if isinstance(result, dict):\n                        exec_stats['node_count'] += 1\n                        exec_stats['nodes_cputime'] += result.get('cputime', 0)\n                        exec_stats['nodes_walltime'] += result.get('walltime', 0)\n                return callback\n            while not queue.empty():\n                if cancel.is_set():\n                    error = RuntimeError('Execution is cancelled')\n                    return (exec_stats, error)\n                exec_data = queue.get()\n                execution = running_executions.get(exec_data.node_id)\n                if execution and (not execution.ready()):\n                    execution.wait()\n                log_metadata.debug(f'Dispatching node execution {exec_data.node_exec_id} for node {exec_data.node_id}')\n                running_executions[exec_data.node_id] = cls.executor.apply_async(cls.on_node_execution, (queue, exec_data), callback=make_exec_callback(exec_data))\n                while queue.empty() and running_executions:\n                    log_metadata.debug(f'Queue empty; running nodes: {list(running_executions.keys())}')\n                    for (node_id, execution) in list(running_executions.items()):\n                        if cancel.is_set():\n                            error = RuntimeError('Execution is cancelled')\n                            return (exec_stats, error)\n                        if not queue.empty():\n                            break\n                        log_metadata.debug(f'Waiting on execution of node {node_id}')\n                        execution.wait(3)\n            log_metadata.info(f'Finished graph execution {graph_exec.graph_exec_id}')\n        except Exception as e:\n            log_metadata.exception(f'Failed graph execution {graph_exec.graph_exec_id}: {e}')\n            error = e\n        finally:\n            if not cancel.is_set():\n                finished = True\n                cancel.set()\n            cancel_thread.join()\n            return (exec_stats, error)\n'\\n    This class contains event handlers for the process pool executor events.\\n\\n    The main events are:\\n        on_node_executor_start: Initialize the process that executes the node.\\n        on_node_execution: Execution logic for a node.\\n\\n        on_graph_executor_start: Initialize the process that executes the graph.\\n        on_graph_execution: Execution logic for a graph.\\n\\n    The execution flow:\\n        1. Graph execution request is added to the queue.\\n        2. Graph executor loop picks the request from the queue.\\n        3. Graph executor loop submits the graph execution request to the executor pool.\\n      [on_graph_execution]\\n        4. Graph executor initialize the node execution queue.\\n        5. Graph executor adds the starting nodes to the node execution queue.\\n        6. Graph executor waits for all nodes to be executed.\\n      [on_node_execution]\\n        7. Node executor picks the node execution request from the queue.\\n        8. Node executor executes the node.\\n        9. Node executor enqueues the next executed nodes to the node execution queue.\\n    '\n@classmethod\ndef on_node_executor_start(cls):\n    configure_logging()\n    set_service_name('NodeExecutor')\n    redis.connect()\n    cls.pid = os.getpid()\n    cls.db_client = get_db_client()\n    cls.creds_manager = IntegrationCredentialsManager()\n    cls.shutdown_lock = threading.Lock()\n    atexit.register(cls.on_node_executor_stop)\n    signal.signal(signal.SIGTERM, lambda _, __: cls.on_node_executor_sigterm())\nconfigure_logging()\nset_service_name('NodeExecutor')\nredis.connect()\ncls.pid = os.getpid()\ncls.db_client = get_db_client()\ncls.creds_manager = IntegrationCredentialsManager()\ncls.shutdown_lock = threading.Lock()\natexit.register(cls.on_node_executor_stop)\nsignal.signal(signal.SIGTERM, lambda _, __: cls.on_node_executor_sigterm())\n@classmethod\ndef on_node_executor_stop(cls):\n    if not cls.shutdown_lock.acquire(blocking=False):\n        return\n    logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Releasing locks...')\n    cls.creds_manager.release_all_locks()\n    logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis...')\n    redis.disconnect()\n    logger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting DB manager...')\n    close_service_client(cls.db_client)\n    logger.info(f'[on_node_executor_stop {cls.pid}] ✅ Finished cleanup')\nnot cls.shutdown_lock.acquire(blocking=False)"
    },
    {
      "id": "n83",
      "type": "block",
      "statements": [
        "return"
      ],
      "code": "return"
    },
    {
      "id": "n84",
      "type": "block",
      "statements": [],
      "code": "\nlogger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Releasing locks...')\ncls.creds_manager.release_all_locks()\nlogger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis...')\nredis.disconnect()\nlogger.info(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting DB manager...')\nclose_service_client(cls.db_client)\nlogger.info(f'[on_node_executor_stop {cls.pid}] ✅ Finished cleanup')\n@classmethod\ndef on_node_executor_sigterm(cls):\n    llprint(f'[on_node_executor_sigterm {cls.pid}] ⚠️ SIGTERM received')\n    if not cls.shutdown_lock.acquire(blocking=False):\n        return\n    llprint(f'[on_node_executor_stop {cls.pid}] ⏳ Releasing locks...')\n    cls.creds_manager.release_all_locks()\n    llprint(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis...')\n    redis.disconnect()\n    llprint(f'[on_node_executor_stop {cls.pid}] ✅ Finished cleanup')\n    sys.exit(0)\nllprint(f'[on_node_executor_sigterm {cls.pid}] ⚠️ SIGTERM received')\nnot cls.shutdown_lock.acquire(blocking=False)"
    },
    {
      "id": "n85",
      "type": "block",
      "statements": [
        "return"
      ],
      "code": "return"
    },
    {
      "id": "n86",
      "type": "block",
      "statements": [],
      "code": "\nllprint(f'[on_node_executor_stop {cls.pid}] ⏳ Releasing locks...')\ncls.creds_manager.release_all_locks()\nllprint(f'[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis...')\nredis.disconnect()\nllprint(f'[on_node_executor_stop {cls.pid}] ✅ Finished cleanup')\nsys.exit(0)\n@classmethod\n@error_logged\ndef on_node_execution(cls, q: ExecutionQueue[NodeExecutionEntry], node_exec: NodeExecutionEntry) -> dict[str, Any]:\n    log_metadata = LogMetadata(user_id=node_exec.user_id, graph_eid=node_exec.graph_exec_id, graph_id=node_exec.graph_id, node_eid=node_exec.node_exec_id, node_id=node_exec.node_id, block_name='-')\n    execution_stats = {}\n    (timing_info, _) = cls._on_node_execution(q, node_exec, log_metadata, execution_stats)\n    execution_stats['walltime'] = timing_info.wall_time\n    execution_stats['cputime'] = timing_info.cpu_time\n    cls.db_client.update_node_execution_stats(node_exec.node_exec_id, execution_stats)\n    return execution_stats\nlog_metadata = LogMetadata(user_id=node_exec.user_id, graph_eid=node_exec.graph_exec_id, graph_id=node_exec.graph_id, node_eid=node_exec.node_exec_id, node_id=node_exec.node_id, block_name='-')\nexecution_stats = {}\n(timing_info, _) = cls._on_node_execution(q, node_exec, log_metadata, execution_stats)\nexecution_stats['walltime'] = timing_info.wall_time\nexecution_stats['cputime'] = timing_info.cpu_time\ncls.db_client.update_node_execution_stats(node_exec.node_exec_id, execution_stats)\nreturn execution_stats"
    },
    {
      "id": "n87",
      "type": "block",
      "statements": [
        "@classmethod\n@time_measured\ndef _on_node_execution(cls, q: ExecutionQueue[NodeExecutionEntry], node_exec: NodeExecutionEntry, log_metadata: LogMetadata, stats: dict[str, Any] | None=None):\n    try:\n        log_metadata.info(f'Start node execution {node_exec.node_exec_id}')\n        for execution in execute_node(cls.db_client, cls.creds_manager, node_exec, stats):\n            q.add(execution)\n        log_metadata.info(f'Finished node execution {node_exec.node_exec_id}')\n    except Exception as e:\n        log_metadata.exception(f'Failed node execution {node_exec.node_exec_id}: {e}')",
        "try:\n    log_metadata.info(f'Start node execution {node_exec.node_exec_id}')\n    for execution in execute_node(cls.db_client, cls.creds_manager, node_exec, stats):\n        q.add(execution)\n    log_metadata.info(f'Finished node execution {node_exec.node_exec_id}')\nexcept Exception as e:\n    log_metadata.exception(f'Failed node execution {node_exec.node_exec_id}: {e}')",
        "log_metadata.info(f'Start node execution {node_exec.node_exec_id}')"
      ],
      "code": "@classmethod\n@time_measured\ndef _on_node_execution(cls, q: ExecutionQueue[NodeExecutionEntry], node_exec: NodeExecutionEntry, log_metadata: LogMetadata, stats: dict[str, Any] | None=None):\n    try:\n        log_metadata.info(f'Start node execution {node_exec.node_exec_id}')\n        for execution in execute_node(cls.db_client, cls.creds_manager, node_exec, stats):\n            q.add(execution)\n        log_metadata.info(f'Finished node execution {node_exec.node_exec_id}')\n    except Exception as e:\n        log_metadata.exception(f'Failed node execution {node_exec.node_exec_id}: {e}')\ntry:\n    log_metadata.info(f'Start node execution {node_exec.node_exec_id}')\n    for execution in execute_node(cls.db_client, cls.creds_manager, node_exec, stats):\n        q.add(execution)\n    log_metadata.info(f'Finished node execution {node_exec.node_exec_id}')\nexcept Exception as e:\n    log_metadata.exception(f'Failed node execution {node_exec.node_exec_id}: {e}')\nlog_metadata.info(f'Start node execution {node_exec.node_exec_id}')"
    },
    {
      "id": "n88",
      "type": "block",
      "statements": [
        "execution",
        "execute_node(cls.db_client, cls.creds_manager, node_exec, stats)"
      ],
      "code": "execution\nexecute_node(cls.db_client, cls.creds_manager, node_exec, stats)"
    },
    {
      "id": "n89",
      "type": "block",
      "statements": [
        "q.add(execution)"
      ],
      "code": "q.add(execution)"
    },
    {
      "id": "n90",
      "type": "block",
      "statements": [
        "log_metadata.info(f'Finished node execution {node_exec.node_exec_id}')",
        "log_metadata.exception(f'Failed node execution {node_exec.node_exec_id}: {e}')",
        "@classmethod\ndef on_graph_executor_start(cls):\n    configure_logging()\n    set_service_name('GraphExecutor')\n    cls.db_client = get_db_client()\n    cls.pool_size = settings.config.num_node_workers\n    cls.pid = os.getpid()\n    cls._init_node_executor_pool()\n    logger.info(f'Graph executor {cls.pid} started with {cls.pool_size} node workers')\n    atexit.register(cls.on_graph_executor_stop)",
        "configure_logging()",
        "set_service_name('GraphExecutor')",
        "cls.db_client = get_db_client()",
        "cls.pool_size = settings.config.num_node_workers",
        "cls.pid = os.getpid()",
        "cls._init_node_executor_pool()",
        "logger.info(f'Graph executor {cls.pid} started with {cls.pool_size} node workers')",
        "atexit.register(cls.on_graph_executor_stop)",
        "@classmethod\ndef on_graph_executor_stop(cls):\n    prefix = f'[on_graph_executor_stop {cls.pid}]'\n    logger.info(f'{prefix} ⏳ Terminating node executor pool...')\n    cls.executor.terminate()\n    logger.info(f'{prefix} ⏳ Disconnecting DB manager...')\n    close_service_client(cls.db_client)\n    logger.info(f'{prefix} ✅ Finished cleanup')",
        "prefix = f'[on_graph_executor_stop {cls.pid}]'",
        "logger.info(f'{prefix} ⏳ Terminating node executor pool...')",
        "cls.executor.terminate()",
        "logger.info(f'{prefix} ⏳ Disconnecting DB manager...')",
        "close_service_client(cls.db_client)",
        "logger.info(f'{prefix} ✅ Finished cleanup')",
        "@classmethod\ndef _init_node_executor_pool(cls):\n    cls.executor = Pool(processes=cls.pool_size, initializer=cls.on_node_executor_start)",
        "cls.executor = Pool(processes=cls.pool_size, initializer=cls.on_node_executor_start)",
        "@classmethod\n@error_logged\ndef on_graph_execution(cls, graph_exec: GraphExecutionEntry, cancel: threading.Event):\n    log_metadata = LogMetadata(user_id=graph_exec.user_id, graph_eid=graph_exec.graph_exec_id, graph_id=graph_exec.graph_id, node_id='*', node_eid='*', block_name='-')\n    (timing_info, (exec_stats, error)) = cls._on_graph_execution(graph_exec, cancel, log_metadata)\n    exec_stats['walltime'] = timing_info.wall_time\n    exec_stats['cputime'] = timing_info.cpu_time\n    exec_stats['error'] = str(error) if error else None\n    result = cls.db_client.update_graph_execution_stats(graph_exec_id=graph_exec.graph_exec_id, stats=exec_stats)\n    cls.db_client.send_execution_update(result)",
        "log_metadata = LogMetadata(user_id=graph_exec.user_id, graph_eid=graph_exec.graph_exec_id, graph_id=graph_exec.graph_id, node_id='*', node_eid='*', block_name='-')",
        "(timing_info, (exec_stats, error)) = cls._on_graph_execution(graph_exec, cancel, log_metadata)",
        "exec_stats['walltime'] = timing_info.wall_time",
        "exec_stats['cputime'] = timing_info.cpu_time",
        "exec_stats['error'] = str(error) if error else None",
        "result = cls.db_client.update_graph_execution_stats(graph_exec_id=graph_exec.graph_exec_id, stats=exec_stats)",
        "cls.db_client.send_execution_update(result)",
        "@classmethod\n@time_measured\ndef _on_graph_execution(cls, graph_exec: GraphExecutionEntry, cancel: threading.Event, log_metadata: LogMetadata) -> tuple[dict[str, Any], Exception | None]:\n    \"\"\"\n        Returns:\n            The execution statistics of the graph execution.\n            The error that occurred during the execution.\n        \"\"\"\n    log_metadata.info(f'Start graph execution {graph_exec.graph_exec_id}')\n    exec_stats = {'nodes_walltime': 0, 'nodes_cputime': 0, 'node_count': 0}\n    error = None\n    finished = False\n\n    def cancel_handler():\n        while not cancel.is_set():\n            cancel.wait(1)\n        if finished:\n            return\n        cls.executor.terminate()\n        log_metadata.info(f'Terminated graph execution {graph_exec.graph_exec_id}')\n        cls._init_node_executor_pool()\n    cancel_thread = threading.Thread(target=cancel_handler)\n    cancel_thread.start()\n    try:\n        queue = ExecutionQueue[NodeExecutionEntry]()\n        for node_exec in graph_exec.start_node_execs:\n            queue.add(node_exec)\n        running_executions: dict[str, AsyncResult] = {}\n\n        def make_exec_callback(exec_data: NodeExecutionEntry):\n            node_id = exec_data.node_id\n\n            def callback(result: object):\n                running_executions.pop(node_id)\n                nonlocal exec_stats\n                if isinstance(result, dict):\n                    exec_stats['node_count'] += 1\n                    exec_stats['nodes_cputime'] += result.get('cputime', 0)\n                    exec_stats['nodes_walltime'] += result.get('walltime', 0)\n            return callback\n        while not queue.empty():\n            if cancel.is_set():\n                error = RuntimeError('Execution is cancelled')\n                return (exec_stats, error)\n            exec_data = queue.get()\n            execution = running_executions.get(exec_data.node_id)\n            if execution and (not execution.ready()):\n                execution.wait()\n            log_metadata.debug(f'Dispatching node execution {exec_data.node_exec_id} for node {exec_data.node_id}')\n            running_executions[exec_data.node_id] = cls.executor.apply_async(cls.on_node_execution, (queue, exec_data), callback=make_exec_callback(exec_data))\n            while queue.empty() and running_executions:\n                log_metadata.debug(f'Queue empty; running nodes: {list(running_executions.keys())}')\n                for (node_id, execution) in list(running_executions.items()):\n                    if cancel.is_set():\n                        error = RuntimeError('Execution is cancelled')\n                        return (exec_stats, error)\n                    if not queue.empty():\n                        break\n                    log_metadata.debug(f'Waiting on execution of node {node_id}')\n                    execution.wait(3)\n        log_metadata.info(f'Finished graph execution {graph_exec.graph_exec_id}')\n    except Exception as e:\n        log_metadata.exception(f'Failed graph execution {graph_exec.graph_exec_id}: {e}')\n        error = e\n    finally:\n        if not cancel.is_set():\n            finished = True\n            cancel.set()\n        cancel_thread.join()\n        return (exec_stats, error)",
        "'\\n        Returns:\\n            The execution statistics of the graph execution.\\n            The error that occurred during the execution.\\n        '",
        "log_metadata.info(f'Start graph execution {graph_exec.graph_exec_id}')",
        "exec_stats = {'nodes_walltime': 0, 'nodes_cputime': 0, 'node_count': 0}",
        "error = None",
        "finished = False",
        "def cancel_handler():\n    while not cancel.is_set():\n        cancel.wait(1)\n    if finished:\n        return\n    cls.executor.terminate()\n    log_metadata.info(f'Terminated graph execution {graph_exec.graph_exec_id}')\n    cls._init_node_executor_pool()"
      ],
      "code": "log_metadata.info(f'Finished node execution {node_exec.node_exec_id}')\nlog_metadata.exception(f'Failed node execution {node_exec.node_exec_id}: {e}')\n@classmethod\ndef on_graph_executor_start(cls):\n    configure_logging()\n    set_service_name('GraphExecutor')\n    cls.db_client = get_db_client()\n    cls.pool_size = settings.config.num_node_workers\n    cls.pid = os.getpid()\n    cls._init_node_executor_pool()\n    logger.info(f'Graph executor {cls.pid} started with {cls.pool_size} node workers')\n    atexit.register(cls.on_graph_executor_stop)\nconfigure_logging()\nset_service_name('GraphExecutor')\ncls.db_client = get_db_client()\ncls.pool_size = settings.config.num_node_workers\ncls.pid = os.getpid()\ncls._init_node_executor_pool()\nlogger.info(f'Graph executor {cls.pid} started with {cls.pool_size} node workers')\natexit.register(cls.on_graph_executor_stop)\n@classmethod\ndef on_graph_executor_stop(cls):\n    prefix = f'[on_graph_executor_stop {cls.pid}]'\n    logger.info(f'{prefix} ⏳ Terminating node executor pool...')\n    cls.executor.terminate()\n    logger.info(f'{prefix} ⏳ Disconnecting DB manager...')\n    close_service_client(cls.db_client)\n    logger.info(f'{prefix} ✅ Finished cleanup')\nprefix = f'[on_graph_executor_stop {cls.pid}]'\nlogger.info(f'{prefix} ⏳ Terminating node executor pool...')\ncls.executor.terminate()\nlogger.info(f'{prefix} ⏳ Disconnecting DB manager...')\nclose_service_client(cls.db_client)\nlogger.info(f'{prefix} ✅ Finished cleanup')\n@classmethod\ndef _init_node_executor_pool(cls):\n    cls.executor = Pool(processes=cls.pool_size, initializer=cls.on_node_executor_start)\ncls.executor = Pool(processes=cls.pool_size, initializer=cls.on_node_executor_start)\n@classmethod\n@error_logged\ndef on_graph_execution(cls, graph_exec: GraphExecutionEntry, cancel: threading.Event):\n    log_metadata = LogMetadata(user_id=graph_exec.user_id, graph_eid=graph_exec.graph_exec_id, graph_id=graph_exec.graph_id, node_id='*', node_eid='*', block_name='-')\n    (timing_info, (exec_stats, error)) = cls._on_graph_execution(graph_exec, cancel, log_metadata)\n    exec_stats['walltime'] = timing_info.wall_time\n    exec_stats['cputime'] = timing_info.cpu_time\n    exec_stats['error'] = str(error) if error else None\n    result = cls.db_client.update_graph_execution_stats(graph_exec_id=graph_exec.graph_exec_id, stats=exec_stats)\n    cls.db_client.send_execution_update(result)\nlog_metadata = LogMetadata(user_id=graph_exec.user_id, graph_eid=graph_exec.graph_exec_id, graph_id=graph_exec.graph_id, node_id='*', node_eid='*', block_name='-')\n(timing_info, (exec_stats, error)) = cls._on_graph_execution(graph_exec, cancel, log_metadata)\nexec_stats['walltime'] = timing_info.wall_time\nexec_stats['cputime'] = timing_info.cpu_time\nexec_stats['error'] = str(error) if error else None\nresult = cls.db_client.update_graph_execution_stats(graph_exec_id=graph_exec.graph_exec_id, stats=exec_stats)\ncls.db_client.send_execution_update(result)\n@classmethod\n@time_measured\ndef _on_graph_execution(cls, graph_exec: GraphExecutionEntry, cancel: threading.Event, log_metadata: LogMetadata) -> tuple[dict[str, Any], Exception | None]:\n    \"\"\"\n        Returns:\n            The execution statistics of the graph execution.\n            The error that occurred during the execution.\n        \"\"\"\n    log_metadata.info(f'Start graph execution {graph_exec.graph_exec_id}')\n    exec_stats = {'nodes_walltime': 0, 'nodes_cputime': 0, 'node_count': 0}\n    error = None\n    finished = False\n\n    def cancel_handler():\n        while not cancel.is_set():\n            cancel.wait(1)\n        if finished:\n            return\n        cls.executor.terminate()\n        log_metadata.info(f'Terminated graph execution {graph_exec.graph_exec_id}')\n        cls._init_node_executor_pool()\n    cancel_thread = threading.Thread(target=cancel_handler)\n    cancel_thread.start()\n    try:\n        queue = ExecutionQueue[NodeExecutionEntry]()\n        for node_exec in graph_exec.start_node_execs:\n            queue.add(node_exec)\n        running_executions: dict[str, AsyncResult] = {}\n\n        def make_exec_callback(exec_data: NodeExecutionEntry):\n            node_id = exec_data.node_id\n\n            def callback(result: object):\n                running_executions.pop(node_id)\n                nonlocal exec_stats\n                if isinstance(result, dict):\n                    exec_stats['node_count'] += 1\n                    exec_stats['nodes_cputime'] += result.get('cputime', 0)\n                    exec_stats['nodes_walltime'] += result.get('walltime', 0)\n            return callback\n        while not queue.empty():\n            if cancel.is_set():\n                error = RuntimeError('Execution is cancelled')\n                return (exec_stats, error)\n            exec_data = queue.get()\n            execution = running_executions.get(exec_data.node_id)\n            if execution and (not execution.ready()):\n                execution.wait()\n            log_metadata.debug(f'Dispatching node execution {exec_data.node_exec_id} for node {exec_data.node_id}')\n            running_executions[exec_data.node_id] = cls.executor.apply_async(cls.on_node_execution, (queue, exec_data), callback=make_exec_callback(exec_data))\n            while queue.empty() and running_executions:\n                log_metadata.debug(f'Queue empty; running nodes: {list(running_executions.keys())}')\n                for (node_id, execution) in list(running_executions.items()):\n                    if cancel.is_set():\n                        error = RuntimeError('Execution is cancelled')\n                        return (exec_stats, error)\n                    if not queue.empty():\n                        break\n                    log_metadata.debug(f'Waiting on execution of node {node_id}')\n                    execution.wait(3)\n        log_metadata.info(f'Finished graph execution {graph_exec.graph_exec_id}')\n    except Exception as e:\n        log_metadata.exception(f'Failed graph execution {graph_exec.graph_exec_id}: {e}')\n        error = e\n    finally:\n        if not cancel.is_set():\n            finished = True\n            cancel.set()\n        cancel_thread.join()\n        return (exec_stats, error)\n'\\n        Returns:\\n            The execution statistics of the graph execution.\\n            The error that occurred during the execution.\\n        '\nlog_metadata.info(f'Start graph execution {graph_exec.graph_exec_id}')\nexec_stats = {'nodes_walltime': 0, 'nodes_cputime': 0, 'node_count': 0}\nerror = None\nfinished = False\ndef cancel_handler():\n    while not cancel.is_set():\n        cancel.wait(1)\n    if finished:\n        return\n    cls.executor.terminate()\n    log_metadata.info(f'Terminated graph execution {graph_exec.graph_exec_id}')\n    cls._init_node_executor_pool()"
    },
    {
      "id": "n91",
      "type": "block",
      "statements": [
        "not cancel.is_set()"
      ],
      "code": "not cancel.is_set()"
    },
    {
      "id": "n92",
      "type": "block",
      "statements": [
        "cancel.wait(1)"
      ],
      "code": "cancel.wait(1)"
    },
    {
      "id": "n93",
      "type": "block",
      "statements": [
        "finished"
      ],
      "code": "finished"
    },
    {
      "id": "n94",
      "type": "block",
      "statements": [
        "return"
      ],
      "code": "return"
    },
    {
      "id": "n95",
      "type": "block",
      "statements": [],
      "code": "\ncls.executor.terminate()\nlog_metadata.info(f'Terminated graph execution {graph_exec.graph_exec_id}')\ncls._init_node_executor_pool()\ncancel_thread = threading.Thread(target=cancel_handler)\ncancel_thread.start()\ntry:\n    queue = ExecutionQueue[NodeExecutionEntry]()\n    for node_exec in graph_exec.start_node_execs:\n        queue.add(node_exec)\n    running_executions: dict[str, AsyncResult] = {}\n\n    def make_exec_callback(exec_data: NodeExecutionEntry):\n        node_id = exec_data.node_id\n\n        def callback(result: object):\n            running_executions.pop(node_id)\n            nonlocal exec_stats\n            if isinstance(result, dict):\n                exec_stats['node_count'] += 1\n                exec_stats['nodes_cputime'] += result.get('cputime', 0)\n                exec_stats['nodes_walltime'] += result.get('walltime', 0)\n        return callback\n    while not queue.empty():\n        if cancel.is_set():\n            error = RuntimeError('Execution is cancelled')\n            return (exec_stats, error)\n        exec_data = queue.get()\n        execution = running_executions.get(exec_data.node_id)\n        if execution and (not execution.ready()):\n            execution.wait()\n        log_metadata.debug(f'Dispatching node execution {exec_data.node_exec_id} for node {exec_data.node_id}')\n        running_executions[exec_data.node_id] = cls.executor.apply_async(cls.on_node_execution, (queue, exec_data), callback=make_exec_callback(exec_data))\n        while queue.empty() and running_executions:\n            log_metadata.debug(f'Queue empty; running nodes: {list(running_executions.keys())}')\n            for (node_id, execution) in list(running_executions.items()):\n                if cancel.is_set():\n                    error = RuntimeError('Execution is cancelled')\n                    return (exec_stats, error)\n                if not queue.empty():\n                    break\n                log_metadata.debug(f'Waiting on execution of node {node_id}')\n                execution.wait(3)\n    log_metadata.info(f'Finished graph execution {graph_exec.graph_exec_id}')\nexcept Exception as e:\n    log_metadata.exception(f'Failed graph execution {graph_exec.graph_exec_id}: {e}')\n    error = e\nfinally:\n    if not cancel.is_set():\n        finished = True\n        cancel.set()\n    cancel_thread.join()\n    return (exec_stats, error)\nqueue = ExecutionQueue[NodeExecutionEntry]()"
    },
    {
      "id": "n96",
      "type": "block",
      "statements": [
        "node_exec",
        "graph_exec.start_node_execs"
      ],
      "code": "node_exec\ngraph_exec.start_node_execs"
    },
    {
      "id": "n97",
      "type": "block",
      "statements": [
        "queue.add(node_exec)"
      ],
      "code": "queue.add(node_exec)"
    },
    {
      "id": "n98",
      "type": "block",
      "statements": [
        "running_executions: dict[str, AsyncResult] = {}",
        "def make_exec_callback(exec_data: NodeExecutionEntry):\n    node_id = exec_data.node_id\n\n    def callback(result: object):\n        running_executions.pop(node_id)\n        nonlocal exec_stats\n        if isinstance(result, dict):\n            exec_stats['node_count'] += 1\n            exec_stats['nodes_cputime'] += result.get('cputime', 0)\n            exec_stats['nodes_walltime'] += result.get('walltime', 0)\n    return callback",
        "node_id = exec_data.node_id",
        "def callback(result: object):\n    running_executions.pop(node_id)\n    nonlocal exec_stats\n    if isinstance(result, dict):\n        exec_stats['node_count'] += 1\n        exec_stats['nodes_cputime'] += result.get('cputime', 0)\n        exec_stats['nodes_walltime'] += result.get('walltime', 0)",
        "running_executions.pop(node_id)",
        "nonlocal exec_stats",
        "isinstance(result, dict)"
      ],
      "code": "running_executions: dict[str, AsyncResult] = {}\ndef make_exec_callback(exec_data: NodeExecutionEntry):\n    node_id = exec_data.node_id\n\n    def callback(result: object):\n        running_executions.pop(node_id)\n        nonlocal exec_stats\n        if isinstance(result, dict):\n            exec_stats['node_count'] += 1\n            exec_stats['nodes_cputime'] += result.get('cputime', 0)\n            exec_stats['nodes_walltime'] += result.get('walltime', 0)\n    return callback\nnode_id = exec_data.node_id\ndef callback(result: object):\n    running_executions.pop(node_id)\n    nonlocal exec_stats\n    if isinstance(result, dict):\n        exec_stats['node_count'] += 1\n        exec_stats['nodes_cputime'] += result.get('cputime', 0)\n        exec_stats['nodes_walltime'] += result.get('walltime', 0)\nrunning_executions.pop(node_id)\nnonlocal exec_stats\nisinstance(result, dict)"
    },
    {
      "id": "n99",
      "type": "block",
      "statements": [
        "exec_stats['node_count'] += 1",
        "exec_stats['nodes_cputime'] += result.get('cputime', 0)",
        "exec_stats['nodes_walltime'] += result.get('walltime', 0)"
      ],
      "code": "exec_stats['node_count'] += 1\nexec_stats['nodes_cputime'] += result.get('cputime', 0)\nexec_stats['nodes_walltime'] += result.get('walltime', 0)"
    },
    {
      "id": "n100",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n101",
      "type": "block",
      "statements": [
        "return callback"
      ],
      "code": "return callback"
    },
    {
      "id": "n102",
      "type": "block",
      "statements": [
        "not queue.empty()"
      ],
      "code": "not queue.empty()"
    },
    {
      "id": "n103",
      "type": "block",
      "statements": [
        "cancel.is_set()"
      ],
      "code": "cancel.is_set()"
    },
    {
      "id": "n104",
      "type": "block",
      "statements": [
        "log_metadata.info(f'Finished graph execution {graph_exec.graph_exec_id}')",
        "log_metadata.exception(f'Failed graph execution {graph_exec.graph_exec_id}: {e}')",
        "error = e",
        "not cancel.is_set()"
      ],
      "code": "log_metadata.info(f'Finished graph execution {graph_exec.graph_exec_id}')\nlog_metadata.exception(f'Failed graph execution {graph_exec.graph_exec_id}: {e}')\nerror = e\nnot cancel.is_set()"
    },
    {
      "id": "n105",
      "type": "block",
      "statements": [
        "error = RuntimeError('Execution is cancelled')",
        "return (exec_stats, error)"
      ],
      "code": "error = RuntimeError('Execution is cancelled')\nreturn (exec_stats, error)"
    },
    {
      "id": "n106",
      "type": "block",
      "statements": [],
      "code": "\nexec_data = queue.get()\nexecution = running_executions.get(exec_data.node_id)\nexecution and (not execution.ready())"
    },
    {
      "id": "n107",
      "type": "block",
      "statements": [
        "execution.wait()"
      ],
      "code": "execution.wait()"
    },
    {
      "id": "n108",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n109",
      "type": "block",
      "statements": [
        "log_metadata.debug(f'Dispatching node execution {exec_data.node_exec_id} for node {exec_data.node_id}')",
        "running_executions[exec_data.node_id] = cls.executor.apply_async(cls.on_node_execution, (queue, exec_data), callback=make_exec_callback(exec_data))"
      ],
      "code": "log_metadata.debug(f'Dispatching node execution {exec_data.node_exec_id} for node {exec_data.node_id}')\nrunning_executions[exec_data.node_id] = cls.executor.apply_async(cls.on_node_execution, (queue, exec_data), callback=make_exec_callback(exec_data))"
    },
    {
      "id": "n110",
      "type": "block",
      "statements": [
        "queue.empty() and running_executions"
      ],
      "code": "queue.empty() and running_executions"
    },
    {
      "id": "n111",
      "type": "block",
      "statements": [
        "log_metadata.debug(f'Queue empty; running nodes: {list(running_executions.keys())}')"
      ],
      "code": "log_metadata.debug(f'Queue empty; running nodes: {list(running_executions.keys())}')"
    },
    {
      "id": "n112",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n113",
      "type": "block",
      "statements": [
        "(node_id, execution)",
        "list(running_executions.items())"
      ],
      "code": "(node_id, execution)\nlist(running_executions.items())"
    },
    {
      "id": "n114",
      "type": "block",
      "statements": [
        "cancel.is_set()"
      ],
      "code": "cancel.is_set()"
    },
    {
      "id": "n115",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n116",
      "type": "block",
      "statements": [
        "error = RuntimeError('Execution is cancelled')",
        "return (exec_stats, error)"
      ],
      "code": "error = RuntimeError('Execution is cancelled')\nreturn (exec_stats, error)"
    },
    {
      "id": "n117",
      "type": "block",
      "statements": [],
      "code": "\nnot queue.empty()"
    },
    {
      "id": "n118",
      "type": "block",
      "statements": [
        "break"
      ],
      "code": "break"
    },
    {
      "id": "n119",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n120",
      "type": "block",
      "statements": [
        "log_metadata.debug(f'Waiting on execution of node {node_id}')",
        "execution.wait(3)"
      ],
      "code": "log_metadata.debug(f'Waiting on execution of node {node_id}')\nexecution.wait(3)"
    },
    {
      "id": "n121",
      "type": "block",
      "statements": [
        "finished = True",
        "cancel.set()"
      ],
      "code": "finished = True\ncancel.set()"
    },
    {
      "id": "n122",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n123",
      "type": "block",
      "statements": [
        "cancel_thread.join()",
        "return (exec_stats, error)"
      ],
      "code": "cancel_thread.join()\nreturn (exec_stats, error)"
    },
    {
      "id": "n124",
      "type": "block",
      "statements": [
        "class ExecutionManager(AppService):\n\n    def __init__(self):\n        super().__init__()\n        self.use_redis = True\n        self.use_supabase = True\n        self.pool_size = settings.config.num_graph_workers\n        self.queue = ExecutionQueue[GraphExecutionEntry]()\n        self.active_graph_runs: dict[str, tuple[Future, threading.Event]] = {}\n\n    @classmethod\n    def get_port(cls) -> int:\n        return settings.config.execution_manager_port\n\n    def run_service(self):\n        from backend.integrations.credentials_store import IntegrationCredentialsStore\n        self.credentials_store = IntegrationCredentialsStore()\n        self.executor = ProcessPoolExecutor(max_workers=self.pool_size, initializer=Executor.on_graph_executor_start)\n        sync_manager = multiprocessing.Manager()\n        logger.info(f'[{self.service_name}] Started with max-{self.pool_size} graph workers')\n        while True:\n            graph_exec_data = self.queue.get()\n            graph_exec_id = graph_exec_data.graph_exec_id\n            logger.debug(f'[ExecutionManager] Dispatching graph execution {graph_exec_id}')\n            cancel_event = sync_manager.Event()\n            future = self.executor.submit(Executor.on_graph_execution, graph_exec_data, cancel_event)\n            self.active_graph_runs[graph_exec_id] = (future, cancel_event)\n            future.add_done_callback(lambda _: self.active_graph_runs.pop(graph_exec_id, None))\n\n    def cleanup(self):\n        logger.info(f'[{__class__.__name__}] ⏳ Shutting down graph executor pool...')\n        self.executor.shutdown(cancel_futures=True)\n        super().cleanup()\n\n    @property\n    def db_client(self) -> 'DatabaseManager':\n        return get_db_client()\n\n    @expose\n    def add_execution(self, graph_id: str, data: BlockInput, user_id: str, graph_version: int | None=None) -> GraphExecutionEntry:\n        graph: GraphModel | None = self.db_client.get_graph(graph_id=graph_id, user_id=user_id, version=graph_version)\n        if not graph:\n            raise ValueError(f'Graph #{graph_id} not found.')\n        graph.validate_graph(for_run=True)\n        self._validate_node_input_credentials(graph, user_id)\n        nodes_input = []\n        for node in graph.starting_nodes:\n            input_data = {}\n            block = get_block(node.block_id)\n            if not block or block.block_type == BlockType.NOTE:\n                continue\n            if block.block_type == BlockType.INPUT:\n                name = node.input_default.get('name')\n                if name and name in data:\n                    input_data = {'value': data[name]}\n            webhook_payload_key = f'webhook_{node.webhook_id}_payload'\n            if block.block_type in (BlockType.WEBHOOK, BlockType.WEBHOOK_MANUAL) and node.webhook_id:\n                if webhook_payload_key not in data:\n                    raise ValueError(f'Node {block.name} #{node.id} webhook payload is missing')\n                input_data = {'payload': data[webhook_payload_key]}\n            (input_data, error) = validate_exec(node, input_data)\n            if input_data is None:\n                raise ValueError(error)\n            else:\n                nodes_input.append((node.id, input_data))\n        (graph_exec_id, node_execs) = self.db_client.create_graph_execution(graph_id=graph_id, graph_version=graph.version, nodes_input=nodes_input, user_id=user_id)\n        starting_node_execs = []\n        for node_exec in node_execs:\n            starting_node_execs.append(NodeExecutionEntry(user_id=user_id, graph_exec_id=node_exec.graph_exec_id, graph_id=node_exec.graph_id, node_exec_id=node_exec.node_exec_id, node_id=node_exec.node_id, data=node_exec.input_data))\n            exec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.QUEUED, node_exec.input_data)\n            self.db_client.send_execution_update(exec_update)\n        graph_exec = GraphExecutionEntry(user_id=user_id, graph_id=graph_id, graph_exec_id=graph_exec_id, start_node_execs=starting_node_execs)\n        self.queue.add(graph_exec)\n        return graph_exec\n\n    @expose\n    def cancel_execution(self, graph_exec_id: str) -> None:\n        \"\"\"\n        Mechanism:\n        1. Set the cancel event\n        2. Graph executor's cancel handler thread detects the event, terminates workers,\n           reinitializes worker pool, and returns.\n        3. Update execution statuses in DB and set `error` outputs to `\"TERMINATED\"`.\n        \"\"\"\n        if graph_exec_id not in self.active_graph_runs:\n            raise Exception(f'Graph execution #{graph_exec_id} not active/running: possibly already completed/cancelled.')\n        (future, cancel_event) = self.active_graph_runs[graph_exec_id]\n        if cancel_event.is_set():\n            return\n        cancel_event.set()\n        future.result()\n        node_execs = self.db_client.get_execution_results(graph_exec_id)\n        for node_exec in node_execs:\n            if node_exec.status not in (ExecutionStatus.COMPLETED, ExecutionStatus.FAILED):\n                self.db_client.upsert_execution_output(node_exec.node_exec_id, 'error', 'TERMINATED')\n                exec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.FAILED)\n                self.db_client.send_execution_update(exec_update)\n\n    def _validate_node_input_credentials(self, graph: GraphModel, user_id: str):\n        \"\"\"Checks all credentials for all nodes of the graph\"\"\"\n        for node in graph.nodes:\n            block = get_block(node.block_id)\n            if not block:\n                raise ValueError(f'Unknown block {node.block_id} for node #{node.id}')\n            model_fields = cast(type[BaseModel], block.input_schema).model_fields\n            if CREDENTIALS_FIELD_NAME not in model_fields:\n                continue\n            field = model_fields[CREDENTIALS_FIELD_NAME]\n            credentials_meta_type = cast(CredentialsMetaInput, field.annotation)\n            credentials_meta = credentials_meta_type.model_validate(node.input_default[CREDENTIALS_FIELD_NAME])\n            credentials = self.credentials_store.get_creds_by_id(user_id, credentials_meta.id)\n            if not credentials:\n                raise ValueError(f'Unknown credentials #{credentials_meta.id} for node #{node.id}')\n            if credentials.provider != credentials_meta.provider or credentials.type != credentials_meta.type:\n                logger.warning(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch: {credentials_meta.type}<>{credentials.type};{credentials_meta.provider}<>{credentials.provider}')\n                raise ValueError(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch')",
        "def __init__(self):\n    super().__init__()\n    self.use_redis = True\n    self.use_supabase = True\n    self.pool_size = settings.config.num_graph_workers\n    self.queue = ExecutionQueue[GraphExecutionEntry]()\n    self.active_graph_runs: dict[str, tuple[Future, threading.Event]] = {}",
        "super().__init__()",
        "self.use_redis = True",
        "self.use_supabase = True",
        "self.pool_size = settings.config.num_graph_workers",
        "self.queue = ExecutionQueue[GraphExecutionEntry]()",
        "self.active_graph_runs: dict[str, tuple[Future, threading.Event]] = {}",
        "@classmethod\ndef get_port(cls) -> int:\n    return settings.config.execution_manager_port",
        "return settings.config.execution_manager_port"
      ],
      "code": "class ExecutionManager(AppService):\n\n    def __init__(self):\n        super().__init__()\n        self.use_redis = True\n        self.use_supabase = True\n        self.pool_size = settings.config.num_graph_workers\n        self.queue = ExecutionQueue[GraphExecutionEntry]()\n        self.active_graph_runs: dict[str, tuple[Future, threading.Event]] = {}\n\n    @classmethod\n    def get_port(cls) -> int:\n        return settings.config.execution_manager_port\n\n    def run_service(self):\n        from backend.integrations.credentials_store import IntegrationCredentialsStore\n        self.credentials_store = IntegrationCredentialsStore()\n        self.executor = ProcessPoolExecutor(max_workers=self.pool_size, initializer=Executor.on_graph_executor_start)\n        sync_manager = multiprocessing.Manager()\n        logger.info(f'[{self.service_name}] Started with max-{self.pool_size} graph workers')\n        while True:\n            graph_exec_data = self.queue.get()\n            graph_exec_id = graph_exec_data.graph_exec_id\n            logger.debug(f'[ExecutionManager] Dispatching graph execution {graph_exec_id}')\n            cancel_event = sync_manager.Event()\n            future = self.executor.submit(Executor.on_graph_execution, graph_exec_data, cancel_event)\n            self.active_graph_runs[graph_exec_id] = (future, cancel_event)\n            future.add_done_callback(lambda _: self.active_graph_runs.pop(graph_exec_id, None))\n\n    def cleanup(self):\n        logger.info(f'[{__class__.__name__}] ⏳ Shutting down graph executor pool...')\n        self.executor.shutdown(cancel_futures=True)\n        super().cleanup()\n\n    @property\n    def db_client(self) -> 'DatabaseManager':\n        return get_db_client()\n\n    @expose\n    def add_execution(self, graph_id: str, data: BlockInput, user_id: str, graph_version: int | None=None) -> GraphExecutionEntry:\n        graph: GraphModel | None = self.db_client.get_graph(graph_id=graph_id, user_id=user_id, version=graph_version)\n        if not graph:\n            raise ValueError(f'Graph #{graph_id} not found.')\n        graph.validate_graph(for_run=True)\n        self._validate_node_input_credentials(graph, user_id)\n        nodes_input = []\n        for node in graph.starting_nodes:\n            input_data = {}\n            block = get_block(node.block_id)\n            if not block or block.block_type == BlockType.NOTE:\n                continue\n            if block.block_type == BlockType.INPUT:\n                name = node.input_default.get('name')\n                if name and name in data:\n                    input_data = {'value': data[name]}\n            webhook_payload_key = f'webhook_{node.webhook_id}_payload'\n            if block.block_type in (BlockType.WEBHOOK, BlockType.WEBHOOK_MANUAL) and node.webhook_id:\n                if webhook_payload_key not in data:\n                    raise ValueError(f'Node {block.name} #{node.id} webhook payload is missing')\n                input_data = {'payload': data[webhook_payload_key]}\n            (input_data, error) = validate_exec(node, input_data)\n            if input_data is None:\n                raise ValueError(error)\n            else:\n                nodes_input.append((node.id, input_data))\n        (graph_exec_id, node_execs) = self.db_client.create_graph_execution(graph_id=graph_id, graph_version=graph.version, nodes_input=nodes_input, user_id=user_id)\n        starting_node_execs = []\n        for node_exec in node_execs:\n            starting_node_execs.append(NodeExecutionEntry(user_id=user_id, graph_exec_id=node_exec.graph_exec_id, graph_id=node_exec.graph_id, node_exec_id=node_exec.node_exec_id, node_id=node_exec.node_id, data=node_exec.input_data))\n            exec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.QUEUED, node_exec.input_data)\n            self.db_client.send_execution_update(exec_update)\n        graph_exec = GraphExecutionEntry(user_id=user_id, graph_id=graph_id, graph_exec_id=graph_exec_id, start_node_execs=starting_node_execs)\n        self.queue.add(graph_exec)\n        return graph_exec\n\n    @expose\n    def cancel_execution(self, graph_exec_id: str) -> None:\n        \"\"\"\n        Mechanism:\n        1. Set the cancel event\n        2. Graph executor's cancel handler thread detects the event, terminates workers,\n           reinitializes worker pool, and returns.\n        3. Update execution statuses in DB and set `error` outputs to `\"TERMINATED\"`.\n        \"\"\"\n        if graph_exec_id not in self.active_graph_runs:\n            raise Exception(f'Graph execution #{graph_exec_id} not active/running: possibly already completed/cancelled.')\n        (future, cancel_event) = self.active_graph_runs[graph_exec_id]\n        if cancel_event.is_set():\n            return\n        cancel_event.set()\n        future.result()\n        node_execs = self.db_client.get_execution_results(graph_exec_id)\n        for node_exec in node_execs:\n            if node_exec.status not in (ExecutionStatus.COMPLETED, ExecutionStatus.FAILED):\n                self.db_client.upsert_execution_output(node_exec.node_exec_id, 'error', 'TERMINATED')\n                exec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.FAILED)\n                self.db_client.send_execution_update(exec_update)\n\n    def _validate_node_input_credentials(self, graph: GraphModel, user_id: str):\n        \"\"\"Checks all credentials for all nodes of the graph\"\"\"\n        for node in graph.nodes:\n            block = get_block(node.block_id)\n            if not block:\n                raise ValueError(f'Unknown block {node.block_id} for node #{node.id}')\n            model_fields = cast(type[BaseModel], block.input_schema).model_fields\n            if CREDENTIALS_FIELD_NAME not in model_fields:\n                continue\n            field = model_fields[CREDENTIALS_FIELD_NAME]\n            credentials_meta_type = cast(CredentialsMetaInput, field.annotation)\n            credentials_meta = credentials_meta_type.model_validate(node.input_default[CREDENTIALS_FIELD_NAME])\n            credentials = self.credentials_store.get_creds_by_id(user_id, credentials_meta.id)\n            if not credentials:\n                raise ValueError(f'Unknown credentials #{credentials_meta.id} for node #{node.id}')\n            if credentials.provider != credentials_meta.provider or credentials.type != credentials_meta.type:\n                logger.warning(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch: {credentials_meta.type}<>{credentials.type};{credentials_meta.provider}<>{credentials.provider}')\n                raise ValueError(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch')\ndef __init__(self):\n    super().__init__()\n    self.use_redis = True\n    self.use_supabase = True\n    self.pool_size = settings.config.num_graph_workers\n    self.queue = ExecutionQueue[GraphExecutionEntry]()\n    self.active_graph_runs: dict[str, tuple[Future, threading.Event]] = {}\nsuper().__init__()\nself.use_redis = True\nself.use_supabase = True\nself.pool_size = settings.config.num_graph_workers\nself.queue = ExecutionQueue[GraphExecutionEntry]()\nself.active_graph_runs: dict[str, tuple[Future, threading.Event]] = {}\n@classmethod\ndef get_port(cls) -> int:\n    return settings.config.execution_manager_port\nreturn settings.config.execution_manager_port"
    },
    {
      "id": "n125",
      "type": "block",
      "statements": [
        "def run_service(self):\n    from backend.integrations.credentials_store import IntegrationCredentialsStore\n    self.credentials_store = IntegrationCredentialsStore()\n    self.executor = ProcessPoolExecutor(max_workers=self.pool_size, initializer=Executor.on_graph_executor_start)\n    sync_manager = multiprocessing.Manager()\n    logger.info(f'[{self.service_name}] Started with max-{self.pool_size} graph workers')\n    while True:\n        graph_exec_data = self.queue.get()\n        graph_exec_id = graph_exec_data.graph_exec_id\n        logger.debug(f'[ExecutionManager] Dispatching graph execution {graph_exec_id}')\n        cancel_event = sync_manager.Event()\n        future = self.executor.submit(Executor.on_graph_execution, graph_exec_data, cancel_event)\n        self.active_graph_runs[graph_exec_id] = (future, cancel_event)\n        future.add_done_callback(lambda _: self.active_graph_runs.pop(graph_exec_id, None))",
        "from backend.integrations.credentials_store import IntegrationCredentialsStore",
        "self.credentials_store = IntegrationCredentialsStore()",
        "self.executor = ProcessPoolExecutor(max_workers=self.pool_size, initializer=Executor.on_graph_executor_start)",
        "sync_manager = multiprocessing.Manager()",
        "logger.info(f'[{self.service_name}] Started with max-{self.pool_size} graph workers')"
      ],
      "code": "def run_service(self):\n    from backend.integrations.credentials_store import IntegrationCredentialsStore\n    self.credentials_store = IntegrationCredentialsStore()\n    self.executor = ProcessPoolExecutor(max_workers=self.pool_size, initializer=Executor.on_graph_executor_start)\n    sync_manager = multiprocessing.Manager()\n    logger.info(f'[{self.service_name}] Started with max-{self.pool_size} graph workers')\n    while True:\n        graph_exec_data = self.queue.get()\n        graph_exec_id = graph_exec_data.graph_exec_id\n        logger.debug(f'[ExecutionManager] Dispatching graph execution {graph_exec_id}')\n        cancel_event = sync_manager.Event()\n        future = self.executor.submit(Executor.on_graph_execution, graph_exec_data, cancel_event)\n        self.active_graph_runs[graph_exec_id] = (future, cancel_event)\n        future.add_done_callback(lambda _: self.active_graph_runs.pop(graph_exec_id, None))\nfrom backend.integrations.credentials_store import IntegrationCredentialsStore\nself.credentials_store = IntegrationCredentialsStore()\nself.executor = ProcessPoolExecutor(max_workers=self.pool_size, initializer=Executor.on_graph_executor_start)\nsync_manager = multiprocessing.Manager()\nlogger.info(f'[{self.service_name}] Started with max-{self.pool_size} graph workers')"
    },
    {
      "id": "n126",
      "type": "block",
      "statements": [
        "True"
      ],
      "code": "True"
    },
    {
      "id": "n127",
      "type": "block",
      "statements": [
        "graph_exec_data = self.queue.get()",
        "graph_exec_id = graph_exec_data.graph_exec_id",
        "logger.debug(f'[ExecutionManager] Dispatching graph execution {graph_exec_id}')",
        "cancel_event = sync_manager.Event()",
        "future = self.executor.submit(Executor.on_graph_execution, graph_exec_data, cancel_event)",
        "self.active_graph_runs[graph_exec_id] = (future, cancel_event)",
        "future.add_done_callback(lambda _: self.active_graph_runs.pop(graph_exec_id, None))"
      ],
      "code": "graph_exec_data = self.queue.get()\ngraph_exec_id = graph_exec_data.graph_exec_id\nlogger.debug(f'[ExecutionManager] Dispatching graph execution {graph_exec_id}')\ncancel_event = sync_manager.Event()\nfuture = self.executor.submit(Executor.on_graph_execution, graph_exec_data, cancel_event)\nself.active_graph_runs[graph_exec_id] = (future, cancel_event)\nfuture.add_done_callback(lambda _: self.active_graph_runs.pop(graph_exec_id, None))"
    },
    {
      "id": "n128",
      "type": "block",
      "statements": [
        "def cleanup(self):\n    logger.info(f'[{__class__.__name__}] ⏳ Shutting down graph executor pool...')\n    self.executor.shutdown(cancel_futures=True)\n    super().cleanup()",
        "logger.info(f'[{__class__.__name__}] ⏳ Shutting down graph executor pool...')",
        "self.executor.shutdown()",
        "super().cleanup()",
        "@property\ndef db_client(self) -> 'DatabaseManager':\n    return get_db_client()",
        "return get_db_client()"
      ],
      "code": "def cleanup(self):\n    logger.info(f'[{__class__.__name__}] ⏳ Shutting down graph executor pool...')\n    self.executor.shutdown(cancel_futures=True)\n    super().cleanup()\nlogger.info(f'[{__class__.__name__}] ⏳ Shutting down graph executor pool...')\nself.executor.shutdown()\nsuper().cleanup()\n@property\ndef db_client(self) -> 'DatabaseManager':\n    return get_db_client()\nreturn get_db_client()"
    },
    {
      "id": "n129",
      "type": "block",
      "statements": [
        "@expose\ndef add_execution(self, graph_id: str, data: BlockInput, user_id: str, graph_version: int | None=None) -> GraphExecutionEntry:\n    graph: GraphModel | None = self.db_client.get_graph(graph_id=graph_id, user_id=user_id, version=graph_version)\n    if not graph:\n        raise ValueError(f'Graph #{graph_id} not found.')\n    graph.validate_graph(for_run=True)\n    self._validate_node_input_credentials(graph, user_id)\n    nodes_input = []\n    for node in graph.starting_nodes:\n        input_data = {}\n        block = get_block(node.block_id)\n        if not block or block.block_type == BlockType.NOTE:\n            continue\n        if block.block_type == BlockType.INPUT:\n            name = node.input_default.get('name')\n            if name and name in data:\n                input_data = {'value': data[name]}\n        webhook_payload_key = f'webhook_{node.webhook_id}_payload'\n        if block.block_type in (BlockType.WEBHOOK, BlockType.WEBHOOK_MANUAL) and node.webhook_id:\n            if webhook_payload_key not in data:\n                raise ValueError(f'Node {block.name} #{node.id} webhook payload is missing')\n            input_data = {'payload': data[webhook_payload_key]}\n        (input_data, error) = validate_exec(node, input_data)\n        if input_data is None:\n            raise ValueError(error)\n        else:\n            nodes_input.append((node.id, input_data))\n    (graph_exec_id, node_execs) = self.db_client.create_graph_execution(graph_id=graph_id, graph_version=graph.version, nodes_input=nodes_input, user_id=user_id)\n    starting_node_execs = []\n    for node_exec in node_execs:\n        starting_node_execs.append(NodeExecutionEntry(user_id=user_id, graph_exec_id=node_exec.graph_exec_id, graph_id=node_exec.graph_id, node_exec_id=node_exec.node_exec_id, node_id=node_exec.node_id, data=node_exec.input_data))\n        exec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.QUEUED, node_exec.input_data)\n        self.db_client.send_execution_update(exec_update)\n    graph_exec = GraphExecutionEntry(user_id=user_id, graph_id=graph_id, graph_exec_id=graph_exec_id, start_node_execs=starting_node_execs)\n    self.queue.add(graph_exec)\n    return graph_exec",
        "graph: GraphModel | None = self.db_client.get_graph(graph_id=graph_id, user_id=user_id, version=graph_version)",
        "not graph"
      ],
      "code": "@expose\ndef add_execution(self, graph_id: str, data: BlockInput, user_id: str, graph_version: int | None=None) -> GraphExecutionEntry:\n    graph: GraphModel | None = self.db_client.get_graph(graph_id=graph_id, user_id=user_id, version=graph_version)\n    if not graph:\n        raise ValueError(f'Graph #{graph_id} not found.')\n    graph.validate_graph(for_run=True)\n    self._validate_node_input_credentials(graph, user_id)\n    nodes_input = []\n    for node in graph.starting_nodes:\n        input_data = {}\n        block = get_block(node.block_id)\n        if not block or block.block_type == BlockType.NOTE:\n            continue\n        if block.block_type == BlockType.INPUT:\n            name = node.input_default.get('name')\n            if name and name in data:\n                input_data = {'value': data[name]}\n        webhook_payload_key = f'webhook_{node.webhook_id}_payload'\n        if block.block_type in (BlockType.WEBHOOK, BlockType.WEBHOOK_MANUAL) and node.webhook_id:\n            if webhook_payload_key not in data:\n                raise ValueError(f'Node {block.name} #{node.id} webhook payload is missing')\n            input_data = {'payload': data[webhook_payload_key]}\n        (input_data, error) = validate_exec(node, input_data)\n        if input_data is None:\n            raise ValueError(error)\n        else:\n            nodes_input.append((node.id, input_data))\n    (graph_exec_id, node_execs) = self.db_client.create_graph_execution(graph_id=graph_id, graph_version=graph.version, nodes_input=nodes_input, user_id=user_id)\n    starting_node_execs = []\n    for node_exec in node_execs:\n        starting_node_execs.append(NodeExecutionEntry(user_id=user_id, graph_exec_id=node_exec.graph_exec_id, graph_id=node_exec.graph_id, node_exec_id=node_exec.node_exec_id, node_id=node_exec.node_id, data=node_exec.input_data))\n        exec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.QUEUED, node_exec.input_data)\n        self.db_client.send_execution_update(exec_update)\n    graph_exec = GraphExecutionEntry(user_id=user_id, graph_id=graph_id, graph_exec_id=graph_exec_id, start_node_execs=starting_node_execs)\n    self.queue.add(graph_exec)\n    return graph_exec\ngraph: GraphModel | None = self.db_client.get_graph(graph_id=graph_id, user_id=user_id, version=graph_version)\nnot graph"
    },
    {
      "id": "n130",
      "type": "block",
      "statements": [
        "raise ValueError(f'Graph #{graph_id} not found.')"
      ],
      "code": "raise ValueError(f'Graph #{graph_id} not found.')"
    },
    {
      "id": "n131",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n132",
      "type": "block",
      "statements": [
        "graph.validate_graph()",
        "self._validate_node_input_credentials(graph, user_id)",
        "nodes_input = []"
      ],
      "code": "graph.validate_graph()\nself._validate_node_input_credentials(graph, user_id)\nnodes_input = []"
    },
    {
      "id": "n133",
      "type": "block",
      "statements": [
        "node",
        "graph.starting_nodes"
      ],
      "code": "node\ngraph.starting_nodes"
    },
    {
      "id": "n134",
      "type": "block",
      "statements": [
        "input_data = {}",
        "block = get_block(node.block_id)",
        "not block or block.block_type == BlockType.NOTE"
      ],
      "code": "input_data = {}\nblock = get_block(node.block_id)\nnot block or block.block_type == BlockType.NOTE"
    },
    {
      "id": "n135",
      "type": "block",
      "statements": [
        "(graph_exec_id, node_execs) = self.db_client.create_graph_execution(graph_id=graph_id, graph_version=graph.version, nodes_input=nodes_input, user_id=user_id)",
        "starting_node_execs = []"
      ],
      "code": "(graph_exec_id, node_execs) = self.db_client.create_graph_execution(graph_id=graph_id, graph_version=graph.version, nodes_input=nodes_input, user_id=user_id)\nstarting_node_execs = []"
    },
    {
      "id": "n136",
      "type": "block",
      "statements": [
        "continue"
      ],
      "code": "continue"
    },
    {
      "id": "n137",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n138",
      "type": "block",
      "statements": [
        "block.block_type Eq BlockType.INPUT"
      ],
      "code": "block.block_type Eq BlockType.INPUT"
    },
    {
      "id": "n139",
      "type": "block",
      "statements": [
        "name = node.input_default.get('name')",
        "name and name in data"
      ],
      "code": "name = node.input_default.get('name')\nname and name in data"
    },
    {
      "id": "n140",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n141",
      "type": "block",
      "statements": [
        "webhook_payload_key = f'webhook_{node.webhook_id}_payload'",
        "block.block_type in (BlockType.WEBHOOK, BlockType.WEBHOOK_MANUAL) and node.webhook_id"
      ],
      "code": "webhook_payload_key = f'webhook_{node.webhook_id}_payload'\nblock.block_type in (BlockType.WEBHOOK, BlockType.WEBHOOK_MANUAL) and node.webhook_id"
    },
    {
      "id": "n142",
      "type": "block",
      "statements": [
        "input_data = {'value': data[name]}"
      ],
      "code": "input_data = {'value': data[name]}"
    },
    {
      "id": "n143",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n144",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n145",
      "type": "block",
      "statements": [
        "webhook_payload_key NotIn data"
      ],
      "code": "webhook_payload_key NotIn data"
    },
    {
      "id": "n146",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n147",
      "type": "block",
      "statements": [
        "(input_data, error) = validate_exec(node, input_data)",
        "input_data Is None"
      ],
      "code": "(input_data, error) = validate_exec(node, input_data)\ninput_data Is None"
    },
    {
      "id": "n148",
      "type": "block",
      "statements": [
        "raise ValueError(f'Node {block.name} #{node.id} webhook payload is missing')"
      ],
      "code": "raise ValueError(f'Node {block.name} #{node.id} webhook payload is missing')"
    },
    {
      "id": "n149",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n150",
      "type": "block",
      "statements": [
        "input_data = {'payload': data[webhook_payload_key]}"
      ],
      "code": "input_data = {'payload': data[webhook_payload_key]}"
    },
    {
      "id": "n151",
      "type": "block",
      "statements": [
        "raise ValueError(error)"
      ],
      "code": "raise ValueError(error)"
    },
    {
      "id": "n152",
      "type": "block",
      "statements": [
        "nodes_input.append((node.id, input_data))"
      ],
      "code": "nodes_input.append((node.id, input_data))"
    },
    {
      "id": "n153",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n154",
      "type": "block",
      "statements": [
        "node_exec",
        "node_execs"
      ],
      "code": "node_exec\nnode_execs"
    },
    {
      "id": "n155",
      "type": "block",
      "statements": [
        "starting_node_execs.append(NodeExecutionEntry())",
        "exec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.QUEUED, node_exec.input_data)",
        "self.db_client.send_execution_update(exec_update)"
      ],
      "code": "starting_node_execs.append(NodeExecutionEntry())\nexec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.QUEUED, node_exec.input_data)\nself.db_client.send_execution_update(exec_update)"
    },
    {
      "id": "n156",
      "type": "block",
      "statements": [
        "graph_exec = GraphExecutionEntry(user_id=user_id, graph_id=graph_id, graph_exec_id=graph_exec_id, start_node_execs=starting_node_execs)",
        "self.queue.add(graph_exec)",
        "return graph_exec"
      ],
      "code": "graph_exec = GraphExecutionEntry(user_id=user_id, graph_id=graph_id, graph_exec_id=graph_exec_id, start_node_execs=starting_node_execs)\nself.queue.add(graph_exec)\nreturn graph_exec"
    },
    {
      "id": "n157",
      "type": "block",
      "statements": [
        "@expose\ndef cancel_execution(self, graph_exec_id: str) -> None:\n    \"\"\"\n        Mechanism:\n        1. Set the cancel event\n        2. Graph executor's cancel handler thread detects the event, terminates workers,\n           reinitializes worker pool, and returns.\n        3. Update execution statuses in DB and set `error` outputs to `\"TERMINATED\"`.\n        \"\"\"\n    if graph_exec_id not in self.active_graph_runs:\n        raise Exception(f'Graph execution #{graph_exec_id} not active/running: possibly already completed/cancelled.')\n    (future, cancel_event) = self.active_graph_runs[graph_exec_id]\n    if cancel_event.is_set():\n        return\n    cancel_event.set()\n    future.result()\n    node_execs = self.db_client.get_execution_results(graph_exec_id)\n    for node_exec in node_execs:\n        if node_exec.status not in (ExecutionStatus.COMPLETED, ExecutionStatus.FAILED):\n            self.db_client.upsert_execution_output(node_exec.node_exec_id, 'error', 'TERMINATED')\n            exec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.FAILED)\n            self.db_client.send_execution_update(exec_update)",
        "'\\n        Mechanism:\\n        1. Set the cancel event\\n        2. Graph executor\\'s cancel handler thread detects the event, terminates workers,\\n           reinitializes worker pool, and returns.\\n        3. Update execution statuses in DB and set `error` outputs to `\"TERMINATED\"`.\\n        '",
        "graph_exec_id NotIn self.active_graph_runs"
      ],
      "code": "@expose\ndef cancel_execution(self, graph_exec_id: str) -> None:\n    \"\"\"\n        Mechanism:\n        1. Set the cancel event\n        2. Graph executor's cancel handler thread detects the event, terminates workers,\n           reinitializes worker pool, and returns.\n        3. Update execution statuses in DB and set `error` outputs to `\"TERMINATED\"`.\n        \"\"\"\n    if graph_exec_id not in self.active_graph_runs:\n        raise Exception(f'Graph execution #{graph_exec_id} not active/running: possibly already completed/cancelled.')\n    (future, cancel_event) = self.active_graph_runs[graph_exec_id]\n    if cancel_event.is_set():\n        return\n    cancel_event.set()\n    future.result()\n    node_execs = self.db_client.get_execution_results(graph_exec_id)\n    for node_exec in node_execs:\n        if node_exec.status not in (ExecutionStatus.COMPLETED, ExecutionStatus.FAILED):\n            self.db_client.upsert_execution_output(node_exec.node_exec_id, 'error', 'TERMINATED')\n            exec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.FAILED)\n            self.db_client.send_execution_update(exec_update)\n'\\n        Mechanism:\\n        1. Set the cancel event\\n        2. Graph executor\\'s cancel handler thread detects the event, terminates workers,\\n           reinitializes worker pool, and returns.\\n        3. Update execution statuses in DB and set `error` outputs to `\"TERMINATED\"`.\\n        '\ngraph_exec_id NotIn self.active_graph_runs"
    },
    {
      "id": "n158",
      "type": "block",
      "statements": [
        "raise Exception(f'Graph execution #{graph_exec_id} not active/running: possibly already completed/cancelled.')"
      ],
      "code": "raise Exception(f'Graph execution #{graph_exec_id} not active/running: possibly already completed/cancelled.')"
    },
    {
      "id": "n159",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n160",
      "type": "block",
      "statements": [
        "(future, cancel_event) = self.active_graph_runs[graph_exec_id]",
        "cancel_event.is_set()"
      ],
      "code": "(future, cancel_event) = self.active_graph_runs[graph_exec_id]\ncancel_event.is_set()"
    },
    {
      "id": "n161",
      "type": "block",
      "statements": [
        "return"
      ],
      "code": "return"
    },
    {
      "id": "n162",
      "type": "block",
      "statements": [],
      "code": "\ncancel_event.set()\nfuture.result()\nnode_execs = self.db_client.get_execution_results(graph_exec_id)"
    },
    {
      "id": "n163",
      "type": "block",
      "statements": [
        "node_exec",
        "node_execs"
      ],
      "code": "node_exec\nnode_execs"
    },
    {
      "id": "n164",
      "type": "block",
      "statements": [
        "node_exec.status NotIn (ExecutionStatus.COMPLETED, ExecutionStatus.FAILED)"
      ],
      "code": "node_exec.status NotIn (ExecutionStatus.COMPLETED, ExecutionStatus.FAILED)"
    },
    {
      "id": "n165",
      "type": "block",
      "statements": [
        "def _validate_node_input_credentials(self, graph: GraphModel, user_id: str):\n    \"\"\"Checks all credentials for all nodes of the graph\"\"\"\n    for node in graph.nodes:\n        block = get_block(node.block_id)\n        if not block:\n            raise ValueError(f'Unknown block {node.block_id} for node #{node.id}')\n        model_fields = cast(type[BaseModel], block.input_schema).model_fields\n        if CREDENTIALS_FIELD_NAME not in model_fields:\n            continue\n        field = model_fields[CREDENTIALS_FIELD_NAME]\n        credentials_meta_type = cast(CredentialsMetaInput, field.annotation)\n        credentials_meta = credentials_meta_type.model_validate(node.input_default[CREDENTIALS_FIELD_NAME])\n        credentials = self.credentials_store.get_creds_by_id(user_id, credentials_meta.id)\n        if not credentials:\n            raise ValueError(f'Unknown credentials #{credentials_meta.id} for node #{node.id}')\n        if credentials.provider != credentials_meta.provider or credentials.type != credentials_meta.type:\n            logger.warning(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch: {credentials_meta.type}<>{credentials.type};{credentials_meta.provider}<>{credentials.provider}')\n            raise ValueError(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch')",
        "'Checks all credentials for all nodes of the graph'"
      ],
      "code": "def _validate_node_input_credentials(self, graph: GraphModel, user_id: str):\n    \"\"\"Checks all credentials for all nodes of the graph\"\"\"\n    for node in graph.nodes:\n        block = get_block(node.block_id)\n        if not block:\n            raise ValueError(f'Unknown block {node.block_id} for node #{node.id}')\n        model_fields = cast(type[BaseModel], block.input_schema).model_fields\n        if CREDENTIALS_FIELD_NAME not in model_fields:\n            continue\n        field = model_fields[CREDENTIALS_FIELD_NAME]\n        credentials_meta_type = cast(CredentialsMetaInput, field.annotation)\n        credentials_meta = credentials_meta_type.model_validate(node.input_default[CREDENTIALS_FIELD_NAME])\n        credentials = self.credentials_store.get_creds_by_id(user_id, credentials_meta.id)\n        if not credentials:\n            raise ValueError(f'Unknown credentials #{credentials_meta.id} for node #{node.id}')\n        if credentials.provider != credentials_meta.provider or credentials.type != credentials_meta.type:\n            logger.warning(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch: {credentials_meta.type}<>{credentials.type};{credentials_meta.provider}<>{credentials.provider}')\n            raise ValueError(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch')\n'Checks all credentials for all nodes of the graph'"
    },
    {
      "id": "n166",
      "type": "block",
      "statements": [
        "self.db_client.upsert_execution_output(node_exec.node_exec_id, 'error', 'TERMINATED')",
        "exec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.FAILED)",
        "self.db_client.send_execution_update(exec_update)"
      ],
      "code": "self.db_client.upsert_execution_output(node_exec.node_exec_id, 'error', 'TERMINATED')\nexec_update = self.db_client.update_execution_status(node_exec.node_exec_id, ExecutionStatus.FAILED)\nself.db_client.send_execution_update(exec_update)"
    },
    {
      "id": "n167",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n168",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n169",
      "type": "block",
      "statements": [
        "node",
        "graph.nodes"
      ],
      "code": "node\ngraph.nodes"
    },
    {
      "id": "n170",
      "type": "block",
      "statements": [
        "block = get_block(node.block_id)",
        "not block"
      ],
      "code": "block = get_block(node.block_id)\nnot block"
    },
    {
      "id": "n171",
      "type": "block",
      "statements": [
        "@thread_cached\ndef get_db_client() -> 'DatabaseManager':\n    from backend.executor import DatabaseManager\n    return get_service_client(DatabaseManager)",
        "from backend.executor import DatabaseManager",
        "return get_service_client(DatabaseManager)"
      ],
      "code": "@thread_cached\ndef get_db_client() -> 'DatabaseManager':\n    from backend.executor import DatabaseManager\n    return get_service_client(DatabaseManager)\nfrom backend.executor import DatabaseManager\nreturn get_service_client(DatabaseManager)"
    },
    {
      "id": "n172",
      "type": "block",
      "statements": [
        "raise ValueError(f'Unknown block {node.block_id} for node #{node.id}')"
      ],
      "code": "raise ValueError(f'Unknown block {node.block_id} for node #{node.id}')"
    },
    {
      "id": "n173",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n174",
      "type": "block",
      "statements": [
        "model_fields = cast(type[BaseModel], block.input_schema).model_fields",
        "CREDENTIALS_FIELD_NAME NotIn model_fields"
      ],
      "code": "model_fields = cast(type[BaseModel], block.input_schema).model_fields\nCREDENTIALS_FIELD_NAME NotIn model_fields"
    },
    {
      "id": "n175",
      "type": "block",
      "statements": [
        "continue"
      ],
      "code": "continue"
    },
    {
      "id": "n176",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n177",
      "type": "block",
      "statements": [
        "field = model_fields[CREDENTIALS_FIELD_NAME]",
        "credentials_meta_type = cast(CredentialsMetaInput, field.annotation)",
        "credentials_meta = credentials_meta_type.model_validate(node.input_default[CREDENTIALS_FIELD_NAME])",
        "credentials = self.credentials_store.get_creds_by_id(user_id, credentials_meta.id)",
        "not credentials"
      ],
      "code": "field = model_fields[CREDENTIALS_FIELD_NAME]\ncredentials_meta_type = cast(CredentialsMetaInput, field.annotation)\ncredentials_meta = credentials_meta_type.model_validate(node.input_default[CREDENTIALS_FIELD_NAME])\ncredentials = self.credentials_store.get_creds_by_id(user_id, credentials_meta.id)\nnot credentials"
    },
    {
      "id": "n178",
      "type": "block",
      "statements": [
        "raise ValueError(f'Unknown credentials #{credentials_meta.id} for node #{node.id}')"
      ],
      "code": "raise ValueError(f'Unknown credentials #{credentials_meta.id} for node #{node.id}')"
    },
    {
      "id": "n179",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n180",
      "type": "block",
      "statements": [
        "credentials.provider != credentials_meta.provider or credentials.type != credentials_meta.type"
      ],
      "code": "credentials.provider != credentials_meta.provider or credentials.type != credentials_meta.type"
    },
    {
      "id": "n181",
      "type": "block",
      "statements": [
        "logger.warning(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch: {credentials_meta.type}<>{credentials.type};{credentials_meta.provider}<>{credentials.provider}')",
        "raise ValueError(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch')"
      ],
      "code": "logger.warning(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch: {credentials_meta.type}<>{credentials.type};{credentials_meta.provider}<>{credentials.provider}')\nraise ValueError(f'Invalid credentials #{credentials.id} for node #{node.id}: type/provider mismatch')"
    },
    {
      "id": "n182",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n183",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n184",
      "type": "block",
      "statements": [
        "@contextmanager\ndef synchronized(key: str, timeout: int=60):\n    lock: RedisLock = redis.get_redis().lock(f'lock:{key}', timeout=timeout)\n    try:\n        lock.acquire()\n        yield\n    finally:\n        if lock.locked():\n            lock.release()",
        "lock: RedisLock = redis.get_redis().lock(f'lock:{key}', timeout=timeout)",
        "try:\n    lock.acquire()\n    yield\nfinally:\n    if lock.locked():\n        lock.release()",
        "lock.acquire()",
        "(yield)",
        "lock.locked()"
      ],
      "code": "@contextmanager\ndef synchronized(key: str, timeout: int=60):\n    lock: RedisLock = redis.get_redis().lock(f'lock:{key}', timeout=timeout)\n    try:\n        lock.acquire()\n        yield\n    finally:\n        if lock.locked():\n            lock.release()\nlock: RedisLock = redis.get_redis().lock(f'lock:{key}', timeout=timeout)\ntry:\n    lock.acquire()\n    yield\nfinally:\n    if lock.locked():\n        lock.release()\nlock.acquire()\n(yield)\nlock.locked()"
    },
    {
      "id": "n185",
      "type": "block",
      "statements": [
        "lock.release()"
      ],
      "code": "lock.release()"
    },
    {
      "id": "n186",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n187",
      "type": "block",
      "statements": [
        "def llprint(message: str):\n    \"\"\"\n    Low-level print/log helper function for use in signal handlers.\n    Regular log/print statements are not allowed in signal handlers.\n    \"\"\"\n    if logger.getEffectiveLevel() == logging.DEBUG:\n        os.write(sys.stdout.fileno(), (message + '\\n').encode())",
        "'\\n    Low-level print/log helper function for use in signal handlers.\\n    Regular log/print statements are not allowed in signal handlers.\\n    '",
        "logger.getEffectiveLevel() Eq logging.DEBUG"
      ],
      "code": "def llprint(message: str):\n    \"\"\"\n    Low-level print/log helper function for use in signal handlers.\n    Regular log/print statements are not allowed in signal handlers.\n    \"\"\"\n    if logger.getEffectiveLevel() == logging.DEBUG:\n        os.write(sys.stdout.fileno(), (message + '\\n').encode())\n'\\n    Low-level print/log helper function for use in signal handlers.\\n    Regular log/print statements are not allowed in signal handlers.\\n    '\nlogger.getEffectiveLevel() Eq logging.DEBUG"
    },
    {
      "id": "n188",
      "type": "block",
      "statements": [
        "os.write(sys.stdout.fileno(), (message + '\\n').encode())"
      ],
      "code": "os.write(sys.stdout.fileno(), (message + '\\n').encode())"
    },
    {
      "id": "n189",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n190",
      "type": "block",
      "statements": [],
      "code": ""
    }
  ],
  "edges": [
    {
      "source": "n130",
      "target": "n132"
    },
    {
      "source": "n54",
      "target": "n53"
    },
    {
      "source": "n103",
      "target": "n105"
    },
    {
      "source": "n74",
      "target": "n75"
    },
    {
      "source": "n82",
      "target": "n84"
    },
    {
      "source": "n118",
      "target": "n120"
    },
    {
      "source": "n0",
      "target": "n1"
    },
    {
      "source": "n14",
      "target": "n15"
    },
    {
      "source": "n65",
      "target": "n73"
    },
    {
      "source": "n67",
      "target": "n69"
    },
    {
      "source": "n136",
      "target": "n138"
    },
    {
      "source": "n50",
      "target": "n51"
    },
    {
      "source": "n157",
      "target": "n159"
    },
    {
      "source": "n107",
      "target": "n109"
    },
    {
      "source": "n84",
      "target": "n85"
    },
    {
      "source": "n132",
      "target": "n133"
    },
    {
      "source": "n98",
      "target": "n100"
    },
    {
      "source": "n133",
      "target": "n135"
    },
    {
      "source": "n35",
      "target": "n36"
    },
    {
      "source": "n158",
      "target": "n160"
    },
    {
      "source": "n160",
      "target": "n162"
    },
    {
      "source": "n53",
      "target": "n55"
    },
    {
      "source": "n19",
      "target": "n20"
    },
    {
      "source": "n12",
      "target": "n13"
    },
    {
      "source": "n34",
      "target": "n36"
    },
    {
      "source": "n31",
      "target": "n33"
    },
    {
      "source": "n71",
      "target": "n72"
    },
    {
      "source": "n189",
      "target": "n190"
    },
    {
      "source": "n49",
      "target": "n50"
    },
    {
      "source": "n152",
      "target": "n153"
    },
    {
      "source": "n187",
      "target": "n188"
    },
    {
      "source": "n168",
      "target": "n163"
    },
    {
      "source": "n88",
      "target": "n89"
    },
    {
      "source": "n88",
      "target": "n90"
    },
    {
      "source": "n5",
      "target": "n6"
    },
    {
      "source": "n98",
      "target": "n99"
    },
    {
      "source": "n96",
      "target": "n98"
    },
    {
      "source": "n75",
      "target": "n77"
    },
    {
      "source": "n121",
      "target": "n123"
    },
    {
      "source": "n93",
      "target": "n94"
    },
    {
      "source": "n69",
      "target": "n65"
    },
    {
      "source": "n91",
      "target": "n92"
    },
    {
      "source": "n137",
      "target": "n138"
    },
    {
      "source": "n125",
      "target": "n126"
    },
    {
      "source": "n154",
      "target": "n155"
    },
    {
      "source": "n22",
      "target": "n24"
    },
    {
      "source": "n131",
      "target": "n132"
    },
    {
      "source": "n153",
      "target": "n133"
    },
    {
      "source": "n104",
      "target": "n121"
    },
    {
      "source": "n183",
      "target": "n169"
    },
    {
      "source": "n184",
      "target": "n185"
    },
    {
      "source": "n41",
      "target": "n42"
    },
    {
      "source": "n169",
      "target": "n171"
    },
    {
      "source": "n15",
      "target": "n17"
    },
    {
      "source": "n166",
      "target": "n168"
    },
    {
      "source": "n141",
      "target": "n145"
    },
    {
      "source": "n134",
      "target": "n136"
    },
    {
      "source": "n146",
      "target": "n147"
    },
    {
      "source": "n17",
      "target": "n18"
    },
    {
      "source": "n141",
      "target": "n146"
    },
    {
      "source": "n65",
      "target": "n74"
    },
    {
      "source": "n2",
      "target": "n3"
    },
    {
      "source": "n19",
      "target": "n21"
    },
    {
      "source": "n117",
      "target": "n119"
    },
    {
      "source": "n108",
      "target": "n109"
    },
    {
      "source": "n169",
      "target": "n170"
    },
    {
      "source": "n138",
      "target": "n140"
    },
    {
      "source": "n24",
      "target": "n19"
    },
    {
      "source": "n155",
      "target": "n154"
    },
    {
      "source": "n138",
      "target": "n139"
    },
    {
      "source": "n174",
      "target": "n176"
    },
    {
      "source": "n129",
      "target": "n130"
    },
    {
      "source": "n44",
      "target": "n43"
    },
    {
      "source": "n117",
      "target": "n118"
    },
    {
      "source": "n157",
      "target": "n158"
    },
    {
      "source": "n102",
      "target": "n103"
    },
    {
      "source": "n15",
      "target": "n16"
    },
    {
      "source": "n96",
      "target": "n97"
    },
    {
      "source": "n29",
      "target": "n30"
    },
    {
      "source": "n154",
      "target": "n156"
    },
    {
      "source": "n9",
      "target": "n11"
    },
    {
      "source": "n79",
      "target": "n81"
    },
    {
      "source": "n115",
      "target": "n110"
    },
    {
      "source": "n170",
      "target": "n173"
    },
    {
      "source": "n187",
      "target": "n189"
    },
    {
      "source": "n37",
      "target": "n39"
    },
    {
      "source": "n28",
      "target": "n30"
    },
    {
      "source": "n126",
      "target": "n127"
    },
    {
      "source": "n114",
      "target": "n116"
    },
    {
      "source": "n11",
      "target": "n12"
    },
    {
      "source": "n134",
      "target": "n137"
    },
    {
      "source": "n7",
      "target": "n9"
    },
    {
      "source": "n140",
      "target": "n141"
    },
    {
      "source": "n26",
      "target": "n25"
    },
    {
      "source": "n144",
      "target": "n141"
    },
    {
      "source": "n165",
      "target": "n169"
    },
    {
      "source": "n60",
      "target": "n62"
    },
    {
      "source": "n33",
      "target": "n34"
    },
    {
      "source": "n180",
      "target": "n182"
    },
    {
      "source": "n39",
      "target": "n40"
    },
    {
      "source": "n184",
      "target": "n186"
    },
    {
      "source": "n164",
      "target": "n166"
    },
    {
      "source": "n92",
      "target": "n91"
    },
    {
      "source": "n45",
      "target": "n42"
    },
    {
      "source": "n163",
      "target": "n165"
    },
    {
      "source": "n182",
      "target": "n183"
    },
    {
      "source": "n139",
      "target": "n143"
    },
    {
      "source": "n186",
      "target": "n187"
    },
    {
      "source": "n93",
      "target": "n95"
    },
    {
      "source": "n30",
      "target": "n32"
    },
    {
      "source": "n56",
      "target": "n58"
    },
    {
      "source": "n151",
      "target": "n153"
    },
    {
      "source": "n55",
      "target": "n56"
    },
    {
      "source": "n143",
      "target": "n144"
    },
    {
      "source": "n74",
      "target": "n76"
    },
    {
      "source": "n106",
      "target": "n108"
    },
    {
      "source": "n122",
      "target": "n123"
    },
    {
      "source": "n47",
      "target": "n49"
    },
    {
      "source": "n77",
      "target": "n78"
    },
    {
      "source": "n102",
      "target": "n104"
    },
    {
      "source": "n22",
      "target": "n23"
    },
    {
      "source": "n53",
      "target": "n54"
    },
    {
      "source": "n180",
      "target": "n181"
    },
    {
      "source": "n163",
      "target": "n164"
    },
    {
      "source": "n176",
      "target": "n177"
    },
    {
      "source": "n89",
      "target": "n88"
    },
    {
      "source": "n181",
      "target": "n183"
    },
    {
      "source": "n149",
      "target": "n150"
    },
    {
      "source": "n160",
      "target": "n161"
    },
    {
      "source": "n147",
      "target": "n151"
    },
    {
      "source": "n82",
      "target": "n83"
    },
    {
      "source": "n172",
      "target": "n174"
    },
    {
      "source": "n177",
      "target": "n178"
    },
    {
      "source": "n173",
      "target": "n174"
    },
    {
      "source": "n68",
      "target": "n71"
    },
    {
      "source": "n95",
      "target": "n96"
    },
    {
      "source": "n90",
      "target": "n91"
    },
    {
      "source": "n129",
      "target": "n131"
    },
    {
      "source": "n127",
      "target": "n126"
    },
    {
      "source": "n170",
      "target": "n172"
    },
    {
      "source": "n135",
      "target": "n154"
    },
    {
      "source": "n164",
      "target": "n167"
    },
    {
      "source": "n30",
      "target": "n31"
    },
    {
      "source": "n47",
      "target": "n48"
    },
    {
      "source": "n120",
      "target": "n113"
    },
    {
      "source": "n150",
      "target": "n147"
    },
    {
      "source": "n175",
      "target": "n177"
    },
    {
      "source": "n51",
      "target": "n53"
    },
    {
      "source": "n79",
      "target": "n80"
    },
    {
      "source": "n42",
      "target": "n47"
    },
    {
      "source": "n148",
      "target": "n150"
    },
    {
      "source": "n177",
      "target": "n179"
    },
    {
      "source": "n60",
      "target": "n61"
    },
    {
      "source": "n62",
      "target": "n63"
    },
    {
      "source": "n147",
      "target": "n152"
    },
    {
      "source": "n167",
      "target": "n168"
    },
    {
      "source": "n145",
      "target": "n149"
    },
    {
      "source": "n57",
      "target": "n58"
    },
    {
      "source": "n10",
      "target": "n12"
    },
    {
      "source": "n72",
      "target": "n67"
    },
    {
      "source": "n37",
      "target": "n38"
    },
    {
      "source": "n25",
      "target": "n26"
    },
    {
      "source": "n27",
      "target": "n29"
    },
    {
      "source": "n1",
      "target": "n3"
    },
    {
      "source": "n27",
      "target": "n28"
    },
    {
      "source": "n58",
      "target": "n50"
    },
    {
      "source": "n70",
      "target": "n72"
    },
    {
      "source": "n110",
      "target": "n111"
    },
    {
      "source": "n66",
      "target": "n65"
    },
    {
      "source": "n119",
      "target": "n120"
    },
    {
      "source": "n114",
      "target": "n117"
    },
    {
      "source": "n39",
      "target": "n41"
    },
    {
      "source": "n21",
      "target": "n25"
    },
    {
      "source": "n0",
      "target": "n2"
    },
    {
      "source": "n97",
      "target": "n96"
    },
    {
      "source": "n111",
      "target": "n113"
    },
    {
      "source": "n109",
      "target": "n110"
    },
    {
      "source": "n185",
      "target": "n187"
    },
    {
      "source": "n145",
      "target": "n148"
    },
    {
      "source": "n179",
      "target": "n180"
    },
    {
      "source": "n159",
      "target": "n160"
    },
    {
      "source": "n91",
      "target": "n93"
    },
    {
      "source": "n103",
      "target": "n106"
    },
    {
      "source": "n178",
      "target": "n180"
    },
    {
      "source": "n87",
      "target": "n88"
    },
    {
      "source": "n23",
      "target": "n22"
    },
    {
      "source": "n104",
      "target": "n122"
    },
    {
      "source": "n113",
      "target": "n115"
    },
    {
      "source": "n13",
      "target": "n15"
    },
    {
      "source": "n68",
      "target": "n70"
    },
    {
      "source": "n16",
      "target": "n18"
    },
    {
      "source": "n33",
      "target": "n35"
    },
    {
      "source": "n142",
      "target": "n144"
    },
    {
      "source": "n40",
      "target": "n43"
    },
    {
      "source": "n43",
      "target": "n44"
    },
    {
      "source": "n84",
      "target": "n86"
    },
    {
      "source": "n7",
      "target": "n8"
    },
    {
      "source": "n50",
      "target": "n52"
    },
    {
      "source": "n113",
      "target": "n114"
    },
    {
      "source": "n126",
      "target": "n128"
    },
    {
      "source": "n64",
      "target": "n67"
    },
    {
      "source": "n62",
      "target": "n64"
    },
    {
      "source": "n12",
      "target": "n14"
    },
    {
      "source": "n55",
      "target": "n57"
    },
    {
      "source": "n100",
      "target": "n101"
    },
    {
      "source": "n42",
      "target": "n46"
    },
    {
      "source": "n162",
      "target": "n163"
    },
    {
      "source": "n32",
      "target": "n33"
    },
    {
      "source": "n106",
      "target": "n107"
    },
    {
      "source": "n9",
      "target": "n10"
    },
    {
      "source": "n67",
      "target": "n68"
    },
    {
      "source": "n112",
      "target": "n102"
    },
    {
      "source": "n18",
      "target": "n19"
    },
    {
      "source": "n110",
      "target": "n112"
    },
    {
      "source": "n174",
      "target": "n175"
    },
    {
      "source": "n77",
      "target": "n79"
    },
    {
      "source": "n139",
      "target": "n142"
    },
    {
      "source": "n43",
      "target": "n45"
    },
    {
      "source": "n76",
      "target": "n77"
    },
    {
      "source": "n99",
      "target": "n101"
    },
    {
      "source": "n25",
      "target": "n27"
    },
    {
      "source": "n20",
      "target": "n22"
    },
    {
      "source": "n188",
      "target": "n190"
    },
    {
      "source": "n5",
      "target": "n7"
    },
    {
      "source": "n133",
      "target": "n134"
    }
  ]
}