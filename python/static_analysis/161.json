{
  "nodes": [
    {
      "id": "n0",
      "type": "block",
      "statements": [
        "import random",
        "from collections import defaultdict",
        "from enum import Enum",
        "from typing import Any, Dict, List, Optional, Union",
        "from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema",
        "from backend.data.model import SchemaField",
        "class SamplingMethod(str, Enum):\n    RANDOM = 'random'\n    SYSTEMATIC = 'systematic'\n    TOP = 'top'\n    BOTTOM = 'bottom'\n    STRATIFIED = 'stratified'\n    WEIGHTED = 'weighted'\n    RESERVOIR = 'reservoir'\n    CLUSTER = 'cluster'",
        "RANDOM = 'random'",
        "SYSTEMATIC = 'systematic'",
        "TOP = 'top'",
        "BOTTOM = 'bottom'",
        "STRATIFIED = 'stratified'",
        "WEIGHTED = 'weighted'",
        "RESERVOIR = 'reservoir'",
        "CLUSTER = 'cluster'",
        "class DataSamplingBlock(Block):\n\n    class Input(BlockSchema):\n        data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(description='The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists.', placeholder=\"{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]\")\n        sample_size: int = SchemaField(description='The number of samples to take from the dataset.', placeholder='10', default=10)\n        sampling_method: SamplingMethod = SchemaField(description='The method to use for sampling.', default=SamplingMethod.RANDOM)\n        accumulate: bool = SchemaField(description='Whether to accumulate data before sampling.', default=False)\n        random_seed: Optional[int] = SchemaField(description='Seed for random number generator (optional).', default=None)\n        stratify_key: Optional[str] = SchemaField(description='Key to use for stratified sampling (required for stratified sampling).', default=None)\n        weight_key: Optional[str] = SchemaField(description='Key to use for weighted sampling (required for weighted sampling).', default=None)\n        cluster_key: Optional[str] = SchemaField(description='Key to use for cluster sampling (required for cluster sampling).', default=None)\n\n    class Output(BlockSchema):\n        sampled_data: List[Union[dict, List[Any]]] = SchemaField(description='The sampled subset of the input data.')\n        sample_indices: List[int] = SchemaField(description='The indices of the sampled data in the original dataset.')\n\n    def __init__(self):\n        super().__init__(id='4a448883-71fa-49cf-91cf-70d793bd7d87', description='This block samples data from a given dataset using various sampling methods.', categories={BlockCategory.LOGIC}, input_schema=DataSamplingBlock.Input, output_schema=DataSamplingBlock.Output, test_input={'data': [{'id': i, 'value': chr(97 + i), 'group': i % 3} for i in range(10)], 'sample_size': 3, 'sampling_method': SamplingMethod.STRATIFIED, 'accumulate': False, 'random_seed': 42, 'stratify_key': 'group'}, test_output=[('sampled_data', [{'id': 0, 'value': 'a', 'group': 0}, {'id': 1, 'value': 'b', 'group': 1}, {'id': 8, 'value': 'i', 'group': 2}]), ('sample_indices', [0, 1, 8])])\n        self.accumulated_data = []\n\n    def run(self, input_data: Input, **kwargs) -> BlockOutput:\n        if input_data.accumulate:\n            if isinstance(input_data.data, dict):\n                self.accumulated_data.append(input_data.data)\n            elif isinstance(input_data.data, list):\n                self.accumulated_data.extend(input_data.data)\n            else:\n                raise ValueError(f'Unsupported data type: {type(input_data.data)}')\n            if len(self.accumulated_data) < input_data.sample_size:\n                return\n            data_to_sample = self.accumulated_data\n        else:\n            data_to_sample = input_data.data if isinstance(input_data.data, list) else [input_data.data]\n        if input_data.random_seed is not None:\n            random.seed(input_data.random_seed)\n        data_size = len(data_to_sample)\n        if input_data.sample_size > data_size:\n            raise ValueError(f'Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).')\n        indices = []\n        if input_data.sampling_method == SamplingMethod.RANDOM:\n            indices = random.sample(range(data_size), input_data.sample_size)\n        elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:\n            step = data_size // input_data.sample_size\n            start = random.randint(0, step - 1)\n            indices = list(range(start, data_size, step))[:input_data.sample_size]\n        elif input_data.sampling_method == SamplingMethod.TOP:\n            indices = list(range(input_data.sample_size))\n        elif input_data.sampling_method == SamplingMethod.BOTTOM:\n            indices = list(range(data_size - input_data.sample_size, data_size))\n        elif input_data.sampling_method == SamplingMethod.STRATIFIED:\n            if not input_data.stratify_key:\n                raise ValueError('Stratify key must be provided for stratified sampling.')\n            strata = defaultdict(list)\n            for (i, item) in enumerate(data_to_sample):\n                if isinstance(item, dict):\n                    strata_value = item.get(input_data.stratify_key)\n                elif hasattr(item, input_data.stratify_key):\n                    strata_value = getattr(item, input_data.stratify_key)\n                else:\n                    raise ValueError(f\"Stratify key '{input_data.stratify_key}' not found in item {item}\")\n                if strata_value is None:\n                    raise ValueError(f\"Stratify value for key '{input_data.stratify_key}' is None\")\n                strata[str(strata_value)].append(i)\n            stratum_sizes = {k: max(1, int(len(v) / data_size * input_data.sample_size)) for (k, v) in strata.items()}\n            while sum(stratum_sizes.values()) != input_data.sample_size:\n                if sum(stratum_sizes.values()) < input_data.sample_size:\n                    stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] += 1\n                else:\n                    stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] -= 1\n            for (stratum, size) in stratum_sizes.items():\n                indices.extend(random.sample(strata[stratum], size))\n        elif input_data.sampling_method == SamplingMethod.WEIGHTED:\n            if not input_data.weight_key:\n                raise ValueError('Weight key must be provided for weighted sampling.')\n            weights = []\n            for item in data_to_sample:\n                if isinstance(item, dict):\n                    weight = item.get(input_data.weight_key)\n                elif hasattr(item, input_data.weight_key):\n                    weight = getattr(item, input_data.weight_key)\n                else:\n                    raise ValueError(f\"Weight key '{input_data.weight_key}' not found in item {item}\")\n                if weight is None:\n                    raise ValueError(f\"Weight value for key '{input_data.weight_key}' is None\")\n                try:\n                    weights.append(float(weight))\n                except ValueError:\n                    raise ValueError(f\"Weight value '{weight}' cannot be converted to a number\")\n            if not weights:\n                raise ValueError(f\"No valid weights found using key '{input_data.weight_key}'\")\n            indices = random.choices(range(data_size), weights=weights, k=input_data.sample_size)\n        elif input_data.sampling_method == SamplingMethod.RESERVOIR:\n            indices = list(range(input_data.sample_size))\n            for i in range(input_data.sample_size, data_size):\n                j = random.randint(0, i)\n                if j < input_data.sample_size:\n                    indices[j] = i\n        elif input_data.sampling_method == SamplingMethod.CLUSTER:\n            if not input_data.cluster_key:\n                raise ValueError('Cluster key must be provided for cluster sampling.')\n            clusters = defaultdict(list)\n            for (i, item) in enumerate(data_to_sample):\n                if isinstance(item, dict):\n                    cluster_value = item.get(input_data.cluster_key)\n                elif hasattr(item, input_data.cluster_key):\n                    cluster_value = getattr(item, input_data.cluster_key)\n                else:\n                    raise TypeError(f\"Item {item} does not have the cluster key '{input_data.cluster_key}'\")\n                clusters[str(cluster_value)].append(i)\n            selected_clusters = []\n            while sum((len(clusters[c]) for c in selected_clusters)) < input_data.sample_size:\n                available_clusters = [c for c in clusters if c not in selected_clusters]\n                if not available_clusters:\n                    break\n                selected_clusters.append(random.choice(available_clusters))\n            for cluster in selected_clusters:\n                indices.extend(clusters[cluster])\n            if len(indices) > input_data.sample_size:\n                indices = random.sample(indices, input_data.sample_size)\n        else:\n            raise ValueError(f'Unknown sampling method: {input_data.sampling_method}')\n        sampled_data = [data_to_sample[i] for i in indices]\n        if input_data.accumulate:\n            self.accumulated_data = []\n        yield ('sampled_data', sampled_data)\n        yield ('sample_indices', indices)",
        "class Input(BlockSchema):\n    data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(description='The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists.', placeholder=\"{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]\")\n    sample_size: int = SchemaField(description='The number of samples to take from the dataset.', placeholder='10', default=10)\n    sampling_method: SamplingMethod = SchemaField(description='The method to use for sampling.', default=SamplingMethod.RANDOM)\n    accumulate: bool = SchemaField(description='Whether to accumulate data before sampling.', default=False)\n    random_seed: Optional[int] = SchemaField(description='Seed for random number generator (optional).', default=None)\n    stratify_key: Optional[str] = SchemaField(description='Key to use for stratified sampling (required for stratified sampling).', default=None)\n    weight_key: Optional[str] = SchemaField(description='Key to use for weighted sampling (required for weighted sampling).', default=None)\n    cluster_key: Optional[str] = SchemaField(description='Key to use for cluster sampling (required for cluster sampling).', default=None)",
        "data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(description='The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists.', placeholder=\"{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]\")",
        "sample_size: int = SchemaField(description='The number of samples to take from the dataset.', placeholder='10', default=10)",
        "sampling_method: SamplingMethod = SchemaField(description='The method to use for sampling.', default=SamplingMethod.RANDOM)",
        "accumulate: bool = SchemaField(description='Whether to accumulate data before sampling.', default=False)",
        "random_seed: Optional[int] = SchemaField(description='Seed for random number generator (optional).', default=None)",
        "stratify_key: Optional[str] = SchemaField(description='Key to use for stratified sampling (required for stratified sampling).', default=None)",
        "weight_key: Optional[str] = SchemaField(description='Key to use for weighted sampling (required for weighted sampling).', default=None)",
        "cluster_key: Optional[str] = SchemaField(description='Key to use for cluster sampling (required for cluster sampling).', default=None)",
        "class Output(BlockSchema):\n    sampled_data: List[Union[dict, List[Any]]] = SchemaField(description='The sampled subset of the input data.')\n    sample_indices: List[int] = SchemaField(description='The indices of the sampled data in the original dataset.')",
        "sampled_data: List[Union[dict, List[Any]]] = SchemaField(description='The sampled subset of the input data.')",
        "sample_indices: List[int] = SchemaField(description='The indices of the sampled data in the original dataset.')",
        "def __init__(self):\n    super().__init__(id='4a448883-71fa-49cf-91cf-70d793bd7d87', description='This block samples data from a given dataset using various sampling methods.', categories={BlockCategory.LOGIC}, input_schema=DataSamplingBlock.Input, output_schema=DataSamplingBlock.Output, test_input={'data': [{'id': i, 'value': chr(97 + i), 'group': i % 3} for i in range(10)], 'sample_size': 3, 'sampling_method': SamplingMethod.STRATIFIED, 'accumulate': False, 'random_seed': 42, 'stratify_key': 'group'}, test_output=[('sampled_data', [{'id': 0, 'value': 'a', 'group': 0}, {'id': 1, 'value': 'b', 'group': 1}, {'id': 8, 'value': 'i', 'group': 2}]), ('sample_indices', [0, 1, 8])])\n    self.accumulated_data = []",
        "super().__init__()",
        "self.accumulated_data = []",
        "def run(self, input_data: Input, **kwargs) -> BlockOutput:\n    if input_data.accumulate:\n        if isinstance(input_data.data, dict):\n            self.accumulated_data.append(input_data.data)\n        elif isinstance(input_data.data, list):\n            self.accumulated_data.extend(input_data.data)\n        else:\n            raise ValueError(f'Unsupported data type: {type(input_data.data)}')\n        if len(self.accumulated_data) < input_data.sample_size:\n            return\n        data_to_sample = self.accumulated_data\n    else:\n        data_to_sample = input_data.data if isinstance(input_data.data, list) else [input_data.data]\n    if input_data.random_seed is not None:\n        random.seed(input_data.random_seed)\n    data_size = len(data_to_sample)\n    if input_data.sample_size > data_size:\n        raise ValueError(f'Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).')\n    indices = []\n    if input_data.sampling_method == SamplingMethod.RANDOM:\n        indices = random.sample(range(data_size), input_data.sample_size)\n    elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:\n        step = data_size // input_data.sample_size\n        start = random.randint(0, step - 1)\n        indices = list(range(start, data_size, step))[:input_data.sample_size]\n    elif input_data.sampling_method == SamplingMethod.TOP:\n        indices = list(range(input_data.sample_size))\n    elif input_data.sampling_method == SamplingMethod.BOTTOM:\n        indices = list(range(data_size - input_data.sample_size, data_size))\n    elif input_data.sampling_method == SamplingMethod.STRATIFIED:\n        if not input_data.stratify_key:\n            raise ValueError('Stratify key must be provided for stratified sampling.')\n        strata = defaultdict(list)\n        for (i, item) in enumerate(data_to_sample):\n            if isinstance(item, dict):\n                strata_value = item.get(input_data.stratify_key)\n            elif hasattr(item, input_data.stratify_key):\n                strata_value = getattr(item, input_data.stratify_key)\n            else:\n                raise ValueError(f\"Stratify key '{input_data.stratify_key}' not found in item {item}\")\n            if strata_value is None:\n                raise ValueError(f\"Stratify value for key '{input_data.stratify_key}' is None\")\n            strata[str(strata_value)].append(i)\n        stratum_sizes = {k: max(1, int(len(v) / data_size * input_data.sample_size)) for (k, v) in strata.items()}\n        while sum(stratum_sizes.values()) != input_data.sample_size:\n            if sum(stratum_sizes.values()) < input_data.sample_size:\n                stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] += 1\n            else:\n                stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] -= 1\n        for (stratum, size) in stratum_sizes.items():\n            indices.extend(random.sample(strata[stratum], size))\n    elif input_data.sampling_method == SamplingMethod.WEIGHTED:\n        if not input_data.weight_key:\n            raise ValueError('Weight key must be provided for weighted sampling.')\n        weights = []\n        for item in data_to_sample:\n            if isinstance(item, dict):\n                weight = item.get(input_data.weight_key)\n            elif hasattr(item, input_data.weight_key):\n                weight = getattr(item, input_data.weight_key)\n            else:\n                raise ValueError(f\"Weight key '{input_data.weight_key}' not found in item {item}\")\n            if weight is None:\n                raise ValueError(f\"Weight value for key '{input_data.weight_key}' is None\")\n            try:\n                weights.append(float(weight))\n            except ValueError:\n                raise ValueError(f\"Weight value '{weight}' cannot be converted to a number\")\n        if not weights:\n            raise ValueError(f\"No valid weights found using key '{input_data.weight_key}'\")\n        indices = random.choices(range(data_size), weights=weights, k=input_data.sample_size)\n    elif input_data.sampling_method == SamplingMethod.RESERVOIR:\n        indices = list(range(input_data.sample_size))\n        for i in range(input_data.sample_size, data_size):\n            j = random.randint(0, i)\n            if j < input_data.sample_size:\n                indices[j] = i\n    elif input_data.sampling_method == SamplingMethod.CLUSTER:\n        if not input_data.cluster_key:\n            raise ValueError('Cluster key must be provided for cluster sampling.')\n        clusters = defaultdict(list)\n        for (i, item) in enumerate(data_to_sample):\n            if isinstance(item, dict):\n                cluster_value = item.get(input_data.cluster_key)\n            elif hasattr(item, input_data.cluster_key):\n                cluster_value = getattr(item, input_data.cluster_key)\n            else:\n                raise TypeError(f\"Item {item} does not have the cluster key '{input_data.cluster_key}'\")\n            clusters[str(cluster_value)].append(i)\n        selected_clusters = []\n        while sum((len(clusters[c]) for c in selected_clusters)) < input_data.sample_size:\n            available_clusters = [c for c in clusters if c not in selected_clusters]\n            if not available_clusters:\n                break\n            selected_clusters.append(random.choice(available_clusters))\n        for cluster in selected_clusters:\n            indices.extend(clusters[cluster])\n        if len(indices) > input_data.sample_size:\n            indices = random.sample(indices, input_data.sample_size)\n    else:\n        raise ValueError(f'Unknown sampling method: {input_data.sampling_method}')\n    sampled_data = [data_to_sample[i] for i in indices]\n    if input_data.accumulate:\n        self.accumulated_data = []\n    yield ('sampled_data', sampled_data)\n    yield ('sample_indices', indices)",
        "input_data.accumulate"
      ],
      "code": "import random\nfrom collections import defaultdict\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union\nfrom backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema\nfrom backend.data.model import SchemaField\nclass SamplingMethod(str, Enum):\n    RANDOM = 'random'\n    SYSTEMATIC = 'systematic'\n    TOP = 'top'\n    BOTTOM = 'bottom'\n    STRATIFIED = 'stratified'\n    WEIGHTED = 'weighted'\n    RESERVOIR = 'reservoir'\n    CLUSTER = 'cluster'\nRANDOM = 'random'\nSYSTEMATIC = 'systematic'\nTOP = 'top'\nBOTTOM = 'bottom'\nSTRATIFIED = 'stratified'\nWEIGHTED = 'weighted'\nRESERVOIR = 'reservoir'\nCLUSTER = 'cluster'\nclass DataSamplingBlock(Block):\n\n    class Input(BlockSchema):\n        data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(description='The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists.', placeholder=\"{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]\")\n        sample_size: int = SchemaField(description='The number of samples to take from the dataset.', placeholder='10', default=10)\n        sampling_method: SamplingMethod = SchemaField(description='The method to use for sampling.', default=SamplingMethod.RANDOM)\n        accumulate: bool = SchemaField(description='Whether to accumulate data before sampling.', default=False)\n        random_seed: Optional[int] = SchemaField(description='Seed for random number generator (optional).', default=None)\n        stratify_key: Optional[str] = SchemaField(description='Key to use for stratified sampling (required for stratified sampling).', default=None)\n        weight_key: Optional[str] = SchemaField(description='Key to use for weighted sampling (required for weighted sampling).', default=None)\n        cluster_key: Optional[str] = SchemaField(description='Key to use for cluster sampling (required for cluster sampling).', default=None)\n\n    class Output(BlockSchema):\n        sampled_data: List[Union[dict, List[Any]]] = SchemaField(description='The sampled subset of the input data.')\n        sample_indices: List[int] = SchemaField(description='The indices of the sampled data in the original dataset.')\n\n    def __init__(self):\n        super().__init__(id='4a448883-71fa-49cf-91cf-70d793bd7d87', description='This block samples data from a given dataset using various sampling methods.', categories={BlockCategory.LOGIC}, input_schema=DataSamplingBlock.Input, output_schema=DataSamplingBlock.Output, test_input={'data': [{'id': i, 'value': chr(97 + i), 'group': i % 3} for i in range(10)], 'sample_size': 3, 'sampling_method': SamplingMethod.STRATIFIED, 'accumulate': False, 'random_seed': 42, 'stratify_key': 'group'}, test_output=[('sampled_data', [{'id': 0, 'value': 'a', 'group': 0}, {'id': 1, 'value': 'b', 'group': 1}, {'id': 8, 'value': 'i', 'group': 2}]), ('sample_indices', [0, 1, 8])])\n        self.accumulated_data = []\n\n    def run(self, input_data: Input, **kwargs) -> BlockOutput:\n        if input_data.accumulate:\n            if isinstance(input_data.data, dict):\n                self.accumulated_data.append(input_data.data)\n            elif isinstance(input_data.data, list):\n                self.accumulated_data.extend(input_data.data)\n            else:\n                raise ValueError(f'Unsupported data type: {type(input_data.data)}')\n            if len(self.accumulated_data) < input_data.sample_size:\n                return\n            data_to_sample = self.accumulated_data\n        else:\n            data_to_sample = input_data.data if isinstance(input_data.data, list) else [input_data.data]\n        if input_data.random_seed is not None:\n            random.seed(input_data.random_seed)\n        data_size = len(data_to_sample)\n        if input_data.sample_size > data_size:\n            raise ValueError(f'Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).')\n        indices = []\n        if input_data.sampling_method == SamplingMethod.RANDOM:\n            indices = random.sample(range(data_size), input_data.sample_size)\n        elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:\n            step = data_size // input_data.sample_size\n            start = random.randint(0, step - 1)\n            indices = list(range(start, data_size, step))[:input_data.sample_size]\n        elif input_data.sampling_method == SamplingMethod.TOP:\n            indices = list(range(input_data.sample_size))\n        elif input_data.sampling_method == SamplingMethod.BOTTOM:\n            indices = list(range(data_size - input_data.sample_size, data_size))\n        elif input_data.sampling_method == SamplingMethod.STRATIFIED:\n            if not input_data.stratify_key:\n                raise ValueError('Stratify key must be provided for stratified sampling.')\n            strata = defaultdict(list)\n            for (i, item) in enumerate(data_to_sample):\n                if isinstance(item, dict):\n                    strata_value = item.get(input_data.stratify_key)\n                elif hasattr(item, input_data.stratify_key):\n                    strata_value = getattr(item, input_data.stratify_key)\n                else:\n                    raise ValueError(f\"Stratify key '{input_data.stratify_key}' not found in item {item}\")\n                if strata_value is None:\n                    raise ValueError(f\"Stratify value for key '{input_data.stratify_key}' is None\")\n                strata[str(strata_value)].append(i)\n            stratum_sizes = {k: max(1, int(len(v) / data_size * input_data.sample_size)) for (k, v) in strata.items()}\n            while sum(stratum_sizes.values()) != input_data.sample_size:\n                if sum(stratum_sizes.values()) < input_data.sample_size:\n                    stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] += 1\n                else:\n                    stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] -= 1\n            for (stratum, size) in stratum_sizes.items():\n                indices.extend(random.sample(strata[stratum], size))\n        elif input_data.sampling_method == SamplingMethod.WEIGHTED:\n            if not input_data.weight_key:\n                raise ValueError('Weight key must be provided for weighted sampling.')\n            weights = []\n            for item in data_to_sample:\n                if isinstance(item, dict):\n                    weight = item.get(input_data.weight_key)\n                elif hasattr(item, input_data.weight_key):\n                    weight = getattr(item, input_data.weight_key)\n                else:\n                    raise ValueError(f\"Weight key '{input_data.weight_key}' not found in item {item}\")\n                if weight is None:\n                    raise ValueError(f\"Weight value for key '{input_data.weight_key}' is None\")\n                try:\n                    weights.append(float(weight))\n                except ValueError:\n                    raise ValueError(f\"Weight value '{weight}' cannot be converted to a number\")\n            if not weights:\n                raise ValueError(f\"No valid weights found using key '{input_data.weight_key}'\")\n            indices = random.choices(range(data_size), weights=weights, k=input_data.sample_size)\n        elif input_data.sampling_method == SamplingMethod.RESERVOIR:\n            indices = list(range(input_data.sample_size))\n            for i in range(input_data.sample_size, data_size):\n                j = random.randint(0, i)\n                if j < input_data.sample_size:\n                    indices[j] = i\n        elif input_data.sampling_method == SamplingMethod.CLUSTER:\n            if not input_data.cluster_key:\n                raise ValueError('Cluster key must be provided for cluster sampling.')\n            clusters = defaultdict(list)\n            for (i, item) in enumerate(data_to_sample):\n                if isinstance(item, dict):\n                    cluster_value = item.get(input_data.cluster_key)\n                elif hasattr(item, input_data.cluster_key):\n                    cluster_value = getattr(item, input_data.cluster_key)\n                else:\n                    raise TypeError(f\"Item {item} does not have the cluster key '{input_data.cluster_key}'\")\n                clusters[str(cluster_value)].append(i)\n            selected_clusters = []\n            while sum((len(clusters[c]) for c in selected_clusters)) < input_data.sample_size:\n                available_clusters = [c for c in clusters if c not in selected_clusters]\n                if not available_clusters:\n                    break\n                selected_clusters.append(random.choice(available_clusters))\n            for cluster in selected_clusters:\n                indices.extend(clusters[cluster])\n            if len(indices) > input_data.sample_size:\n                indices = random.sample(indices, input_data.sample_size)\n        else:\n            raise ValueError(f'Unknown sampling method: {input_data.sampling_method}')\n        sampled_data = [data_to_sample[i] for i in indices]\n        if input_data.accumulate:\n            self.accumulated_data = []\n        yield ('sampled_data', sampled_data)\n        yield ('sample_indices', indices)\nclass Input(BlockSchema):\n    data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(description='The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists.', placeholder=\"{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]\")\n    sample_size: int = SchemaField(description='The number of samples to take from the dataset.', placeholder='10', default=10)\n    sampling_method: SamplingMethod = SchemaField(description='The method to use for sampling.', default=SamplingMethod.RANDOM)\n    accumulate: bool = SchemaField(description='Whether to accumulate data before sampling.', default=False)\n    random_seed: Optional[int] = SchemaField(description='Seed for random number generator (optional).', default=None)\n    stratify_key: Optional[str] = SchemaField(description='Key to use for stratified sampling (required for stratified sampling).', default=None)\n    weight_key: Optional[str] = SchemaField(description='Key to use for weighted sampling (required for weighted sampling).', default=None)\n    cluster_key: Optional[str] = SchemaField(description='Key to use for cluster sampling (required for cluster sampling).', default=None)\ndata: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(description='The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists.', placeholder=\"{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]\")\nsample_size: int = SchemaField(description='The number of samples to take from the dataset.', placeholder='10', default=10)\nsampling_method: SamplingMethod = SchemaField(description='The method to use for sampling.', default=SamplingMethod.RANDOM)\naccumulate: bool = SchemaField(description='Whether to accumulate data before sampling.', default=False)\nrandom_seed: Optional[int] = SchemaField(description='Seed for random number generator (optional).', default=None)\nstratify_key: Optional[str] = SchemaField(description='Key to use for stratified sampling (required for stratified sampling).', default=None)\nweight_key: Optional[str] = SchemaField(description='Key to use for weighted sampling (required for weighted sampling).', default=None)\ncluster_key: Optional[str] = SchemaField(description='Key to use for cluster sampling (required for cluster sampling).', default=None)\nclass Output(BlockSchema):\n    sampled_data: List[Union[dict, List[Any]]] = SchemaField(description='The sampled subset of the input data.')\n    sample_indices: List[int] = SchemaField(description='The indices of the sampled data in the original dataset.')\nsampled_data: List[Union[dict, List[Any]]] = SchemaField(description='The sampled subset of the input data.')\nsample_indices: List[int] = SchemaField(description='The indices of the sampled data in the original dataset.')\ndef __init__(self):\n    super().__init__(id='4a448883-71fa-49cf-91cf-70d793bd7d87', description='This block samples data from a given dataset using various sampling methods.', categories={BlockCategory.LOGIC}, input_schema=DataSamplingBlock.Input, output_schema=DataSamplingBlock.Output, test_input={'data': [{'id': i, 'value': chr(97 + i), 'group': i % 3} for i in range(10)], 'sample_size': 3, 'sampling_method': SamplingMethod.STRATIFIED, 'accumulate': False, 'random_seed': 42, 'stratify_key': 'group'}, test_output=[('sampled_data', [{'id': 0, 'value': 'a', 'group': 0}, {'id': 1, 'value': 'b', 'group': 1}, {'id': 8, 'value': 'i', 'group': 2}]), ('sample_indices', [0, 1, 8])])\n    self.accumulated_data = []\nsuper().__init__()\nself.accumulated_data = []\ndef run(self, input_data: Input, **kwargs) -> BlockOutput:\n    if input_data.accumulate:\n        if isinstance(input_data.data, dict):\n            self.accumulated_data.append(input_data.data)\n        elif isinstance(input_data.data, list):\n            self.accumulated_data.extend(input_data.data)\n        else:\n            raise ValueError(f'Unsupported data type: {type(input_data.data)}')\n        if len(self.accumulated_data) < input_data.sample_size:\n            return\n        data_to_sample = self.accumulated_data\n    else:\n        data_to_sample = input_data.data if isinstance(input_data.data, list) else [input_data.data]\n    if input_data.random_seed is not None:\n        random.seed(input_data.random_seed)\n    data_size = len(data_to_sample)\n    if input_data.sample_size > data_size:\n        raise ValueError(f'Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).')\n    indices = []\n    if input_data.sampling_method == SamplingMethod.RANDOM:\n        indices = random.sample(range(data_size), input_data.sample_size)\n    elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:\n        step = data_size // input_data.sample_size\n        start = random.randint(0, step - 1)\n        indices = list(range(start, data_size, step))[:input_data.sample_size]\n    elif input_data.sampling_method == SamplingMethod.TOP:\n        indices = list(range(input_data.sample_size))\n    elif input_data.sampling_method == SamplingMethod.BOTTOM:\n        indices = list(range(data_size - input_data.sample_size, data_size))\n    elif input_data.sampling_method == SamplingMethod.STRATIFIED:\n        if not input_data.stratify_key:\n            raise ValueError('Stratify key must be provided for stratified sampling.')\n        strata = defaultdict(list)\n        for (i, item) in enumerate(data_to_sample):\n            if isinstance(item, dict):\n                strata_value = item.get(input_data.stratify_key)\n            elif hasattr(item, input_data.stratify_key):\n                strata_value = getattr(item, input_data.stratify_key)\n            else:\n                raise ValueError(f\"Stratify key '{input_data.stratify_key}' not found in item {item}\")\n            if strata_value is None:\n                raise ValueError(f\"Stratify value for key '{input_data.stratify_key}' is None\")\n            strata[str(strata_value)].append(i)\n        stratum_sizes = {k: max(1, int(len(v) / data_size * input_data.sample_size)) for (k, v) in strata.items()}\n        while sum(stratum_sizes.values()) != input_data.sample_size:\n            if sum(stratum_sizes.values()) < input_data.sample_size:\n                stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] += 1\n            else:\n                stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] -= 1\n        for (stratum, size) in stratum_sizes.items():\n            indices.extend(random.sample(strata[stratum], size))\n    elif input_data.sampling_method == SamplingMethod.WEIGHTED:\n        if not input_data.weight_key:\n            raise ValueError('Weight key must be provided for weighted sampling.')\n        weights = []\n        for item in data_to_sample:\n            if isinstance(item, dict):\n                weight = item.get(input_data.weight_key)\n            elif hasattr(item, input_data.weight_key):\n                weight = getattr(item, input_data.weight_key)\n            else:\n                raise ValueError(f\"Weight key '{input_data.weight_key}' not found in item {item}\")\n            if weight is None:\n                raise ValueError(f\"Weight value for key '{input_data.weight_key}' is None\")\n            try:\n                weights.append(float(weight))\n            except ValueError:\n                raise ValueError(f\"Weight value '{weight}' cannot be converted to a number\")\n        if not weights:\n            raise ValueError(f\"No valid weights found using key '{input_data.weight_key}'\")\n        indices = random.choices(range(data_size), weights=weights, k=input_data.sample_size)\n    elif input_data.sampling_method == SamplingMethod.RESERVOIR:\n        indices = list(range(input_data.sample_size))\n        for i in range(input_data.sample_size, data_size):\n            j = random.randint(0, i)\n            if j < input_data.sample_size:\n                indices[j] = i\n    elif input_data.sampling_method == SamplingMethod.CLUSTER:\n        if not input_data.cluster_key:\n            raise ValueError('Cluster key must be provided for cluster sampling.')\n        clusters = defaultdict(list)\n        for (i, item) in enumerate(data_to_sample):\n            if isinstance(item, dict):\n                cluster_value = item.get(input_data.cluster_key)\n            elif hasattr(item, input_data.cluster_key):\n                cluster_value = getattr(item, input_data.cluster_key)\n            else:\n                raise TypeError(f\"Item {item} does not have the cluster key '{input_data.cluster_key}'\")\n            clusters[str(cluster_value)].append(i)\n        selected_clusters = []\n        while sum((len(clusters[c]) for c in selected_clusters)) < input_data.sample_size:\n            available_clusters = [c for c in clusters if c not in selected_clusters]\n            if not available_clusters:\n                break\n            selected_clusters.append(random.choice(available_clusters))\n        for cluster in selected_clusters:\n            indices.extend(clusters[cluster])\n        if len(indices) > input_data.sample_size:\n            indices = random.sample(indices, input_data.sample_size)\n    else:\n        raise ValueError(f'Unknown sampling method: {input_data.sampling_method}')\n    sampled_data = [data_to_sample[i] for i in indices]\n    if input_data.accumulate:\n        self.accumulated_data = []\n    yield ('sampled_data', sampled_data)\n    yield ('sample_indices', indices)\ninput_data.accumulate"
    },
    {
      "id": "n1",
      "type": "block",
      "statements": [
        "isinstance(input_data.data, dict)"
      ],
      "code": "isinstance(input_data.data, dict)"
    },
    {
      "id": "n2",
      "type": "block",
      "statements": [
        "data_to_sample = input_data.data if isinstance(input_data.data, list) else [input_data.data]"
      ],
      "code": "data_to_sample = input_data.data if isinstance(input_data.data, list) else [input_data.data]"
    },
    {
      "id": "n3",
      "type": "block",
      "statements": [
        "input_data.random_seed IsNot None"
      ],
      "code": "input_data.random_seed IsNot None"
    },
    {
      "id": "n4",
      "type": "block",
      "statements": [
        "self.accumulated_data.append(input_data.data)"
      ],
      "code": "self.accumulated_data.append(input_data.data)"
    },
    {
      "id": "n5",
      "type": "block",
      "statements": [
        "isinstance(input_data.data, list)"
      ],
      "code": "isinstance(input_data.data, list)"
    },
    {
      "id": "n6",
      "type": "block",
      "statements": [
        "len(self.accumulated_data) Lt input_data.sample_size"
      ],
      "code": "len(self.accumulated_data) Lt input_data.sample_size"
    },
    {
      "id": "n7",
      "type": "block",
      "statements": [
        "self.accumulated_data.extend(input_data.data)"
      ],
      "code": "self.accumulated_data.extend(input_data.data)"
    },
    {
      "id": "n8",
      "type": "block",
      "statements": [
        "raise ValueError(f'Unsupported data type: {type(input_data.data)}')"
      ],
      "code": "raise ValueError(f'Unsupported data type: {type(input_data.data)}')"
    },
    {
      "id": "n9",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n10",
      "type": "block",
      "statements": [
        "return"
      ],
      "code": "return"
    },
    {
      "id": "n11",
      "type": "block",
      "statements": [],
      "code": "\ndata_to_sample = self.accumulated_data"
    },
    {
      "id": "n12",
      "type": "block",
      "statements": [
        "random.seed(input_data.random_seed)"
      ],
      "code": "random.seed(input_data.random_seed)"
    },
    {
      "id": "n13",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n14",
      "type": "block",
      "statements": [
        "data_size = len(data_to_sample)",
        "input_data.sample_size Gt data_size"
      ],
      "code": "data_size = len(data_to_sample)\ninput_data.sample_size Gt data_size"
    },
    {
      "id": "n15",
      "type": "block",
      "statements": [
        "raise ValueError(f'Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).')"
      ],
      "code": "raise ValueError(f'Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).')"
    },
    {
      "id": "n16",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n17",
      "type": "block",
      "statements": [
        "indices = []",
        "input_data.sampling_method Eq SamplingMethod.RANDOM"
      ],
      "code": "indices = []\ninput_data.sampling_method Eq SamplingMethod.RANDOM"
    },
    {
      "id": "n18",
      "type": "block",
      "statements": [
        "indices = random.sample(range(data_size), input_data.sample_size)"
      ],
      "code": "indices = random.sample(range(data_size), input_data.sample_size)"
    },
    {
      "id": "n19",
      "type": "block",
      "statements": [
        "input_data.sampling_method Eq SamplingMethod.SYSTEMATIC"
      ],
      "code": "input_data.sampling_method Eq SamplingMethod.SYSTEMATIC"
    },
    {
      "id": "n20",
      "type": "block",
      "statements": [
        "sampled_data = [data_to_sample[i] for i in indices]",
        "input_data.accumulate"
      ],
      "code": "sampled_data = [data_to_sample[i] for i in indices]\ninput_data.accumulate"
    },
    {
      "id": "n21",
      "type": "block",
      "statements": [
        "step = data_size // input_data.sample_size",
        "start = random.randint(0, step - 1)",
        "indices = list(range(start, data_size, step))[:input_data.sample_size]"
      ],
      "code": "step = data_size // input_data.sample_size\nstart = random.randint(0, step - 1)\nindices = list(range(start, data_size, step))[:input_data.sample_size]"
    },
    {
      "id": "n22",
      "type": "block",
      "statements": [
        "input_data.sampling_method Eq SamplingMethod.TOP"
      ],
      "code": "input_data.sampling_method Eq SamplingMethod.TOP"
    },
    {
      "id": "n23",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n24",
      "type": "block",
      "statements": [
        "indices = list(range(input_data.sample_size))"
      ],
      "code": "indices = list(range(input_data.sample_size))"
    },
    {
      "id": "n25",
      "type": "block",
      "statements": [
        "input_data.sampling_method Eq SamplingMethod.BOTTOM"
      ],
      "code": "input_data.sampling_method Eq SamplingMethod.BOTTOM"
    },
    {
      "id": "n26",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n27",
      "type": "block",
      "statements": [
        "indices = list(range(data_size - input_data.sample_size, data_size))"
      ],
      "code": "indices = list(range(data_size - input_data.sample_size, data_size))"
    },
    {
      "id": "n28",
      "type": "block",
      "statements": [
        "input_data.sampling_method Eq SamplingMethod.STRATIFIED"
      ],
      "code": "input_data.sampling_method Eq SamplingMethod.STRATIFIED"
    },
    {
      "id": "n29",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n30",
      "type": "block",
      "statements": [
        "not input_data.stratify_key"
      ],
      "code": "not input_data.stratify_key"
    },
    {
      "id": "n31",
      "type": "block",
      "statements": [
        "input_data.sampling_method Eq SamplingMethod.WEIGHTED"
      ],
      "code": "input_data.sampling_method Eq SamplingMethod.WEIGHTED"
    },
    {
      "id": "n32",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n33",
      "type": "block",
      "statements": [
        "raise ValueError('Stratify key must be provided for stratified sampling.')"
      ],
      "code": "raise ValueError('Stratify key must be provided for stratified sampling.')"
    },
    {
      "id": "n34",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n35",
      "type": "block",
      "statements": [
        "strata = defaultdict(list)"
      ],
      "code": "strata = defaultdict(list)"
    },
    {
      "id": "n36",
      "type": "block",
      "statements": [
        "(i, item)",
        "enumerate(data_to_sample)"
      ],
      "code": "(i, item)\nenumerate(data_to_sample)"
    },
    {
      "id": "n37",
      "type": "block",
      "statements": [
        "isinstance(item, dict)"
      ],
      "code": "isinstance(item, dict)"
    },
    {
      "id": "n38",
      "type": "block",
      "statements": [
        "stratum_sizes = {k: max(1, int(len(v) / data_size * input_data.sample_size)) for (k, v) in strata.items()}"
      ],
      "code": "stratum_sizes = {k: max(1, int(len(v) / data_size * input_data.sample_size)) for (k, v) in strata.items()}"
    },
    {
      "id": "n39",
      "type": "block",
      "statements": [
        "strata_value = item.get(input_data.stratify_key)"
      ],
      "code": "strata_value = item.get(input_data.stratify_key)"
    },
    {
      "id": "n40",
      "type": "block",
      "statements": [
        "hasattr(item, input_data.stratify_key)"
      ],
      "code": "hasattr(item, input_data.stratify_key)"
    },
    {
      "id": "n41",
      "type": "block",
      "statements": [
        "strata_value Is None"
      ],
      "code": "strata_value Is None"
    },
    {
      "id": "n42",
      "type": "block",
      "statements": [
        "strata_value = getattr(item, input_data.stratify_key)"
      ],
      "code": "strata_value = getattr(item, input_data.stratify_key)"
    },
    {
      "id": "n43",
      "type": "block",
      "statements": [
        "raise ValueError(f\"Stratify key '{input_data.stratify_key}' not found in item {item}\")"
      ],
      "code": "raise ValueError(f\"Stratify key '{input_data.stratify_key}' not found in item {item}\")"
    },
    {
      "id": "n44",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n45",
      "type": "block",
      "statements": [
        "raise ValueError(f\"Stratify value for key '{input_data.stratify_key}' is None\")"
      ],
      "code": "raise ValueError(f\"Stratify value for key '{input_data.stratify_key}' is None\")"
    },
    {
      "id": "n46",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n47",
      "type": "block",
      "statements": [
        "strata[str(strata_value)].append(i)"
      ],
      "code": "strata[str(strata_value)].append(i)"
    },
    {
      "id": "n48",
      "type": "block",
      "statements": [
        "sum(stratum_sizes.values()) NotEq input_data.sample_size"
      ],
      "code": "sum(stratum_sizes.values()) NotEq input_data.sample_size"
    },
    {
      "id": "n49",
      "type": "block",
      "statements": [
        "sum(stratum_sizes.values()) Lt input_data.sample_size"
      ],
      "code": "sum(stratum_sizes.values()) Lt input_data.sample_size"
    },
    {
      "id": "n50",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n51",
      "type": "block",
      "statements": [
        "stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] += 1"
      ],
      "code": "stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] += 1"
    },
    {
      "id": "n52",
      "type": "block",
      "statements": [
        "stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] -= 1"
      ],
      "code": "stratum_sizes[max(stratum_sizes, key=lambda k: stratum_sizes[k])] -= 1"
    },
    {
      "id": "n53",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n54",
      "type": "block",
      "statements": [
        "(stratum, size)",
        "stratum_sizes.items()"
      ],
      "code": "(stratum, size)\nstratum_sizes.items()"
    },
    {
      "id": "n55",
      "type": "block",
      "statements": [
        "indices.extend(random.sample(strata[stratum], size))"
      ],
      "code": "indices.extend(random.sample(strata[stratum], size))"
    },
    {
      "id": "n56",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n57",
      "type": "block",
      "statements": [
        "not input_data.weight_key"
      ],
      "code": "not input_data.weight_key"
    },
    {
      "id": "n58",
      "type": "block",
      "statements": [
        "input_data.sampling_method Eq SamplingMethod.RESERVOIR"
      ],
      "code": "input_data.sampling_method Eq SamplingMethod.RESERVOIR"
    },
    {
      "id": "n59",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n60",
      "type": "block",
      "statements": [
        "raise ValueError('Weight key must be provided for weighted sampling.')"
      ],
      "code": "raise ValueError('Weight key must be provided for weighted sampling.')"
    },
    {
      "id": "n61",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n62",
      "type": "block",
      "statements": [
        "weights = []"
      ],
      "code": "weights = []"
    },
    {
      "id": "n63",
      "type": "block",
      "statements": [
        "item",
        "data_to_sample"
      ],
      "code": "item\ndata_to_sample"
    },
    {
      "id": "n64",
      "type": "block",
      "statements": [
        "isinstance(item, dict)"
      ],
      "code": "isinstance(item, dict)"
    },
    {
      "id": "n65",
      "type": "block",
      "statements": [
        "not weights"
      ],
      "code": "not weights"
    },
    {
      "id": "n66",
      "type": "block",
      "statements": [
        "weight = item.get(input_data.weight_key)"
      ],
      "code": "weight = item.get(input_data.weight_key)"
    },
    {
      "id": "n67",
      "type": "block",
      "statements": [
        "hasattr(item, input_data.weight_key)"
      ],
      "code": "hasattr(item, input_data.weight_key)"
    },
    {
      "id": "n68",
      "type": "block",
      "statements": [
        "weight Is None"
      ],
      "code": "weight Is None"
    },
    {
      "id": "n69",
      "type": "block",
      "statements": [
        "weight = getattr(item, input_data.weight_key)"
      ],
      "code": "weight = getattr(item, input_data.weight_key)"
    },
    {
      "id": "n70",
      "type": "block",
      "statements": [
        "raise ValueError(f\"Weight key '{input_data.weight_key}' not found in item {item}\")"
      ],
      "code": "raise ValueError(f\"Weight key '{input_data.weight_key}' not found in item {item}\")"
    },
    {
      "id": "n71",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n72",
      "type": "block",
      "statements": [
        "raise ValueError(f\"Weight value for key '{input_data.weight_key}' is None\")"
      ],
      "code": "raise ValueError(f\"Weight value for key '{input_data.weight_key}' is None\")"
    },
    {
      "id": "n73",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n74",
      "type": "block",
      "statements": [
        "try:\n    weights.append(float(weight))\nexcept ValueError:\n    raise ValueError(f\"Weight value '{weight}' cannot be converted to a number\")",
        "weights.append(float(weight))",
        "raise ValueError(f\"Weight value '{weight}' cannot be converted to a number\")"
      ],
      "code": "try:\n    weights.append(float(weight))\nexcept ValueError:\n    raise ValueError(f\"Weight value '{weight}' cannot be converted to a number\")\nweights.append(float(weight))\nraise ValueError(f\"Weight value '{weight}' cannot be converted to a number\")"
    },
    {
      "id": "n75",
      "type": "block",
      "statements": [
        "raise ValueError(f\"No valid weights found using key '{input_data.weight_key}'\")"
      ],
      "code": "raise ValueError(f\"No valid weights found using key '{input_data.weight_key}'\")"
    },
    {
      "id": "n76",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n77",
      "type": "block",
      "statements": [
        "indices = random.choices(range(data_size), weights=weights, k=input_data.sample_size)"
      ],
      "code": "indices = random.choices(range(data_size), weights=weights, k=input_data.sample_size)"
    },
    {
      "id": "n78",
      "type": "block",
      "statements": [
        "indices = list(range(input_data.sample_size))"
      ],
      "code": "indices = list(range(input_data.sample_size))"
    },
    {
      "id": "n79",
      "type": "block",
      "statements": [
        "input_data.sampling_method Eq SamplingMethod.CLUSTER"
      ],
      "code": "input_data.sampling_method Eq SamplingMethod.CLUSTER"
    },
    {
      "id": "n80",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n81",
      "type": "block",
      "statements": [
        "i",
        "range(input_data.sample_size, data_size)"
      ],
      "code": "i\nrange(input_data.sample_size, data_size)"
    },
    {
      "id": "n82",
      "type": "block",
      "statements": [
        "j = random.randint(0, i)",
        "j Lt input_data.sample_size"
      ],
      "code": "j = random.randint(0, i)\nj Lt input_data.sample_size"
    },
    {
      "id": "n83",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n84",
      "type": "block",
      "statements": [
        "indices[j] = i"
      ],
      "code": "indices[j] = i"
    },
    {
      "id": "n85",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n86",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n87",
      "type": "block",
      "statements": [
        "not input_data.cluster_key"
      ],
      "code": "not input_data.cluster_key"
    },
    {
      "id": "n88",
      "type": "block",
      "statements": [
        "raise ValueError(f'Unknown sampling method: {input_data.sampling_method}')"
      ],
      "code": "raise ValueError(f'Unknown sampling method: {input_data.sampling_method}')"
    },
    {
      "id": "n89",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n90",
      "type": "block",
      "statements": [
        "raise ValueError('Cluster key must be provided for cluster sampling.')"
      ],
      "code": "raise ValueError('Cluster key must be provided for cluster sampling.')"
    },
    {
      "id": "n91",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n92",
      "type": "block",
      "statements": [
        "clusters = defaultdict(list)"
      ],
      "code": "clusters = defaultdict(list)"
    },
    {
      "id": "n93",
      "type": "block",
      "statements": [
        "(i, item)",
        "enumerate(data_to_sample)"
      ],
      "code": "(i, item)\nenumerate(data_to_sample)"
    },
    {
      "id": "n94",
      "type": "block",
      "statements": [
        "isinstance(item, dict)"
      ],
      "code": "isinstance(item, dict)"
    },
    {
      "id": "n95",
      "type": "block",
      "statements": [
        "selected_clusters = []"
      ],
      "code": "selected_clusters = []"
    },
    {
      "id": "n96",
      "type": "block",
      "statements": [
        "cluster_value = item.get(input_data.cluster_key)"
      ],
      "code": "cluster_value = item.get(input_data.cluster_key)"
    },
    {
      "id": "n97",
      "type": "block",
      "statements": [
        "hasattr(item, input_data.cluster_key)"
      ],
      "code": "hasattr(item, input_data.cluster_key)"
    },
    {
      "id": "n98",
      "type": "block",
      "statements": [
        "clusters[str(cluster_value)].append(i)"
      ],
      "code": "clusters[str(cluster_value)].append(i)"
    },
    {
      "id": "n99",
      "type": "block",
      "statements": [
        "cluster_value = getattr(item, input_data.cluster_key)"
      ],
      "code": "cluster_value = getattr(item, input_data.cluster_key)"
    },
    {
      "id": "n100",
      "type": "block",
      "statements": [
        "raise TypeError(f\"Item {item} does not have the cluster key '{input_data.cluster_key}'\")"
      ],
      "code": "raise TypeError(f\"Item {item} does not have the cluster key '{input_data.cluster_key}'\")"
    },
    {
      "id": "n101",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n102",
      "type": "block",
      "statements": [
        "sum((len(clusters[c]) for c in selected_clusters)) Lt input_data.sample_size"
      ],
      "code": "sum((len(clusters[c]) for c in selected_clusters)) Lt input_data.sample_size"
    },
    {
      "id": "n103",
      "type": "block",
      "statements": [
        "available_clusters = [c for c in clusters if c not in selected_clusters]",
        "not available_clusters"
      ],
      "code": "available_clusters = [c for c in clusters if c not in selected_clusters]\nnot available_clusters"
    },
    {
      "id": "n104",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n105",
      "type": "block",
      "statements": [
        "break"
      ],
      "code": "break"
    },
    {
      "id": "n106",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n107",
      "type": "block",
      "statements": [
        "selected_clusters.append(random.choice(available_clusters))"
      ],
      "code": "selected_clusters.append(random.choice(available_clusters))"
    },
    {
      "id": "n108",
      "type": "block",
      "statements": [
        "cluster",
        "selected_clusters"
      ],
      "code": "cluster\nselected_clusters"
    },
    {
      "id": "n109",
      "type": "block",
      "statements": [
        "indices.extend(clusters[cluster])"
      ],
      "code": "indices.extend(clusters[cluster])"
    },
    {
      "id": "n110",
      "type": "block",
      "statements": [
        "len(indices) Gt input_data.sample_size"
      ],
      "code": "len(indices) Gt input_data.sample_size"
    },
    {
      "id": "n111",
      "type": "block",
      "statements": [
        "indices = random.sample(indices, input_data.sample_size)"
      ],
      "code": "indices = random.sample(indices, input_data.sample_size)"
    },
    {
      "id": "n112",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n113",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n114",
      "type": "block",
      "statements": [
        "self.accumulated_data = []"
      ],
      "code": "self.accumulated_data = []"
    },
    {
      "id": "n115",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n116",
      "type": "block",
      "statements": [
        "(yield ('sampled_data', sampled_data))",
        "(yield ('sample_indices', indices))"
      ],
      "code": "(yield ('sampled_data', sampled_data))\n(yield ('sample_indices', indices))"
    }
  ],
  "edges": [
    {
      "source": "n103",
      "target": "n105"
    },
    {
      "source": "n70",
      "target": "n71"
    },
    {
      "source": "n95",
      "target": "n102"
    },
    {
      "source": "n82",
      "target": "n84"
    },
    {
      "source": "n14",
      "target": "n15"
    },
    {
      "source": "n0",
      "target": "n1"
    },
    {
      "source": "n67",
      "target": "n69"
    },
    {
      "source": "n3",
      "target": "n13"
    },
    {
      "source": "n17",
      "target": "n19"
    },
    {
      "source": "n13",
      "target": "n14"
    },
    {
      "source": "n38",
      "target": "n48"
    },
    {
      "source": "n31",
      "target": "n58"
    },
    {
      "source": "n35",
      "target": "n36"
    },
    {
      "source": "n86",
      "target": "n81"
    },
    {
      "source": "n8",
      "target": "n9"
    },
    {
      "source": "n56",
      "target": "n32"
    },
    {
      "source": "n104",
      "target": "n108"
    },
    {
      "source": "n112",
      "target": "n113"
    },
    {
      "source": "n58",
      "target": "n79"
    },
    {
      "source": "n21",
      "target": "n23"
    },
    {
      "source": "n19",
      "target": "n22"
    },
    {
      "source": "n20",
      "target": "n115"
    },
    {
      "source": "n85",
      "target": "n86"
    },
    {
      "source": "n115",
      "target": "n116"
    },
    {
      "source": "n22",
      "target": "n25"
    },
    {
      "source": "n88",
      "target": "n89"
    },
    {
      "source": "n20",
      "target": "n114"
    },
    {
      "source": "n81",
      "target": "n82"
    },
    {
      "source": "n96",
      "target": "n98"
    },
    {
      "source": "n5",
      "target": "n8"
    },
    {
      "source": "n18",
      "target": "n20"
    },
    {
      "source": "n75",
      "target": "n77"
    },
    {
      "source": "n93",
      "target": "n94"
    },
    {
      "source": "n91",
      "target": "n92"
    },
    {
      "source": "n36",
      "target": "n37"
    },
    {
      "source": "n61",
      "target": "n62"
    },
    {
      "source": "n22",
      "target": "n24"
    },
    {
      "source": "n82",
      "target": "n85"
    },
    {
      "source": "n97",
      "target": "n99"
    },
    {
      "source": "n15",
      "target": "n17"
    },
    {
      "source": "n68",
      "target": "n73"
    },
    {
      "source": "n63",
      "target": "n64"
    },
    {
      "source": "n89",
      "target": "n80"
    },
    {
      "source": "n17",
      "target": "n18"
    },
    {
      "source": "n74",
      "target": "n63"
    },
    {
      "source": "n65",
      "target": "n75"
    },
    {
      "source": "n34",
      "target": "n35"
    },
    {
      "source": "n68",
      "target": "n72"
    },
    {
      "source": "n2",
      "target": "n3"
    },
    {
      "source": "n19",
      "target": "n21"
    },
    {
      "source": "n69",
      "target": "n71"
    },
    {
      "source": "n108",
      "target": "n109"
    },
    {
      "source": "n41",
      "target": "n45"
    },
    {
      "source": "n1",
      "target": "n5"
    },
    {
      "source": "n101",
      "target": "n98"
    },
    {
      "source": "n81",
      "target": "n83"
    },
    {
      "source": "n25",
      "target": "n28"
    },
    {
      "source": "n102",
      "target": "n103"
    },
    {
      "source": "n87",
      "target": "n91"
    },
    {
      "source": "n37",
      "target": "n39"
    },
    {
      "source": "n48",
      "target": "n50"
    },
    {
      "source": "n28",
      "target": "n30"
    },
    {
      "source": "n6",
      "target": "n10"
    },
    {
      "source": "n114",
      "target": "n116"
    },
    {
      "source": "n58",
      "target": "n78"
    },
    {
      "source": "n7",
      "target": "n9"
    },
    {
      "source": "n53",
      "target": "n48"
    },
    {
      "source": "n57",
      "target": "n60"
    },
    {
      "source": "n42",
      "target": "n44"
    },
    {
      "source": "n47",
      "target": "n36"
    },
    {
      "source": "n80",
      "target": "n59"
    },
    {
      "source": "n94",
      "target": "n96"
    },
    {
      "source": "n60",
      "target": "n62"
    },
    {
      "source": "n73",
      "target": "n74"
    },
    {
      "source": "n1",
      "target": "n4"
    },
    {
      "source": "n98",
      "target": "n93"
    },
    {
      "source": "n66",
      "target": "n68"
    },
    {
      "source": "n29",
      "target": "n26"
    },
    {
      "source": "n28",
      "target": "n31"
    },
    {
      "source": "n11",
      "target": "n3"
    },
    {
      "source": "n93",
      "target": "n95"
    },
    {
      "source": "n32",
      "target": "n29"
    },
    {
      "source": "n79",
      "target": "n87"
    },
    {
      "source": "n90",
      "target": "n92"
    },
    {
      "source": "n77",
      "target": "n59"
    },
    {
      "source": "n23",
      "target": "n20"
    },
    {
      "source": "n72",
      "target": "n74"
    },
    {
      "source": "n30",
      "target": "n34"
    },
    {
      "source": "n102",
      "target": "n104"
    },
    {
      "source": "n50",
      "target": "n54"
    },
    {
      "source": "n71",
      "target": "n68"
    },
    {
      "source": "n44",
      "target": "n41"
    },
    {
      "source": "n113",
      "target": "n89"
    },
    {
      "source": "n37",
      "target": "n40"
    },
    {
      "source": "n4",
      "target": "n6"
    },
    {
      "source": "n109",
      "target": "n108"
    },
    {
      "source": "n51",
      "target": "n53"
    },
    {
      "source": "n14",
      "target": "n16"
    },
    {
      "source": "n62",
      "target": "n63"
    },
    {
      "source": "n54",
      "target": "n56"
    },
    {
      "source": "n41",
      "target": "n46"
    },
    {
      "source": "n64",
      "target": "n66"
    },
    {
      "source": "n27",
      "target": "n29"
    },
    {
      "source": "n40",
      "target": "n42"
    },
    {
      "source": "n30",
      "target": "n33"
    },
    {
      "source": "n24",
      "target": "n26"
    },
    {
      "source": "n110",
      "target": "n111"
    },
    {
      "source": "n108",
      "target": "n110"
    },
    {
      "source": "n49",
      "target": "n52"
    },
    {
      "source": "n9",
      "target": "n6"
    },
    {
      "source": "n3",
      "target": "n12"
    },
    {
      "source": "n78",
      "target": "n81"
    },
    {
      "source": "n63",
      "target": "n65"
    },
    {
      "source": "n39",
      "target": "n41"
    },
    {
      "source": "n0",
      "target": "n2"
    },
    {
      "source": "n48",
      "target": "n49"
    },
    {
      "source": "n54",
      "target": "n55"
    },
    {
      "source": "n111",
      "target": "n113"
    },
    {
      "source": "n52",
      "target": "n53"
    },
    {
      "source": "n55",
      "target": "n54"
    },
    {
      "source": "n105",
      "target": "n107"
    },
    {
      "source": "n67",
      "target": "n70"
    },
    {
      "source": "n103",
      "target": "n106"
    },
    {
      "source": "n65",
      "target": "n76"
    },
    {
      "source": "n26",
      "target": "n23"
    },
    {
      "source": "n83",
      "target": "n80"
    },
    {
      "source": "n31",
      "target": "n57"
    },
    {
      "source": "n57",
      "target": "n61"
    },
    {
      "source": "n16",
      "target": "n17"
    },
    {
      "source": "n107",
      "target": "n102"
    },
    {
      "source": "n33",
      "target": "n35"
    },
    {
      "source": "n97",
      "target": "n100"
    },
    {
      "source": "n43",
      "target": "n44"
    },
    {
      "source": "n59",
      "target": "n32"
    },
    {
      "source": "n40",
      "target": "n43"
    },
    {
      "source": "n84",
      "target": "n86"
    },
    {
      "source": "n100",
      "target": "n101"
    },
    {
      "source": "n64",
      "target": "n67"
    },
    {
      "source": "n12",
      "target": "n14"
    },
    {
      "source": "n6",
      "target": "n11"
    },
    {
      "source": "n46",
      "target": "n47"
    },
    {
      "source": "n36",
      "target": "n38"
    },
    {
      "source": "n49",
      "target": "n51"
    },
    {
      "source": "n106",
      "target": "n107"
    },
    {
      "source": "n45",
      "target": "n47"
    },
    {
      "source": "n79",
      "target": "n88"
    },
    {
      "source": "n110",
      "target": "n112"
    },
    {
      "source": "n87",
      "target": "n90"
    },
    {
      "source": "n76",
      "target": "n77"
    },
    {
      "source": "n99",
      "target": "n101"
    },
    {
      "source": "n25",
      "target": "n27"
    },
    {
      "source": "n94",
      "target": "n97"
    },
    {
      "source": "n5",
      "target": "n7"
    },
    {
      "source": "n92",
      "target": "n93"
    }
  ]
}