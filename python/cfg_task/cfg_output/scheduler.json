{
  "nodes": [
    {
      "id": "1",
      "type": "block",
      "statements": [
        "import logging",
        "import os",
        "from urllib.parse import parse_qs, urlencode, urlparse, urlunparse",
        "from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED",
        "from apscheduler.job import Job as JobObj",
        "from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore",
        "from apscheduler.schedulers.blocking import BlockingScheduler",
        "from apscheduler.triggers.cron import CronTrigger",
        "from autogpt_libs.utils.cache import thread_cached",
        "from dotenv import load_dotenv",
        "from pydantic import BaseModel",
        "from sqlalchemy import MetaData, create_engine",
        "from backend.data.block import BlockInput",
        "from backend.executor.manager import ExecutionManager",
        "from backend.util.service import AppService, expose, get_service_client",
        "from backend.util.settings import Config",
        "def _extract_schema_from_url(database_url) -> tuple[str, str]:\n    \"\"\"\n    Extracts the schema from the DATABASE_URL and returns the schema and cleaned URL.\n    \"\"\"\n    parsed_url = urlparse(database_url)\n    query_params = parse_qs(parsed_url.query)\n    schema_list = query_params.pop('schema', None)\n    schema = schema_list[0] if schema_list else 'public'\n    new_query = urlencode(query_params, doseq=True)\n    new_parsed_url = parsed_url._replace(query=new_query)\n    database_url_clean = str(urlunparse(new_parsed_url))\n    return (schema, database_url_clean)",
        "'\\n    Extracts the schema from the DATABASE_URL and returns the schema and cleaned URL.\\n    '",
        "parsed_url = urlparse(database_url)",
        "query_params = parse_qs(parsed_url.query)",
        "schema_list = query_params.pop('schema', None)",
        "schema = schema_list[0] if schema_list else 'public'",
        "new_query = urlencode(query_params, doseq=True)",
        "new_parsed_url = parsed_url._replace(query=new_query)",
        "database_url_clean = str(urlunparse(new_parsed_url))",
        "return (schema, database_url_clean)"
      ]
    },
    {
      "id": "2",
      "type": "block",
      "statements": [
        "logger = logging.getLogger(__name__)",
        "config = Config()",
        "def log(msg, **kwargs):\n    logger.info('[ExecutionScheduler] ' + msg, **kwargs)",
        "logger.info('[ExecutionScheduler] ' Add msg)",
        "def job_listener(event):\n    \"\"\"Logs job execution outcomes for better monitoring.\"\"\"\n    if event.exception:\n        log(f'Job {event.job_id} failed.')\n    else:\n        log(f'Job {event.job_id} completed successfully.')",
        "'Logs job execution outcomes for better monitoring.'",
        "event.exception"
      ]
    },
    {
      "id": "3",
      "type": "block",
      "statements": [
        "log(f'Job {event.job_id} failed.')"
      ]
    },
    {
      "id": "4",
      "type": "block",
      "statements": [
        "log(f'Job {event.job_id} completed successfully.')"
      ]
    },
    {
      "id": "5",
      "type": "block",
      "statements": [
        "@thread_cached\ndef get_execution_client() -> ExecutionManager:\n    return get_service_client(ExecutionManager)",
        "return get_service_client(ExecutionManager)"
      ]
    },
    {
      "id": "6",
      "type": "block",
      "statements": [
        "def execute_graph(**kwargs):\n    args = JobArgs(**kwargs)\n    try:\n        log(f'Executing recurring job for graph #{args.graph_id}')\n        get_execution_client().add_execution(args.graph_id, args.input_data, args.user_id)\n    except Exception as e:\n        logger.exception(f'Error executing graph {args.graph_id}: {e}')",
        "args = JobArgs(**kwargs)",
        "try:\n    log(f'Executing recurring job for graph #{args.graph_id}')\n    get_execution_client().add_execution(args.graph_id, args.input_data, args.user_id)\nexcept Exception as e:\n    logger.exception(f'Error executing graph {args.graph_id}: {e}')",
        "log(f'Executing recurring job for graph #{args.graph_id}')",
        "get_execution_client().add_execution(args.graph_id, args.input_data, args.user_id)",
        "logger.exception(f'Error executing graph {args.graph_id}: {e}')",
        "class JobArgs(BaseModel):\n    graph_id: str\n    input_data: BlockInput\n    user_id: str\n    graph_version: int\n    cron: str",
        "graph_id: str",
        "input_data: BlockInput",
        "user_id: str",
        "graph_version: int",
        "cron: str",
        "class JobInfo(JobArgs):\n    id: str\n    name: str\n    next_run_time: str\n\n    @staticmethod\n    def from_db(job_args: JobArgs, job_obj: JobObj) -> 'JobInfo':\n        return JobInfo(id=job_obj.id, name=job_obj.name, next_run_time=job_obj.next_run_time.isoformat(), **job_args.model_dump())",
        "id: str",
        "name: str",
        "next_run_time: str",
        "@staticmethod\ndef from_db(job_args: JobArgs, job_obj: JobObj) -> 'JobInfo':\n    return JobInfo(id=job_obj.id, name=job_obj.name, next_run_time=job_obj.next_run_time.isoformat(), **job_args.model_dump())",
        "return JobInfo(id=job_obj.id, name=job_obj.name, next_run_time=job_obj.next_run_time.isoformat(), **job_args.model_dump())"
      ]
    },
    {
      "id": "7",
      "type": "block",
      "statements": [
        "class ExecutionScheduler(AppService):\n    scheduler: BlockingScheduler\n\n    @classmethod\n    def get_port(cls) -> int:\n        return config.execution_scheduler_port\n\n    @property\n    @thread_cached\n    def execution_client(self) -> ExecutionManager:\n        return get_service_client(ExecutionManager)\n\n    def run_service(self):\n        load_dotenv()\n        (db_schema, db_url) = _extract_schema_from_url(os.getenv('DATABASE_URL'))\n        self.scheduler = BlockingScheduler(jobstores={'default': SQLAlchemyJobStore(engine=create_engine(db_url), metadata=MetaData(schema=db_schema))})\n        self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)\n        self.scheduler.start()\n\n    @expose\n    def add_execution_schedule(self, graph_id: str, graph_version: int, cron: str, input_data: BlockInput, user_id: str) -> JobInfo:\n        job_args = JobArgs(graph_id=graph_id, input_data=input_data, user_id=user_id, graph_version=graph_version, cron=cron)\n        job = self.scheduler.add_job(execute_graph, CronTrigger.from_crontab(cron), kwargs=job_args.model_dump(), replace_existing=True)\n        log(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")\n        return JobInfo.from_db(job_args, job)\n\n    @expose\n    def delete_schedule(self, schedule_id: str, user_id: str) -> JobInfo:\n        job = self.scheduler.get_job(schedule_id)\n        if not job:\n            log(f'Job {schedule_id} not found.')\n            raise ValueError(f'Job #{schedule_id} not found.')\n        job_args = JobArgs(**job.kwargs)\n        if job_args.user_id != user_id:\n            raise ValueError(\"User ID does not match the job's user ID.\")\n        log(f'Deleting job {schedule_id}')\n        job.remove()\n        return JobInfo.from_db(job_args, job)\n\n    @expose\n    def get_execution_schedules(self, graph_id: str | None=None, user_id: str | None=None) -> list[JobInfo]:\n        schedules = []\n        for job in self.scheduler.get_jobs():\n            job_args = JobArgs(**job.kwargs)\n            if job.next_run_time is not None and (graph_id is None or job_args.graph_id == graph_id) and (user_id is None or job_args.user_id == user_id):\n                schedules.append(JobInfo.from_db(job_args, job))\n        return schedules",
        "scheduler: BlockingScheduler",
        "@classmethod\ndef get_port(cls) -> int:\n    return config.execution_scheduler_port",
        "return config.execution_scheduler_port"
      ]
    },
    {
      "id": "8",
      "type": "block",
      "statements": [
        "@property\n@thread_cached\ndef execution_client(self) -> ExecutionManager:\n    return get_service_client(ExecutionManager)",
        "return get_service_client(ExecutionManager)"
      ]
    },
    {
      "id": "9",
      "type": "block",
      "statements": [
        "def run_service(self):\n    load_dotenv()\n    (db_schema, db_url) = _extract_schema_from_url(os.getenv('DATABASE_URL'))\n    self.scheduler = BlockingScheduler(jobstores={'default': SQLAlchemyJobStore(engine=create_engine(db_url), metadata=MetaData(schema=db_schema))})\n    self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)\n    self.scheduler.start()",
        "load_dotenv()",
        "(db_schema, db_url) = _extract_schema_from_url(os.getenv('DATABASE_URL'))",
        "self.scheduler = BlockingScheduler(jobstores={'default': SQLAlchemyJobStore(engine=create_engine(db_url), metadata=MetaData(schema=db_schema))})",
        "self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED BitOr EVENT_JOB_ERROR)",
        "self.scheduler.start()",
        "@expose\ndef add_execution_schedule(self, graph_id: str, graph_version: int, cron: str, input_data: BlockInput, user_id: str) -> JobInfo:\n    job_args = JobArgs(graph_id=graph_id, input_data=input_data, user_id=user_id, graph_version=graph_version, cron=cron)\n    job = self.scheduler.add_job(execute_graph, CronTrigger.from_crontab(cron), kwargs=job_args.model_dump(), replace_existing=True)\n    log(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")\n    return JobInfo.from_db(job_args, job)",
        "job_args = JobArgs(graph_id=graph_id, input_data=input_data, user_id=user_id, graph_version=graph_version, cron=cron)",
        "job = self.scheduler.add_job(execute_graph, CronTrigger.from_crontab(cron), kwargs=job_args.model_dump(), replace_existing=True)",
        "log(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")",
        "return JobInfo.from_db(job_args, job)"
      ]
    },
    {
      "id": "10",
      "type": "block",
      "statements": [
        "@expose\ndef delete_schedule(self, schedule_id: str, user_id: str) -> JobInfo:\n    job = self.scheduler.get_job(schedule_id)\n    if not job:\n        log(f'Job {schedule_id} not found.')\n        raise ValueError(f'Job #{schedule_id} not found.')\n    job_args = JobArgs(**job.kwargs)\n    if job_args.user_id != user_id:\n        raise ValueError(\"User ID does not match the job's user ID.\")\n    log(f'Deleting job {schedule_id}')\n    job.remove()\n    return JobInfo.from_db(job_args, job)",
        "job = self.scheduler.get_job(schedule_id)",
        "not job"
      ]
    },
    {
      "id": "11",
      "type": "block",
      "statements": [
        "log(f'Job {schedule_id} not found.')",
        "raise ValueError(f'Job #{schedule_id} not found.')"
      ]
    },
    {
      "id": "12",
      "type": "block",
      "statements": []
    },
    {
      "id": "13",
      "type": "block",
      "statements": [
        "job_args = JobArgs(**job.kwargs)",
        "job_args.user_id NotEq user_id"
      ]
    },
    {
      "id": "14",
      "type": "block",
      "statements": [
        "raise ValueError(\"User ID does not match the job's user ID.\")"
      ]
    },
    {
      "id": "15",
      "type": "block",
      "statements": []
    },
    {
      "id": "16",
      "type": "block",
      "statements": [
        "log(f'Deleting job {schedule_id}')",
        "job.remove()",
        "return JobInfo.from_db(job_args, job)"
      ]
    },
    {
      "id": "17",
      "type": "block",
      "statements": [
        "@expose\ndef get_execution_schedules(self, graph_id: str | None=None, user_id: str | None=None) -> list[JobInfo]:\n    schedules = []\n    for job in self.scheduler.get_jobs():\n        job_args = JobArgs(**job.kwargs)\n        if job.next_run_time is not None and (graph_id is None or job_args.graph_id == graph_id) and (user_id is None or job_args.user_id == user_id):\n            schedules.append(JobInfo.from_db(job_args, job))\n    return schedules",
        "schedules = []"
      ]
    },
    {
      "id": "18",
      "type": "block",
      "statements": [
        "job",
        "self.scheduler.get_jobs()"
      ]
    },
    {
      "id": "19",
      "type": "block",
      "statements": [
        "job_args = JobArgs(**job.kwargs)",
        "job.next_run_time is not None and (graph_id is None or job_args.graph_id == graph_id) and (user_id is None or job_args.user_id == user_id)"
      ]
    },
    {
      "id": "20",
      "type": "block",
      "statements": [
        "return schedules"
      ]
    },
    {
      "id": "21",
      "type": "block",
      "statements": [
        "schedules.append(JobInfo.from_db(job_args, job))"
      ]
    },
    {
      "id": "22",
      "type": "block",
      "statements": []
    },
    {
      "id": "23",
      "type": "block",
      "statements": []
    }
  ],
  "edges": [
    {
      "source": "2",
      "target": "3",
      "type": "true"
    },
    {
      "source": "2",
      "target": "4",
      "type": "false"
    },
    {
      "source": "3",
      "target": "5",
      "type": "next"
    },
    {
      "source": "4",
      "target": "5",
      "type": "next"
    },
    {
      "source": "10",
      "target": "11",
      "type": "true"
    },
    {
      "source": "10",
      "target": "12",
      "type": "false"
    },
    {
      "source": "11",
      "target": "13",
      "type": "next"
    },
    {
      "source": "12",
      "target": "13",
      "type": "next"
    },
    {
      "source": "13",
      "target": "14",
      "type": "true"
    },
    {
      "source": "13",
      "target": "15",
      "type": "false"
    },
    {
      "source": "14",
      "target": "16",
      "type": "next"
    },
    {
      "source": "15",
      "target": "16",
      "type": "next"
    },
    {
      "source": "17",
      "target": "18",
      "type": "next"
    },
    {
      "source": "18",
      "target": "19",
      "type": "true"
    },
    {
      "source": "18",
      "target": "20",
      "type": "false"
    },
    {
      "source": "19",
      "target": "21",
      "type": "true"
    },
    {
      "source": "19",
      "target": "22",
      "type": "false"
    },
    {
      "source": "21",
      "target": "23",
      "type": "next"
    },
    {
      "source": "22",
      "target": "23",
      "type": "next"
    },
    {
      "source": "23",
      "target": "18",
      "type": "next"
    }
  ]
}