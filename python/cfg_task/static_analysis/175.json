{
  "nodes": [
    {
      "id": "n0",
      "type": "block",
      "statements": [
        "import logging",
        "import os",
        "from urllib.parse import parse_qs, urlencode, urlparse, urlunparse",
        "from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED",
        "from apscheduler.job import Job as JobObj",
        "from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore",
        "from apscheduler.schedulers.blocking import BlockingScheduler",
        "from apscheduler.triggers.cron import CronTrigger",
        "from autogpt_libs.utils.cache import thread_cached",
        "from dotenv import load_dotenv",
        "from pydantic import BaseModel",
        "from sqlalchemy import MetaData, create_engine",
        "from backend.data.block import BlockInput",
        "from backend.executor.manager import ExecutionManager",
        "from backend.util.service import AppService, expose, get_service_client",
        "from backend.util.settings import Config",
        "def _extract_schema_from_url(database_url) -> tuple[str, str]:\n    \"\"\"\n    Extracts the schema from the DATABASE_URL and returns the schema and cleaned URL.\n    \"\"\"\n    parsed_url = urlparse(database_url)\n    query_params = parse_qs(parsed_url.query)\n    schema_list = query_params.pop('schema', None)\n    schema = schema_list[0] if schema_list else 'public'\n    new_query = urlencode(query_params, doseq=True)\n    new_parsed_url = parsed_url._replace(query=new_query)\n    database_url_clean = str(urlunparse(new_parsed_url))\n    return (schema, database_url_clean)",
        "'\\n    Extracts the schema from the DATABASE_URL and returns the schema and cleaned URL.\\n    '",
        "parsed_url = urlparse(database_url)",
        "query_params = parse_qs(parsed_url.query)",
        "schema_list = query_params.pop('schema', None)",
        "schema = schema_list[0] if schema_list else 'public'",
        "new_query = urlencode(query_params, doseq=True)",
        "new_parsed_url = parsed_url._replace(query=new_query)",
        "database_url_clean = str(urlunparse(new_parsed_url))",
        "return (schema, database_url_clean)"
      ],
      "code": "import logging\nimport os\nfrom urllib.parse import parse_qs, urlencode, urlparse, urlunparse\nfrom apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED\nfrom apscheduler.job import Job as JobObj\nfrom apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.triggers.cron import CronTrigger\nfrom autogpt_libs.utils.cache import thread_cached\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom sqlalchemy import MetaData, create_engine\nfrom backend.data.block import BlockInput\nfrom backend.executor.manager import ExecutionManager\nfrom backend.util.service import AppService, expose, get_service_client\nfrom backend.util.settings import Config\ndef _extract_schema_from_url(database_url) -> tuple[str, str]:\n    \"\"\"\n    Extracts the schema from the DATABASE_URL and returns the schema and cleaned URL.\n    \"\"\"\n    parsed_url = urlparse(database_url)\n    query_params = parse_qs(parsed_url.query)\n    schema_list = query_params.pop('schema', None)\n    schema = schema_list[0] if schema_list else 'public'\n    new_query = urlencode(query_params, doseq=True)\n    new_parsed_url = parsed_url._replace(query=new_query)\n    database_url_clean = str(urlunparse(new_parsed_url))\n    return (schema, database_url_clean)\n'\\n    Extracts the schema from the DATABASE_URL and returns the schema and cleaned URL.\\n    '\nparsed_url = urlparse(database_url)\nquery_params = parse_qs(parsed_url.query)\nschema_list = query_params.pop('schema', None)\nschema = schema_list[0] if schema_list else 'public'\nnew_query = urlencode(query_params, doseq=True)\nnew_parsed_url = parsed_url._replace(query=new_query)\ndatabase_url_clean = str(urlunparse(new_parsed_url))\nreturn (schema, database_url_clean)"
    },
    {
      "id": "n1",
      "type": "block",
      "statements": [
        "logger = logging.getLogger(__name__)",
        "config = Config()",
        "def log(msg, **kwargs):\n    logger.info('[ExecutionScheduler] ' + msg, **kwargs)",
        "logger.info('[ExecutionScheduler] ' Add msg)",
        "def job_listener(event):\n    \"\"\"Logs job execution outcomes for better monitoring.\"\"\"\n    if event.exception:\n        log(f'Job {event.job_id} failed.')\n    else:\n        log(f'Job {event.job_id} completed successfully.')",
        "'Logs job execution outcomes for better monitoring.'",
        "event.exception"
      ],
      "code": "logger = logging.getLogger(__name__)\nconfig = Config()\ndef log(msg, **kwargs):\n    logger.info('[ExecutionScheduler] ' + msg, **kwargs)\nlogger.info('[ExecutionScheduler] ' Add msg)\ndef job_listener(event):\n    \"\"\"Logs job execution outcomes for better monitoring.\"\"\"\n    if event.exception:\n        log(f'Job {event.job_id} failed.')\n    else:\n        log(f'Job {event.job_id} completed successfully.')\n'Logs job execution outcomes for better monitoring.'\nevent.exception"
    },
    {
      "id": "n2",
      "type": "block",
      "statements": [
        "log(f'Job {event.job_id} failed.')"
      ],
      "code": "log(f'Job {event.job_id} failed.')"
    },
    {
      "id": "n3",
      "type": "block",
      "statements": [
        "log(f'Job {event.job_id} completed successfully.')"
      ],
      "code": "log(f'Job {event.job_id} completed successfully.')"
    },
    {
      "id": "n4",
      "type": "block",
      "statements": [
        "@thread_cached\ndef get_execution_client() -> ExecutionManager:\n    return get_service_client(ExecutionManager)",
        "return get_service_client(ExecutionManager)"
      ],
      "code": "@thread_cached\ndef get_execution_client() -> ExecutionManager:\n    return get_service_client(ExecutionManager)\nreturn get_service_client(ExecutionManager)"
    },
    {
      "id": "n5",
      "type": "block",
      "statements": [
        "def execute_graph(**kwargs):\n    args = JobArgs(**kwargs)\n    try:\n        log(f'Executing recurring job for graph #{args.graph_id}')\n        get_execution_client().add_execution(args.graph_id, args.input_data, args.user_id)\n    except Exception as e:\n        logger.exception(f'Error executing graph {args.graph_id}: {e}')",
        "args = JobArgs(**kwargs)",
        "try:\n    log(f'Executing recurring job for graph #{args.graph_id}')\n    get_execution_client().add_execution(args.graph_id, args.input_data, args.user_id)\nexcept Exception as e:\n    logger.exception(f'Error executing graph {args.graph_id}: {e}')",
        "log(f'Executing recurring job for graph #{args.graph_id}')",
        "get_execution_client().add_execution(args.graph_id, args.input_data, args.user_id)",
        "logger.exception(f'Error executing graph {args.graph_id}: {e}')",
        "class JobArgs(BaseModel):\n    graph_id: str\n    input_data: BlockInput\n    user_id: str\n    graph_version: int\n    cron: str",
        "graph_id: str",
        "input_data: BlockInput",
        "user_id: str",
        "graph_version: int",
        "cron: str",
        "class JobInfo(JobArgs):\n    id: str\n    name: str\n    next_run_time: str\n\n    @staticmethod\n    def from_db(job_args: JobArgs, job_obj: JobObj) -> 'JobInfo':\n        return JobInfo(id=job_obj.id, name=job_obj.name, next_run_time=job_obj.next_run_time.isoformat(), **job_args.model_dump())",
        "id: str",
        "name: str",
        "next_run_time: str",
        "@staticmethod\ndef from_db(job_args: JobArgs, job_obj: JobObj) -> 'JobInfo':\n    return JobInfo(id=job_obj.id, name=job_obj.name, next_run_time=job_obj.next_run_time.isoformat(), **job_args.model_dump())",
        "return JobInfo(id=job_obj.id, name=job_obj.name, next_run_time=job_obj.next_run_time.isoformat(), **job_args.model_dump())"
      ],
      "code": "def execute_graph(**kwargs):\n    args = JobArgs(**kwargs)\n    try:\n        log(f'Executing recurring job for graph #{args.graph_id}')\n        get_execution_client().add_execution(args.graph_id, args.input_data, args.user_id)\n    except Exception as e:\n        logger.exception(f'Error executing graph {args.graph_id}: {e}')\nargs = JobArgs(**kwargs)\ntry:\n    log(f'Executing recurring job for graph #{args.graph_id}')\n    get_execution_client().add_execution(args.graph_id, args.input_data, args.user_id)\nexcept Exception as e:\n    logger.exception(f'Error executing graph {args.graph_id}: {e}')\nlog(f'Executing recurring job for graph #{args.graph_id}')\nget_execution_client().add_execution(args.graph_id, args.input_data, args.user_id)\nlogger.exception(f'Error executing graph {args.graph_id}: {e}')\nclass JobArgs(BaseModel):\n    graph_id: str\n    input_data: BlockInput\n    user_id: str\n    graph_version: int\n    cron: str\ngraph_id: str\ninput_data: BlockInput\nuser_id: str\ngraph_version: int\ncron: str\nclass JobInfo(JobArgs):\n    id: str\n    name: str\n    next_run_time: str\n\n    @staticmethod\n    def from_db(job_args: JobArgs, job_obj: JobObj) -> 'JobInfo':\n        return JobInfo(id=job_obj.id, name=job_obj.name, next_run_time=job_obj.next_run_time.isoformat(), **job_args.model_dump())\nid: str\nname: str\nnext_run_time: str\n@staticmethod\ndef from_db(job_args: JobArgs, job_obj: JobObj) -> 'JobInfo':\n    return JobInfo(id=job_obj.id, name=job_obj.name, next_run_time=job_obj.next_run_time.isoformat(), **job_args.model_dump())\nreturn JobInfo(id=job_obj.id, name=job_obj.name, next_run_time=job_obj.next_run_time.isoformat(), **job_args.model_dump())"
    },
    {
      "id": "n6",
      "type": "block",
      "statements": [
        "class ExecutionScheduler(AppService):\n    scheduler: BlockingScheduler\n\n    @classmethod\n    def get_port(cls) -> int:\n        return config.execution_scheduler_port\n\n    @property\n    @thread_cached\n    def execution_client(self) -> ExecutionManager:\n        return get_service_client(ExecutionManager)\n\n    def run_service(self):\n        load_dotenv()\n        (db_schema, db_url) = _extract_schema_from_url(os.getenv('DATABASE_URL'))\n        self.scheduler = BlockingScheduler(jobstores={'default': SQLAlchemyJobStore(engine=create_engine(db_url), metadata=MetaData(schema=db_schema))})\n        self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)\n        self.scheduler.start()\n\n    @expose\n    def add_execution_schedule(self, graph_id: str, graph_version: int, cron: str, input_data: BlockInput, user_id: str) -> JobInfo:\n        job_args = JobArgs(graph_id=graph_id, input_data=input_data, user_id=user_id, graph_version=graph_version, cron=cron)\n        job = self.scheduler.add_job(execute_graph, CronTrigger.from_crontab(cron), kwargs=job_args.model_dump(), replace_existing=True)\n        log(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")\n        return JobInfo.from_db(job_args, job)\n\n    @expose\n    def delete_schedule(self, schedule_id: str, user_id: str) -> JobInfo:\n        job = self.scheduler.get_job(schedule_id)\n        if not job:\n            log(f'Job {schedule_id} not found.')\n            raise ValueError(f'Job #{schedule_id} not found.')\n        job_args = JobArgs(**job.kwargs)\n        if job_args.user_id != user_id:\n            raise ValueError(\"User ID does not match the job's user ID.\")\n        log(f'Deleting job {schedule_id}')\n        job.remove()\n        return JobInfo.from_db(job_args, job)\n\n    @expose\n    def get_execution_schedules(self, graph_id: str | None=None, user_id: str | None=None) -> list[JobInfo]:\n        schedules = []\n        for job in self.scheduler.get_jobs():\n            job_args = JobArgs(**job.kwargs)\n            if job.next_run_time is not None and (graph_id is None or job_args.graph_id == graph_id) and (user_id is None or job_args.user_id == user_id):\n                schedules.append(JobInfo.from_db(job_args, job))\n        return schedules",
        "scheduler: BlockingScheduler",
        "@classmethod\ndef get_port(cls) -> int:\n    return config.execution_scheduler_port",
        "return config.execution_scheduler_port"
      ],
      "code": "class ExecutionScheduler(AppService):\n    scheduler: BlockingScheduler\n\n    @classmethod\n    def get_port(cls) -> int:\n        return config.execution_scheduler_port\n\n    @property\n    @thread_cached\n    def execution_client(self) -> ExecutionManager:\n        return get_service_client(ExecutionManager)\n\n    def run_service(self):\n        load_dotenv()\n        (db_schema, db_url) = _extract_schema_from_url(os.getenv('DATABASE_URL'))\n        self.scheduler = BlockingScheduler(jobstores={'default': SQLAlchemyJobStore(engine=create_engine(db_url), metadata=MetaData(schema=db_schema))})\n        self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)\n        self.scheduler.start()\n\n    @expose\n    def add_execution_schedule(self, graph_id: str, graph_version: int, cron: str, input_data: BlockInput, user_id: str) -> JobInfo:\n        job_args = JobArgs(graph_id=graph_id, input_data=input_data, user_id=user_id, graph_version=graph_version, cron=cron)\n        job = self.scheduler.add_job(execute_graph, CronTrigger.from_crontab(cron), kwargs=job_args.model_dump(), replace_existing=True)\n        log(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")\n        return JobInfo.from_db(job_args, job)\n\n    @expose\n    def delete_schedule(self, schedule_id: str, user_id: str) -> JobInfo:\n        job = self.scheduler.get_job(schedule_id)\n        if not job:\n            log(f'Job {schedule_id} not found.')\n            raise ValueError(f'Job #{schedule_id} not found.')\n        job_args = JobArgs(**job.kwargs)\n        if job_args.user_id != user_id:\n            raise ValueError(\"User ID does not match the job's user ID.\")\n        log(f'Deleting job {schedule_id}')\n        job.remove()\n        return JobInfo.from_db(job_args, job)\n\n    @expose\n    def get_execution_schedules(self, graph_id: str | None=None, user_id: str | None=None) -> list[JobInfo]:\n        schedules = []\n        for job in self.scheduler.get_jobs():\n            job_args = JobArgs(**job.kwargs)\n            if job.next_run_time is not None and (graph_id is None or job_args.graph_id == graph_id) and (user_id is None or job_args.user_id == user_id):\n                schedules.append(JobInfo.from_db(job_args, job))\n        return schedules\nscheduler: BlockingScheduler\n@classmethod\ndef get_port(cls) -> int:\n    return config.execution_scheduler_port\nreturn config.execution_scheduler_port"
    },
    {
      "id": "n7",
      "type": "block",
      "statements": [
        "@property\n@thread_cached\ndef execution_client(self) -> ExecutionManager:\n    return get_service_client(ExecutionManager)",
        "return get_service_client(ExecutionManager)"
      ],
      "code": "@property\n@thread_cached\ndef execution_client(self) -> ExecutionManager:\n    return get_service_client(ExecutionManager)\nreturn get_service_client(ExecutionManager)"
    },
    {
      "id": "n8",
      "type": "block",
      "statements": [
        "def run_service(self):\n    load_dotenv()\n    (db_schema, db_url) = _extract_schema_from_url(os.getenv('DATABASE_URL'))\n    self.scheduler = BlockingScheduler(jobstores={'default': SQLAlchemyJobStore(engine=create_engine(db_url), metadata=MetaData(schema=db_schema))})\n    self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)\n    self.scheduler.start()",
        "load_dotenv()",
        "(db_schema, db_url) = _extract_schema_from_url(os.getenv('DATABASE_URL'))",
        "self.scheduler = BlockingScheduler(jobstores={'default': SQLAlchemyJobStore(engine=create_engine(db_url), metadata=MetaData(schema=db_schema))})",
        "self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED BitOr EVENT_JOB_ERROR)",
        "self.scheduler.start()",
        "@expose\ndef add_execution_schedule(self, graph_id: str, graph_version: int, cron: str, input_data: BlockInput, user_id: str) -> JobInfo:\n    job_args = JobArgs(graph_id=graph_id, input_data=input_data, user_id=user_id, graph_version=graph_version, cron=cron)\n    job = self.scheduler.add_job(execute_graph, CronTrigger.from_crontab(cron), kwargs=job_args.model_dump(), replace_existing=True)\n    log(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")\n    return JobInfo.from_db(job_args, job)",
        "job_args = JobArgs(graph_id=graph_id, input_data=input_data, user_id=user_id, graph_version=graph_version, cron=cron)",
        "job = self.scheduler.add_job(execute_graph, CronTrigger.from_crontab(cron), kwargs=job_args.model_dump(), replace_existing=True)",
        "log(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")",
        "return JobInfo.from_db(job_args, job)"
      ],
      "code": "def run_service(self):\n    load_dotenv()\n    (db_schema, db_url) = _extract_schema_from_url(os.getenv('DATABASE_URL'))\n    self.scheduler = BlockingScheduler(jobstores={'default': SQLAlchemyJobStore(engine=create_engine(db_url), metadata=MetaData(schema=db_schema))})\n    self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)\n    self.scheduler.start()\nload_dotenv()\n(db_schema, db_url) = _extract_schema_from_url(os.getenv('DATABASE_URL'))\nself.scheduler = BlockingScheduler(jobstores={'default': SQLAlchemyJobStore(engine=create_engine(db_url), metadata=MetaData(schema=db_schema))})\nself.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED BitOr EVENT_JOB_ERROR)\nself.scheduler.start()\n@expose\ndef add_execution_schedule(self, graph_id: str, graph_version: int, cron: str, input_data: BlockInput, user_id: str) -> JobInfo:\n    job_args = JobArgs(graph_id=graph_id, input_data=input_data, user_id=user_id, graph_version=graph_version, cron=cron)\n    job = self.scheduler.add_job(execute_graph, CronTrigger.from_crontab(cron), kwargs=job_args.model_dump(), replace_existing=True)\n    log(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")\n    return JobInfo.from_db(job_args, job)\njob_args = JobArgs(graph_id=graph_id, input_data=input_data, user_id=user_id, graph_version=graph_version, cron=cron)\njob = self.scheduler.add_job(execute_graph, CronTrigger.from_crontab(cron), kwargs=job_args.model_dump(), replace_existing=True)\nlog(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")\nreturn JobInfo.from_db(job_args, job)"
    },
    {
      "id": "n9",
      "type": "block",
      "statements": [
        "@expose\ndef delete_schedule(self, schedule_id: str, user_id: str) -> JobInfo:\n    job = self.scheduler.get_job(schedule_id)\n    if not job:\n        log(f'Job {schedule_id} not found.')\n        raise ValueError(f'Job #{schedule_id} not found.')\n    job_args = JobArgs(**job.kwargs)\n    if job_args.user_id != user_id:\n        raise ValueError(\"User ID does not match the job's user ID.\")\n    log(f'Deleting job {schedule_id}')\n    job.remove()\n    return JobInfo.from_db(job_args, job)",
        "job = self.scheduler.get_job(schedule_id)",
        "not job"
      ],
      "code": "@expose\ndef delete_schedule(self, schedule_id: str, user_id: str) -> JobInfo:\n    job = self.scheduler.get_job(schedule_id)\n    if not job:\n        log(f'Job {schedule_id} not found.')\n        raise ValueError(f'Job #{schedule_id} not found.')\n    job_args = JobArgs(**job.kwargs)\n    if job_args.user_id != user_id:\n        raise ValueError(\"User ID does not match the job's user ID.\")\n    log(f'Deleting job {schedule_id}')\n    job.remove()\n    return JobInfo.from_db(job_args, job)\njob = self.scheduler.get_job(schedule_id)\nnot job"
    },
    {
      "id": "n10",
      "type": "block",
      "statements": [
        "log(f'Job {schedule_id} not found.')",
        "raise ValueError(f'Job #{schedule_id} not found.')"
      ],
      "code": "log(f'Job {schedule_id} not found.')\nraise ValueError(f'Job #{schedule_id} not found.')"
    },
    {
      "id": "n11",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n12",
      "type": "block",
      "statements": [
        "job_args = JobArgs(**job.kwargs)",
        "job_args.user_id NotEq user_id"
      ],
      "code": "job_args = JobArgs(**job.kwargs)\njob_args.user_id NotEq user_id"
    },
    {
      "id": "n13",
      "type": "block",
      "statements": [
        "raise ValueError(\"User ID does not match the job's user ID.\")"
      ],
      "code": "raise ValueError(\"User ID does not match the job's user ID.\")"
    },
    {
      "id": "n14",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n15",
      "type": "block",
      "statements": [
        "log(f'Deleting job {schedule_id}')",
        "job.remove()",
        "return JobInfo.from_db(job_args, job)"
      ],
      "code": "log(f'Deleting job {schedule_id}')\njob.remove()\nreturn JobInfo.from_db(job_args, job)"
    },
    {
      "id": "n16",
      "type": "block",
      "statements": [
        "@expose\ndef get_execution_schedules(self, graph_id: str | None=None, user_id: str | None=None) -> list[JobInfo]:\n    schedules = []\n    for job in self.scheduler.get_jobs():\n        job_args = JobArgs(**job.kwargs)\n        if job.next_run_time is not None and (graph_id is None or job_args.graph_id == graph_id) and (user_id is None or job_args.user_id == user_id):\n            schedules.append(JobInfo.from_db(job_args, job))\n    return schedules",
        "schedules = []"
      ],
      "code": "@expose\ndef get_execution_schedules(self, graph_id: str | None=None, user_id: str | None=None) -> list[JobInfo]:\n    schedules = []\n    for job in self.scheduler.get_jobs():\n        job_args = JobArgs(**job.kwargs)\n        if job.next_run_time is not None and (graph_id is None or job_args.graph_id == graph_id) and (user_id is None or job_args.user_id == user_id):\n            schedules.append(JobInfo.from_db(job_args, job))\n    return schedules\nschedules = []"
    },
    {
      "id": "n17",
      "type": "block",
      "statements": [
        "job",
        "self.scheduler.get_jobs()"
      ],
      "code": "job\nself.scheduler.get_jobs()"
    },
    {
      "id": "n18",
      "type": "block",
      "statements": [
        "job_args = JobArgs(**job.kwargs)",
        "job.next_run_time is not None and (graph_id is None or job_args.graph_id == graph_id) and (user_id is None or job_args.user_id == user_id)"
      ],
      "code": "job_args = JobArgs(**job.kwargs)\njob.next_run_time is not None and (graph_id is None or job_args.graph_id == graph_id) and (user_id is None or job_args.user_id == user_id)"
    },
    {
      "id": "n19",
      "type": "block",
      "statements": [
        "return schedules"
      ],
      "code": "return schedules"
    },
    {
      "id": "n20",
      "type": "block",
      "statements": [
        "schedules.append(JobInfo.from_db(job_args, job))"
      ],
      "code": "schedules.append(JobInfo.from_db(job_args, job))"
    },
    {
      "id": "n21",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n22",
      "type": "block",
      "statements": [],
      "code": ""
    }
  ],
  "edges": [
    {
      "source": "n14",
      "target": "n15"
    },
    {
      "source": "n17",
      "target": "n18"
    },
    {
      "source": "n21",
      "target": "n22"
    },
    {
      "source": "n17",
      "target": "n19"
    },
    {
      "source": "n2",
      "target": "n4"
    },
    {
      "source": "n16",
      "target": "n17"
    },
    {
      "source": "n13",
      "target": "n15"
    },
    {
      "source": "n3",
      "target": "n4"
    },
    {
      "source": "n1",
      "target": "n2"
    },
    {
      "source": "n12",
      "target": "n13"
    },
    {
      "source": "n22",
      "target": "n17"
    },
    {
      "source": "n12",
      "target": "n14"
    },
    {
      "source": "n9",
      "target": "n11"
    },
    {
      "source": "n9",
      "target": "n10"
    },
    {
      "source": "n18",
      "target": "n21"
    },
    {
      "source": "n10",
      "target": "n12"
    },
    {
      "source": "n11",
      "target": "n12"
    },
    {
      "source": "n18",
      "target": "n20"
    },
    {
      "source": "n1",
      "target": "n3"
    },
    {
      "source": "n20",
      "target": "n22"
    }
  ]
}