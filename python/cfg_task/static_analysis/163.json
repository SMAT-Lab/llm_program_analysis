{
  "nodes": [
    {
      "id": "n0",
      "type": "block",
      "statements": [
        "class PagesDataStore(object):\n\n    def __init__(self, db):\n        self.db = db\n        pass\n\n    def add_link_to_crawl(self, url):\n        \"\"\"Add the given link to `links_to_crawl`.\"\"\"\n        pass\n\n    def remove_link_to_crawl(self, url):\n        \"\"\"Remove the given link from `links_to_crawl`.\"\"\"\n        pass\n\n    def reduce_priority_link_to_crawl(self, url):\n        \"\"\"Reduce the priority of a link in `links_to_crawl` to avoid cycles.\"\"\"\n        pass\n\n    def extract_max_priority_page(self):\n        \"\"\"Return the highest priority link in `links_to_crawl`.\"\"\"\n        pass\n\n    def insert_crawled_link(self, url, signature):\n        \"\"\"Add the given link to `crawled_links`.\"\"\"\n        pass\n\n    def crawled_similar(self, signature):\n        \"\"\"Determine if we've already crawled a page matching the given signature\"\"\"\n        pass",
        "def __init__(self, db):\n    self.db = db\n    pass",
        "self.db = db",
        "pass",
        "def add_link_to_crawl(self, url):\n    \"\"\"Add the given link to `links_to_crawl`.\"\"\"\n    pass",
        "'Add the given link to `links_to_crawl`.'",
        "pass",
        "def remove_link_to_crawl(self, url):\n    \"\"\"Remove the given link from `links_to_crawl`.\"\"\"\n    pass",
        "'Remove the given link from `links_to_crawl`.'",
        "pass",
        "def reduce_priority_link_to_crawl(self, url):\n    \"\"\"Reduce the priority of a link in `links_to_crawl` to avoid cycles.\"\"\"\n    pass",
        "'Reduce the priority of a link in `links_to_crawl` to avoid cycles.'",
        "pass",
        "def extract_max_priority_page(self):\n    \"\"\"Return the highest priority link in `links_to_crawl`.\"\"\"\n    pass",
        "'Return the highest priority link in `links_to_crawl`.'",
        "pass",
        "def insert_crawled_link(self, url, signature):\n    \"\"\"Add the given link to `crawled_links`.\"\"\"\n    pass",
        "'Add the given link to `crawled_links`.'",
        "pass",
        "def crawled_similar(self, signature):\n    \"\"\"Determine if we've already crawled a page matching the given signature\"\"\"\n    pass",
        "\"Determine if we've already crawled a page matching the given signature\"",
        "pass",
        "class Page(object):\n\n    def __init__(self, url, contents, child_urls):\n        self.url = url\n        self.contents = contents\n        self.child_urls = child_urls\n        self.signature = self.create_signature()\n\n    def create_signature(self):\n        pass",
        "def __init__(self, url, contents, child_urls):\n    self.url = url\n    self.contents = contents\n    self.child_urls = child_urls\n    self.signature = self.create_signature()",
        "self.url = url",
        "self.contents = contents",
        "self.child_urls = child_urls",
        "self.signature = self.create_signature()",
        "def create_signature(self):\n    pass",
        "pass",
        "class Crawler(object):\n\n    def __init__(self, pages, data_store, reverse_index_queue, doc_index_queue):\n        self.pages = pages\n        self.data_store = data_store\n        self.reverse_index_queue = reverse_index_queue\n        self.doc_index_queue = doc_index_queue\n\n    def crawl_page(self, page):\n        for url in page.child_urls:\n            self.data_store.add_link_to_crawl(url)\n        self.reverse_index_queue.generate(page)\n        self.doc_index_queue.generate(page)\n        self.data_store.remove_link_to_crawl(page.url)\n        self.data_store.insert_crawled_link(page.url, page.signature)\n\n    def crawl(self):\n        while True:\n            page = self.data_store.extract_max_priority_page()\n            if page is None:\n                break\n            if self.data_store.crawled_similar(page.signature):\n                self.data_store.reduce_priority_link_to_crawl(page.url)\n            else:\n                self.crawl_page(page)\n            page = self.data_store.extract_max_priority_page()",
        "def __init__(self, pages, data_store, reverse_index_queue, doc_index_queue):\n    self.pages = pages\n    self.data_store = data_store\n    self.reverse_index_queue = reverse_index_queue\n    self.doc_index_queue = doc_index_queue",
        "self.pages = pages",
        "self.data_store = data_store",
        "self.reverse_index_queue = reverse_index_queue",
        "self.doc_index_queue = doc_index_queue",
        "def crawl_page(self, page):\n    for url in page.child_urls:\n        self.data_store.add_link_to_crawl(url)\n    self.reverse_index_queue.generate(page)\n    self.doc_index_queue.generate(page)\n    self.data_store.remove_link_to_crawl(page.url)\n    self.data_store.insert_crawled_link(page.url, page.signature)"
      ],
      "code": "class PagesDataStore(object):\n\n    def __init__(self, db):\n        self.db = db\n        pass\n\n    def add_link_to_crawl(self, url):\n        \"\"\"Add the given link to `links_to_crawl`.\"\"\"\n        pass\n\n    def remove_link_to_crawl(self, url):\n        \"\"\"Remove the given link from `links_to_crawl`.\"\"\"\n        pass\n\n    def reduce_priority_link_to_crawl(self, url):\n        \"\"\"Reduce the priority of a link in `links_to_crawl` to avoid cycles.\"\"\"\n        pass\n\n    def extract_max_priority_page(self):\n        \"\"\"Return the highest priority link in `links_to_crawl`.\"\"\"\n        pass\n\n    def insert_crawled_link(self, url, signature):\n        \"\"\"Add the given link to `crawled_links`.\"\"\"\n        pass\n\n    def crawled_similar(self, signature):\n        \"\"\"Determine if we've already crawled a page matching the given signature\"\"\"\n        pass\ndef __init__(self, db):\n    self.db = db\n    pass\nself.db = db\npass\ndef add_link_to_crawl(self, url):\n    \"\"\"Add the given link to `links_to_crawl`.\"\"\"\n    pass\n'Add the given link to `links_to_crawl`.'\npass\ndef remove_link_to_crawl(self, url):\n    \"\"\"Remove the given link from `links_to_crawl`.\"\"\"\n    pass\n'Remove the given link from `links_to_crawl`.'\npass\ndef reduce_priority_link_to_crawl(self, url):\n    \"\"\"Reduce the priority of a link in `links_to_crawl` to avoid cycles.\"\"\"\n    pass\n'Reduce the priority of a link in `links_to_crawl` to avoid cycles.'\npass\ndef extract_max_priority_page(self):\n    \"\"\"Return the highest priority link in `links_to_crawl`.\"\"\"\n    pass\n'Return the highest priority link in `links_to_crawl`.'\npass\ndef insert_crawled_link(self, url, signature):\n    \"\"\"Add the given link to `crawled_links`.\"\"\"\n    pass\n'Add the given link to `crawled_links`.'\npass\ndef crawled_similar(self, signature):\n    \"\"\"Determine if we've already crawled a page matching the given signature\"\"\"\n    pass\n\"Determine if we've already crawled a page matching the given signature\"\npass\nclass Page(object):\n\n    def __init__(self, url, contents, child_urls):\n        self.url = url\n        self.contents = contents\n        self.child_urls = child_urls\n        self.signature = self.create_signature()\n\n    def create_signature(self):\n        pass\ndef __init__(self, url, contents, child_urls):\n    self.url = url\n    self.contents = contents\n    self.child_urls = child_urls\n    self.signature = self.create_signature()\nself.url = url\nself.contents = contents\nself.child_urls = child_urls\nself.signature = self.create_signature()\ndef create_signature(self):\n    pass\npass\nclass Crawler(object):\n\n    def __init__(self, pages, data_store, reverse_index_queue, doc_index_queue):\n        self.pages = pages\n        self.data_store = data_store\n        self.reverse_index_queue = reverse_index_queue\n        self.doc_index_queue = doc_index_queue\n\n    def crawl_page(self, page):\n        for url in page.child_urls:\n            self.data_store.add_link_to_crawl(url)\n        self.reverse_index_queue.generate(page)\n        self.doc_index_queue.generate(page)\n        self.data_store.remove_link_to_crawl(page.url)\n        self.data_store.insert_crawled_link(page.url, page.signature)\n\n    def crawl(self):\n        while True:\n            page = self.data_store.extract_max_priority_page()\n            if page is None:\n                break\n            if self.data_store.crawled_similar(page.signature):\n                self.data_store.reduce_priority_link_to_crawl(page.url)\n            else:\n                self.crawl_page(page)\n            page = self.data_store.extract_max_priority_page()\ndef __init__(self, pages, data_store, reverse_index_queue, doc_index_queue):\n    self.pages = pages\n    self.data_store = data_store\n    self.reverse_index_queue = reverse_index_queue\n    self.doc_index_queue = doc_index_queue\nself.pages = pages\nself.data_store = data_store\nself.reverse_index_queue = reverse_index_queue\nself.doc_index_queue = doc_index_queue\ndef crawl_page(self, page):\n    for url in page.child_urls:\n        self.data_store.add_link_to_crawl(url)\n    self.reverse_index_queue.generate(page)\n    self.doc_index_queue.generate(page)\n    self.data_store.remove_link_to_crawl(page.url)\n    self.data_store.insert_crawled_link(page.url, page.signature)"
    },
    {
      "id": "n1",
      "type": "block",
      "statements": [
        "url",
        "page.child_urls"
      ],
      "code": "url\npage.child_urls"
    },
    {
      "id": "n2",
      "type": "block",
      "statements": [
        "self.data_store.add_link_to_crawl(url)"
      ],
      "code": "self.data_store.add_link_to_crawl(url)"
    },
    {
      "id": "n3",
      "type": "block",
      "statements": [
        "self.reverse_index_queue.generate(page)",
        "self.doc_index_queue.generate(page)",
        "self.data_store.remove_link_to_crawl(page.url)",
        "self.data_store.insert_crawled_link(page.url, page.signature)",
        "def crawl(self):\n    while True:\n        page = self.data_store.extract_max_priority_page()\n        if page is None:\n            break\n        if self.data_store.crawled_similar(page.signature):\n            self.data_store.reduce_priority_link_to_crawl(page.url)\n        else:\n            self.crawl_page(page)\n        page = self.data_store.extract_max_priority_page()"
      ],
      "code": "self.reverse_index_queue.generate(page)\nself.doc_index_queue.generate(page)\nself.data_store.remove_link_to_crawl(page.url)\nself.data_store.insert_crawled_link(page.url, page.signature)\ndef crawl(self):\n    while True:\n        page = self.data_store.extract_max_priority_page()\n        if page is None:\n            break\n        if self.data_store.crawled_similar(page.signature):\n            self.data_store.reduce_priority_link_to_crawl(page.url)\n        else:\n            self.crawl_page(page)\n        page = self.data_store.extract_max_priority_page()"
    },
    {
      "id": "n4",
      "type": "block",
      "statements": [
        "True"
      ],
      "code": "True"
    },
    {
      "id": "n5",
      "type": "block",
      "statements": [
        "page = self.data_store.extract_max_priority_page()",
        "page Is None"
      ],
      "code": "page = self.data_store.extract_max_priority_page()\npage Is None"
    },
    {
      "id": "n6",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n7",
      "type": "block",
      "statements": [
        "break"
      ],
      "code": "break"
    },
    {
      "id": "n8",
      "type": "block",
      "statements": [],
      "code": ""
    },
    {
      "id": "n9",
      "type": "block",
      "statements": [
        "self.data_store.crawled_similar(page.signature)"
      ],
      "code": "self.data_store.crawled_similar(page.signature)"
    },
    {
      "id": "n10",
      "type": "block",
      "statements": [
        "self.data_store.reduce_priority_link_to_crawl(page.url)"
      ],
      "code": "self.data_store.reduce_priority_link_to_crawl(page.url)"
    },
    {
      "id": "n11",
      "type": "block",
      "statements": [
        "self.crawl_page(page)"
      ],
      "code": "self.crawl_page(page)"
    },
    {
      "id": "n12",
      "type": "block",
      "statements": [
        "page = self.data_store.extract_max_priority_page()"
      ],
      "code": "page = self.data_store.extract_max_priority_page()"
    }
  ],
  "edges": [
    {
      "source": "n5",
      "target": "n8"
    },
    {
      "source": "n9",
      "target": "n11"
    },
    {
      "source": "n1",
      "target": "n3"
    },
    {
      "source": "n0",
      "target": "n1"
    },
    {
      "source": "n8",
      "target": "n9"
    },
    {
      "source": "n3",
      "target": "n4"
    },
    {
      "source": "n12",
      "target": "n4"
    },
    {
      "source": "n9",
      "target": "n10"
    },
    {
      "source": "n7",
      "target": "n9"
    },
    {
      "source": "n1",
      "target": "n2"
    },
    {
      "source": "n4",
      "target": "n5"
    },
    {
      "source": "n5",
      "target": "n7"
    },
    {
      "source": "n10",
      "target": "n12"
    },
    {
      "source": "n11",
      "target": "n12"
    },
    {
      "source": "n4",
      "target": "n6"
    },
    {
      "source": "n2",
      "target": "n1"
    }
  ]
}