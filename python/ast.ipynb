{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "def ensure_tree_sitter_python():\n",
    "    \"\"\"确保 tree-sitter-python 语言库已经构建\"\"\"\n",
    "    build_path = Path('build')\n",
    "    so_path = build_path / 'my-languages.so'\n",
    "    repo_path = Path('tree-sitter-python')\n",
    "\n",
    "    # 如果.so文件已存在，直接返回路径\n",
    "    if so_path.exists():\n",
    "        return str(so_path.absolute())\n",
    "\n",
    "    # 创建build目录\n",
    "    build_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # 克隆仓库（如果不存在）\n",
    "    if not repo_path.exists():\n",
    "        subprocess.run([\n",
    "            'git', 'clone',\n",
    "            'https://github.com/tree-sitter/tree-sitter-python.git'\n",
    "        ], check=True)\n",
    "\n",
    "    # 构建语言库\n",
    "    Language.build_library(\n",
    "        str(so_path),\n",
    "        [str(repo_path)]\n",
    "    )\n",
    "\n",
    "    return str(so_path.absolute())\n",
    "\n",
    "def setup_parser():\n",
    "    \"\"\"设置并返回配置好的Parser\"\"\"\n",
    "    so_path = ensure_tree_sitter_python()\n",
    "    PY_LANGUAGE = Language(so_path, 'python')\n",
    "    parser = Parser()\n",
    "    parser.set_language(PY_LANGUAGE)\n",
    "    return parser\n",
    "\n",
    "def tokenize_code(code: str, start_index: int = 0):\n",
    "    \"\"\"分词函数\"\"\"\n",
    "    code = code.replace('\\n', ' \\\\n ').replace('\\t', ' \\\\t ').replace('\\r', ' \\\\r ')\n",
    "    token_pattern = r'[A-Za-z_]\\w*|[$$\\{\\}\\;\\,\\=\\+\\-\\*/]|[0-9]+|\"[^\"]*\"|\\'[^\\']*\\'|\\\\[ntr]|\\S'\n",
    "    raw_tokens = re.findall(token_pattern, code)\n",
    "    \n",
    "    token_objects = {}\n",
    "    for idx, tk in enumerate(raw_tokens):\n",
    "        if tk == '\\\\n':\n",
    "            tk = '\\n'\n",
    "        elif tk == '\\\\t':\n",
    "            tk = '\\t'\n",
    "        elif tk == '\\\\r':\n",
    "            tk = '\\r'\n",
    "        token_objects[idx + start_index] = tk\n",
    "    \n",
    "    return token_objects\n",
    "\n",
    "def get_token_index(pos: int, code: str, token_map: dict) -> Optional[int]:\n",
    "    \"\"\"获取给定位置对应的token索引\"\"\"\n",
    "    current_pos = 0\n",
    "    for idx, token in token_map.items():\n",
    "        token_len = len(token)\n",
    "        if current_pos <= pos < current_pos + token_len:\n",
    "            return idx\n",
    "        current_pos += token_len + 1  # +1 for space\n",
    "    return None\n",
    "\n",
    "def tree_to_dict(node, code: str, token_map: dict):\n",
    "    \"\"\"将 tree-sitter 节点转换为字典结构\"\"\"\n",
    "    # 获取节点的文本值\n",
    "    value = code[node.start_byte:node.end_byte].strip()\n",
    "    \n",
    "    # 获取token范围\n",
    "    start_token = get_token_index(node.start_byte, code, token_map)\n",
    "    end_token = get_token_index(node.end_byte - 1, code, token_map)\n",
    "    \n",
    "    result = {\n",
    "        \"value\": value,\n",
    "        \"children\": []\n",
    "    }\n",
    "    \n",
    "    # 递归处理子节点\n",
    "    cursor = node.walk()\n",
    "    if cursor.goto_first_child():\n",
    "        while True:\n",
    "            child_node = cursor.node\n",
    "            if child_node.is_named:  # 只处理named节点\n",
    "                child_dict = tree_to_dict(child_node, code, token_map)\n",
    "                if child_dict:\n",
    "                    result[\"children\"].append(child_dict)\n",
    "            if not cursor.goto_next_sibling():\n",
    "                break\n",
    "    \n",
    "    return result\n",
    "\n",
    "def parse_code(code: str):\n",
    "    \"\"\"解析Python代码并返回AST\"\"\"\n",
    "    # 设置解析器\n",
    "    parser = setup_parser()\n",
    "    \n",
    "    # 解析代码\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    \n",
    "    # 生成token映射\n",
    "    token_map = tokenize_code(code)\n",
    "    \n",
    "    # 转换为字典结构\n",
    "    return tree_to_dict(tree.root_node, code, token_map)\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    data_dir = \"data\"\n",
    "    AST_dir = \"AST\"\n",
    "    if not os.path.exists(AST_dir):\n",
    "        os.makedirs(AST_dir)\n",
    "    py_files = [f for f in os.listdir(data_dir) if f.endswith('.py')]\n",
    "    for file in py_files:\n",
    "        with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            code = f.read()\n",
    "        result = parse_code(code)\n",
    "        with open(os.path.join(AST_dir, file.replace('.py', '.json')), 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scalpel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
