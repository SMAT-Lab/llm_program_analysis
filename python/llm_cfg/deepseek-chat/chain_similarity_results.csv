Index,Original Code,Generated Code,Static Similarity,LLM Similarity
0,"from prisma.models import User

from backend.blocks.basic import AgentInputBlock, PrintToConsoleBlock
from backend.blocks.text import FillTextTemplateBlock
from backend.data import graph
from backend.data.graph import create_graph
from backend.data.user import get_or_create_user
from backend.util.test import SpinTestServer, wait_execution


async def create_test_user() -> User:
    test_user_data = {
        ""sub"": ""ef3b97d7-1161-4eb4-92b2-10c24fb154c1"",
        ""email"": ""testuser#example.com"",
        ""name"": ""Test User"",
    }
    user = await get_or_create_user(test_user_data)
    return user


def create_test_graph() -> graph.Graph:
    """"""
    InputBlock
               \
                 ---- FillTextTemplateBlock ---- PrintToConsoleBlock
               /
    InputBlock
    """"""
    nodes = [
        graph.Node(
            block_id=AgentInputBlock().id,
            input_default={""name"": ""input_1""},
        ),
        graph.Node(
            block_id=AgentInputBlock().id,
            input_default={""name"": ""input_2""},
        ),
        graph.Node(
            block_id=FillTextTemplateBlock().id,
            input_default={
                ""format"": ""{a}, {b}{c}"",
                ""values_#_c"": ""!!!"",
            },
        ),
        graph.Node(block_id=PrintToConsoleBlock().id),
    ]
    links = [
        graph.Link(
            source_id=nodes[0].id,
            sink_id=nodes[2].id,
            source_name=""result"",
            sink_name=""values_#_a"",
        ),
        graph.Link(
            source_id=nodes[1].id,
            sink_id=nodes[2].id,
            source_name=""result"",
            sink_name=""values_#_b"",
        ),
        graph.Link(
            source_id=nodes[2].id,
            sink_id=nodes[3].id,
            source_name=""output"",
            sink_name=""text"",
        ),
    ]

    return graph.Graph(
        name=""TestGraph"",
        description=""Test graph"",
        nodes=nodes,
        links=links,
    )


async def sample_agent():
    async with SpinTestServer() as server:
        test_user = await create_test_user()
        test_graph = await create_graph(create_test_graph(), test_user.id)
        input_data = {""input_1"": ""Hello"", ""input_2"": ""World""}
        response = await server.agent_server.test_execute_graph(
            test_graph.id, input_data, test_user.id
        )
        print(response)
        result = await wait_execution(test_user.id, test_graph.id, response[""id""], 10)
        print(result)


if __name__ == ""__main__"":
    import asyncio

    asyncio.run(sample_agent())
","from prisma.models import User
from backend.blocks.basic import AgentInputBlock, PrintToConsoleBlock
from backend.blocks.text import FillTextTemplateBlock
from backend.data import graph
from backend.data.graph import create_graph
from backend.data.user import get_or_create_user
from backend.util.test import SpinTestServer, wait_execution

async def create_test_user() -> User:
    test_user_data = {
        ""sub"": ""ef3b97d7-1161-4eb4-92b2-10c24fb154c1"",
        ""email"": ""testuser#example.com"",
        ""name"": ""Test User"",
    }
    user = await get_or_create_user(test_user_data)
    return user

def create_test_graph() -> graph.Graph:
    """"""
    InputBlock
               \
                 ---- FillTextTemplateBlock ---- PrintToConsoleBlock
               /
    InputBlock
    """"""
    nodes = [
        graph.Node(
            block_id=AgentInputBlock().id,
            input_default={""name"": ""input_1""},
        ),
        graph.Node(
            block_id=AgentInputBlock().id,
            input_default={""name"": ""input_2""},
        ),
        graph.Node(
            block_id=FillTextTemplateBlock().id,
            input_default={
                ""format"": ""{a}, {b}{c}"",
                ""values_#_c"": ""!!!"",
            },
        ),
        graph.Node(block_id=PrintToConsoleBlock().id),
    ]
    links = [
        graph.Link(
            source_id=nodes[0].id,
            sink_id=nodes[2].id,
            source_name=""result"",
            sink_name=""values_#_a"",
        ),
        graph.Link(
            source_id=nodes[1].id,
            sink_id=nodes[2].id,
            source_name=""result"",
            sink_name=""values_#_b"",
        ),
        graph.Link(
            source_id=nodes[2].id,
            sink_id=nodes[3].id,
            source_name=""output"",
            sink_name=""text"",
        ),
    ]
    return graph.Graph(
        name=""TestGraph"",
        description=""Test graph"",
        nodes=nodes,
        links=links,
    )

async def sample_agent():
    async with SpinTestServer() as server:
        test_user = await create_test_user()
        test_graph = await create_graph(create_test_graph(), test_user.id)
        input_data = {""input_1"": ""Hello"", ""input_2"": ""World""}

        response = await server.agent_server.test_execute_graph(
            test_graph.id, input_data, test_user.id
        )
        print(response)

        result = await wait_execution(test_user.id, test_graph.id, response[""id""], 10)
        print(result)

if __name__ == ""__main__"":
    import asyncio

    asyncio.run(sample_agent())","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
1,"from enum import Enum
from typing import Any, Dict, Literal, Optional

from pydantic import SecretStr
from requests.exceptions import RequestException

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName
from backend.util.request import requests

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""ideogram"",
    api_key=SecretStr(""mock-ideogram-api-key""),
    title=""Mock Ideogram API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class IdeogramModelName(str, Enum):
    V2 = ""V_2""
    V1 = ""V_1""
    V1_TURBO = ""V_1_TURBO""
    V2_TURBO = ""V_2_TURBO""


class MagicPromptOption(str, Enum):
    AUTO = ""AUTO""
    ON = ""ON""
    OFF = ""OFF""


class StyleType(str, Enum):
    AUTO = ""AUTO""
    GENERAL = ""GENERAL""
    REALISTIC = ""REALISTIC""
    DESIGN = ""DESIGN""
    RENDER_3D = ""RENDER_3D""
    ANIME = ""ANIME""


class ColorPalettePreset(str, Enum):
    NONE = ""NONE""
    EMBER = ""EMBER""
    FRESH = ""FRESH""
    JUNGLE = ""JUNGLE""
    MAGIC = ""MAGIC""
    MELON = ""MELON""
    MOSAIC = ""MOSAIC""
    PASTEL = ""PASTEL""
    ULTRAMARINE = ""ULTRAMARINE""


class AspectRatio(str, Enum):
    ASPECT_10_16 = ""ASPECT_10_16""
    ASPECT_16_10 = ""ASPECT_16_10""
    ASPECT_9_16 = ""ASPECT_9_16""
    ASPECT_16_9 = ""ASPECT_16_9""
    ASPECT_3_2 = ""ASPECT_3_2""
    ASPECT_2_3 = ""ASPECT_2_3""
    ASPECT_4_3 = ""ASPECT_4_3""
    ASPECT_3_4 = ""ASPECT_3_4""
    ASPECT_1_1 = ""ASPECT_1_1""
    ASPECT_1_3 = ""ASPECT_1_3""
    ASPECT_3_1 = ""ASPECT_3_1""


class UpscaleOption(str, Enum):
    AI_UPSCALE = ""AI Upscale""
    NO_UPSCALE = ""No Upscale""


class IdeogramModelBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.IDEOGRAM], Literal[""api_key""]
        ] = CredentialsField(
            description=""The Ideogram integration can be used with any API key with sufficient permissions for the blocks it is used on."",
        )
        prompt: str = SchemaField(
            description=""Text prompt for image generation"",
            placeholder=""e.g., 'A futuristic cityscape at sunset'"",
            title=""Prompt"",
        )
        ideogram_model_name: IdeogramModelName = SchemaField(
            description=""The name of the Image Generation Model, e.g., V_2"",
            default=IdeogramModelName.V2,
            title=""Image Generation Model"",
            advanced=False,
        )
        aspect_ratio: AspectRatio = SchemaField(
            description=""Aspect ratio for the generated image"",
            default=AspectRatio.ASPECT_1_1,
            title=""Aspect Ratio"",
            advanced=False,
        )
        upscale: UpscaleOption = SchemaField(
            description=""Upscale the generated image"",
            default=UpscaleOption.NO_UPSCALE,
            title=""Upscale Image"",
            advanced=False,
        )
        magic_prompt_option: MagicPromptOption = SchemaField(
            description=""Whether to use MagicPrompt for enhancing the request"",
            default=MagicPromptOption.AUTO,
            title=""Magic Prompt Option"",
            advanced=True,
        )
        seed: Optional[int] = SchemaField(
            description=""Random seed. Set for reproducible generation"",
            default=None,
            title=""Seed"",
            advanced=True,
        )
        style_type: StyleType = SchemaField(
            description=""Style type to apply, applicable for V_2 and above"",
            default=StyleType.AUTO,
            title=""Style Type"",
            advanced=True,
        )
        negative_prompt: Optional[str] = SchemaField(
            description=""Description of what to exclude from the image"",
            default=None,
            title=""Negative Prompt"",
            advanced=True,
        )
        color_palette_name: ColorPalettePreset = SchemaField(
            description=""Color palette preset name, choose 'None' to skip"",
            default=ColorPalettePreset.NONE,
            title=""Color Palette Preset"",
            advanced=True,
        )

    class Output(BlockSchema):
        result: str = SchemaField(description=""Generated image URL"")
        error: str = SchemaField(description=""Error message if the model run failed"")

    def __init__(self):
        super().__init__(
            id=""6ab085e2-20b3-4055-bc3e-08036e01eca6"",
            description=""This block runs Ideogram models with both simple and advanced settings."",
            categories={BlockCategory.AI},
            input_schema=IdeogramModelBlock.Input,
            output_schema=IdeogramModelBlock.Output,
            test_input={
                ""ideogram_model_name"": IdeogramModelName.V2,
                ""prompt"": ""A futuristic cityscape at sunset"",
                ""aspect_ratio"": AspectRatio.ASPECT_1_1,
                ""upscale"": UpscaleOption.NO_UPSCALE,
                ""magic_prompt_option"": MagicPromptOption.AUTO,
                ""seed"": None,
                ""style_type"": StyleType.AUTO,
                ""negative_prompt"": None,
                ""color_palette_name"": ColorPalettePreset.NONE,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (
                    ""result"",
                    ""https://ideogram.ai/api/images/test-generated-image-url.png"",
                ),
            ],
            test_mock={
                ""run_model"": lambda api_key, model_name, prompt, seed, aspect_ratio, magic_prompt_option, style_type, negative_prompt, color_palette_name: ""https://ideogram.ai/api/images/test-generated-image-url.png"",
                ""upscale_image"": lambda api_key, image_url: ""https://ideogram.ai/api/images/test-upscaled-image-url.png"",
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        seed = input_data.seed

        # Step 1: Generate the image
        result = self.run_model(
            api_key=credentials.api_key,
            model_name=input_data.ideogram_model_name.value,
            prompt=input_data.prompt,
            seed=seed,
            aspect_ratio=input_data.aspect_ratio.value,
            magic_prompt_option=input_data.magic_prompt_option.value,
            style_type=input_data.style_type.value,
            negative_prompt=input_data.negative_prompt,
            color_palette_name=input_data.color_palette_name.value,
        )

        # Step 2: Upscale the image if requested
        if input_data.upscale == UpscaleOption.AI_UPSCALE:
            result = self.upscale_image(
                api_key=credentials.api_key,
                image_url=result,
            )

        yield ""result"", result

    def run_model(
        self,
        api_key: SecretStr,
        model_name: str,
        prompt: str,
        seed: Optional[int],
        aspect_ratio: str,
        magic_prompt_option: str,
        style_type: str,
        negative_prompt: Optional[str],
        color_palette_name: str,
    ):
        url = ""https://api.ideogram.ai/generate""
        headers = {
            ""Api-Key"": api_key.get_secret_value(),
            ""Content-Type"": ""application/json"",
        }

        data: Dict[str, Any] = {
            ""image_request"": {
                ""prompt"": prompt,
                ""model"": model_name,
                ""aspect_ratio"": aspect_ratio,
                ""magic_prompt_option"": magic_prompt_option,
                ""style_type"": style_type,
            }
        }

        if seed is not None:
            data[""image_request""][""seed""] = seed

        if negative_prompt:
            data[""image_request""][""negative_prompt""] = negative_prompt

        if color_palette_name != ""NONE"":
            data[""image_request""][""color_palette""] = {""name"": color_palette_name}

        try:
            response = requests.post(url, json=data, headers=headers)
            return response.json()[""data""][0][""url""]
        except RequestException as e:
            raise Exception(f""Failed to fetch image: {str(e)}"")

    def upscale_image(self, api_key: SecretStr, image_url: str):
        url = ""https://api.ideogram.ai/upscale""
        headers = {
            ""Api-Key"": api_key.get_secret_value(),
        }

        try:
            # Step 1: Download the image from the provided URL
            image_response = requests.get(image_url)

            # Step 2: Send the downloaded image to the upscale API
            files = {
                ""image_file"": (""image.png"", image_response.content, ""image/png""),
            }

            response = requests.post(
                url,
                headers=headers,
                data={
                    ""image_request"": ""{}"",  # Empty JSON object
                },
                files=files,
            )

            return response.json()[""data""][0][""url""]

        except RequestException as e:
            raise Exception(f""Failed to upscale image: {str(e)}"")
","from enum import Enum
from typing import Any, Dict, Literal, Optional

from pydantic import SecretStr
from requests.exceptions import RequestException

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName
from backend.util.request import requests

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""ideogram"",
    api_key=SecretStr(""mock-ideogram-api-key""),
    title=""Mock Ideogram API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}

class IdeogramModelName(str, Enum):
    V2 = ""V_2""
    V1 = ""V_1""
    V1_TURBO = ""V_1_TURBO""
    V2_TURBO = ""V_2_TURBO""

class MagicPromptOption(str, Enum):
    AUTO = ""AUTO""
    ON = ""ON""
    OFF = ""OFF""

class StyleType(str, Enum):
    AUTO = ""AUTO""
    GENERAL = ""GENERAL""
    REALISTIC = ""REALISTIC""
    DESIGN = ""DESIGN""
    RENDER_3D = ""RENDER_3D""
    ANIME = ""ANIME""

class ColorPalettePreset(str, Enum):
    NONE = ""NONE""
    EMBER = ""EMBER""
    FRESH = ""FRESH""
    JUNGLE = ""JUNGLE""
    MAGIC = ""MAGIC""
    MELON = ""MELON""
    MOSAIC = ""MOSAIC""
    PASTEL = ""PASTEL""
    ULTRAMARINE = ""ULTRAMARINE""

class AspectRatio(str, Enum):
    ASPECT_10_16 = ""ASPECT_10_16""
    ASPECT_16_10 = ""ASPECT_16_10""
    ASPECT_9_16 = ""ASPECT_9_16""
    ASPECT_16_9 = ""ASPECT_16_9""
    ASPECT_3_2 = ""ASPECT_3_2""
    ASPECT_2_3 = ""ASPECT_2_3""
    ASPECT_4_3 = ""ASPECT_4_3""
    ASPECT_3_4 = ""ASPECT_3_4""
    ASPECT_1_1 = ""ASPECT_1_1""
    ASPECT_1_3 = ""ASPECT_1_3""
    ASPECT_3_1 = ""ASPECT_3_1""

class UpscaleOption(str, Enum):
    AI_UPSCALE = ""AI Upscale""
    NO_UPSCALE = ""No Upscale""

class IdeogramModelBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.IDEOGRAM], Literal[""api_key""]
        ] = CredentialsField(
            description=""The Ideogram integration can be used with any API key with sufficient permissions for the blocks it is used on."",
        )
        prompt: str = SchemaField(
            description=""Text prompt for image generation"",
            placeholder=""e.g., 'A futuristic cityscape at sunset'"",
            title=""Prompt"",
        )
        ideogram_model_name: IdeogramModelName = SchemaField(
            description=""The name of the Image Generation Model, e.g., V_2"",
            default=IdeogramModelName.V2,
            title=""Image Generation Model"",
            advanced=False,
        )
        aspect_ratio: AspectRatio = SchemaField(
            description=""Aspect ratio for the generated image"",
            default=AspectRatio.ASPECT_1_1,
            title=""Aspect Ratio"",
            advanced=False,
        )
        upscale: UpscaleOption = SchemaField(
            description=""Upscale the generated image"",
            default=UpscaleOption.NO_UPSCALE,
            title=""Upscale Image"",
            advanced=False,
        )
        magic_prompt_option: MagicPromptOption = SchemaField(
            description=""Whether to use MagicPrompt for enhancing the request"",
            default=MagicPromptOption.AUTO,
            title=""Magic Prompt Option"",
            advanced=True,
        )
        seed: Optional[int] = SchemaField(
            description=""Random seed. Set for reproducible generation"",
            default=None,
            title=""Seed"",
            advanced=True,
        )
        style_type: StyleType = SchemaField(
            description=""Style type to apply, applicable for V_2 and above"",
            default=StyleType.AUTO,
            title=""Style Type"",
            advanced=True,
        )
        negative_prompt: Optional[str] = SchemaField(
            description=""Description of what to exclude from the image"",
            default=None,
            title=""Negative Prompt"",
            advanced=True,
        )
        color_palette_name: ColorPalettePreset = SchemaField(
            description=""Color palette preset name, choose 'None' to skip"",
            default=ColorPalettePreset.NONE,
            title=""Color Palette Preset"",
            advanced=True,
        )

    class Output(BlockSchema):
        result: str = SchemaField(description=""Generated image URL"")
        error: str = SchemaField(description=""Error message if the model run failed"")

    def __init__(self):
        super().__init__(
            id=""6ab085e2-20b3-4055-bc3e-08036e01eca6"",
            description=""This block runs Ideogram models with both simple and advanced settings."",
            categories={BlockCategory.AI},
            input_schema=IdeogramModelBlock.Input,
            output_schema=IdeogramModelBlock.Output,
            test_input={
                ""ideogram_model_name"": IdeogramModelName.V2,
                ""prompt"": ""A futuristic cityscape at sunset"",
                ""aspect_ratio"": AspectRatio.ASPECT_1_1,
                ""upscale"": UpscaleOption.NO_UPSCALE,
                ""magic_prompt_option"": MagicPromptOption.AUTO,
                ""seed"": None,
                ""style_type"": StyleType.AUTO,
                ""negative_prompt"": None,
                ""color_palette_name"": ColorPalettePreset.NONE,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (
                    ""result"",
                    ""https://ideogram.ai/api/images/test-generated-image-url.png"",
                ),
            ],
            test_mock={
                ""run_model"": lambda api_key, model_name, prompt, seed, aspect_ratio, magic_prompt_option, style_type, negative_prompt, color_palette_name: ""https://ideogram.ai/api/images/test-generated-image-url.png"",
                ""upscale_image"": lambda api_key, image_url: ""https://ideogram.ai/api/images/test-upscaled-image-url.png"",
            },
            test_credentials=TEST_CREDENTIALS,
        )
    
    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        seed = input_data.seed
        # Step 1: Generate the image
        result = self.run_model(
            api_key=credentials.api_key,
            model_name=input_data.ideogram_model_name.value,
            prompt=input_data.prompt,
            seed=seed,
            aspect_ratio=input_data.aspect_ratio.value,
            magic_prompt_option=input_data.magic_prompt_option.value,
            style_type=input_data.style_type.value,
            negative_prompt=input_data.negative_prompt,
            color_palette_name=input_data.color_palette_name.value,
        )
        # Step 2: Upscale the image if requested
        if input_data.upscale == UpscaleOption.AI_UPSCALE:
            result = self.upscale_image(
                api_key=credentials.api_key,
                image_url=result,
            )
        yield ""result"", result
    
    def run_model(
        self,
        api_key: SecretStr,
        model_name: str,
        prompt: str,
        seed: Optional[int],
        aspect_ratio: str,
        magic_prompt_option: str,
        style_type: str,
        negative_prompt: Optional[str],
        color_palette_name: str,
    ):
        url = ""https://api.ideogram.ai/generate""
        headers = {
            ""Api-Key"": api_key.get_secret_value(),
            ""Content-Type"": ""application/json"",
        }
        data: Dict[str, Any] = {
            ""image_request"": {
                ""prompt"": prompt,
                ""model"": model_name,
                ""aspect_ratio"": aspect_ratio,
                ""magic_prompt_option"": magic_prompt_option,
                ""style_type"": style_type,
            }
        }
        if seed is not None:
            data[""image_request""][""seed""] = seed
        if negative_prompt:
            data[""image_request""][""negative_prompt""] = negative_prompt
        if color_palette_name != ""NONE"":
            data[""image_request""][""color_palette""] = {""name"": color_palette_name}
        try:
            response = requests.post(url, json=data, headers=headers)
            return response.json()[""data""][0][""url""]
        except RequestException as e:
            raise Exception(f""Failed to fetch image: {str(e)}"")

    def upscale_image(self, api_key: SecretStr, image_url: str):
        url = ""https://api.ideogram.ai/upscale""
        headers = {
            ""Api-Key"": api_key.get_secret_value(),
        }
        try:
            # Step 1: Download the image from the provided URL
            image_response = requests.get(image_url)
            # Step 2: Send the downloaded image to the upscale API
            files = {
                ""image_file"": (""image.png"", image_response.content, ""image/png""),
            }
            response = requests.post(
                url,
                headers=headers,
                data={
                    ""image_request"": ""{}"",  # Empty JSON object
                },
                files=files,
            )
            return response.json()[""data""][0][""url""]
        except RequestException as e:
            raise Exception(f""Failed to upscale image: {str(e)}"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
2,"from backend.blocks.jina._auth import (
    JinaCredentials,
    JinaCredentialsField,
    JinaCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class JinaEmbeddingBlock(Block):
    class Input(BlockSchema):
        texts: list = SchemaField(description=""List of texts to embed"")
        credentials: JinaCredentialsInput = JinaCredentialsField()
        model: str = SchemaField(
            description=""Jina embedding model to use"",
            default=""jina-embeddings-v2-base-en"",
        )

    class Output(BlockSchema):
        embeddings: list = SchemaField(description=""List of embeddings"")

    def __init__(self):
        super().__init__(
            id=""7c56b3ab-62e7-43a2-a2dc-4ec4245660b6"",
            description=""Generates embeddings using Jina AI"",
            categories={BlockCategory.AI},
            input_schema=JinaEmbeddingBlock.Input,
            output_schema=JinaEmbeddingBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://api.jina.ai/v1/embeddings""
        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }
        data = {""input"": input_data.texts, ""model"": input_data.model}
        response = requests.post(url, headers=headers, json=data)
        embeddings = [e[""embedding""] for e in response.json()[""data""]]
        yield ""embeddings"", embeddings
","from backend.blocks.jina._auth import (
    JinaCredentials,
    JinaCredentialsField,
    JinaCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class JinaEmbeddingBlock(Block):
    class Input(BlockSchema):
        texts: list = SchemaField(description=""List of texts to embed"")
        credentials: JinaCredentialsInput = JinaCredentialsField()
        model: str = SchemaField(
            description=""Jina embedding model to use"",
            default=""jina-embeddings-v2-base-en"",
        )

    class Output(BlockSchema):
        embeddings: list = SchemaField(description=""List of embeddings"")

    def __init__(self):
        super().__init__(
            id=""7c56b3ab-62e7-43a2-a2dc-4ec4245660b6"",
            description=""Generates embeddings using Jina AI"",
            categories={BlockCategory.AI},
            input_schema=JinaEmbeddingBlock.Input,
            output_schema=JinaEmbeddingBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://api.jina.ai/v1/embeddings""
        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }
        data = {""input"": input_data.texts, ""model"": input_data.model}
        response = requests.post(url, headers=headers, json=data)
        embeddings = [e[""embedding""] for e in response.json()[""data""]]
        yield ""embeddings"", embeddings","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
3,"from typing import Type

from backend.blocks.ai_music_generator import AIMusicGeneratorBlock
from backend.blocks.ai_shortform_video_block import AIShortformVideoCreatorBlock
from backend.blocks.ideogram import IdeogramModelBlock
from backend.blocks.jina.embeddings import JinaEmbeddingBlock
from backend.blocks.jina.search import ExtractWebsiteContentBlock, SearchTheWebBlock
from backend.blocks.llm import (
    MODEL_METADATA,
    AIConversationBlock,
    AIListGeneratorBlock,
    AIStructuredResponseGeneratorBlock,
    AITextGeneratorBlock,
    AITextSummarizerBlock,
    LlmModel,
)
from backend.blocks.replicate_flux_advanced import ReplicateFluxAdvancedModelBlock
from backend.blocks.talking_head import CreateTalkingAvatarVideoBlock
from backend.blocks.text_to_speech_block import UnrealTextToSpeechBlock
from backend.data.block import Block
from backend.data.cost import BlockCost, BlockCostType
from backend.integrations.credentials_store import (
    anthropic_credentials,
    did_credentials,
    groq_credentials,
    ideogram_credentials,
    jina_credentials,
    open_router_credentials,
    openai_credentials,
    replicate_credentials,
    revid_credentials,
    unreal_credentials,
)

# =============== Configure the cost for each LLM Model call =============== #

MODEL_COST: dict[LlmModel, int] = {
    LlmModel.O1_PREVIEW: 16,
    LlmModel.O1_MINI: 4,
    LlmModel.GPT4O_MINI: 1,
    LlmModel.GPT4O: 3,
    LlmModel.GPT4_TURBO: 10,
    LlmModel.GPT3_5_TURBO: 1,
    LlmModel.CLAUDE_3_5_SONNET: 4,
    LlmModel.CLAUDE_3_HAIKU: 1,
    LlmModel.LLAMA3_8B: 1,
    LlmModel.LLAMA3_70B: 1,
    LlmModel.MIXTRAL_8X7B: 1,
    LlmModel.GEMMA_7B: 1,
    LlmModel.GEMMA2_9B: 1,
    LlmModel.LLAMA3_1_405B: 1,
    LlmModel.LLAMA3_1_70B: 1,
    LlmModel.LLAMA3_1_8B: 1,
    LlmModel.OLLAMA_LLAMA3_8B: 1,
    LlmModel.OLLAMA_LLAMA3_405B: 1,
    LlmModel.OLLAMA_DOLPHIN: 1,
    LlmModel.GEMINI_FLASH_1_5_8B: 1,
    LlmModel.GROK_BETA: 5,
    LlmModel.MISTRAL_NEMO: 1,
    LlmModel.COHERE_COMMAND_R_08_2024: 1,
    LlmModel.COHERE_COMMAND_R_PLUS_08_2024: 3,
    LlmModel.EVA_QWEN_2_5_32B: 1,
    LlmModel.DEEPSEEK_CHAT: 2,
    LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: 1,
    LlmModel.QWEN_QWQ_32B_PREVIEW: 2,
    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: 1,
    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: 1,
    LlmModel.AMAZON_NOVA_LITE_V1: 1,
    LlmModel.AMAZON_NOVA_MICRO_V1: 1,
    LlmModel.AMAZON_NOVA_PRO_V1: 1,
    LlmModel.MICROSOFT_WIZARDLM_2_8X22B: 1,
    LlmModel.GRYPHE_MYTHOMAX_L2_13B: 1,
}

for model in LlmModel:
    if model not in MODEL_COST:
        raise ValueError(f""Missing MODEL_COST for model: {model}"")


LLM_COST = (
    # Anthropic Models
    [
        BlockCost(
            cost_type=BlockCostType.RUN,
            cost_filter={
                ""model"": model,
                ""credentials"": {
                    ""id"": anthropic_credentials.id,
                    ""provider"": anthropic_credentials.provider,
                    ""type"": anthropic_credentials.type,
                },
            },
            cost_amount=cost,
        )
        for model, cost in MODEL_COST.items()
        if MODEL_METADATA[model].provider == ""anthropic""
    ]
    # OpenAI Models
    + [
        BlockCost(
            cost_type=BlockCostType.RUN,
            cost_filter={
                ""model"": model,
                ""credentials"": {
                    ""id"": openai_credentials.id,
                    ""provider"": openai_credentials.provider,
                    ""type"": openai_credentials.type,
                },
            },
            cost_amount=cost,
        )
        for model, cost in MODEL_COST.items()
        if MODEL_METADATA[model].provider == ""openai""
    ]
    # Groq Models
    + [
        BlockCost(
            cost_type=BlockCostType.RUN,
            cost_filter={
                ""model"": model,
                ""credentials"": {""id"": groq_credentials.id},
            },
            cost_amount=cost,
        )
        for model, cost in MODEL_COST.items()
        if MODEL_METADATA[model].provider == ""groq""
    ]
    # Open Router Models
    + [
        BlockCost(
            cost_type=BlockCostType.RUN,
            cost_filter={
                ""model"": model,
                ""credentials"": {
                    ""id"": open_router_credentials.id,
                    ""provider"": open_router_credentials.provider,
                    ""type"": open_router_credentials.type,
                },
            },
            cost_amount=cost,
        )
        for model, cost in MODEL_COST.items()
        if MODEL_METADATA[model].provider == ""open_router""
    ]
)

# =============== This is the exhaustive list of cost for each Block =============== #

BLOCK_COSTS: dict[Type[Block], list[BlockCost]] = {
    AIConversationBlock: LLM_COST,
    AITextGeneratorBlock: LLM_COST,
    AIStructuredResponseGeneratorBlock: LLM_COST,
    AITextSummarizerBlock: LLM_COST,
    AIListGeneratorBlock: LLM_COST,
    CreateTalkingAvatarVideoBlock: [
        BlockCost(
            cost_amount=15,
            cost_filter={
                ""credentials"": {
                    ""id"": did_credentials.id,
                    ""provider"": did_credentials.provider,
                    ""type"": did_credentials.type,
                }
            },
        )
    ],
    SearchTheWebBlock: [
        BlockCost(
            cost_amount=1,
            cost_filter={
                ""credentials"": {
                    ""id"": jina_credentials.id,
                    ""provider"": jina_credentials.provider,
                    ""type"": jina_credentials.type,
                }
            },
        )
    ],
    ExtractWebsiteContentBlock: [
        BlockCost(
            cost_amount=1,
            cost_filter={
                ""raw_content"": False,
                ""credentials"": {
                    ""id"": jina_credentials.id,
                    ""provider"": jina_credentials.provider,
                    ""type"": jina_credentials.type,
                },
            },
        )
    ],
    IdeogramModelBlock: [
        BlockCost(
            cost_amount=16,
            cost_filter={
                ""credentials"": {
                    ""id"": ideogram_credentials.id,
                    ""provider"": ideogram_credentials.provider,
                    ""type"": ideogram_credentials.type,
                }
            },
        )
    ],
    AIShortformVideoCreatorBlock: [
        BlockCost(
            cost_amount=50,
            cost_filter={
                ""credentials"": {
                    ""id"": revid_credentials.id,
                    ""provider"": revid_credentials.provider,
                    ""type"": revid_credentials.type,
                }
            },
        )
    ],
    ReplicateFluxAdvancedModelBlock: [
        BlockCost(
            cost_amount=10,
            cost_filter={
                ""credentials"": {
                    ""id"": replicate_credentials.id,
                    ""provider"": replicate_credentials.provider,
                    ""type"": replicate_credentials.type,
                }
            },
        )
    ],
    AIMusicGeneratorBlock: [
        BlockCost(
            cost_amount=11,
            cost_filter={
                ""credentials"": {
                    ""id"": replicate_credentials.id,
                    ""provider"": replicate_credentials.provider,
                    ""type"": replicate_credentials.type,
                }
            },
        )
    ],
    JinaEmbeddingBlock: [
        BlockCost(
            cost_amount=12,
            cost_filter={
                ""credentials"": {
                    ""id"": jina_credentials.id,
                    ""provider"": jina_credentials.provider,
                    ""type"": jina_credentials.type,
                }
            },
        )
    ],
    UnrealTextToSpeechBlock: [
        BlockCost(
            cost_amount=5,
            cost_filter={
                ""credentials"": {
                    ""id"": unreal_credentials.id,
                    ""provider"": unreal_credentials.provider,
                    ""type"": unreal_credentials.type,
                }
            },
        )
    ],
}
","from typing import Type
from backend.blocks.ai_music_generator import AIMusicGeneratorBlock
from backend.blocks.ai_shortform_video_block import AIShortformVideoCreatorBlock
from backend.blocks.ideogram import IdeogramModelBlock
from backend.blocks.jina.embeddings import JinaEmbeddingBlock
from backend.blocks.jina.search import ExtractWebsiteContentBlock, SearchTheWebBlock
from backend.blocks.llm import (
    MODEL_METADATA,
    AIConversationBlock,
    AIListGeneratorBlock,
    AIStructuredResponseGeneratorBlock,
    AITextGeneratorBlock,
    AITextSummarizerBlock,
    LlmModel,
)
from backend.blocks.replicate_flux_advanced import ReplicateFluxAdvancedModelBlock
from backend.blocks.talking_head import CreateTalkingAvatarVideoBlock
from backend.blocks.text_to_speech_block import UnrealTextToSpeechBlock
from backend.data.block import Block
from backend.data.cost import BlockCost, BlockCostType
from backend.integrations.credentials_store import (
    anthropic_credentials,
    did_credentials,
    groq_credentials,
    ideogram_credentials,
    jina_credentials,
    open_router_credentials,
    openai_credentials,
    replicate_credentials,
    revid_credentials,
    unreal_credentials,
)

# =============== Configure the cost for each LLM Model call =============== #

MODEL_COST: dict[LlmModel, int] = {
    LlmModel.O1_PREVIEW: 16,
    LlmModel.O1_MINI: 4,
    LlmModel.GPT4O_MINI: 1,
    LlmModel.GPT4O: 3,
    LlmModel.GPT4_TURBO: 10,
    LlmModel.GPT3_5_TURBO: 1,
    LlmModel.CLAUDE_3_5_SONNET: 4,
    LlmModel.CLAUDE_3_HAIKU: 1,
    LlmModel.LLAMA3_8B: 1,
    LlmModel.LLAMA3_70B: 1,
    LlmModel.MIXTRAL_8X7B: 1,
    LlmModel.GEMMA_7B: 1,
    LlmModel.GEMMA2_9B: 1,
    LlmModel.LLAMA3_1_405B: 1,
    LlmModel.LLAMA3_1_70B: 1,
    LlmModel.LLAMA3_1_8B: 1,
    LlmModel.OLLAMA_LLAMA3_8B: 1,
    LlmModel.OLLAMA_LLAMA3_405B: 1,
    LlmModel.OLLAMA_DOLPHIN: 1,
    LlmModel.GEMINI_FLASH_1_5_8B: 1,
    LlmModel.GROK_BETA: 5,
    LlmModel.MISTRAL_NEMO: 1,
    LlmModel.COHERE_COMMAND_R_08_2024: 1,
    LlmModel.COHERE_COMMAND_R_PLUS_08_2024: 3,
    LlmModel.EVA_QWEN_2_5_32B: 1,
    LlmModel.DEEPSEEK_CHAT: 2,
    LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: 1,
    LlmModel.QWEN_QWQ_32B_PREVIEW: 2,
    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: 1,
    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: 1,
    LlmModel.AMAZON_NOVA_LITE_V1: 1,
    LlmModel.AMAZON_NOVA_MICRO_V1: 1,
    LlmModel.AMAZON_NOVA_PRO_V1: 1,
    LlmModel.MICROSOFT_WIZARDLM_2_8X22B: 1,
    LlmModel.GRYPHE_MYTHOMAX_L2_13B: 1,
}

for model in LlmModel:
    if model not in MODEL_COST:
        raise ValueError(f""Missing MODEL_COST for model: {model}"")

LLM_COST = (
    # Anthropic Models
    [
        BlockCost(
            cost_type=BlockCostType.RUN,
            cost_filter={
                ""model"": model,
                ""credentials"": {
                    ""id"": anthropic_credentials.id,
                    ""provider"": anthropic_credentials.provider,
                    ""type"": anthropic_credentials.type,
                },
            },
            cost_amount=cost,
        )
        for model, cost in MODEL_COST.items()
        if MODEL_METADATA[model].provider == ""anthropic""
    ]
    # OpenAI Models
    + [
        BlockCost(
            cost_type=BlockCostType.RUN,
            cost_filter={
                ""model"": model,
                ""credentials"": {
                    ""id"": openai_credentials.id,
                    ""provider"": openai_credentials.provider,
                    ""type"": openai_credentials.type,
                },
            },
            cost_amount=cost,
        )
        for model, cost in MODEL_COST.items()
        if MODEL_METADATA[model].provider == ""openai""
    ]
    # Groq Models
    + [
        BlockCost(
            cost_type=BlockCostType.RUN,
            cost_filter={
                ""model"": model,
                ""credentials"": {""id"": groq_credentials.id},
            },
            cost_amount=cost,
        )
        for model, cost in MODEL_COST.items()
        if MODEL_METADATA[model].provider == ""groq""
    ]
    # Open Router Models
    + [
        BlockCost(
            cost_type=BlockCostType.RUN,
            cost_filter={
                ""model"": model,
                ""credentials"": {
                    ""id"": open_router_credentials.id,
                    ""provider"": open_router_credentials.provider,
                    ""type"": open_router_credentials.type,
                },
            },
            cost_amount=cost,
        )
        for model, cost in MODEL_COST.items()
        if MODEL_METADATA[model].provider == ""open_router""
    ]
)

BLOCK_COSTS: dict[Type[Block], list[BlockCost]] = {
    AIConversationBlock: LLM_COST,
    AITextGeneratorBlock: LLM_COST,
    AIStructuredResponseGeneratorBlock: LLM_COST,
    AITextSummarizerBlock: LLM_COST,
    AIListGeneratorBlock: LLM_COST,
    CreateTalkingAvatarVideoBlock: [
        BlockCost(
            cost_amount=15,
            cost_filter={
                ""credentials"": {
                    ""id"": did_credentials.id,
                    ""provider"": did_credentials.provider,
                    ""type"": did_credentials.type,
                }
            },
        )
    ],
    SearchTheWebBlock: [
        BlockCost(
            cost_amount=1,
            cost_filter={
                ""credentials"": {
                    ""id"": jina_credentials.id,
                    ""provider"": jina_credentials.provider,
                    ""type"": jina_credentials.type,
                }
            },
        )
    ],
    ExtractWebsiteContentBlock: [
        BlockCost(
            cost_amount=1,
            cost_filter={
                ""raw_content"": False,
                ""credentials"": {
                    ""id"": jina_credentials.id,
                    ""provider"": jina_credentials.provider,
                    ""type"": jina_credentials.type,
                },
            },
        )
    ],
    IdeogramModelBlock: [
        BlockCost(
            cost_amount=16,
            cost_filter={
                ""credentials"": {
                    ""id"": ideogram_credentials.id,
                    ""provider"": ideogram_credentials.provider,
                    ""type"": ideogram_credentials.type,
                }
            },
        )
    ],
    AIShortformVideoCreatorBlock: [
        BlockCost(
            cost_amount=50,
            cost_filter={
                ""credentials"": {
                    ""id"": revid_credentials.id,
                    ""provider"": revid_credentials.provider,
                    ""type"": revid_credentials.type,
                }
            },
        )
    ],
    ReplicateFluxAdvancedModelBlock: [
        BlockCost(
            cost_amount=10,
            cost_filter={
                ""credentials"": {
                    ""id"": replicate_credentials.id,
                    ""provider"": replicate_credentials.provider,
                    ""type"": replicate_credentials.type,
                }
            },
        )
    ],
    AIMusicGeneratorBlock: [
        BlockCost(
            cost_amount=11,
            cost_filter={
                ""credentials"": {
                    ""id"": replicate_credentials.id,
                    ""provider"": replicate_credentials.provider,
                    ""type"": replicate_credentials.type,
                }
            },
        )
    ],
    JinaEmbeddingBlock: [
        BlockCost(
            cost_amount=12,
            cost_filter={
                ""credentials"": {
                    ""id"": jina_credentials.id,
                    ""provider"": jina_credentials.provider,
                    ""type"": jina_credentials.type,
                }
            },
        )
    ],
    UnrealTextToSpeechBlock: [
        BlockCost(
            cost_amount=5,
            cost_filter={
                ""credentials"": {
                    ""id"": unreal_credentials.id,
                    ""provider"": unreal_credentials.provider,
                    ""type"": unreal_credentials.type,
                }
            },
        )
    ],
}","{'final_score': 98.32, 'length_similarity': 98.93, 'token_similarity': 97.57, 'structure_similarity': 99.61, 'function_similarity': 100, 'complexity_similarity': 92.86, 'variable_similarity': 100}",
4,"import atexit
import logging
import multiprocessing
import os
import signal
import sys
import threading
from concurrent.futures import Future, ProcessPoolExecutor
from contextlib import contextmanager
from multiprocessing.pool import AsyncResult, Pool
from typing import TYPE_CHECKING, Any, Generator, TypeVar, cast

from pydantic import BaseModel
from redis.lock import Lock as RedisLock

if TYPE_CHECKING:
    from backend.executor import DatabaseManager

from autogpt_libs.utils.cache import thread_cached

from backend.blocks.agent import AgentExecutorBlock
from backend.data import redis
from backend.data.block import Block, BlockData, BlockInput, BlockType, get_block
from backend.data.execution import (
    ExecutionQueue,
    ExecutionResult,
    ExecutionStatus,
    GraphExecutionEntry,
    NodeExecutionEntry,
    merge_execution_input,
    parse_execution_output,
)
from backend.data.graph import GraphModel, Link, Node
from backend.data.model import CREDENTIALS_FIELD_NAME, CredentialsMetaInput
from backend.integrations.creds_manager import IntegrationCredentialsManager
from backend.util import json
from backend.util.decorator import error_logged, time_measured
from backend.util.logging import configure_logging
from backend.util.process import set_service_name
from backend.util.service import (
    AppService,
    close_service_client,
    expose,
    get_service_client,
)
from backend.util.settings import Settings
from backend.util.type import convert

logger = logging.getLogger(__name__)
settings = Settings()


class LogMetadata:
    def __init__(
        self,
        user_id: str,
        graph_eid: str,
        graph_id: str,
        node_eid: str,
        node_id: str,
        block_name: str,
    ):
        self.metadata = {
            ""component"": ""ExecutionManager"",
            ""user_id"": user_id,
            ""graph_eid"": graph_eid,
            ""graph_id"": graph_id,
            ""node_eid"": node_eid,
            ""node_id"": node_id,
            ""block_name"": block_name,
        }
        self.prefix = f""[ExecutionManager|uid:{user_id}|gid:{graph_id}|nid:{node_id}]|geid:{graph_eid}|nid:{node_eid}|{block_name}]""

    def info(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.info(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def warning(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.warning(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def error(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.error(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def debug(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.debug(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def exception(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.exception(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def _wrap(self, msg: str, **extra):
        return f""{self.prefix} {msg} {extra}""


T = TypeVar(""T"")
ExecutionStream = Generator[NodeExecutionEntry, None, None]


def execute_node(
    db_client: ""DatabaseManager"",
    creds_manager: IntegrationCredentialsManager,
    data: NodeExecutionEntry,
    execution_stats: dict[str, Any] | None = None,
) -> ExecutionStream:
    """"""
    Execute a node in the graph. This will trigger a block execution on a node,
    persist the execution result, and return the subsequent node to be executed.

    Args:
        db_client: The client to send execution updates to the server.
        creds_manager: The manager to acquire and release credentials.
        data: The execution data for executing the current node.
        execution_stats: The execution statistics to be updated.

    Returns:
        The subsequent node to be enqueued, or None if there is no subsequent node.
    """"""
    user_id = data.user_id
    graph_exec_id = data.graph_exec_id
    graph_id = data.graph_id
    node_exec_id = data.node_exec_id
    node_id = data.node_id

    def update_execution(status: ExecutionStatus) -> ExecutionResult:
        exec_update = db_client.update_execution_status(node_exec_id, status)
        db_client.send_execution_update(exec_update)
        return exec_update

    node = db_client.get_node(node_id)

    node_block = get_block(node.block_id)
    if not node_block:
        logger.error(f""Block {node.block_id} not found."")
        return

    log_metadata = LogMetadata(
        user_id=user_id,
        graph_eid=graph_exec_id,
        graph_id=graph_id,
        node_eid=node_exec_id,
        node_id=node_id,
        block_name=node_block.name,
    )

    # Sanity check: validate the execution input.
    input_data, error = validate_exec(node, data.data, resolve_input=False)
    if input_data is None:
        log_metadata.error(f""Skip execution, input validation error: {error}"")
        db_client.upsert_execution_output(node_exec_id, ""error"", error)
        update_execution(ExecutionStatus.FAILED)
        return

    # Re-shape the input data for agent block.
    # AgentExecutorBlock specially separate the node input_data & its input_default.
    if isinstance(node_block, AgentExecutorBlock):
        input_data = {**node.input_default, ""data"": input_data}

    # Execute the node
    input_data_str = json.dumps(input_data)
    input_size = len(input_data_str)
    log_metadata.info(""Executed node with input"", input=input_data_str)
    update_execution(ExecutionStatus.RUNNING)

    extra_exec_kwargs = {}
    # Last-minute fetch credentials + acquire a system-wide read-write lock to prevent
    # changes during execution. ⚠️ This means a set of credentials can only be used by
    # one (running) block at a time; simultaneous execution of blocks using same
    # credentials is not supported.
    creds_lock = None
    if CREDENTIALS_FIELD_NAME in input_data:
        credentials_meta = CredentialsMetaInput(**input_data[CREDENTIALS_FIELD_NAME])
        credentials, creds_lock = creds_manager.acquire(user_id, credentials_meta.id)
        extra_exec_kwargs[""credentials""] = credentials

    output_size = 0
    end_status = ExecutionStatus.COMPLETED
    credit = db_client.get_or_refill_credit(user_id)
    if credit < 0:
        raise ValueError(f""Insufficient credit: {credit}"")

    try:
        for output_name, output_data in node_block.execute(
            input_data, **extra_exec_kwargs
        ):
            output_size += len(json.dumps(output_data))
            log_metadata.info(""Node produced output"", **{output_name: output_data})
            db_client.upsert_execution_output(node_exec_id, output_name, output_data)

            for execution in _enqueue_next_nodes(
                db_client=db_client,
                node=node,
                output=(output_name, output_data),
                user_id=user_id,
                graph_exec_id=graph_exec_id,
                graph_id=graph_id,
                log_metadata=log_metadata,
            ):
                yield execution

    except Exception as e:
        end_status = ExecutionStatus.FAILED
        error_msg = str(e)
        log_metadata.exception(f""Node execution failed with error {error_msg}"")
        db_client.upsert_execution_output(node_exec_id, ""error"", error_msg)

        for execution in _enqueue_next_nodes(
            db_client=db_client,
            node=node,
            output=(""error"", error_msg),
            user_id=user_id,
            graph_exec_id=graph_exec_id,
            graph_id=graph_id,
            log_metadata=log_metadata,
        ):
            yield execution

        raise e
    finally:
        # Ensure credentials are released even if execution fails
        if creds_lock:
            try:
                creds_lock.release()
            except Exception as e:
                log_metadata.error(f""Failed to release credentials lock: {e}"")

        # Update execution status and spend credits
        res = update_execution(end_status)
        if end_status == ExecutionStatus.COMPLETED:
            s = input_size + output_size
            t = (
                (res.end_time - res.start_time).total_seconds()
                if res.end_time and res.start_time
                else 0
            )
            db_client.spend_credits(user_id, credit, node_block.id, input_data, s, t)

        # Update execution stats
        if execution_stats is not None:
            execution_stats.update(node_block.execution_stats)
            execution_stats[""input_size""] = input_size
            execution_stats[""output_size""] = output_size


def _enqueue_next_nodes(
    db_client: ""DatabaseManager"",
    node: Node,
    output: BlockData,
    user_id: str,
    graph_exec_id: str,
    graph_id: str,
    log_metadata: LogMetadata,
) -> list[NodeExecutionEntry]:
    def add_enqueued_execution(
        node_exec_id: str, node_id: str, data: BlockInput
    ) -> NodeExecutionEntry:
        exec_update = db_client.update_execution_status(
            node_exec_id, ExecutionStatus.QUEUED, data
        )
        db_client.send_execution_update(exec_update)
        return NodeExecutionEntry(
            user_id=user_id,
            graph_exec_id=graph_exec_id,
            graph_id=graph_id,
            node_exec_id=node_exec_id,
            node_id=node_id,
            data=data,
        )

    def register_next_executions(node_link: Link) -> list[NodeExecutionEntry]:
        enqueued_executions = []
        next_output_name = node_link.source_name
        next_input_name = node_link.sink_name
        next_node_id = node_link.sink_id

        next_data = parse_execution_output(output, next_output_name)
        if next_data is None:
            return enqueued_executions

        next_node = db_client.get_node(next_node_id)

        # Multiple node can register the same next node, we need this to be atomic
        # To avoid same execution to be enqueued multiple times,
        # Or the same input to be consumed multiple times.
        with synchronized(f""upsert_input-{next_node_id}-{graph_exec_id}""):
            # Add output data to the earliest incomplete execution, or create a new one.
            next_node_exec_id, next_node_input = db_client.upsert_execution_input(
                node_id=next_node_id,
                graph_exec_id=graph_exec_id,
                input_name=next_input_name,
                input_data=next_data,
            )

            # Complete missing static input pins data using the last execution input.
            static_link_names = {
                link.sink_name
                for link in next_node.input_links
                if link.is_static and link.sink_name not in next_node_input
            }
            if static_link_names and (
                latest_execution := db_client.get_latest_execution(
                    next_node_id, graph_exec_id
                )
            ):
                for name in static_link_names:
                    next_node_input[name] = latest_execution.input_data.get(name)

            # Validate the input data for the next node.
            next_node_input, validation_msg = validate_exec(next_node, next_node_input)
            suffix = f""{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}""

            # Incomplete input data, skip queueing the execution.
            if not next_node_input:
                log_metadata.warning(f""Skipped queueing {suffix}"")
                return enqueued_executions

            # Input is complete, enqueue the execution.
            log_metadata.info(f""Enqueued {suffix}"")
            enqueued_executions.append(
                add_enqueued_execution(next_node_exec_id, next_node_id, next_node_input)
            )

            # Next execution stops here if the link is not static.
            if not node_link.is_static:
                return enqueued_executions

            # If link is static, there could be some incomplete executions waiting for it.
            # Load and complete the input missing input data, and try to re-enqueue them.
            for iexec in db_client.get_incomplete_executions(
                next_node_id, graph_exec_id
            ):
                idata = iexec.input_data
                ineid = iexec.node_exec_id

                static_link_names = {
                    link.sink_name
                    for link in next_node.input_links
                    if link.is_static and link.sink_name not in idata
                }
                for input_name in static_link_names:
                    idata[input_name] = next_node_input[input_name]

                idata, msg = validate_exec(next_node, idata)
                suffix = f""{next_output_name}>{next_input_name}~{ineid}:{msg}""
                if not idata:
                    log_metadata.info(f""Enqueueing static-link skipped: {suffix}"")
                    continue
                log_metadata.info(f""Enqueueing static-link execution {suffix}"")
                enqueued_executions.append(
                    add_enqueued_execution(iexec.node_exec_id, next_node_id, idata)
                )
            return enqueued_executions

    return [
        execution
        for link in node.output_links
        for execution in register_next_executions(link)
    ]


def validate_exec(
    node: Node,
    data: BlockInput,
    resolve_input: bool = True,
) -> tuple[BlockInput | None, str]:
    """"""
    Validate the input data for a node execution.

    Args:
        node: The node to execute.
        data: The input data for the node execution.
        resolve_input: Whether to resolve dynamic pins into dict/list/object.

    Returns:
        A tuple of the validated data and the block name.
        If the data is invalid, the first element will be None, and the second element
        will be an error message.
        If the data is valid, the first element will be the resolved input data, and
        the second element will be the block name.
    """"""
    node_block: Block | None = get_block(node.block_id)
    if not node_block:
        return None, f""Block for {node.block_id} not found.""

    if isinstance(node_block, AgentExecutorBlock):
        # Validate the execution metadata for the agent executor block.
        try:
            exec_data = AgentExecutorBlock.Input(**node.input_default)
        except Exception as e:
            return None, f""Input data doesn't match {node_block.name}: {str(e)}""

        # Validation input
        input_schema = exec_data.input_schema
        required_fields = set(input_schema[""required""])
        input_default = exec_data.data
    else:
        # Convert non-matching data types to the expected input schema.
        for name, data_type in node_block.input_schema.__annotations__.items():
            if (value := data.get(name)) and (type(value) is not data_type):
                data[name] = convert(value, data_type)

        # Validation input
        input_schema = node_block.input_schema.jsonschema()
        required_fields = node_block.input_schema.get_required_fields()
        input_default = node.input_default

    # Input data (without default values) should contain all required fields.
    error_prefix = f""Input data missing or mismatch for `{node_block.name}`:""
    input_fields_from_nodes = {link.sink_name for link in node.input_links}
    if not input_fields_from_nodes.issubset(data):
        return None, f""{error_prefix} {input_fields_from_nodes - set(data)}""

    # Merge input data with default values and resolve dynamic dict/list/object pins.
    data = {**input_default, **data}
    if resolve_input:
        data = merge_execution_input(data)

    # Input data post-merge should contain all required fields from the schema.
    if not required_fields.issubset(data):
        return None, f""{error_prefix} {required_fields - set(data)}""

    # Last validation: Validate the input values against the schema.
    if error := json.validate_with_jsonschema(schema=input_schema, data=data):
        error_message = f""{error_prefix} {error}""
        logger.error(error_message)
        return None, error_message

    return data, node_block.name


class Executor:
    """"""
    This class contains event handlers for the process pool executor events.

    The main events are:
        on_node_executor_start: Initialize the process that executes the node.
        on_node_execution: Execution logic for a node.

        on_graph_executor_start: Initialize the process that executes the graph.
        on_graph_execution: Execution logic for a graph.

    The execution flow:
        1. Graph execution request is added to the queue.
        2. Graph executor loop picks the request from the queue.
        3. Graph executor loop submits the graph execution request to the executor pool.
      [on_graph_execution]
        4. Graph executor initialize the node execution queue.
        5. Graph executor adds the starting nodes to the node execution queue.
        6. Graph executor waits for all nodes to be executed.
      [on_node_execution]
        7. Node executor picks the node execution request from the queue.
        8. Node executor executes the node.
        9. Node executor enqueues the next executed nodes to the node execution queue.
    """"""

    @classmethod
    def on_node_executor_start(cls):
        configure_logging()
        set_service_name(""NodeExecutor"")
        redis.connect()
        cls.pid = os.getpid()
        cls.db_client = get_db_client()
        cls.creds_manager = IntegrationCredentialsManager()

        # Set up shutdown handlers
        cls.shutdown_lock = threading.Lock()
        atexit.register(cls.on_node_executor_stop)  # handle regular shutdown
        signal.signal(  # handle termination
            signal.SIGTERM, lambda _, __: cls.on_node_executor_sigterm()
        )

    @classmethod
    def on_node_executor_stop(cls):
        if not cls.shutdown_lock.acquire(blocking=False):
            return  # already shutting down

        logger.info(f""[on_node_executor_stop {cls.pid}] ⏳ Releasing locks..."")
        cls.creds_manager.release_all_locks()
        logger.info(f""[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis..."")
        redis.disconnect()
        logger.info(f""[on_node_executor_stop {cls.pid}] ⏳ Disconnecting DB manager..."")
        close_service_client(cls.db_client)
        logger.info(f""[on_node_executor_stop {cls.pid}] ✅ Finished cleanup"")

    @classmethod
    def on_node_executor_sigterm(cls):
        llprint(f""[on_node_executor_sigterm {cls.pid}] ⚠️ SIGTERM received"")
        if not cls.shutdown_lock.acquire(blocking=False):
            return  # already shutting down

        llprint(f""[on_node_executor_stop {cls.pid}] ⏳ Releasing locks..."")
        cls.creds_manager.release_all_locks()
        llprint(f""[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis..."")
        redis.disconnect()
        llprint(f""[on_node_executor_stop {cls.pid}] ✅ Finished cleanup"")
        sys.exit(0)

    @classmethod
    @error_logged
    def on_node_execution(
        cls,
        q: ExecutionQueue[NodeExecutionEntry],
        node_exec: NodeExecutionEntry,
    ) -> dict[str, Any]:
        log_metadata = LogMetadata(
            user_id=node_exec.user_id,
            graph_eid=node_exec.graph_exec_id,
            graph_id=node_exec.graph_id,
            node_eid=node_exec.node_exec_id,
            node_id=node_exec.node_id,
            block_name=""-"",
        )

        execution_stats = {}
        timing_info, _ = cls._on_node_execution(
            q, node_exec, log_metadata, execution_stats
        )
        execution_stats[""walltime""] = timing_info.wall_time
        execution_stats[""cputime""] = timing_info.cpu_time

        cls.db_client.update_node_execution_stats(
            node_exec.node_exec_id, execution_stats
        )
        return execution_stats

    @classmethod
    @time_measured
    def _on_node_execution(
        cls,
        q: ExecutionQueue[NodeExecutionEntry],
        node_exec: NodeExecutionEntry,
        log_metadata: LogMetadata,
        stats: dict[str, Any] | None = None,
    ):
        try:
            log_metadata.info(f""Start node execution {node_exec.node_exec_id}"")
            for execution in execute_node(
                cls.db_client, cls.creds_manager, node_exec, stats
            ):
                q.add(execution)
            log_metadata.info(f""Finished node execution {node_exec.node_exec_id}"")
        except Exception as e:
            log_metadata.exception(
                f""Failed node execution {node_exec.node_exec_id}: {e}""
            )

    @classmethod
    def on_graph_executor_start(cls):
        configure_logging()
        set_service_name(""GraphExecutor"")

        cls.db_client = get_db_client()
        cls.pool_size = settings.config.num_node_workers
        cls.pid = os.getpid()
        cls._init_node_executor_pool()
        logger.info(
            f""Graph executor {cls.pid} started with {cls.pool_size} node workers""
        )

        # Set up shutdown handler
        atexit.register(cls.on_graph_executor_stop)

    @classmethod
    def on_graph_executor_stop(cls):
        prefix = f""[on_graph_executor_stop {cls.pid}]""
        logger.info(f""{prefix} ⏳ Terminating node executor pool..."")
        cls.executor.terminate()
        logger.info(f""{prefix} ⏳ Disconnecting DB manager..."")
        close_service_client(cls.db_client)
        logger.info(f""{prefix} ✅ Finished cleanup"")

    @classmethod
    def _init_node_executor_pool(cls):
        cls.executor = Pool(
            processes=cls.pool_size,
            initializer=cls.on_node_executor_start,
        )

    @classmethod
    @error_logged
    def on_graph_execution(
        cls, graph_exec: GraphExecutionEntry, cancel: threading.Event
    ):
        log_metadata = LogMetadata(
            user_id=graph_exec.user_id,
            graph_eid=graph_exec.graph_exec_id,
            graph_id=graph_exec.graph_id,
            node_id=""*"",
            node_eid=""*"",
            block_name=""-"",
        )
        timing_info, (exec_stats, error) = cls._on_graph_execution(
            graph_exec, cancel, log_metadata
        )
        exec_stats[""walltime""] = timing_info.wall_time
        exec_stats[""cputime""] = timing_info.cpu_time
        exec_stats[""error""] = str(error) if error else None
        result = cls.db_client.update_graph_execution_stats(
            graph_exec_id=graph_exec.graph_exec_id,
            stats=exec_stats,
        )
        cls.db_client.send_execution_update(result)

    @classmethod
    @time_measured
    def _on_graph_execution(
        cls,
        graph_exec: GraphExecutionEntry,
        cancel: threading.Event,
        log_metadata: LogMetadata,
    ) -> tuple[dict[str, Any], Exception | None]:
        """"""
        Returns:
            The execution statistics of the graph execution.
            The error that occurred during the execution.
        """"""
        log_metadata.info(f""Start graph execution {graph_exec.graph_exec_id}"")
        exec_stats = {
            ""nodes_walltime"": 0,
            ""nodes_cputime"": 0,
            ""node_count"": 0,
        }
        error = None
        finished = False

        def cancel_handler():
            while not cancel.is_set():
                cancel.wait(1)
            if finished:
                return
            cls.executor.terminate()
            log_metadata.info(f""Terminated graph execution {graph_exec.graph_exec_id}"")
            cls._init_node_executor_pool()

        cancel_thread = threading.Thread(target=cancel_handler)
        cancel_thread.start()

        try:
            queue = ExecutionQueue[NodeExecutionEntry]()
            for node_exec in graph_exec.start_node_execs:
                queue.add(node_exec)

            running_executions: dict[str, AsyncResult] = {}

            def make_exec_callback(exec_data: NodeExecutionEntry):
                node_id = exec_data.node_id

                def callback(result: object):
                    running_executions.pop(node_id)
                    nonlocal exec_stats
                    if isinstance(result, dict):
                        exec_stats[""node_count""] += 1
                        exec_stats[""nodes_cputime""] += result.get(""cputime"", 0)
                        exec_stats[""nodes_walltime""] += result.get(""walltime"", 0)

                return callback

            while not queue.empty():
                if cancel.is_set():
                    error = RuntimeError(""Execution is cancelled"")
                    return exec_stats, error

                exec_data = queue.get()

                # Avoid parallel execution of the same node.
                execution = running_executions.get(exec_data.node_id)
                if execution and not execution.ready():
                    # TODO (performance improvement):
                    #   Wait for the completion of the same node execution is blocking.
                    #   To improve this we need a separate queue for each node.
                    #   Re-enqueueing the data back to the queue will disrupt the order.
                    execution.wait()

                log_metadata.debug(
                    f""Dispatching node execution {exec_data.node_exec_id} ""
                    f""for node {exec_data.node_id}"",
                )
                running_executions[exec_data.node_id] = cls.executor.apply_async(
                    cls.on_node_execution,
                    (queue, exec_data),
                    callback=make_exec_callback(exec_data),
                )

                # Avoid terminating graph execution when some nodes are still running.
                while queue.empty() and running_executions:
                    log_metadata.debug(
                        f""Queue empty; running nodes: {list(running_executions.keys())}""
                    )
                    for node_id, execution in list(running_executions.items()):
                        if cancel.is_set():
                            error = RuntimeError(""Execution is cancelled"")
                            return exec_stats, error

                        if not queue.empty():
                            break  # yield to parent loop to execute new queue items

                        log_metadata.debug(f""Waiting on execution of node {node_id}"")
                        execution.wait(3)

            log_metadata.info(f""Finished graph execution {graph_exec.graph_exec_id}"")
        except Exception as e:
            log_metadata.exception(
                f""Failed graph execution {graph_exec.graph_exec_id}: {e}""
            )
            error = e
        finally:
            if not cancel.is_set():
                finished = True
                cancel.set()
            cancel_thread.join()
            return exec_stats, error


class ExecutionManager(AppService):
    def __init__(self):
        super().__init__()
        self.use_redis = True
        self.use_supabase = True
        self.pool_size = settings.config.num_graph_workers
        self.queue = ExecutionQueue[GraphExecutionEntry]()
        self.active_graph_runs: dict[str, tuple[Future, threading.Event]] = {}

    @classmethod
    def get_port(cls) -> int:
        return settings.config.execution_manager_port

    def run_service(self):
        from backend.integrations.credentials_store import IntegrationCredentialsStore

        self.credentials_store = IntegrationCredentialsStore()
        self.executor = ProcessPoolExecutor(
            max_workers=self.pool_size,
            initializer=Executor.on_graph_executor_start,
        )
        sync_manager = multiprocessing.Manager()
        logger.info(
            f""[{self.service_name}] Started with max-{self.pool_size} graph workers""
        )
        while True:
            graph_exec_data = self.queue.get()
            graph_exec_id = graph_exec_data.graph_exec_id
            logger.debug(
                f""[ExecutionManager] Dispatching graph execution {graph_exec_id}""
            )
            cancel_event = sync_manager.Event()
            future = self.executor.submit(
                Executor.on_graph_execution, graph_exec_data, cancel_event
            )
            self.active_graph_runs[graph_exec_id] = (future, cancel_event)
            future.add_done_callback(
                lambda _: self.active_graph_runs.pop(graph_exec_id, None)
            )

    def cleanup(self):
        logger.info(f""[{__class__.__name__}] ⏳ Shutting down graph executor pool..."")
        self.executor.shutdown(cancel_futures=True)

        super().cleanup()

    @property
    def db_client(self) -> ""DatabaseManager"":
        return get_db_client()

    @expose
    def add_execution(
        self,
        graph_id: str,
        data: BlockInput,
        user_id: str,
        graph_version: int | None = None,
    ) -> GraphExecutionEntry:
        graph: GraphModel | None = self.db_client.get_graph(
            graph_id=graph_id, user_id=user_id, version=graph_version
        )
        if not graph:
            raise ValueError(f""Graph #{graph_id} not found."")

        graph.validate_graph(for_run=True)
        self._validate_node_input_credentials(graph, user_id)

        nodes_input = []
        for node in graph.starting_nodes:
            input_data = {}
            block = get_block(node.block_id)

            # Invalid block & Note block should never be executed.
            if not block or block.block_type == BlockType.NOTE:
                continue

            # Extract request input data, and assign it to the input pin.
            if block.block_type == BlockType.INPUT:
                name = node.input_default.get(""name"")
                if name and name in data:
                    input_data = {""value"": data[name]}

            # Extract webhook payload, and assign it to the input pin
            webhook_payload_key = f""webhook_{node.webhook_id}_payload""
            if (
                block.block_type in (BlockType.WEBHOOK, BlockType.WEBHOOK_MANUAL)
                and node.webhook_id
            ):
                if webhook_payload_key not in data:
                    raise ValueError(
                        f""Node {block.name} #{node.id} webhook payload is missing""
                    )
                input_data = {""payload"": data[webhook_payload_key]}

            input_data, error = validate_exec(node, input_data)
            if input_data is None:
                raise ValueError(error)
            else:
                nodes_input.append((node.id, input_data))

        graph_exec_id, node_execs = self.db_client.create_graph_execution(
            graph_id=graph_id,
            graph_version=graph.version,
            nodes_input=nodes_input,
            user_id=user_id,
        )

        starting_node_execs = []
        for node_exec in node_execs:
            starting_node_execs.append(
                NodeExecutionEntry(
                    user_id=user_id,
                    graph_exec_id=node_exec.graph_exec_id,
                    graph_id=node_exec.graph_id,
                    node_exec_id=node_exec.node_exec_id,
                    node_id=node_exec.node_id,
                    data=node_exec.input_data,
                )
            )
            exec_update = self.db_client.update_execution_status(
                node_exec.node_exec_id, ExecutionStatus.QUEUED, node_exec.input_data
            )
            self.db_client.send_execution_update(exec_update)

        graph_exec = GraphExecutionEntry(
            user_id=user_id,
            graph_id=graph_id,
            graph_exec_id=graph_exec_id,
            start_node_execs=starting_node_execs,
        )
        self.queue.add(graph_exec)

        return graph_exec

    @expose
    def cancel_execution(self, graph_exec_id: str) -> None:
        """"""
        Mechanism:
        1. Set the cancel event
        2. Graph executor's cancel handler thread detects the event, terminates workers,
           reinitializes worker pool, and returns.
        3. Update execution statuses in DB and set `error` outputs to `""TERMINATED""`.
        """"""
        if graph_exec_id not in self.active_graph_runs:
            raise Exception(
                f""Graph execution #{graph_exec_id} not active/running: ""
                ""possibly already completed/cancelled.""
            )

        future, cancel_event = self.active_graph_runs[graph_exec_id]
        if cancel_event.is_set():
            return

        cancel_event.set()
        future.result()

        # Update the status of the unfinished node executions
        node_execs = self.db_client.get_execution_results(graph_exec_id)
        for node_exec in node_execs:
            if node_exec.status not in (
                ExecutionStatus.COMPLETED,
                ExecutionStatus.FAILED,
            ):
                self.db_client.upsert_execution_output(
                    node_exec.node_exec_id, ""error"", ""TERMINATED""
                )
                exec_update = self.db_client.update_execution_status(
                    node_exec.node_exec_id, ExecutionStatus.FAILED
                )
                self.db_client.send_execution_update(exec_update)

    def _validate_node_input_credentials(self, graph: GraphModel, user_id: str):
        """"""Checks all credentials for all nodes of the graph""""""

        for node in graph.nodes:
            block = get_block(node.block_id)
            if not block:
                raise ValueError(f""Unknown block {node.block_id} for node #{node.id}"")

            # Find any fields of type CredentialsMetaInput
            model_fields = cast(type[BaseModel], block.input_schema).model_fields
            if CREDENTIALS_FIELD_NAME not in model_fields:
                continue

            field = model_fields[CREDENTIALS_FIELD_NAME]

            # The BlockSchema class enforces that a `credentials` field is always a
            # `CredentialsMetaInput`, so we can safely assume this here.
            credentials_meta_type = cast(CredentialsMetaInput, field.annotation)
            credentials_meta = credentials_meta_type.model_validate(
                node.input_default[CREDENTIALS_FIELD_NAME]
            )
            # Fetch the corresponding Credentials and perform sanity checks
            credentials = self.credentials_store.get_creds_by_id(
                user_id, credentials_meta.id
            )
            if not credentials:
                raise ValueError(
                    f""Unknown credentials #{credentials_meta.id} ""
                    f""for node #{node.id}""
                )
            if (
                credentials.provider != credentials_meta.provider
                or credentials.type != credentials_meta.type
            ):
                logger.warning(
                    f""Invalid credentials #{credentials.id} for node #{node.id}: ""
                    ""type/provider mismatch: ""
                    f""{credentials_meta.type}<>{credentials.type};""
                    f""{credentials_meta.provider}<>{credentials.provider}""
                )
                raise ValueError(
                    f""Invalid credentials #{credentials.id} for node #{node.id}: ""
                    ""type/provider mismatch""
                )


# ------- UTILITIES ------- #


@thread_cached
def get_db_client() -> ""DatabaseManager"":
    from backend.executor import DatabaseManager

    return get_service_client(DatabaseManager)


@contextmanager
def synchronized(key: str, timeout: int = 60):
    lock: RedisLock = redis.get_redis().lock(f""lock:{key}"", timeout=timeout)
    try:
        lock.acquire()
        yield
    finally:
        if lock.locked():
            lock.release()


def llprint(message: str):
    """"""
    Low-level print/log helper function for use in signal handlers.
    Regular log/print statements are not allowed in signal handlers.
    """"""
    if logger.getEffectiveLevel() == logging.DEBUG:
        os.write(sys.stdout.fileno(), (message + ""\n"").encode())
","import atexit
import logging
import multiprocessing
import os
import signal
import sys
import threading
from concurrent.futures import Future, ProcessPoolExecutor
from contextlib import contextmanager
from multiprocessing.pool import AsyncResult, Pool
from typing import TYPE_CHECKING, Any, Generator, TypeVar, cast

from pydantic import BaseModel
from redis.lock import Lock as RedisLock

if TYPE_CHECKING:
    from backend.executor import DatabaseManager

from autogpt_libs.utils.cache import thread_cached

from backend.blocks.agent import AgentExecutorBlock
from backend.data import redis
from backend.data.block import Block, BlockData, BlockInput, BlockType, get_block

from backend.data.execution import (
    ExecutionQueue,
    ExecutionResult,
    ExecutionStatus,
    GraphExecutionEntry,
    NodeExecutionEntry,
    merge_execution_input,
    parse_execution_output,
)

from backend.data.graph import GraphModel, Link, Node
from backend.data.model import CREDENTIALS_FIELD_NAME, CredentialsMetaInput
from backend.integrations.creds_manager import IntegrationCredentialsManager

from backend.util import json
from backend.util.decorator import error_logged, time_measured
from backend.util.logging import configure_logging

from backend.util.process import set_service_name
from backend.util.service import (
    AppService,
    close_service_client,
    expose,
    get_service_client,
)

from backend.util.settings import Settings
from backend.util.type import convert

logger = logging.getLogger(__name__)
settings = Settings()

class LogMetadata:
    def __init__(
        self,
        user_id: str,
        graph_eid: str,
        graph_id: str,
        node_eid: str,
        node_id: str,
        block_name: str,
    ):
        self.metadata = {
            ""component"": ""ExecutionManager"",
            ""user_id"": user_id,
            ""graph_eid"": graph_eid,
            ""graph_id"": graph_id,
            ""node_eid"": node_eid,
            ""node_id"": node_id,
            ""block_name"": block_name,
        }
        self.prefix = f""[ExecutionManager|uid:{user_id}|gid:{graph_id}|nid:{node_id}]|geid:{graph_eid}|nid:{node_eid}|{block_name}]""

    def info(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.info(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def warning(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.warning(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def error(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.error(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def debug(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.debug(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def exception(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.exception(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def _wrap(self, msg: str, **extra):
        return f""{self.prefix} {msg} {extra}""

T = TypeVar(""T"")
ExecutionStream = Generator[NodeExecutionEntry, None, None]

def execute_node(
    db_client: ""DatabaseManager"",
    creds_manager: IntegrationCredentialsManager,
    data: NodeExecutionEntry,
    execution_stats: dict[str, Any] | None = None,
) -> ExecutionStream:
    """"""
    Execute a node in the graph. This will trigger a block execution on a node,
    persist the execution result, and return the subsequent node to be executed.

    Args:
        db_client: The client to send execution updates to the server.
        creds_manager: The manager to acquire and release credentials.
        data: The execution data for executing the current node.
        execution_stats: The execution statistics to be updated.

    Returns:
        The subsequent node to be enqueued, or None if there is no subsequent node.
    """"""

    user_id = data.user_id
    graph_exec_id = data.graph_exec_id
    graph_id = data.graph_id
    node_exec_id = data.node_exec_id
    node_id = data.node_id

    def update_execution(status: ExecutionStatus) -> ExecutionResult:
        exec_update = db_client.update_execution_status(node_exec_id, status)
        db_client.send_execution_update(exec_update)
        return exec_update

    node = db_client.get_node(node_id)

    node_block = get_block(node.block_id)
    if not node_block:
        logger.error(f""Block {node.block_id} not found."")
        return

    log_metadata = LogMetadata(
        user_id=user_id,
        graph_eid=graph_exec_id,
        graph_id=graph_id,
        node_eid=node_exec_id,
        node_id=node_id,
        block_name=node_block.name,
    )

    # Sanity check: validate the execution input.
    input_data, error = validate_exec(node, data.data, resolve_input=False)
    if input_data is None:
        log_metadata.error(f""Skip execution, input validation error: {error}"")
        db_client.upsert_execution_output(node_exec_id, ""error"", error)
        update_execution(ExecutionStatus.FAILED)
        return

    # Re-shape the input data for agent block.
    # AgentExecutorBlock specially separate the node input_data & its input_default.
    if isinstance(node_block, AgentExecutorBlock):
        input_data = {**node.input_default, ""data"": input_data}

    # Execute the node
    input_data_str = json.dumps(input_data)
    input_size = len(input_data_str)
    log_metadata.info(""Executed node with input"", input=input_data_str)
    update_execution(ExecutionStatus.RUNNING)

    extra_exec_kwargs = {}
    # Last-minute fetch credentials + acquire a system-wide read-write lock to prevent
    # changes during execution. ⚠️ This means a set of credentials can only be used by
    # one (running) block at a time; simultaneous execution of blocks using same
    # credentials is not supported.
    creds_lock = None
    if CREDENTIALS_FIELD_NAME in input_data:
        credentials_meta = CredentialsMetaInput(**input_data[CREDENTIALS_FIELD_NAME])
        credentials, creds_lock = creds_manager.acquire(user_id, credentials_meta.id)
        extra_exec_kwargs[""credentials""] = credentials

    output_size = 0
    end_status = ExecutionStatus.COMPLETED
    credit = db_client.get_or_refill_credit(user_id)
    if credit < 0:
        raise ValueError(f""Insufficient credit: {credit}"")

    try:
        for output_name, output_data in node_block.execute(
            input_data, **extra_exec_kwargs
        ):
            output_size += len(json.dumps(output_data))
            log_metadata.info(""Node produced output"", **{output_name: output_data})
            db_client.upsert_execution_output(node_exec_id, output_name, output_data)

            for execution in _enqueue_next_nodes(
                db_client=db_client,
                node=node,
                output=(output_name, output_data),
                user_id=user_id,
                graph_exec_id=graph_exec_id,
                graph_id=graph_id,
                log_metadata=log_metadata,
            ):
                yield execution

    except Exception as e:
        end_status = ExecutionStatus.FAILED
        error_msg = str(e)
        log_metadata.exception(f""Node execution failed with error {error_msg}"")
        db_client.upsert_execution_output(node_exec_id, ""error"", error_msg)

        for execution in _enqueue_next_nodes(
            db_client=db_client,
            node=node,
            output=(""error"", error_msg),
            user_id=user_id,
            graph_exec_id=graph_exec_id,
            graph_id=graph_id,
            log_metadata=log_metadata,
        ):
            yield execution

        raise e
    finally:
        # Ensure credentials are released even if execution fails
        if creds_lock:
            try:
                creds_lock.release()
            except Exception as e:
                log_metadata.error(f""Failed to release credentials lock: {e}"")

        # Update execution status and spend credits
        res = update_execution(end_status)
        if end_status == ExecutionStatus.COMPLETED:
            s = input_size + output_size
            t = (
                (res.end_time - res.start_time).total_seconds()
                if res.end_time and res.start_time
                else 0
            )
            db_client.spend_credits(user_id, credit, node_block.id, input_data, s, t)

        # Update execution stats
        if execution_stats is not None:
            execution_stats.update(node_block.execution_stats)
            execution_stats[""input_size""] = input_size
            execution_stats[""output_size""] = output_size

def _enqueue_next_nodes(
    db_client: ""DatabaseManager"",
    node: Node,
    output: BlockData,
    user_id: str,
    graph_exec_id: str,
    graph_id: str,
    log_metadata: LogMetadata,
) -> list[NodeExecutionEntry]:
    def add_enqueued_execution(
        node_exec_id: str, node_id: str, data: BlockInput
    ) -> NodeExecutionEntry:
        exec_update = db_client.update_execution_status(
            node_exec_id, ExecutionStatus.QUEUED, data
        )
        db_client.send_execution_update(exec_update)
        return NodeExecutionEntry(
            user_id=user_id,
            graph_exec_id=graph_exec_id,
            graph_id=graph_id,
            node_exec_id=node_exec_id,
            node_id=node_id,
            data=data,
        )

    def register_next_executions(node_link: Link) -> list[NodeExecutionEntry]:
        enqueued_executions = []
        next_output_name = node_link.source_name
        next_input_name = node_link.sink_name
        next_node_id = node_link.sink_id

        next_data = parse_execution_output(output, next_output_name)
        if next_data is None:
            return enqueued_executions

        next_node = db_client.get_node(next_node_id)

        with synchronized(f""upsert_input-{next_node_id}-{graph_exec_id}""):
            # Add output data to the earliest incomplete execution, or create a new one.
            next_node_exec_id, next_node_input = db_client.upsert_execution_input(
                node_id=next_node_id,
                graph_exec_id=graph_exec_id,
                input_name=next_input_name,
                input_data=next_data,
            )

            static_link_names = {
                link.sink_name
                for link in next_node.input_links
                if link.is_static and link.sink_name not in next_node_input
            }

            if static_link_names and (
                latest_execution := db_client.get_latest_execution(
                    next_node_id, graph_exec_id
                )
            ):
                for name in static_link_names:
                    next_node_input[name] = latest_execution.input_data.get(name)

            # Validate the input data for the next node.
            next_node_input, validation_msg = validate_exec(next_node, next_node_input)
            suffix = f""{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}""

            # Incomplete input data, skip queueing the execution.
            if not next_node_input:
                log_metadata.warning(f""Skipped queueing {suffix}"")
                return enqueued_executions

            # Input is complete, enqueue the execution.
            log_metadata.info(f""Enqueued {suffix}"")
            enqueued_executions.append(
                add_enqueued_execution(next_node_exec_id, next_node_id, next_node_input)
            )

            # Next execution stops here if the link is not static.
            if not node_link.is_static:
                return enqueued_executions

            # If link is static, there could be some incomplete executions waiting for it.
            # Load and complete the input missing input data, and try to re-enqueue them.
            for iexec in db_client.get_incomplete_executions(
                next_node_id, graph_exec_id
            ):
                idata = iexec.input_data
                ineid = iexec.node_exec_id

                static_link_names = {
                    link.sink_name
                    for link in next_node.input_links
                    if link.is_static and link.sink_name not in idata
                }
                for input_name in static_link_names:
                    idata[input_name] = next_node_input[input_name]

                idata, msg = validate_exec(next_node, idata)
                suffix = f""{next_output_name}>{next_input_name}~{ineid}:{msg}""
                if not idata:
                    log_metadata.info(f""Enqueueing static-link skipped: {suffix}"")
                    continue
                log_metadata.info(f""Enqueueing static-link execution {suffix}"")
                enqueued_executions.append(
                    add_enqueued_execution(iexec.node_exec_id, next_node_id, idata)
                )
            return enqueued_executions

    return [
        execution
        for link in node.output_links
        for execution in register_next_executions(link)
    ]

def validate_exec(
    node: Node,
    data: BlockInput,
    resolve_input: bool = True,
) -> tuple[BlockInput | None, str]:
    """"""
    Validate the input data for a node execution.

    Args:
        node: The node to execute.
        data: The input data for the node execution.
        resolve_input: Whether to resolve dynamic pins into dict/list/object.

    Returns:
        A tuple of the validated data and the block name.
        If the data is invalid, the first element will be None, and the second element
        will be an error message.
        If the data is valid, the first element will be the resolved input data, and
        the second element will be the block name.
    """"""
    node_block: Block | None = get_block(node.block_id)
    if not node_block:
        return None, f""Block for {node.block_id} not found.""

    if isinstance(node_block, AgentExecutorBlock):
        # Validate the execution metadata for the agent executor block.
        try:
            exec_data = AgentExecutorBlock.Input(**node.input_default)
        except Exception as e:
            return None, f""Input data doesn't match {node_block.name}: {str(e)}""

        # Validation input
        input_schema = exec_data.input_schema
        required_fields = set(input_schema[""required""])
        input_default = exec_data.data

        # Convert non-matching data types to the expected input schema.
        for name, data_type in node_block.input_schema.__annotations__.items():
            if (value := data.get(name)) and (type(value) is not data_type):
                data[name] = convert(value, data_type)

    # Input data (without default values) should contain all required fields.
    error_prefix = f""Input data missing or mismatch for `{node_block.name}`:""
    input_fields_from_nodes = {link.sink_name for link in node.input_links}
    if not input_fields_from_nodes.issubset(data):
        return None, f""{error_prefix} {input_fields_from_nodes - set(data)}""

    # Merge input data with default values and resolve dynamic dict/list/object pins.
    data = {**input_default, **data}
    if resolve_input:
        data = merge_execution_input(data)

    # Input data post-merge should contain all required fields from the schema.
    if not required_fields.issubset(data):
        return None, f""{error_prefix} {required_fields - set(data)}""

    # Last validation: Validate the input values against the schema.
    if error := json.validate_with_jsonschema(schema=input_schema, data=data):
        error_message = f""{error_prefix} {error}""
        logger.error(error_message)
        return None, error_message

    return data, node_block.name

class Executor:
    """"""
    This class contains event handlers for the process pool executor events.

    The main events are:
        on_node_executor_start: Initialize the process that executes the node.
        on_node_execution: Execution logic for a node.

        on_graph_executor_start: Initialize the process that executes the graph.
        on_graph_execution: Execution logic for a graph.

    The execution flow:
        1. Graph execution request is added to the queue.
        2. Graph executor loop picks the request from the queue.
        3. Graph executor loop submits the graph execution request to the executor pool.
      [on_graph_execution]
        4. Graph executor initialize the node execution queue.
        5. Graph executor adds the starting nodes to the node execution queue.
        6. Graph executor waits for all nodes to be executed.
      [on_node_execution]
        7. Node executor picks the node execution request from the queue.
        8. Node executor executes the node.
        9. Node executor enqueues the next executed nodes to the node execution queue.
    """"""
    @classmethod
    def on_node_executor_start(cls):
        configure_logging()
        set_service_name(""NodeExecutor"")
        redis.connect()
        cls.pid = os.getpid()
        cls.db_client = get_db_client()
        cls.creds_manager = IntegrationCredentialsManager()

        # Set up shutdown handlers
        cls.shutdown_lock = threading.Lock()
        atexit.register(cls.on_node_executor_stop)  # handle regular shutdown
        signal.signal(  # handle termination
            signal.SIGTERM, lambda _, __: cls.on_node_executor_sigterm()
        )

    @classmethod
    def on_node_executor_stop(cls):
        if not cls.shutdown_lock.acquire(blocking=False):
            return  # already shutting down

        logger.info(f""[on_node_executor_stop {cls.pid}] ⏳ Releasing locks..."")
        cls.creds_manager.release_all_locks()
        logger.info(f""[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis..."")
        redis.disconnect()
        logger.info(f""[on_node_executor_stop {cls.pid}] ⏳ Disconnecting DB manager..."")
        close_service_client(cls.db_client)
        logger.info(f""[on_node_executor_stop {cls.pid}] ✅ Finished cleanup"")

    @classmethod
    def on_node_executor_sigterm(cls):
        llprint(f""[on_node_executor_sigterm {cls.pid}] ⚠️ SIGTERM received"")
        if not cls.shutdown_lock.acquire(blocking=False):
            return  # already shutting down

        llprint(f""[on_node_executor_stop {cls.pid}] ⏳ Releasing locks..."")
        cls.creds_manager.release_all_locks()
        llprint(f""[on_node_executor_stop {cls.pid}] ⏳ Disconnecting Redis..."")
        redis.disconnect()
        llprint(f""[on_node_executor_stop {cls.pid}] ✅ Finished cleanup"")
        sys.exit(0)

    @classmethod
    @error_logged
    def on_node_execution(
        cls,
        q: ExecutionQueue[NodeExecutionEntry],
        node_exec: NodeExecutionEntry,
    ) -> dict[str, Any]:
        log_metadata = LogMetadata(
            user_id=node_exec.user_id,
            graph_eid=node_exec.graph_exec_id,
            graph_id=node_exec.graph_id,
            node_eid=node_exec.node_exec_id,
            node_id=node_exec.node_id,
            block_name=""-"",
        )

        execution_stats = {}
        timing_info, _ = cls._on_node_execution(
            q, node_exec, log_metadata, execution_stats
        )
        execution_stats[""walltime""] = timing_info.wall_time
        execution_stats[""cputime""] = timing_info.cpu_time

        cls.db_client.update_node_execution_stats(
            node_exec.node_exec_id, execution_stats
        )
        return execution_stats

    @classmethod
    @time_measured
    def _on_node_execution(
        cls,
        q: ExecutionQueue[NodeExecutionEntry],
        node_exec: NodeExecutionEntry,
        log_metadata: LogMetadata,
        stats: dict[str, Any] | None = None,
    ):
        try:
            log_metadata.info(f""Start node execution {node_exec.node_exec_id}"")
            for execution in execute_node(
                cls.db_client, cls.creds_manager, node_exec, stats
            ):
                q.add(execution)
            log_metadata.info(f""Finished node execution {node_exec.node_exec_id}"")
        except Exception as e:
            log_metadata.exception(
                f""Failed node execution {node_exec.node_exec_id}: {e}""
            )

    @classmethod
    def on_graph_executor_start(cls):
        configure_logging()
        set_service_name(""GraphExecutor"")

        cls.db_client = get_db_client()
        cls.pool_size = settings.config.num_node_workers
        cls.pid = os.getpid()
        cls._init_node_executor_pool()
        logger.info(
            f""Graph executor {cls.pid} started with {cls.pool_size} node workers""
        )

        # Set up shutdown handler
        atexit.register(cls.on_graph_executor_stop)

    @classmethod
    def on_graph_executor_stop(cls):
        prefix = f""[on_graph_executor_stop {cls.pid}]""
        logger.info(f""{prefix} ⏳ Terminating node executor pool..."")
        cls.executor.terminate()
        logger.info(f""{prefix} ⏳ Disconnecting DB manager..."")
        close_service_client(cls.db_client)
        logger.info(f""{prefix} ✅ Finished cleanup"")

    @classmethod
    def _init_node_executor_pool(cls):
        cls.executor = Pool(
            processes=cls.pool_size,
            initializer=cls.on_node_executor_start,
        )

    @classmethod
    @error_logged
    def on_graph_execution(
        cls, graph_exec: GraphExecutionEntry, cancel: threading.Event
    ):
        log_metadata = LogMetadata(
            user_id=graph_exec.user_id,
            graph_eid=graph_exec.graph_exec_id,
            graph_id=graph_exec.graph_id,
            node_id=""*"",
            node_eid=""*"",
            block_name=""-"",
        )
        timing_info, (exec_stats, error) = cls._on_graph_execution(
            graph_exec, cancel, log_metadata
        )
        exec_stats[""walltime""] = timing_info.wall_time
        exec_stats[""cputime""] = timing_info.cpu_time
        exec_stats[""error""] = str(error) if error else None
        result = cls.db_client.update_graph_execution_stats(
            graph_exec_id=graph_exec.graph_exec_id,
            stats=exec_stats,
        )
        cls.db_client.send_execution_update(result)

    @classmethod
    @time_measured
    def _on_graph_execution(
        cls,
        graph_exec: GraphExecutionEntry,
        cancel: threading.Event,
        log_metadata: LogMetadata,
    ) -> tuple[dict[str, Any], Exception | None]:
        """"""
        Returns:
            The execution statistics of the graph execution.
            The error that occurred during the execution.
        """"""
        log_metadata.info(f""Start graph execution {graph_exec.graph_exec_id}"")
        exec_stats = {
            ""nodes_walltime"": 0,
            ""nodes_cputime"": 0,
            ""node_count"": 0,
        }
        error = None
        finished = False

        def cancel_handler():
            while not cancel.is_set():
                cancel.wait(1)
            if finished:
                return
            cls.executor.terminate()
            log_metadata.info(f""Terminated graph execution {graph_exec.graph_exec_id}"")
            cls._init_node_executor_pool()

        cancel_thread = threading.Thread(target=cancel_handler)
        cancel_thread.start()

        try:
            queue = ExecutionQueue[NodeExecutionEntry]()
            for node_exec in graph_exec.start_node_execs:
                queue.add(node_exec)

            running_executions: dict[str, AsyncResult] = {}

            def make_exec_callback(exec_data: NodeExecutionEntry):
                node_id = exec_data.node_id

                def callback(result: object):
                    running_executions.pop(node_id)
                    nonlocal exec_stats
                    if isinstance(result, dict):
                        exec_stats[""node_count""] += 1
                        exec_stats[""nodes_cputime""] += result.get(""cputime"", 0)
                        exec_stats[""nodes_walltime""] += result.get(""walltime"", 0)

                return callback

            while not queue.empty():
                if cancel.is_set():
                    error = RuntimeError(""Execution is cancelled"")
                    return exec_stats, error

                exec_data = queue.get()

                # Avoid parallel execution of the same node.
                execution = running_executions.get(exec_data.node_id)
                if execution and not execution.ready():
                    # TODO (performance improvement):
                    #   Wait for the completion of the same node execution is blocking.
                    #   To improve this we need a separate queue for each node.
                    #   Re-enqueueing the data back to the queue will disrupt the order.
                    execution.wait()

                log_metadata.debug(
                    f""Dispatching node execution {exec_data.node_exec_id} ""
                    f""for node {exec_data.node_id}"",
                )
                running_executions[exec_data.node_id] = cls.executor.apply_async(
                    cls.on_node_execution,
                    (queue, exec_data),
                    callback=make_exec_callback(exec_data),
                )

                # Avoid terminating graph execution when some nodes are still running.
                while queue.empty() and running_executions:
                    log_metadata.debug(
                        f""Queue empty; running nodes: {list(running_executions.keys())}""
                    )
                    for node_id, execution in list(running_executions.items()):
                        if cancel.is_set():
                            error = RuntimeError(""Execution is cancelled"")
                            return exec_stats, error

                        if not queue.empty():
                            break  # yield to parent loop to execute new queue items

                        log_metadata.debug(f""Waiting on execution of node {node_id}"")
                        execution.wait(3)

            log_metadata.info(f""Finished graph execution {graph_exec.graph_exec_id}"")
        except Exception as e:
            log_metadata.exception(
                f""Failed graph execution {graph_exec.graph_exec_id}: {e}""
            )
            error = e
        finally:
            if not cancel.is_set():
                finished = True
                cancel.set()
            cancel_thread.join()
            return exec_stats, error

class ExecutionManager(AppService):
    def __init__(self):
        super().__init__()
        self.use_redis = True
        self.use_supabase = True
        self.pool_size = settings.config.num_graph_workers
        self.queue = ExecutionQueue[GraphExecutionEntry]()
        self.active_graph_runs: dict[str, tuple[Future, threading.Event]] = {}

    @classmethod
    def get_port(cls) -> int:
        return settings.config.execution_manager_port

    def run_service(self):
        from backend.integrations.credentials_store import IntegrationCredentialsStore

        self.credentials_store = IntegrationCredentialsStore()
        self.executor = ProcessPoolExecutor(
            max_workers=self.pool_size,
            initializer=Executor.on_graph_executor_start,
        )
        sync_manager = multiprocessing.Manager()
        logger.info(
            f""[{self.service_name}] Started with max-{self.pool_size} graph workers""
        )

        while True:
            graph_exec_data = self.queue.get()
            graph_exec_id = graph_exec_data.graph_exec_id
            logger.debug(
                f""[ExecutionManager] Dispatching graph execution {graph_exec_id}""
            )
            cancel_event = sync_manager.Event()
            future = self.executor.submit(
                Executor.on_graph_execution, graph_exec_data, cancel_event
            )
            self.active_graph_runs[graph_exec_id] = (future, cancel_event)
            future.add_done_callback(
                lambda _: self.active_graph_runs.pop(graph_exec_id, None)
            )

    def cleanup(self):
        logger.info(f""[{__class__.__name__}] ⏳ Shutting down graph executor pool..."")
        self.executor.shutdown(cancel_futures=True)

        super().cleanup()

    @property
    def db_client(self) -> ""DatabaseManager"":
        return get_db_client()

    @expose
    def add_execution(
        self,
        graph_id: str,
        data: BlockInput,
        user_id: str,
        graph_version: int | None = None,
    ) -> GraphExecutionEntry:
        graph: GraphModel | None = self.db_client.get_graph(
            graph_id=graph_id, user_id=user_id, version=graph_version
        )
        if not graph:
            raise ValueError(f""Graph #{graph_id} not found."")

        graph.validate_graph(for_run=True)
        self._validate_node_input_credentials(graph, user_id)

        nodes_input = []
        for node in graph.starting_nodes:
            input_data = {}
            block = get_block(node.block_id)

            # Invalid block & Note block should never be executed.
            if not block or block.block_type == BlockType.NOTE:
                continue

            # Extract request input data, and assign it to the input pin.
            if block.block_type == BlockType.INPUT:
                name = node.input_default.get(""name"")
                if name and name in data:
                    input_data = {""value"": data[name]}

            # Extract webhook payload, and assign it to the input pin
            webhook_payload_key = f""webhook_{node.webhook_id}_payload""
            if (
                block.block_type in (BlockType.WEBHOOK, BlockType.WEBHOOK_MANUAL)
                and node.webhook_id
            ):
                if webhook_payload_key not in data:
                    raise ValueError(
                        f""Node {block.name} #{node.id} webhook payload is missing""
                    )
                input_data = {""payload"": data[webhook_payload_key]}

            input_data, error = validate_exec(node, input_data)
            if input_data is None:
                raise ValueError(error)
            else:
                nodes_input.append((node.id, input_data))

        graph_exec_id, node_execs = self.db_client.create_graph_execution(
            graph_id=graph_id,
            graph_version=graph.version,
            nodes_input=nodes_input,
            user_id=user_id,
        )

        starting_node_execs = []
        for node_exec in node_execs:
            starting_node_execs.append(
                NodeExecutionEntry(
                    user_id=user_id,
                    graph_exec_id=node_exec.graph_exec_id,
                    graph_id=node_exec.graph_id,
                    node_exec_id=node_exec.node_exec_id,
                    node_id=node_exec.node_id,
                    data=node_exec.input_data,
                )
            )
            exec_update = self.db_client.update_execution_status(
                node_exec.node_exec_id, ExecutionStatus.QUEUED, node_exec.input_data
            )
            self.db_client.send_execution_update(exec_update)

        graph_exec = GraphExecutionEntry(
            user_id=user_id,
            graph_id=graph_id,
            graph_exec_id=graph_exec_id,
            start_node_execs=starting_node_execs,
        )
        self.queue.add(graph_exec)

        return graph_exec

    @expose
    def cancel_execution(self, graph_exec_id: str) -> None:
        """"""
        Mechanism:
        1. Set the cancel event
        2. Graph executor's cancel handler thread detects the event, terminates workers,
           reinitializes worker pool, and returns.
        3. Update execution statuses in DB and set `error` outputs to `""TERMINATED""`.
        """"""
        if graph_exec_id not in self.active_graph_runs:
            raise Exception(
                f""Graph execution #{graph_exec_id} not active/running: ""
                ""possibly already completed/cancelled.""
            )

        future, cancel_event = self.active_graph_runs[graph_exec_id]
        if cancel_event.is_set():
            return

        cancel_event.set()
        future.result()

        # Update the status of the unfinished node executions
        node_execs = self.db_client.get_execution_results(graph_exec_id)
        for node_exec in node_execs:
            if node_exec.status not in (
                ExecutionStatus.COMPLETED,
                ExecutionStatus.FAILED,
            ):
                self.db_client.upsert_execution_output(
                    node_exec.node_exec_id, ""error"", ""TERMINATED""
                )
                exec_update = self.db_client.update_execution_status(
                    node_exec.node_exec_id, ExecutionStatus.FAILED
                )
                self.db_client.send_execution_update(exec_update)

    def _validate_node_input_credentials(self, graph: GraphModel, user_id: str):
        """"""Checks all credentials for all nodes of the graph""""""

        for node in graph.nodes:
            block = get_block(node.block_id)
            if not block:
                raise ValueError(f""Unknown block {node.block_id} for node #{node.id}"")

            # Find any fields of type CredentialsMetaInput
            model_fields = cast(type[BaseModel], block.input_schema).model_fields
            if CREDENTIALS_FIELD_NAME not in model_fields:
                continue

            field = model_fields[CREDENTIALS_FIELD_NAME]

            # The BlockSchema class enforces that a `credentials` field is always a
            # `CredentialsMetaInput`, so we can safely assume this here.
            credentials_meta_type = cast(CredentialsMetaInput, field.annotation)
            credentials_meta = credentials_meta_type.model_validate(
                node.input_default[CREDENTIALS_FIELD_NAME]
            )
            # Fetch the corresponding Credentials and perform sanity checks
            credentials = self.credentials_store.get_creds_by_id(
                user_id, credentials_meta.id
            )
            if not credentials:
                raise ValueError(
                    f""Unknown credentials #{credentials_meta.id} ""
                    f""for node #{node.id}""
                )
            if (
                credentials.provider != credentials_meta.provider
                or credentials.type != credentials_meta.type
            ):
                logger.warning(
                    f""Invalid credentials #{credentials.id} for node #{node.id}: ""
                    ""type/provider mismatch: ""
                    f""{credentials_meta.type}<>{credentials.type};""
                    f""{credentials_meta.provider}<>{credentials.provider}""
                )
                raise ValueError(
                    f""Invalid credentials #{credentials.id} for node #{node.id}: ""
                    ""type/provider mismatch""
                )

# ------- UTILITIES ------- #

@thread_cached
def get_db_client() -> ""DatabaseManager"":
    from backend.executor import DatabaseManager

    return get_service_client(DatabaseManager)

@contextmanager
def synchronized(key: str, timeout: int = 60):
    lock: RedisLock = redis.get_redis().lock(f""lock:{key}"", timeout=timeout)
    try:
        lock.acquire()
        yield
    finally:
        if lock.locked():
            lock.release()

def llprint(message: str):
    """"""
    Low-level print/log helper function for use in signal handlers.
    Regular log/print statements are not allowed in signal handlers.
    """"""
    if logger.getEffectiveLevel() == logging.DEBUG:
        os.write(sys.stdout.fileno(), (message + ""\n"").encode())","{'final_score': 99.14, 'length_similarity': 98.59, 'token_similarity': 98.58, 'structure_similarity': 98.91, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
5,"""""""
The sum-of-subsetsproblem states that a set of non-negative integers, and a
value M, determine all possible subsets of the given set whose summation sum
equal to given M.

Summation of the chosen numbers must be equal to given number M and one number
can be used only once.
""""""

from __future__ import annotations


def generate_sum_of_subsets_soln(nums: list[int], max_sum: int) -> list[list[int]]:
    result: list[list[int]] = []
    path: list[int] = []
    num_index = 0
    remaining_nums_sum = sum(nums)
    create_state_space_tree(nums, max_sum, num_index, path, result, remaining_nums_sum)
    return result


def create_state_space_tree(
    nums: list[int],
    max_sum: int,
    num_index: int,
    path: list[int],
    result: list[list[int]],
    remaining_nums_sum: int,
) -> None:
    """"""
    Creates a state space tree to iterate through each branch using DFS.
    It terminates the branching of a node when any of the two conditions
    given below satisfy.
    This algorithm follows depth-fist-search and backtracks when the node is not
    branchable.

    """"""
    if sum(path) > max_sum or (remaining_nums_sum + sum(path)) < max_sum:
        return
    if sum(path) == max_sum:
        result.append(path)
        return
    for index in range(num_index, len(nums)):
        create_state_space_tree(
            nums,
            max_sum,
            index + 1,
            [*path, nums[index]],
            result,
            remaining_nums_sum - nums[index],
        )


""""""
remove the comment to take an input from the user

print(""Enter the elements"")
nums = list(map(int, input().split()))
print(""Enter max_sum sum"")
max_sum = int(input())

""""""
nums = [3, 34, 4, 12, 5, 2]
max_sum = 9
result = generate_sum_of_subsets_soln(nums, max_sum)
print(*result)
","""""""
The sum-of-subsets problem states that a set of non-negative integers, and a
value M, determine all possible subsets of the given set whose summation sum
equal to given M.

Summation of the chosen numbers must be equal to given number M and one number
can be used only once.
""""""

from __future__ import annotations

def generate_sum_of_subsets_soln(nums: list[int], max_sum: int) -> list[list[int]]:
    result: list[list[int]] = []
    path: list[int] = []
    num_index = 0
    remaining_nums_sum = sum(nums)
    create_state_space_tree(nums, max_sum, num_index, path, result, remaining_nums_sum)
    return result

def create_state_space_tree(
    nums: list[int],
    max_sum: int,
    num_index: int,
    path: list[int],
    result: list[list[int]],
    remaining_nums_sum: int,
) -> None:
    """"""
    Creates a state space tree to iterate through each branch using DFS.
    It terminates the branching of a node when any of the two conditions
    given below satisfy.
    This algorithm follows depth-first-search and backtracks when the node is not
    branchable.
    """"""
    if sum(path) > max_sum or (remaining_nums_sum + sum(path)) < max_sum:
        return

    if sum(path) == max_sum:
        result.append(path)
        return

    for index in range(num_index, len(nums)):
        create_state_space_tree(
            nums,
            max_sum,
            index + 1,
            [*path, nums[index]],
            result,
            remaining_nums_sum - nums[index],
        )

""""""
remove the comment to take an input from the user

# print(""Enter the elements"")
# nums = list(map(int, input().split()))

# print(""Enter max_sum sum"")
# max_sum = int(input())
""""""

nums = [3, 34, 4, 12, 5, 2]
max_sum = 9

result = generate_sum_of_subsets_soln(nums, max_sum)
print(*result)","{'final_score': 98.69, 'length_similarity': 99.44, 'token_similarity': 95.92, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
6,"from urllib.parse import quote

import requests

from backend.blocks.jina._auth import (
    JinaCredentials,
    JinaCredentialsField,
    JinaCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class FactCheckerBlock(Block):
    class Input(BlockSchema):
        statement: str = SchemaField(
            description=""The statement to check for factuality""
        )
        credentials: JinaCredentialsInput = JinaCredentialsField()

    class Output(BlockSchema):
        factuality: float = SchemaField(
            description=""The factuality score of the statement""
        )
        result: bool = SchemaField(description=""The result of the factuality check"")
        reason: str = SchemaField(description=""The reason for the factuality result"")
        error: str = SchemaField(description=""Error message if the check fails"")

    def __init__(self):
        super().__init__(
            id=""d38b6c5e-9968-4271-8423-6cfe60d6e7e6"",
            description=""This block checks the factuality of a given statement using Jina AI's Grounding API."",
            categories={BlockCategory.SEARCH},
            input_schema=FactCheckerBlock.Input,
            output_schema=FactCheckerBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        encoded_statement = quote(input_data.statement)
        url = f""https://g.jina.ai/{encoded_statement}""

        headers = {
            ""Accept"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }

        response = requests.get(url, headers=headers)
        response.raise_for_status()
        data = response.json()

        if ""data"" in data:
            data = data[""data""]
            yield ""factuality"", data[""factuality""]
            yield ""result"", data[""result""]
            yield ""reason"", data[""reason""]
        else:
            raise RuntimeError(f""Expected 'data' key not found in response: {data}"")
","from urllib.parse import quote
import requests
from backend.blocks.jina._auth import (
    JinaCredentials,
    JinaCredentialsField,
    JinaCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

class FactCheckerBlock(Block):
    class Input(BlockSchema):
        statement: str = SchemaField(
            description=""The statement to check for factuality""
        )
        credentials: JinaCredentialsInput = JinaCredentialsField()

    class Output(BlockSchema):
        factuality: float = SchemaField(
            description=""The factuality score of the statement""
        )
        result: bool = SchemaField(description=""The result of the factuality check"")
        reason: str = SchemaField(description=""The reason for the factuality result"")
        error: str = SchemaField(description=""Error message if the check fails"")

    def __init__(self):
        super().__init__(
            id=""d38b6c5e-9968-4271-8423-6cfe60d6e7e6"",
            description=""This block checks the factuality of a given statement using Jina AI's Grounding API."",
            categories={BlockCategory.SEARCH},
            input_schema=FactCheckerBlock.Input,
            output_schema=FactCheckerBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        encoded_statement = quote(input_data.statement)
        url = f""https://g.jina.ai/{encoded_statement}""

        headers = {
            ""Accept"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }

        response = requests.get(url, headers=headers)
        response.raise_for_status()
        data = response.json()

        if ""data"" in data:
            data = data[""data""]
            yield ""factuality"", data[""factuality""]
            yield ""result"", data[""result""]
            yield ""reason"", data[""reason""]
        else:
            raise RuntimeError(f""Expected 'data' key not found in response: {data}"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
7,"from abc import ABC, abstractmethod
from datetime import datetime, timezone

from prisma import Json
from prisma.enums import CreditTransactionType
from prisma.errors import UniqueViolationError
from prisma.models import CreditTransaction

from backend.data.block import Block, BlockInput, get_block
from backend.data.block_cost_config import BLOCK_COSTS
from backend.data.cost import BlockCost, BlockCostType
from backend.util.settings import Config

config = Config()


class UserCreditBase(ABC):
    def __init__(self, num_user_credits_refill: int):
        self.num_user_credits_refill = num_user_credits_refill

    @abstractmethod
    async def get_or_refill_credit(self, user_id: str) -> int:
        """"""
        Get the current credit for the user and refill if no transaction has been made in the current cycle.

        Returns:
            int: The current credit for the user.
        """"""
        pass

    @abstractmethod
    async def spend_credits(
        self,
        user_id: str,
        user_credit: int,
        block_id: str,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
    ) -> int:
        """"""
        Spend the credits for the user based on the block usage.

        Args:
            user_id (str): The user ID.
            user_credit (int): The current credit for the user.
            block_id (str): The block ID.
            input_data (BlockInput): The input data for the block.
            data_size (float): The size of the data being processed.
            run_time (float): The time taken to run the block.

        Returns:
            int: amount of credit spent
        """"""
        pass

    @abstractmethod
    async def top_up_credits(self, user_id: str, amount: int):
        """"""
        Top up the credits for the user.

        Args:
            user_id (str): The user ID.
            amount (int): The amount to top up.
        """"""
        pass


class UserCredit(UserCreditBase):
    async def get_or_refill_credit(self, user_id: str) -> int:
        cur_time = self.time_now()
        cur_month = cur_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        nxt_month = (
            cur_month.replace(month=cur_month.month + 1)
            if cur_month.month < 12
            else cur_month.replace(year=cur_month.year + 1, month=1)
        )

        user_credit = await CreditTransaction.prisma().group_by(
            by=[""userId""],
            sum={""amount"": True},
            where={
                ""userId"": user_id,
                ""createdAt"": {""gte"": cur_month, ""lt"": nxt_month},
                ""isActive"": True,
            },
        )

        if user_credit:
            credit_sum = user_credit[0].get(""_sum"") or {}
            return credit_sum.get(""amount"", 0)

        key = f""MONTHLY-CREDIT-TOP-UP-{cur_month}""

        try:
            await CreditTransaction.prisma().create(
                data={
                    ""amount"": self.num_user_credits_refill,
                    ""type"": CreditTransactionType.TOP_UP,
                    ""userId"": user_id,
                    ""transactionKey"": key,
                    ""createdAt"": self.time_now(),
                }
            )
        except UniqueViolationError:
            pass  # Already refilled this month

        return self.num_user_credits_refill

    @staticmethod
    def time_now():
        return datetime.now(timezone.utc)

    def _block_usage_cost(
        self,
        block: Block,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
    ) -> tuple[int, BlockInput]:
        block_costs = BLOCK_COSTS.get(type(block))
        if not block_costs:
            return 0, {}

        for block_cost in block_costs:
            if not self._is_cost_filter_match(block_cost.cost_filter, input_data):
                continue

            if block_cost.cost_type == BlockCostType.RUN:
                return block_cost.cost_amount, block_cost.cost_filter

            if block_cost.cost_type == BlockCostType.SECOND:
                return (
                    int(run_time * block_cost.cost_amount),
                    block_cost.cost_filter,
                )

            if block_cost.cost_type == BlockCostType.BYTE:
                return (
                    int(data_size * block_cost.cost_amount),
                    block_cost.cost_filter,
                )

        return 0, {}

    def _is_cost_filter_match(
        self, cost_filter: BlockInput, input_data: BlockInput
    ) -> bool:
        """"""
        Filter rules:
          - If costFilter is an object, then check if costFilter is the subset of inputValues
          - Otherwise, check if costFilter is equal to inputValues.
          - Undefined, null, and empty string are considered as equal.
        """"""
        if not isinstance(cost_filter, dict) or not isinstance(input_data, dict):
            return cost_filter == input_data

        return all(
            (not input_data.get(k) and not v)
            or (input_data.get(k) and self._is_cost_filter_match(v, input_data[k]))
            for k, v in cost_filter.items()
        )

    async def spend_credits(
        self,
        user_id: str,
        user_credit: int,
        block_id: str,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
        validate_balance: bool = True,
    ) -> int:
        block = get_block(block_id)
        if not block:
            raise ValueError(f""Block not found: {block_id}"")

        cost, matching_filter = self._block_usage_cost(
            block=block, input_data=input_data, data_size=data_size, run_time=run_time
        )
        if cost <= 0:
            return 0

        if validate_balance and user_credit < cost:
            raise ValueError(f""Insufficient credit: {user_credit} < {cost}"")

        await CreditTransaction.prisma().create(
            data={
                ""userId"": user_id,
                ""amount"": -cost,
                ""type"": CreditTransactionType.USAGE,
                ""blockId"": block.id,
                ""metadata"": Json(
                    {
                        ""block"": block.name,
                        ""input"": matching_filter,
                    }
                ),
                ""createdAt"": self.time_now(),
            }
        )
        return cost

    async def top_up_credits(self, user_id: str, amount: int):
        await CreditTransaction.prisma().create(
            data={
                ""userId"": user_id,
                ""amount"": amount,
                ""type"": CreditTransactionType.TOP_UP,
                ""createdAt"": self.time_now(),
            }
        )


class DisabledUserCredit(UserCreditBase):
    async def get_or_refill_credit(self, *args, **kwargs) -> int:
        return 0

    async def spend_credits(self, *args, **kwargs) -> int:
        return 0

    async def top_up_credits(self, *args, **kwargs):
        pass


def get_user_credit_model() -> UserCreditBase:
    if config.enable_credit.lower() == ""true"":
        return UserCredit(config.num_user_credits_refill)
    else:
        return DisabledUserCredit(0)


def get_block_costs() -> dict[str, list[BlockCost]]:
    return {block().id: costs for block, costs in BLOCK_COSTS.items()}
","from abc import ABC, abstractmethod
from datetime import datetime, timezone

from prisma import Json
from prisma.enums import CreditTransactionType
from prisma.errors import UniqueViolationError
from prisma.models import CreditTransaction

from backend.data.block import Block, BlockInput, get_block
from backend.data.block_cost_config import BLOCK_COSTS
from backend.data.cost import BlockCost, BlockCostType
from backend.util.settings import Config

config = Config()

class UserCreditBase(ABC):
    def __init__(self, num_user_credits_refill: int):
        self.num_user_credits_refill = num_user_credits_refill

    @abstractmethod
    async def get_or_refill_credit(self, user_id: str) -> int:
        """"""
        Get the current credit for the user and refill if no transaction has been made in the current cycle.

        Returns:
            int: The current credit for the user.
        """"""
        pass

    @abstractmethod
    async def spend_credits(
        self,
        user_id: str,
        user_credit: int,
        block_id: str,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
    ) -> int:
        """"""
        Spend the credits for the user based on the block usage.

        Args:
            user_id (str): The user ID.
            user_credit (int): The current credit for the user.
            block_id (str): The block ID.
            input_data (BlockInput): The input data for the block.
            data_size (float): The size of the data being processed.
            run_time (float): The time taken to run the block.

        Returns:
            int: amount of credit spent
        """"""
        pass

    @abstractmethod
    async def top_up_credits(self, user_id: str, amount: int):
        """"""
        Top up the credits for the user.

        Args:
            user_id (str): The user ID.
            amount (int): The amount to top up.
        """"""
        pass

class UserCredit(UserCreditBase):
    async def get_or_refill_credit(self, user_id: str) -> int:
        cur_time = self.time_now()
        cur_month = cur_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        nxt_month = (
            cur_month.replace(month=cur_month.month + 1)
            if cur_month.month < 12
            else cur_month.replace(year=cur_month.year + 1, month=1)
        )
        user_credit = await CreditTransaction.prisma().group_by(
            by=[""userId""],
            sum={""amount"": True},
            where={
                ""userId"": user_id,
                ""createdAt"": {""gte"": cur_month, ""lt"": nxt_month},
                ""isActive"": True,
            },
        )
        if user_credit:
            credit_sum = user_credit[0].get(""_sum"") or {}
            return credit_sum.get(""amount"", 0)
        key = f""MONTHLY-CREDIT-TOP-UP-{cur_month}""
        try:
            await CreditTransaction.prisma().create(
                data={
                    ""amount"": self.num_user_credits_refill,
                    ""type"": CreditTransactionType.TOP_UP,
                    ""userId"": user_id,
                    ""transactionKey"": key,
                    ""createdAt"": self.time_now(),
                }
            )
        except UniqueViolationError:
            pass  # Already refilled this month
        return self.num_user_credits_refill

    @staticmethod
    def time_now():
        return datetime.now(timezone.utc)

    def _block_usage_cost(
        self,
        block: Block,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
    ) -> tuple[int, BlockInput]:
        block_costs = BLOCK_COSTS.get(type(block))
        if not block_costs:
            return 0, {}
        for block_cost in block_costs:
            if not self._is_cost_filter_match(block_cost.cost_filter, input_data):
                continue
            if block_cost.cost_type == BlockCostType.RUN:
                return block_cost.cost_amount, block_cost.cost_filter
            if block_cost.cost_type == BlockCostType.SECOND:
                return (
                    int(run_time * block_cost.cost_amount),
                    block_cost.cost_filter,
                )
            if block_cost.cost_type == BlockCostType.BYTE:
                return (
                    int(data_size * block_cost.cost_amount),
                    block_cost.cost_filter,
                )
        return 0, {}

    def _is_cost_filter_match(
        self, cost_filter: BlockInput, input_data: BlockInput
    ) -> bool:
        """"""
        Filter rules:
          - If costFilter is an object, then check if costFilter is the subset of inputValues
          - Otherwise, check if costFilter is equal to inputValues.
          - Undefined, null, and empty string are considered as equal.
        """"""
        if not isinstance(cost_filter, dict) or not isinstance(input_data, dict):
            return cost_filter == input_data
        return all(
            (not input_data.get(k) and not v)
            or (input_data.get(k) and self._is_cost_filter_match(v, input_data[k]))
            for k, v in cost_filter.items()
        )

    async def spend_credits(
        self,
        user_id: str,
        user_credit: int,
        block_id: str,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
        validate_balance: bool = True,
    ) -> int:
        block = get_block(block_id)
        if not block:
            raise ValueError(f""Block not found: {block_id}"")
        cost, matching_filter = self._block_usage_cost(
            block=block, input_data=input_data, data_size=data_size, run_time=run_time
        )
        if cost <= 0:
            return 0
        if validate_balance and user_credit < cost:
            raise ValueError(f""Insufficient credit: {user_credit} < {cost}"")
        await CreditTransaction.prisma().create(
            data={
                ""userId"": user_id,
                ""amount"": -cost,
                ""type"": CreditTransactionType.USAGE,
                ""blockId"": block.id,
                ""metadata"": Json(
                    {
                        ""block"": block.name,
                        ""input"": matching_filter,
                    }
                ),
                ""createdAt"": self.time_now(),
            }
        )
        return cost

    async def top_up_credits(self, user_id: str, amount: int):
        await CreditTransaction.prisma().create(
            data={
                ""userId"": user_id,
                ""amount"": amount,
                ""type"": CreditTransactionType.TOP_UP,
                ""createdAt"": self.time_now(),
            }
        )

class DisabledUserCredit(UserCreditBase):
    async def get_or_refill_credit(self, *args, **kwargs) -> int:
        return 0

    async def spend_credits(self, *args, **kwargs) -> int:
        return 0

    async def top_up_credits(self, *args, **kwargs):
        pass

def get_user_credit_model() -> UserCreditBase:
    if config.enable_credit.lower() == ""true"":
        return UserCredit(config.num_user_credits_refill)
    else:
        return DisabledUserCredit(0)

def get_block_costs() -> dict[str, list[BlockCost]]:
    return {block().id: costs for block, costs in BLOCK_COSTS.items()}","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
8,"import logging

from autogpt_libs.utils.cache import thread_cached

from backend.data.block import (
    Block,
    BlockCategory,
    BlockInput,
    BlockOutput,
    BlockSchema,
    BlockType,
    get_block,
)
from backend.data.execution import ExecutionStatus
from backend.data.model import SchemaField

logger = logging.getLogger(__name__)


@thread_cached
def get_executor_manager_client():
    from backend.executor import ExecutionManager
    from backend.util.service import get_service_client

    return get_service_client(ExecutionManager)


@thread_cached
def get_event_bus():
    from backend.data.execution import RedisExecutionEventBus

    return RedisExecutionEventBus()


class AgentExecutorBlock(Block):
    class Input(BlockSchema):
        user_id: str = SchemaField(description=""User ID"")
        graph_id: str = SchemaField(description=""Graph ID"")
        graph_version: int = SchemaField(description=""Graph Version"")

        data: BlockInput = SchemaField(description=""Input data for the graph"")
        input_schema: dict = SchemaField(description=""Input schema for the graph"")
        output_schema: dict = SchemaField(description=""Output schema for the graph"")

    class Output(BlockSchema):
        pass

    def __init__(self):
        super().__init__(
            id=""e189baac-8c20-45a1-94a7-55177ea42565"",
            description=""Executes an existing agent inside your agent"",
            input_schema=AgentExecutorBlock.Input,
            output_schema=AgentExecutorBlock.Output,
            block_type=BlockType.AGENT,
            categories={BlockCategory.AGENT},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        executor_manager = get_executor_manager_client()
        event_bus = get_event_bus()

        graph_exec = executor_manager.add_execution(
            graph_id=input_data.graph_id,
            graph_version=input_data.graph_version,
            user_id=input_data.user_id,
            data=input_data.data,
        )
        log_id = f""Graph #{input_data.graph_id}-V{input_data.graph_version}, exec-id: {graph_exec.graph_exec_id}""
        logger.info(f""Starting execution of {log_id}"")

        for event in event_bus.listen(
            graph_id=graph_exec.graph_id, graph_exec_id=graph_exec.graph_exec_id
        ):
            logger.info(
                f""Execution {log_id} produced input {event.input_data} output {event.output_data}""
            )

            if not event.node_id:
                if event.status in [ExecutionStatus.COMPLETED, ExecutionStatus.FAILED]:
                    logger.info(f""Execution {log_id} ended with status {event.status}"")
                    break
                else:
                    continue

            if not event.block_id:
                logger.warning(f""{log_id} received event without block_id {event}"")
                continue

            block = get_block(event.block_id)
            if not block or block.block_type != BlockType.OUTPUT:
                continue

            output_name = event.input_data.get(""name"")
            if not output_name:
                logger.warning(f""{log_id} produced an output with no name {event}"")
                continue

            for output_data in event.output_data.get(""output"", []):
                logger.info(f""Execution {log_id} produced {output_name}: {output_data}"")
                yield output_name, output_data
","import logging
from autogpt_libs.utils.cache import thread_cached
from backend.data.block import (
    Block,
    BlockCategory,
    BlockInput,
    BlockOutput,
    BlockSchema,
    BlockType,
    get_block,
)
from backend.data.execution import ExecutionStatus
from backend.data.model import SchemaField

logger = logging.getLogger(__name__)

@thread_cached
def get_executor_manager_client():
    from backend.executor import ExecutionManager
    from backend.util.service import get_service_client

    return get_service_client(ExecutionManager)

@thread_cached
def get_event_bus():
    from backend.data.execution import RedisExecutionEventBus

    return RedisExecutionEventBus()

class AgentExecutorBlock(Block):
    class Input(BlockSchema):
        user_id: str = SchemaField(description=""User ID"")
        graph_id: str = SchemaField(description=""Graph ID"")
        graph_version: int = SchemaField(description=""Graph Version"")

        data: BlockInput = SchemaField(description=""Input data for the graph"")
        input_schema: dict = SchemaField(description=""Input schema for the graph"")
        output_schema: dict = SchemaField(description=""Output schema for the graph"")

    class Output(BlockSchema):
        pass

    def __init__(self):
        super().__init__(
            id=""e189baac-8c20-45a1-94a7-55177ea42565"",
            description=""Executes an existing agent inside your agent"",
            input_schema=AgentExecutorBlock.Input,
            output_schema=AgentExecutorBlock.Output,
            block_type=BlockType.AGENT,
            categories={BlockCategory.AGENT},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        executor_manager = get_executor_manager_client()
        event_bus = get_event_bus()

        graph_exec = executor_manager.add_execution(
            graph_id=input_data.graph_id,
            graph_version=input_data.graph_version,
            user_id=input_data.user_id,
            data=input_data.data,
        )

        log_id = f""Graph #{input_data.graph_id}-V{input_data.graph_version}, exec-id: {graph_exec.graph_exec_id}""
        logger.info(f""Starting execution of {log_id}"")

        for event in event_bus.listen(
            graph_id=graph_exec.graph_id, graph_exec_id=graph_exec.graph_exec_id
        ):
            logger.info(
                f""Execution {log_id} produced input {event.input_data} output {event.output_data}""
            )

            if not event.node_id:
                if event.status in [ExecutionStatus.COMPLETED, ExecutionStatus.FAILED]:
                    logger.info(f""Execution {log_id} ended with status {event.status}"")
                    break
                else:
                    continue

            if not event.block_id:
                logger.warning(f""{log_id} received event without block_id {event}"")
                continue

            block = get_block(event.block_id)
            if not block or block.block_type != BlockType.OUTPUT:
                continue

            output_name = event.input_data.get(""name"")
            if not output_name:
                logger.warning(f""{log_id} produced an output with no name {event}"")
                continue

            for output_data in event.output_data.get(""output"", []):
                logger.info(f""Execution {log_id} produced {output_name}: {output_data}"")
                yield output_name, output_data","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
9,"import threading
from typing import Callable, ParamSpec, TypeVar

P = ParamSpec(""P"")
R = TypeVar(""R"")


def thread_cached(func: Callable[P, R]) -> Callable[P, R]:
    thread_local = threading.local()

    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
        cache = getattr(thread_local, ""cache"", None)
        if cache is None:
            cache = thread_local.cache = {}
        key = (args, tuple(sorted(kwargs.items())))
        if key not in cache:
            cache[key] = func(*args, **kwargs)
        return cache[key]

    return wrapper
","import threading
from typing import Callable, ParamSpec, TypeVar

P = ParamSpec(""P"")
R = TypeVar(""R"")

def thread_cached(func: Callable[P, R]) -> Callable[P, R]:
    thread_local = threading.local()
    
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
        cache = getattr(thread_local, ""cache"", None)
        if cache is None:
            cache = thread_local.cache = {}
        key = (args, tuple(sorted(kwargs.items())))
        if key not in cache:
            cache[key] = func(*args, **kwargs)
        return cache[key]
    
    return wrapper","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
10,"import json
import os
import requests
import sys
import time
from typing import Dict, List, Tuple

CHECK_INTERVAL = 30


def get_environment_variables() -> Tuple[str, str, str, str, str]:
    """"""Retrieve and return necessary environment variables.""""""
    try:
        with open(os.environ[""GITHUB_EVENT_PATH""]) as f:
            event = json.load(f)

        # Handle both PR and merge group events
        if ""pull_request"" in event:
            sha = event[""pull_request""][""head""][""sha""]
        else:
            sha = os.environ[""GITHUB_SHA""]

        return (
            os.environ[""GITHUB_API_URL""],
            os.environ[""GITHUB_REPOSITORY""],
            sha,
            os.environ[""GITHUB_TOKEN""],
            os.environ[""GITHUB_RUN_ID""],
        )
    except KeyError as e:
        print(f""Error: Missing required environment variable or event data: {e}"")
        sys.exit(1)


def make_api_request(url: str, headers: Dict[str, str]) -> Dict:
    """"""Make an API request and return the JSON response.""""""
    try:
        print(""Making API request to:"", url)
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        print(f""Error: API request failed. {e}"")
        sys.exit(1)


def process_check_runs(check_runs: List[Dict]) -> Tuple[bool, bool]:
    """"""Process check runs and return their status.""""""
    runs_in_progress = False
    all_others_passed = True

    for run in check_runs:
        if str(run[""name""]) != ""Check PR Status"":
            status = run[""status""]
            conclusion = run[""conclusion""]

            if status == ""completed"":
                if conclusion not in [""success"", ""skipped"", ""neutral""]:
                    all_others_passed = False
                    print(
                        f""Check run {run['name']} (ID: {run['id']}) has conclusion: {conclusion}""
                    )
            else:
                runs_in_progress = True
                print(f""Check run {run['name']} (ID: {run['id']}) is still {status}."")
                all_others_passed = False
        else:
            print(
                f""Skipping check run {run['name']} (ID: {run['id']}) as it is the current run.""
            )

    return runs_in_progress, all_others_passed


def main():
    api_url, repo, sha, github_token, current_run_id = get_environment_variables()

    endpoint = f""{api_url}/repos/{repo}/commits/{sha}/check-runs""
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
    }
    if github_token:
        headers[""Authorization""] = f""token {github_token}""

    print(f""Current run ID: {current_run_id}"")

    while True:
        data = make_api_request(endpoint, headers)

        check_runs = data[""check_runs""]

        print(""Processing check runs..."")

        print(check_runs)

        runs_in_progress, all_others_passed = process_check_runs(check_runs)

        if not runs_in_progress:
            break

        print(
            ""Some check runs are still in progress. ""
            f""Waiting {CHECK_INTERVAL} seconds before checking again...""
        )
        time.sleep(CHECK_INTERVAL)

    if all_others_passed:
        print(""All other completed check runs have passed. This check passes."")
        sys.exit(0)
    else:
        print(""Some check runs have failed or have not completed. This check fails."")
        sys.exit(1)


if __name__ == ""__main__"":
    main()
","import json
import os
import requests
import sys
import time
from typing import Dict, List, Tuple

CHECK_INTERVAL = 30

def get_environment_variables() -> Tuple[str, str, str, str, str]:
    """"""Retrieve and return necessary environment variables.""""""
    try:
        with open(os.environ[""GITHUB_EVENT_PATH""]) as f:
            event = json.load(f)

        # Handle both PR and merge group events
        if ""pull_request"" in event:
            sha = event[""pull_request""][""head""][""sha""]
        else:
            sha = os.environ[""GITHUB_SHA""]

        return (
            os.environ[""GITHUB_API_URL""],
            os.environ[""GITHUB_REPOSITORY""],
            sha,
            os.environ[""GITHUB_TOKEN""],
            os.environ[""GITHUB_RUN_ID""],
        )
    except KeyError as e:
        print(f""Error: Missing required environment variable or event data: {e}"")
        sys.exit(1)

def make_api_request(url: str, headers: Dict[str, str]) -> Dict:
    """"""Make an API request and return the JSON response.""""""
    try:
        print(""Making API request to:"", url)
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        print(f""Error: API request failed. {e}"")
        sys.exit(1)

def process_check_runs(check_runs: List[Dict]) -> Tuple[bool, bool]:
    """"""Process check runs and return their status.""""""
    runs_in_progress = False
    all_others_passed = True

    for run in check_runs:
        if str(run[""name""]) != ""Check PR Status"":
            status = run[""status""]
            conclusion = run[""conclusion""]

            if status == ""completed"":
                if conclusion not in [""success"", ""skipped"", ""neutral""]:
                    all_others_passed = False
                    print(
                        f""Check run {run['name']} (ID: {run['id']}) has conclusion: {conclusion}""
                    )
            else:
                runs_in_progress = True
                print(f""Check run {run['name']} (ID: {run['id']}) is still {status}."")
                all_others_passed = False
        else:
            print(
                f""Skipping check run {run['name']} (ID: {run['id']}) as it is the current run.""
            )

    return runs_in_progress, all_others_passed

def main():
    api_url, repo, sha, github_token, current_run_id = get_environment_variables()
    endpoint = f""{api_url}/repos/{repo}/commits/{sha}/check-runs""
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
    }
    if github_token:
        headers[""Authorization""] = f""token {github_token}""

    print(f""Current run ID: {current_run_id}"")

    while True:
        data = make_api_request(endpoint, headers)
        check_runs = data[""check_runs""]

        print(""Processing check runs..."")
        print(check_runs)

        runs_in_progress, all_others_passed = process_check_runs(check_runs)

        if not runs_in_progress:
            break

        print(
            ""Some check runs are still in progress. ""
            f""Waiting {CHECK_INTERVAL} seconds before checking again...""
        )
        time.sleep(CHECK_INTERVAL)

    if all_others_passed:
        print(""All other completed check runs have passed. This check passes."")
        sys.exit(0)
    else:
        print(""Some check runs have failed or have not completed. This check fails."")
        sys.exit(1)

if __name__ == ""__main__"":
    main()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
11,"import functools
import logging
import os
import time
from typing import Callable, ParamSpec, Tuple, TypeVar

from pydantic import BaseModel


class TimingInfo(BaseModel):
    cpu_time: float
    wall_time: float


def _start_measurement() -> Tuple[float, float]:
    return time.time(), os.times()[0] + os.times()[1]


def _end_measurement(
    start_wall_time: float, start_cpu_time: float
) -> Tuple[float, float]:
    end_wall_time = time.time()
    end_cpu_time = os.times()[0] + os.times()[1]
    return end_wall_time - start_wall_time, end_cpu_time - start_cpu_time


P = ParamSpec(""P"")
T = TypeVar(""T"")

logger = logging.getLogger(__name__)


def time_measured(func: Callable[P, T]) -> Callable[P, Tuple[TimingInfo, T]]:
    """"""
    Decorator to measure the time taken by a function to execute.
    """"""

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> Tuple[TimingInfo, T]:
        start_wall_time, start_cpu_time = _start_measurement()
        try:
            result = func(*args, **kwargs)
        finally:
            wall_duration, cpu_duration = _end_measurement(
                start_wall_time, start_cpu_time
            )
            timing_info = TimingInfo(cpu_time=cpu_duration, wall_time=wall_duration)
        return timing_info, result

    return wrapper


def error_logged(func: Callable[P, T]) -> Callable[P, T | None]:
    """"""
    Decorator to suppress and log any exceptions raised by a function.
    """"""

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T | None:
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.exception(
                f""Error when calling function {func.__name__} with arguments {args} {kwargs}: {e}""
            )

    return wrapper
","import functools
import logging
import os
import time
from typing import Callable, ParamSpec, Tuple, TypeVar
from pydantic import BaseModel

class TimingInfo(BaseModel):
    cpu_time: float
    wall_time: float

def _start_measurement() -> Tuple[float, float]:
    return time.time(), os.times()[0] + os.times()[1]

def _end_measurement(
    start_wall_time: float, start_cpu_time: float
) -> Tuple[float, float]:
    end_wall_time = time.time()
    end_cpu_time = os.times()[0] + os.times()[1]
    return end_wall_time - start_wall_time, end_cpu_time - start_cpu_time

P = ParamSpec(""P"")
T = TypeVar(""T"")

logger = logging.getLogger(__name__)

def time_measured(func: Callable[P, T]) -> Callable[P, Tuple[TimingInfo, T]]:
    """"""
    Decorator to measure the time taken by a function to execute.
    """"""
    
    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> Tuple[TimingInfo, T]:
        start_wall_time, start_cpu_time = _start_measurement()
        try:
            result = func(*args, **kwargs)
        finally:
            wall_duration, cpu_duration = _end_measurement(
                start_wall_time, start_cpu_time
            )
            timing_info = TimingInfo(cpu_time=cpu_duration, wall_time=wall_duration)
        return timing_info, result

    return wrapper

def error_logged(func: Callable[P, T]) -> Callable[P, T | None]:
    """"""
    Decorator to suppress and log any exceptions raised by a function.
    """"""
    
    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T | None:
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.exception(
                f""Error when calling function {func.__name__} with arguments {args} {kwargs}: {e}""
            )

    return wrapper","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
12,"import base64

from typing_extensions import TypedDict

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

from ._api import get_api
from ._auth import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GithubCredentials,
    GithubCredentialsField,
    GithubCredentialsInput,
)


class GithubListTagsBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class TagItem(TypedDict):
            name: str
            url: str

        tag: TagItem = SchemaField(
            title=""Tag"", description=""Tags with their name and file tree browser URL""
        )
        error: str = SchemaField(description=""Error message if listing tags failed"")

    def __init__(self):
        super().__init__(
            id=""358924e7-9a11-4d1a-a0f2-13c67fe59e2e"",
            description=""This block lists all tags for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListTagsBlock.Input,
            output_schema=GithubListTagsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""tag"",
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/tree/v1.0.0"",
                    },
                )
            ],
            test_mock={
                ""list_tags"": lambda *args, **kwargs: [
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/tree/v1.0.0"",
                    }
                ]
            },
        )

    @staticmethod
    def list_tags(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.TagItem]:
        api = get_api(credentials)
        tags_url = repo_url + ""/tags""
        response = api.get(tags_url)
        data = response.json()
        repo_path = repo_url.replace(""https://github.com/"", """")
        tags: list[GithubListTagsBlock.Output.TagItem] = [
            {
                ""name"": tag[""name""],
                ""url"": f""https://github.com/{repo_path}/tree/{tag['name']}"",
            }
            for tag in data
        ]
        return tags

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        tags = self.list_tags(
            credentials,
            input_data.repo_url,
        )
        yield from ((""tag"", tag) for tag in tags)


class GithubListBranchesBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class BranchItem(TypedDict):
            name: str
            url: str

        branch: BranchItem = SchemaField(
            title=""Branch"",
            description=""Branches with their name and file tree browser URL"",
        )
        error: str = SchemaField(description=""Error message if listing branches failed"")

    def __init__(self):
        super().__init__(
            id=""74243e49-2bec-4916-8bf4-db43d44aead5"",
            description=""This block lists all branches for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListBranchesBlock.Input,
            output_schema=GithubListBranchesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""branch"",
                    {
                        ""name"": ""main"",
                        ""url"": ""https://github.com/owner/repo/tree/main"",
                    },
                )
            ],
            test_mock={
                ""list_branches"": lambda *args, **kwargs: [
                    {
                        ""name"": ""main"",
                        ""url"": ""https://github.com/owner/repo/tree/main"",
                    }
                ]
            },
        )

    @staticmethod
    def list_branches(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.BranchItem]:
        api = get_api(credentials)
        branches_url = repo_url + ""/branches""
        response = api.get(branches_url)
        data = response.json()
        repo_path = repo_url.replace(""https://github.com/"", """")
        branches: list[GithubListBranchesBlock.Output.BranchItem] = [
            {
                ""name"": branch[""name""],
                ""url"": f""https://github.com/{repo_path}/tree/{branch['name']}"",
            }
            for branch in data
        ]
        return branches

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        branches = self.list_branches(
            credentials,
            input_data.repo_url,
        )
        yield from ((""branch"", branch) for branch in branches)


class GithubListDiscussionsBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        num_discussions: int = SchemaField(
            description=""Number of discussions to fetch"", default=5
        )

    class Output(BlockSchema):
        class DiscussionItem(TypedDict):
            title: str
            url: str

        discussion: DiscussionItem = SchemaField(
            title=""Discussion"", description=""Discussions with their title and URL""
        )
        error: str = SchemaField(
            description=""Error message if listing discussions failed""
        )

    def __init__(self):
        super().__init__(
            id=""3ef1a419-3d76-4e07-b761-de9dad4d51d7"",
            description=""This block lists recent discussions for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListDiscussionsBlock.Input,
            output_schema=GithubListDiscussionsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""num_discussions"": 3,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""discussion"",
                    {
                        ""title"": ""Discussion 1"",
                        ""url"": ""https://github.com/owner/repo/discussions/1"",
                    },
                )
            ],
            test_mock={
                ""list_discussions"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Discussion 1"",
                        ""url"": ""https://github.com/owner/repo/discussions/1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_discussions(
        credentials: GithubCredentials, repo_url: str, num_discussions: int
    ) -> list[Output.DiscussionItem]:
        api = get_api(credentials)
        # GitHub GraphQL API endpoint is different; we'll use api.post with custom URL
        repo_path = repo_url.replace(""https://github.com/"", """")
        owner, repo = repo_path.split(""/"")
        query = """"""
        query($owner: String!, $repo: String!, $num: Int!) {
            repository(owner: $owner, name: $repo) {
                discussions(first: $num) {
                    nodes {
                        title
                        url
                    }
                }
            }
        }
        """"""
        variables = {""owner"": owner, ""repo"": repo, ""num"": num_discussions}
        response = api.post(
            ""https://api.github.com/graphql"",
            json={""query"": query, ""variables"": variables},
        )
        data = response.json()
        discussions: list[GithubListDiscussionsBlock.Output.DiscussionItem] = [
            {""title"": discussion[""title""], ""url"": discussion[""url""]}
            for discussion in data[""data""][""repository""][""discussions""][""nodes""]
        ]
        return discussions

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        discussions = self.list_discussions(
            credentials, input_data.repo_url, input_data.num_discussions
        )
        yield from ((""discussion"", discussion) for discussion in discussions)


class GithubListReleasesBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class ReleaseItem(TypedDict):
            name: str
            url: str

        release: ReleaseItem = SchemaField(
            title=""Release"",
            description=""Releases with their name and file tree browser URL"",
        )
        error: str = SchemaField(description=""Error message if listing releases failed"")

    def __init__(self):
        super().__init__(
            id=""3460367a-6ba7-4645-8ce6-47b05d040b92"",
            description=""This block lists all releases for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListReleasesBlock.Input,
            output_schema=GithubListReleasesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""release"",
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/releases/tag/v1.0.0"",
                    },
                )
            ],
            test_mock={
                ""list_releases"": lambda *args, **kwargs: [
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/releases/tag/v1.0.0"",
                    }
                ]
            },
        )

    @staticmethod
    def list_releases(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.ReleaseItem]:
        api = get_api(credentials)
        releases_url = repo_url + ""/releases""
        response = api.get(releases_url)
        data = response.json()
        releases: list[GithubListReleasesBlock.Output.ReleaseItem] = [
            {""name"": release[""name""], ""url"": release[""html_url""]} for release in data
        ]
        return releases

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        releases = self.list_releases(
            credentials,
            input_data.repo_url,
        )
        yield from ((""release"", release) for release in releases)


class GithubReadFileBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        file_path: str = SchemaField(
            description=""Path to the file in the repository"",
            placeholder=""path/to/file"",
        )
        branch: str = SchemaField(
            description=""Branch to read from"",
            placeholder=""branch_name"",
            default=""master"",
        )

    class Output(BlockSchema):
        text_content: str = SchemaField(
            description=""Content of the file (decoded as UTF-8 text)""
        )
        raw_content: str = SchemaField(
            description=""Raw base64-encoded content of the file""
        )
        size: int = SchemaField(description=""The size of the file (in bytes)"")
        error: str = SchemaField(description=""Error message if the file reading failed"")

    def __init__(self):
        super().__init__(
            id=""87ce6c27-5752-4bbc-8e26-6da40a3dcfd3"",
            description=""This block reads the content of a specified file from a GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadFileBlock.Input,
            output_schema=GithubReadFileBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""file_path"": ""path/to/file"",
                ""branch"": ""master"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""raw_content"", ""RmlsZSBjb250ZW50""),
                (""text_content"", ""File content""),
                (""size"", 13),
            ],
            test_mock={""read_file"": lambda *args, **kwargs: (""RmlsZSBjb250ZW50"", 13)},
        )

    @staticmethod
    def read_file(
        credentials: GithubCredentials, repo_url: str, file_path: str, branch: str
    ) -> tuple[str, int]:
        api = get_api(credentials)
        content_url = repo_url + f""/contents/{file_path}?ref={branch}""
        response = api.get(content_url)
        content = response.json()

        if isinstance(content, list):
            # Multiple entries of different types exist at this path
            if not (file := next((f for f in content if f[""type""] == ""file""), None)):
                raise TypeError(""Not a file"")
            content = file

        if content[""type""] != ""file"":
            raise TypeError(""Not a file"")

        return content[""content""], content[""size""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        raw_content, size = self.read_file(
            credentials,
            input_data.repo_url,
            input_data.file_path.lstrip(""/""),
            input_data.branch,
        )
        yield ""raw_content"", raw_content
        yield ""text_content"", base64.b64decode(raw_content).decode(""utf-8"")
        yield ""size"", size


class GithubReadFolderBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        folder_path: str = SchemaField(
            description=""Path to the folder in the repository"",
            placeholder=""path/to/folder"",
        )
        branch: str = SchemaField(
            description=""Branch name to read from (defaults to master)"",
            placeholder=""branch_name"",
            default=""master"",
        )

    class Output(BlockSchema):
        class DirEntry(TypedDict):
            name: str
            path: str

        class FileEntry(TypedDict):
            name: str
            path: str
            size: int

        file: FileEntry = SchemaField(description=""Files in the folder"")
        dir: DirEntry = SchemaField(description=""Directories in the folder"")
        error: str = SchemaField(
            description=""Error message if reading the folder failed""
        )

    def __init__(self):
        super().__init__(
            id=""1355f863-2db3-4d75-9fba-f91e8a8ca400"",
            description=""This block reads the content of a specified folder from a GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadFolderBlock.Input,
            output_schema=GithubReadFolderBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""folder_path"": ""path/to/folder"",
                ""branch"": ""master"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""file"",
                    {
                        ""name"": ""file1.txt"",
                        ""path"": ""path/to/folder/file1.txt"",
                        ""size"": 1337,
                    },
                ),
                (""dir"", {""name"": ""dir2"", ""path"": ""path/to/folder/dir2""}),
            ],
            test_mock={
                ""read_folder"": lambda *args, **kwargs: (
                    [
                        {
                            ""name"": ""file1.txt"",
                            ""path"": ""path/to/folder/file1.txt"",
                            ""size"": 1337,
                        }
                    ],
                    [{""name"": ""dir2"", ""path"": ""path/to/folder/dir2""}],
                )
            },
        )

    @staticmethod
    def read_folder(
        credentials: GithubCredentials, repo_url: str, folder_path: str, branch: str
    ) -> tuple[list[Output.FileEntry], list[Output.DirEntry]]:
        api = get_api(credentials)
        contents_url = repo_url + f""/contents/{folder_path}?ref={branch}""
        response = api.get(contents_url)
        content = response.json()

        if not isinstance(content, list):
            raise TypeError(""Not a folder"")

        files = [
            GithubReadFolderBlock.Output.FileEntry(
                name=entry[""name""],
                path=entry[""path""],
                size=entry[""size""],
            )
            for entry in content
            if entry[""type""] == ""file""
        ]
        dirs = [
            GithubReadFolderBlock.Output.DirEntry(
                name=entry[""name""],
                path=entry[""path""],
            )
            for entry in content
            if entry[""type""] == ""dir""
        ]

        return files, dirs

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        files, dirs = self.read_folder(
            credentials,
            input_data.repo_url,
            input_data.folder_path.lstrip(""/""),
            input_data.branch,
        )
        yield from ((""file"", file) for file in files)
        yield from ((""dir"", dir) for dir in dirs)


class GithubMakeBranchBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        new_branch: str = SchemaField(
            description=""Name of the new branch"",
            placeholder=""new_branch_name"",
        )
        source_branch: str = SchemaField(
            description=""Name of the source branch"",
            placeholder=""source_branch_name"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the branch creation operation"")
        error: str = SchemaField(
            description=""Error message if the branch creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""944cc076-95e7-4d1b-b6b6-b15d8ee5448d"",
            description=""This block creates a new branch from a specified source branch."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakeBranchBlock.Input,
            output_schema=GithubMakeBranchBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""new_branch"": ""new_branch_name"",
                ""source_branch"": ""source_branch_name"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Branch created successfully"")],
            test_mock={
                ""create_branch"": lambda *args, **kwargs: ""Branch created successfully""
            },
        )

    @staticmethod
    def create_branch(
        credentials: GithubCredentials,
        repo_url: str,
        new_branch: str,
        source_branch: str,
    ) -> str:
        api = get_api(credentials)
        # Get the SHA of the source branch
        ref_url = repo_url + f""/git/refs/heads/{source_branch}""
        response = api.get(ref_url)
        sha = response.json()[""object""][""sha""]

        # Create the new branch
        create_ref_url = repo_url + ""/git/refs""
        data = {""ref"": f""refs/heads/{new_branch}"", ""sha"": sha}
        response = api.post(create_ref_url, json=data)
        return ""Branch created successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.create_branch(
            credentials,
            input_data.repo_url,
            input_data.new_branch,
            input_data.source_branch,
        )
        yield ""status"", status


class GithubDeleteBranchBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        branch: str = SchemaField(
            description=""Name of the branch to delete"",
            placeholder=""branch_name"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the branch deletion operation"")
        error: str = SchemaField(
            description=""Error message if the branch deletion failed""
        )

    def __init__(self):
        super().__init__(
            id=""0d4130f7-e0ab-4d55-adc3-0a40225e80f4"",
            description=""This block deletes a specified branch."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubDeleteBranchBlock.Input,
            output_schema=GithubDeleteBranchBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""branch"": ""branch_name"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Branch deleted successfully"")],
            test_mock={
                ""delete_branch"": lambda *args, **kwargs: ""Branch deleted successfully""
            },
        )

    @staticmethod
    def delete_branch(
        credentials: GithubCredentials, repo_url: str, branch: str
    ) -> str:
        api = get_api(credentials)
        ref_url = repo_url + f""/git/refs/heads/{branch}""
        api.delete(ref_url)
        return ""Branch deleted successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.delete_branch(
            credentials,
            input_data.repo_url,
            input_data.branch,
        )
        yield ""status"", status
","import base64
from typing_extensions import TypedDict
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from ._api import get_api
from ._auth import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GithubCredentials,
    GithubCredentialsField,
    GithubCredentialsInput,
)

class GithubListTagsBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class TagItem(TypedDict):
            name: str
            url: str

        tag: TagItem = SchemaField(
            title=""Tag"", description=""Tags with their name and file tree browser URL""
        )
        error: str = SchemaField(description=""Error message if listing tags failed"")

    def __init__(self):
        super().__init__(
            id=""358924e7-9a11-4d1a-a0f2-13c67fe59e2e"",
            description=""This block lists all tags for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListTagsBlock.Input,
            output_schema=GithubListTagsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""tag"",
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/tree/v1.0.0"",
                    },
                )
            ],
            test_mock={
                ""list_tags"": lambda *args, **kwargs: [
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/tree/v1.0.0"",
                    }
                ]
            },
        )

    @staticmethod
    def list_tags(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.TagItem]:
        api = get_api(credentials)
        tags_url = repo_url + ""/tags""
        response = api.get(tags_url)
        data = response.json()
        repo_path = repo_url.replace(""https://github.com/"", """")
        tags: list[GithubListTagsBlock.Output.TagItem] = [
            {
                ""name"": tag[""name""],
                ""url"": f""https://github.com/{repo_path}/tree/{tag['name']}"",
            }
            for tag in data
        ]
        return tags

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        tags = self.list_tags(
            credentials,
            input_data.repo_url,
        )
        yield from ((""tag"", tag) for tag in tags)

class GithubListBranchesBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class BranchItem(TypedDict):
            name: str
            url: str

        branch: BranchItem = SchemaField(
            title=""Branch"",
            description=""Branches with their name and file tree browser URL"",
        )
        error: str = SchemaField(description=""Error message if listing branches failed"")

    def __init__(self):
        super().__init__(
            id=""74243e49-2bec-4916-8bf4-db43d44aead5"",
            description=""This block lists all branches for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListBranchesBlock.Input,
            output_schema=GithubListBranchesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""branch"",
                    {
                        ""name"": ""main"",
                        ""url"": ""https://github.com/owner/repo/tree/main"",
                    },
                )
            ],
            test_mock={
                ""list_branches"": lambda *args, **kwargs: [
                    {
                        ""name"": ""main"",
                        ""url"": ""https://github.com/owner/repo/tree/main"",
                    }
                ]
            },
        )

    @staticmethod
    def list_branches(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.BranchItem]:
        api = get_api(credentials)
        branches_url = repo_url + ""/branches""
        response = api.get(branches_url)
        data = response.json()
        repo_path = repo_url.replace(""https://github.com/"", """")
        branches: list[GithubListBranchesBlock.Output.BranchItem] = [
            {
                ""name"": branch[""name""],
                ""url"": f""https://github.com/{repo_path}/tree/{branch['name']}"",
            }
            for branch in data
        ]
        return branches

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        branches = self.list_branches(
            credentials,
            input_data.repo_url,
        )
        yield from ((""branch"", branch) for branch in branches)

class GithubListDiscussionsBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        num_discussions: int = SchemaField(
            description=""Number of discussions to fetch"", default=5
        )

    class Output(BlockSchema):
        class DiscussionItem(TypedDict):
            title: str
            url: str

        discussion: DiscussionItem = SchemaField(
            title=""Discussion"", description=""Discussions with their title and URL""
        )
        error: str = SchemaField(
            description=""Error message if listing discussions failed""
        )

    def __init__(self):
        super().__init__(
            id=""3ef1a419-3d76-4e07-b761-de9dad4d51d7"",
            description=""This block lists recent discussions for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListDiscussionsBlock.Input,
            output_schema=GithubListDiscussionsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""num_discussions"": 3,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""discussion"",
                    {
                        ""title"": ""Discussion 1"",
                        ""url"": ""https://github.com/owner/repo/discussions/1"",
                    },
                )
            ],
            test_mock={
                ""list_discussions"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Discussion 1"",
                        ""url"": ""https://github.com/owner/repo/discussions/1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_discussions(
        credentials: GithubCredentials, repo_url: str, num_discussions: int
    ) -> list[Output.DiscussionItem]:
        api = get_api(credentials)
        # GitHub GraphQL API endpoint is different; we'll use api.post with custom URL
        repo_path = repo_url.replace(""https://github.com/"", """")
        owner, repo = repo_path.split(""/"")
        query = """"""
        query($owner: String!, $repo: String!, $num: Int!) {
            repository(owner: $owner, name: $repo) {
                discussions(first: $num) {
                    nodes {
                        title
                        url
                    }
                }
            }
        }
        """"""
        variables = {""owner"": owner, ""repo"": repo, ""num"": num_discussions}
        response = api.post(
            ""https://api.github.com/graphql"",
            json={""query"": query, ""variables"": variables},
        )
        data = response.json()
        discussions: list[GithubListDiscussionsBlock.Output.DiscussionItem] = [
            {""title"": discussion[""title""], ""url"": discussion[""url""]}
            for discussion in data[""data""][""repository""][""discussions""][""nodes""]
        ]
        return discussions

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        discussions = self.list_discussions(
            credentials, input_data.repo_url, input_data.num_discussions
        )
        yield from ((""discussion"", discussion) for discussion in discussions)

class GithubListReleasesBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class ReleaseItem(TypedDict):
            name: str
            url: str

        release: ReleaseItem = SchemaField(
            title=""Release"",
            description=""Releases with their name and file tree browser URL"",
        )
        error: str = SchemaField(description=""Error message if listing releases failed"")

    def __init__(self):
        super().__init__(
            id=""3460367a-6ba7-4645-8ce6-47b05d040b92"",
            description=""This block lists all releases for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListReleasesBlock.Input,
            output_schema=GithubListReleasesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""release"",
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/releases/tag/v1.0.0"",
                    },
                )
            ],
            test_mock={
                ""list_releases"": lambda *args, **kwargs: [
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/releases/tag/v1.0.0"",
                    }
                ]
            },
        )

    @staticmethod
    def list_releases(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.ReleaseItem]:
        api = get_api(credentials)
        releases_url = repo_url + ""/releases""
        response = api.get(releases_url)
        data = response.json()
        releases: list[GithubListReleasesBlock.Output.ReleaseItem] = [
            {""name"": release[""name""], ""url"": release[""html_url""]} for release in data
        ]
        return releases

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        releases = self.list_releases(
            credentials,
            input_data.repo_url,
        )
        yield from ((""release"", release) for release in releases)

class GithubReadFileBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        file_path: str = SchemaField(
            description=""Path to the file in the repository"",
            placeholder=""path/to/file"",
        )
        branch: str = SchemaField(
            description=""Branch to read from"",
            placeholder=""branch_name"",
            default=""master"",
        )

    class Output(BlockSchema):
        text_content: str = SchemaField(
            description=""Content of the file (decoded as UTF-8 text)""
        )
        raw_content: str = SchemaField(
            description=""Raw base64-encoded content of the file""
        )
        size: int = SchemaField(description=""The size of the file (in bytes)"")
        error: str = SchemaField(description=""Error message if the file reading failed"")

    def __init__(self):
        super().__init__(
            id=""87ce6c27-5752-4bbc-8e26-6da40a3dcfd3"",
            description=""This block reads the content of a specified file from a GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadFileBlock.Input,
            output_schema=GithubReadFileBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""file_path"": ""path/to/file"",
                ""branch"": ""master"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""raw_content"", ""RmlsZSBjb250ZW50""),
                (""text_content"", ""File content""),
                (""size"", 13),
            ],
            test_mock={""read_file"": lambda *args, **kwargs: (""RmlsZSBjb250ZW50"", 13)},
        )

    @staticmethod
    def read_file(
        credentials: GithubCredentials, repo_url: str, file_path: str, branch: str
    ) -> tuple[str, int]:
        api = get_api(credentials)
        content_url = repo_url + f""/contents/{file_path}?ref={branch}""
        response = api.get(content_url)
        content = response.json()

        if isinstance(content, list):
            # Multiple entries of different types exist at this path
            if not (file := next((f for f in content if f[""type""] == ""file""), None)):
                raise TypeError(""Not a file"")
            content = file

        if content[""type""] != ""file"":
            raise TypeError(""Not a file"")

        return content[""content""], content[""size""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        raw_content, size = self.read_file(
            credentials,
            input_data.repo_url,
            input_data.file_path.lstrip(""/""),
            input_data.branch,
        )
        yield ""raw_content"", raw_content
        yield ""text_content"", base64.b64decode(raw_content).decode(""utf-8"")
        yield ""size"", size

class GithubReadFolderBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        folder_path: str = SchemaField(
            description=""Path to the folder in the repository"",
            placeholder=""path/to/folder"",
        )
        branch: str = SchemaField(
            description=""Branch name to read from (defaults to master)"",
            placeholder=""branch_name"",
            default=""master"",
        )

    class Output(BlockSchema):
        class DirEntry(TypedDict):
            name: str
            path: str

        class FileEntry(TypedDict):
            name: str
            path: str
            size: int

        file: FileEntry = SchemaField(description=""Files in the folder"")
        dir: DirEntry = SchemaField(description=""Directories in the folder"")
        error: str = SchemaField(
            description=""Error message if reading the folder failed""
        )

    def __init__(self):
        super().__init__(
            id=""1355f863-2db3-4d75-9fba-f91e8a8ca400"",
            description=""This block reads the content of a specified folder from a GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadFolderBlock.Input,
            output_schema=GithubReadFolderBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""folder_path"": ""path/to/folder"",
                ""branch"": ""master"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""file"",
                    {
                        ""name"": ""file1.txt"",
                        ""path"": ""path/to/folder/file1.txt"",
                        ""size"": 1337,
                    },
                ),
                (""dir"", {""name"": ""dir2"", ""path"": ""path/to/folder/dir2""}),
            ],
            test_mock={
                ""read_folder"": lambda *args, **kwargs: (
                    [
                        {
                            ""name"": ""file1.txt"",
                            ""path"": ""path/to/folder/file1.txt"",
                            ""size"": 1337,
                        }
                    ],
                    [{""name"": ""dir2"", ""path"": ""path/to/folder/dir2""}],
                )
            },
        )

    @staticmethod
    def read_folder(
        credentials: GithubCredentials, repo_url: str, folder_path: str, branch: str
    ) -> tuple[list[Output.FileEntry], list[Output.DirEntry]]:
        api = get_api(credentials)
        contents_url = repo_url + f""/contents/{folder_path}?ref={branch}""
        response = api.get(contents_url)
        content = response.json()

        if not isinstance(content, list):
            raise TypeError(""Not a folder"")

        files = [
            GithubReadFolderBlock.Output.FileEntry(
                name=entry[""name""],
                path=entry[""path""],
                size=entry[""size""],
            )
            for entry in content
            if entry[""type""] == ""file""
        ]

        dirs = [
            GithubReadFolderBlock.Output.DirEntry(
                name=entry[""name""],
                path=entry[""path""],
            )
            for entry in content
            if entry[""type""] == ""dir""
        ]

        return files, dirs

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        files, dirs = self.read_folder(
            credentials,
            input_data.repo_url,
            input_data.folder_path.lstrip(""/""),
            input_data.branch,
        )
        yield from ((""file"", file) for file in files)
        yield from ((""dir"", dir) for dir in dirs)

class GithubMakeBranchBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        new_branch: str = SchemaField(
            description=""Name of the new branch"",
            placeholder=""new_branch_name"",
        )
        source_branch: str = SchemaField(
            description=""Name of the source branch"",
            placeholder=""source_branch_name"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the branch creation operation"")
        error: str = SchemaField(
            description=""Error message if the branch creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""944cc076-95e7-4d1b-b6b6-b15d8ee5448d"",
            description=""This block creates a new branch from a specified source branch."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakeBranchBlock.Input,
            output_schema=GithubMakeBranchBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""new_branch"": ""new_branch_name"",
                ""source_branch"": ""source_branch_name"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Branch created successfully"")],
            test_mock={
                ""create_branch"": lambda *args, **kwargs: ""Branch created successfully""
            },
        )

    @staticmethod
    def create_branch(
        credentials: GithubCredentials,
        repo_url: str,
        new_branch: str,
        source_branch: str,
    ) -> str:
        api = get_api(credentials)
        # Get the SHA of the source branch
        ref_url = repo_url + f""/git/refs/heads/{source_branch}""
        response = api.get(ref_url)
        sha = response.json()[""object""][""sha""]

        # Create the new branch
        create_ref_url = repo_url + ""/git/refs""
        data = {""ref"": f""refs/heads/{new_branch}"", ""sha"": sha}
        response = api.post(create_ref_url, json=data)
        return ""Branch created successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.create_branch(
            credentials,
            input_data.repo_url,
            input_data.new_branch,
            input_data.source_branch,
        )
        yield ""status"", status

class GithubDeleteBranchBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        branch: str = SchemaField(
            description=""Name of the branch to delete"",
            placeholder=""branch_name"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the branch deletion operation"")
        error: str = SchemaField(
            description=""Error message if the branch deletion failed""
        )

    def __init__(self):
        super().__init__(
            id=""0d4130f7-e0ab-4d55-adc3-0a40225e80f4"",
            description=""This block deletes a specified branch."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubDeleteBranchBlock.Input,
            output_schema=GithubDeleteBranchBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""branch"": ""branch_name"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Branch deleted successfully"")],
            test_mock={
                ""delete_branch"": lambda *args, **kwargs: ""Branch deleted successfully""
            },
        )

    @staticmethod
    def delete_branch(
        credentials: GithubCredentials, repo_url: str, branch: str
    ) -> str:
        api = get_api(credentials)
        ref_url = repo_url + f""/git/refs/heads/{branch}""
        api.delete(ref_url)
        return ""Branch deleted successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.delete_branch(
            credentials,
            input_data.repo_url,
            input_data.branch,
        )
        yield ""status"", status","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
13,"from typing import Any, Literal

from pydantic import SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName
from backend.util.request import requests

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""unreal_speech"",
    api_key=SecretStr(""mock-unreal-speech-api-key""),
    title=""Mock Unreal Speech API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class UnrealTextToSpeechBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""The text to be converted to speech"",
            placeholder=""Enter the text you want to convert to speech"",
        )
        voice_id: str = SchemaField(
            description=""The voice ID to use for text-to-speech conversion"",
            placeholder=""Scarlett"",
            default=""Scarlett"",
        )
        credentials: CredentialsMetaInput[
            Literal[ProviderName.UNREAL_SPEECH], Literal[""api_key""]
        ] = CredentialsField(
            description=""The Unreal Speech integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )

    class Output(BlockSchema):
        mp3_url: str = SchemaField(description=""The URL of the generated MP3 file"")
        error: str = SchemaField(description=""Error message if the API call failed"")

    def __init__(self):
        super().__init__(
            id=""4ff1ff6d-cc40-4caa-ae69-011daa20c378"",
            description=""Converts text to speech using the Unreal Speech API"",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=UnrealTextToSpeechBlock.Input,
            output_schema=UnrealTextToSpeechBlock.Output,
            test_input={
                ""text"": ""This is a test of the text to speech API."",
                ""voice_id"": ""Scarlett"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[(""mp3_url"", ""https://example.com/test.mp3"")],
            test_mock={
                ""call_unreal_speech_api"": lambda *args, **kwargs: {
                    ""OutputUri"": ""https://example.com/test.mp3""
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )

    @staticmethod
    def call_unreal_speech_api(
        api_key: SecretStr, text: str, voice_id: str
    ) -> dict[str, Any]:
        url = ""https://api.v7.unrealspeech.com/speech""
        headers = {
            ""Authorization"": f""Bearer {api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
        }
        data = {
            ""Text"": text,
            ""VoiceId"": voice_id,
            ""Bitrate"": ""192k"",
            ""Speed"": ""0"",
            ""Pitch"": ""1"",
            ""TimestampType"": ""sentence"",
        }

        response = requests.post(url, headers=headers, json=data)
        return response.json()

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        api_response = self.call_unreal_speech_api(
            credentials.api_key,
            input_data.text,
            input_data.voice_id,
        )
        yield ""mp3_url"", api_response[""OutputUri""]
","from typing import Any, Literal
from pydantic import SecretStr
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName
from backend.util.request import requests

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""unreal_speech"",
    api_key=SecretStr(""mock-unreal-speech-api-key""),
    title=""Mock Unreal Speech API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}

class UnrealTextToSpeechBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""The text to be converted to speech"",
            placeholder=""Enter the text you want to convert to speech"",
        )
        voice_id: str = SchemaField(
            description=""The voice ID to use for text-to-speech conversion"",
            placeholder=""Scarlett"",
            default=""Scarlett"",
        )
        credentials: CredentialsMetaInput[
            Literal[ProviderName.UNREAL_SPEECH], Literal[""api_key""]
        ] = CredentialsField(
            description=""The Unreal Speech integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )

    class Output(BlockSchema):
        mp3_url: str = SchemaField(description=""The URL of the generated MP3 file"")
        error: str = SchemaField(description=""Error message if the API call failed"")

    def __init__(self):
        super().__init__(
            id=""4ff1ff6d-cc40-4caa-ae69-011daa20c378"",
            description=""Converts text to speech using the Unreal Speech API"",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=UnrealTextToSpeechBlock.Input,
            output_schema=UnrealTextToSpeechBlock.Output,
            test_input={
                ""text"": ""This is a test of the text to speech API."",
                ""voice_id"": ""Scarlett"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[(""mp3_url"", ""https://example.com/test.mp3"")],
            test_mock={
                ""call_unreal_speech_api"": lambda *args, **kwargs: {
                    ""OutputUri"": ""https://example.com/test.mp3""
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )

    @staticmethod
    def call_unreal_speech_api(
        api_key: SecretStr, text: str, voice_id: str
    ) -> dict[str, Any]:
        url = ""https://api.v7.unrealspeech.com/speech""
        headers = {
            ""Authorization"": f""Bearer {api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
        }
        data = {
            ""Text"": text,
            ""VoiceId"": voice_id,
            ""Bitrate"": ""192k"",
            ""Speed"": ""0"",
            ""Pitch"": ""1"",
            ""TimestampType"": ""sentence"",
        }

        response = requests.post(url, headers=headers, json=data)
        return response.json()

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        api_response = self.call_unreal_speech_api(
            credentials.api_key,
            input_data.text,
            input_data.voice_id,
        )
        yield ""mp3_url"", api_response[""OutputUri""]","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
14,"import re

from jinja2 import BaseLoader
from jinja2.sandbox import SandboxedEnvironment


class TextFormatter:
    def __init__(self):
        # Create a sandboxed environment
        self.env = SandboxedEnvironment(loader=BaseLoader(), autoescape=True)

        # Clear any registered filters, tests, and globals to minimize attack surface
        self.env.filters.clear()
        self.env.tests.clear()
        self.env.globals.clear()

    def format_string(self, template_str: str, values=None, **kwargs) -> str:
        # For python.format compatibility: replace all {...} with {{..}}.
        # But avoid replacing {{...}} to {{{...}}}.
        template_str = re.sub(r""(?<!{){[ a-zA-Z0-9_]+}"", r""{\g<0>}"", template_str)
        template = self.env.from_string(template_str)
        return template.render(values or {}, **kwargs)
","import re
from jinja2 import BaseLoader
from jinja2.sandbox import SandboxedEnvironment

class TextFormatter:
    def __init__(self):
        # Create a sandboxed environment
        self.env = SandboxedEnvironment(loader=BaseLoader(), autoescape=True)

        # Clear any registered filters, tests, and globals to minimize attack surface
        self.env.filters.clear()
        self.env.tests.clear()
        self.env.globals.clear()

    def format_string(self, template_str: str, values=None, **kwargs) -> str:
        # For python.format compatibility: replace all {...} with {{..}}.
        # But avoid replacing {{...}} to {{{...}}}.
        template_str = re.sub(r""(?<!{){[ a-zA-Z0-9_]+}"", r""{\g<0>}"", template_str)
        template = self.env.from_string(template_str)
        return template.render(values or {}, **kwargs)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
15,"from __future__ import annotations


class IIRFilter:
    r""""""
    N-Order IIR filter
    Assumes working with float samples normalized on [-1, 1]

    ---

    Implementation details:
    Based on the 2nd-order function from
    https://en.wikipedia.org/wiki/Digital_biquad_filter,
    this generalized N-order function was made.

    Using the following transfer function
        .. math:: H(z)=\frac{b_{0}+b_{1}z^{-1}+b_{2}z^{-2}+...+b_{k}z^{-k}}
                  {a_{0}+a_{1}z^{-1}+a_{2}z^{-2}+...+a_{k}z^{-k}}

    we can rewrite this to
        .. math:: y[n]={\frac{1}{a_{0}}}
                  \left(\left(b_{0}x[n]+b_{1}x[n-1]+b_{2}x[n-2]+...+b_{k}x[n-k]\right)-
                  \left(a_{1}y[n-1]+a_{2}y[n-2]+...+a_{k}y[n-k]\right)\right)
    """"""

    def __init__(self, order: int) -> None:
        self.order = order

        # a_{0} ... a_{k}
        self.a_coeffs = [1.0] + [0.0] * order
        # b_{0} ... b_{k}
        self.b_coeffs = [1.0] + [0.0] * order

        # x[n-1] ... x[n-k]
        self.input_history = [0.0] * self.order
        # y[n-1] ... y[n-k]
        self.output_history = [0.0] * self.order

    def set_coefficients(self, a_coeffs: list[float], b_coeffs: list[float]) -> None:
        """"""
        Set the coefficients for the IIR filter.
        These should both be of size `order` + 1.
        :math:`a_0` may be left out, and it will use 1.0 as default value.

        This method works well with scipy's filter design functions

        >>> # Make a 2nd-order 1000Hz butterworth lowpass filter
        >>> import scipy.signal
        >>> b_coeffs, a_coeffs = scipy.signal.butter(2, 1000,
        ...                                          btype='lowpass',
        ...                                          fs=48000)
        >>> filt = IIRFilter(2)
        >>> filt.set_coefficients(a_coeffs, b_coeffs)
        """"""
        if len(a_coeffs) < self.order:
            a_coeffs = [1.0, *a_coeffs]

        if len(a_coeffs) != self.order + 1:
            msg = (
                f""Expected a_coeffs to have {self.order + 1} elements ""
                f""for {self.order}-order filter, got {len(a_coeffs)}""
            )
            raise ValueError(msg)

        if len(b_coeffs) != self.order + 1:
            msg = (
                f""Expected b_coeffs to have {self.order + 1} elements ""
                f""for {self.order}-order filter, got {len(a_coeffs)}""
            )
            raise ValueError(msg)

        self.a_coeffs = a_coeffs
        self.b_coeffs = b_coeffs

    def process(self, sample: float) -> float:
        """"""
        Calculate :math:`y[n]`

        >>> filt = IIRFilter(2)
        >>> filt.process(0)
        0.0
        """"""
        result = 0.0

        # Start at index 1 and do index 0 at the end.
        for i in range(1, self.order + 1):
            result += (
                self.b_coeffs[i] * self.input_history[i - 1]
                - self.a_coeffs[i] * self.output_history[i - 1]
            )

        result = (result + self.b_coeffs[0] * sample) / self.a_coeffs[0]

        self.input_history[1:] = self.input_history[:-1]
        self.output_history[1:] = self.output_history[:-1]

        self.input_history[0] = sample
        self.output_history[0] = result

        return result
","from __future__ import annotations

class IIRFilter:
    r""""""
    N-Order IIR filter
    Assumes working with float samples normalized on [-1, 1]

    ---

    Implementation details:
    Based on the 2nd-order function from
    https://en.wikipedia.org/wiki/Digital_biquad_filter,
    this generalized N-order function was made.

    Using the following transfer function
        .. math:: H(z)=\frac{b_{0}+b_{1}z^{-1}+b_{2}z^{-2}+...+b_{k}z^{-k}}
                  {a_{0}+a_{1}z^{-1}+a_{2}z^{-2}+...+a_{k}z^{-k}}

    we can rewrite this to
        .. math:: y[n]={\frac{1}{a_{0}}}
                  \left(\left(b_{0}x[n]+b_{1}x[n-1]+b_{2}x[n-2]+...+b_{k}x[n-k]\right)-
                  \left(a_{1}y[n-1]+a_{2}y[n-2]+...+a_{k}y[n-k]\right)\right)
    """"""

    def __init__(self, order: int) -> None:
        self.order = order

        # a_{0} ... a_{k}
        self.a_coeffs = [1.0] + [0.0] * order
        # b_{0} ... b_{k}
        self.b_coeffs = [1.0] + [0.0] * order

        # x[n-1] ... x[n-k]
        self.input_history = [0.0] * self.order
        # y[n-1] ... y[n-k]
        self.output_history = [0.0] * self.order

    def set_coefficients(self, a_coeffs: list[float], b_coeffs: list[float]) -> None:
        """"""
        Set the coefficients for the IIR filter.
        These should both be of size `order` + 1.
        :math:`a_0` may be left out, and it will use 1.0 as default value.

        This method works well with scipy's filter design functions

        >>> # Make a 2nd-order 1000Hz butterworth lowpass filter
        >>> import scipy.signal
        >>> b_coeffs, a_coeffs = scipy.signal.butter(2, 1000,
        ...                                          btype='lowpass',
        ...                                          fs=48000)
        >>> filt = IIRFilter(2)
        >>> filt.set_coefficients(a_coeffs, b_coeffs)
        """"""
        if len(a_coeffs) < self.order:
            a_coeffs = [1.0, *a_coeffs]

        if len(a_coeffs) != self.order + 1:
            msg = (
                f""Expected a_coeffs to have {self.order + 1} elements ""
                f""for {self.order}-order filter, got {len(a_coeffs)}""
            )
            raise ValueError(msg)

        if len(b_coeffs) != self.order + 1:
            msg = (
                f""Expected b_coeffs to have {self.order + 1} elements ""
                f""for {self.order}-order filter, got {len(a_coeffs)}""
            )
            raise ValueError(msg)

        self.a_coeffs = a_coeffs
        self.b_coeffs = b_coeffs

    def process(self, sample: float) -> float:
        """"""
        Calculate :math:`y[n]`

        >>> filt = IIRFilter(2)
        >>> filt.process(0)
        0.0
        """"""
        result = 0.0

        # Start at index 1 and do index 0 at the end.
        for i in range(1, self.order + 1):
            result += (
                self.b_coeffs[i] * self.input_history[i - 1]
                - self.a_coeffs[i] * self.output_history[i - 1]
            )

        result = (result + self.b_coeffs[0] * sample) / self.a_coeffs[0]

        self.input_history[1:] = self.input_history[:-1]
        self.output_history[1:] = self.output_history[:-1]

        self.input_history[0] = sample
        self.output_history[0] = result

        return result","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
16,"from backend.blocks.hubspot._auth import (
    HubSpotCredentials,
    HubSpotCredentialsField,
    HubSpotCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class HubSpotContactBlock(Block):
    class Input(BlockSchema):
        credentials: HubSpotCredentialsInput = HubSpotCredentialsField()
        operation: str = SchemaField(
            description=""Operation to perform (create, update, get)"", default=""get""
        )
        contact_data: dict = SchemaField(
            description=""Contact data for create/update operations"", default={}
        )
        email: str = SchemaField(
            description=""Email address for get/update operations"", default=""""
        )

    class Output(BlockSchema):
        contact: dict = SchemaField(description=""Contact information"")
        status: str = SchemaField(description=""Operation status"")

    def __init__(self):
        super().__init__(
            id=""5267326e-c4c1-4016-9f54-4e72ad02f813"",
            description=""Manages HubSpot contacts - create, update, and retrieve contact information"",
            categories={BlockCategory.CRM},
            input_schema=HubSpotContactBlock.Input,
            output_schema=HubSpotContactBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: HubSpotCredentials, **kwargs
    ) -> BlockOutput:
        base_url = ""https://api.hubapi.com/crm/v3/objects/contacts""
        headers = {
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
        }

        if input_data.operation == ""create"":
            response = requests.post(
                base_url, headers=headers, json={""properties"": input_data.contact_data}
            )
            result = response.json()
            yield ""contact"", result
            yield ""status"", ""created""

        elif input_data.operation == ""get"":
            # Search for contact by email
            search_url = f""{base_url}/search""
            search_data = {
                ""filterGroups"": [
                    {
                        ""filters"": [
                            {
                                ""propertyName"": ""email"",
                                ""operator"": ""EQ"",
                                ""value"": input_data.email,
                            }
                        ]
                    }
                ]
            }
            response = requests.post(search_url, headers=headers, json=search_data)
            result = response.json()
            yield ""contact"", result.get(""results"", [{}])[0]
            yield ""status"", ""retrieved""

        elif input_data.operation == ""update"":
            search_response = requests.post(
                f""{base_url}/search"",
                headers=headers,
                json={
                    ""filterGroups"": [
                        {
                            ""filters"": [
                                {
                                    ""propertyName"": ""email"",
                                    ""operator"": ""EQ"",
                                    ""value"": input_data.email,
                                }
                            ]
                        }
                    ]
                },
            )
            contact_id = search_response.json().get(""results"", [{}])[0].get(""id"")

            if contact_id:
                response = requests.patch(
                    f""{base_url}/{contact_id}"",
                    headers=headers,
                    json={""properties"": input_data.contact_data},
                )
                result = response.json()
                yield ""contact"", result
                yield ""status"", ""updated""
            else:
                yield ""contact"", {}
                yield ""status"", ""contact_not_found""
","from backend.blocks.hubspot._auth import (
    HubSpotCredentials,
    HubSpotCredentialsField,
    HubSpotCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class HubSpotContactBlock(Block):
    class Input(BlockSchema):
        credentials: HubSpotCredentialsInput = HubSpotCredentialsField()
        operation: str = SchemaField(
            description=""Operation to perform (create, update, get)"", default=""get""
        )
        contact_data: dict = SchemaField(
            description=""Contact data for create/update operations"", default={}
        )
        email: str = SchemaField(
            description=""Email address for get/update operations"", default=""""
        )

    class Output(BlockSchema):
        contact: dict = SchemaField(description=""Contact information"")
        status: str = SchemaField(description=""Operation status"")

    def __init__(self):
        super().__init__(
            id=""5267326e-c4c1-4016-9f54-4e72ad02f813"",
            description=""Manages HubSpot contacts - create, update, and retrieve contact information"",
            categories={BlockCategory.CRM},
            input_schema=HubSpotContactBlock.Input,
            output_schema=HubSpotContactBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: HubSpotCredentials, **kwargs
    ) -> BlockOutput:
        base_url = ""https://api.hubapi.com/crm/v3/objects/contacts""
        headers = {
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
        }

        if input_data.operation == ""create"":
            response = requests.post(
                base_url, headers=headers, json={""properties"": input_data.contact_data}
            )
            result = response.json()
            yield ""contact"", result
            yield ""status"", ""created""

        elif input_data.operation == ""get"":
            # Search for contact by email
            search_url = f""{base_url}/search""
            search_data = {
                ""filterGroups"": [
                    {
                        ""filters"": [
                            {
                                ""propertyName"": ""email"",
                                ""operator"": ""EQ"",
                                ""value"": input_data.email,
                            }
                        ]
                    }
                ]
            }
            response = requests.post(search_url, headers=headers, json=search_data)
            result = response.json()
            yield ""contact"", result.get(""results"", [{}])[0]
            yield ""status"", ""retrieved""

        elif input_data.operation == ""update"":
            search_response = requests.post(
                f""{base_url}/search"",
                headers=headers,
                json={
                    ""filterGroups"": [
                        {
                            ""filters"": [
                                {
                                    ""propertyName"": ""email"",
                                    ""operator"": ""EQ"",
                                    ""value"": input_data.email,
                                }
                            ]
                        }
                    ]
                },
            )
            contact_id = search_response.json().get(""results"", [{}])[0].get(""id"")

            if contact_id:
                response = requests.patch(
                    f""{base_url}/{contact_id}"",
                    headers=headers,
                    json={""properties"": input_data.contact_data},
                )
                result = response.json()
                yield ""contact"", result
                yield ""status"", ""updated""
            else:
                yield ""contact"", {}
                yield ""status"", ""contact_not_found""","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
17,"""""""
Author  : Alexander Pantyukhin
Date    : November 24, 2022

Task:
Given an m x n grid of characters board and a string word,
return true if word exists in the grid.

The word can be constructed from letters of sequentially adjacent cells,
where adjacent cells are horizontally or vertically neighboring.
The same letter cell may not be used more than once.

Example:

Matrix:
---------
|A|B|C|E|
|S|F|C|S|
|A|D|E|E|
---------

Word:
""ABCCED""

Result:
True

Implementation notes: Use backtracking approach.
At each point, check all neighbors to try to find the next letter of the word.

leetcode: https://leetcode.com/problems/word-search/

""""""


def get_point_key(len_board: int, len_board_column: int, row: int, column: int) -> int:
    """"""
    Returns the hash key of matrix indexes.

    >>> get_point_key(10, 20, 1, 0)
    200
    """"""

    return len_board * len_board_column * row + column


def exits_word(
    board: list[list[str]],
    word: str,
    row: int,
    column: int,
    word_index: int,
    visited_points_set: set[int],
) -> bool:
    """"""
    Return True if it's possible to search the word suffix
    starting from the word_index.

    >>> exits_word([[""A""]], ""B"", 0, 0, 0, set())
    False
    """"""

    if board[row][column] != word[word_index]:
        return False

    if word_index == len(word) - 1:
        return True

    traverts_directions = [(0, 1), (0, -1), (-1, 0), (1, 0)]
    len_board = len(board)
    len_board_column = len(board[0])
    for direction in traverts_directions:
        next_i = row + direction[0]
        next_j = column + direction[1]
        if not (0 <= next_i < len_board and 0 <= next_j < len_board_column):
            continue

        key = get_point_key(len_board, len_board_column, next_i, next_j)
        if key in visited_points_set:
            continue

        visited_points_set.add(key)
        if exits_word(board, word, next_i, next_j, word_index + 1, visited_points_set):
            return True

        visited_points_set.remove(key)

    return False


def word_exists(board: list[list[str]], word: str) -> bool:
    """"""
    >>> word_exists([[""A"",""B"",""C"",""E""],[""S"",""F"",""C"",""S""],[""A"",""D"",""E"",""E""]], ""ABCCED"")
    True
    >>> word_exists([[""A"",""B"",""C"",""E""],[""S"",""F"",""C"",""S""],[""A"",""D"",""E"",""E""]], ""SEE"")
    True
    >>> word_exists([[""A"",""B"",""C"",""E""],[""S"",""F"",""C"",""S""],[""A"",""D"",""E"",""E""]], ""ABCB"")
    False
    >>> word_exists([[""A""]], ""A"")
    True
    >>> word_exists([[""B"", ""A"", ""A""], [""A"", ""A"", ""A""], [""A"", ""B"", ""A""]], ""ABB"")
    False
    >>> word_exists([[""A""]], 123)
    Traceback (most recent call last):
        ...
    ValueError: The word parameter should be a string of length greater than 0.
    >>> word_exists([[""A""]], """")
    Traceback (most recent call last):
        ...
    ValueError: The word parameter should be a string of length greater than 0.
    >>> word_exists([[]], ""AB"")
    Traceback (most recent call last):
        ...
    ValueError: The board should be a non empty matrix of single chars strings.
    >>> word_exists([], ""AB"")
    Traceback (most recent call last):
        ...
    ValueError: The board should be a non empty matrix of single chars strings.
    >>> word_exists([[""A""], [21]], ""AB"")
    Traceback (most recent call last):
        ...
    ValueError: The board should be a non empty matrix of single chars strings.
    """"""

    # Validate board
    board_error_message = (
        ""The board should be a non empty matrix of single chars strings.""
    )

    len_board = len(board)
    if not isinstance(board, list) or len(board) == 0:
        raise ValueError(board_error_message)

    for row in board:
        if not isinstance(row, list) or len(row) == 0:
            raise ValueError(board_error_message)

        for item in row:
            if not isinstance(item, str) or len(item) != 1:
                raise ValueError(board_error_message)

    # Validate word
    if not isinstance(word, str) or len(word) == 0:
        raise ValueError(
            ""The word parameter should be a string of length greater than 0.""
        )

    len_board_column = len(board[0])
    for i in range(len_board):
        for j in range(len_board_column):
            if exits_word(
                board, word, i, j, 0, {get_point_key(len_board, len_board_column, i, j)}
            ):
                return True

    return False


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","""""""
Author  : Alexander Pantyukhin
Date    : November 24, 2022

Task:
Given an m x n grid of characters board and a string word,
return true if word exists in the grid.

The word can be constructed from letters of sequentially adjacent cells,
where adjacent cells are horizontally or vertically neighboring.
The same letter cell may not be used more than once.

Example:

Matrix:
---------
|A|B|C|E|
|S|F|C|S|
|A|D|E|E|
---------

Word:
""ABCCED""

Result:
True

Implementation notes: Use backtracking approach.
At each point, check all neighbors to try to find the next letter of the word.

leetcode: https://leetcode.com/problems/word-search/

""""""



def get_point_key(len_board: int, len_board_column: int, row: int, column: int) -> int:
    """"""
    Returns the hash key of matrix indexes.

    >>> get_point_key(10, 20, 1, 0)
    200
    """"""
    return len_board * len_board_column * row + column


def exits_word(
    board: list[list[str]],
    word: str,
    row: int,
    column: int,
    word_index: int,
    visited_points_set: set[int],
) -> bool:
    """"""
    Return True if it's possible to search the word suffix
    starting from the word_index.

    >>> exits_word([[""A""]], ""B"", 0, 0, 0, set())
    False
    """"""
    if board[row][column] != word[word_index]:
        return False
    if word_index == len(word) - 1:
        return True
    traverts_directions = [(0, 1), (0, -1), (-1, 0), (1, 0)]
    len_board = len(board)
    len_board_column = len(board[0])

    for direction in traverts_directions:
        next_i = row + direction[0]
        next_j = column + direction[1]
        if not (0 <= next_i < len_board and 0 <= next_j < len_board_column):
            continue
        key = get_point_key(len_board, len_board_column, next_i, next_j)
        if key in visited_points_set:
            continue
        visited_points_set.add(key)
        if exits_word(board, word, next_i, next_j, word_index + 1, visited_points_set):
            return True
        visited_points_set.remove(key)
    return False


def word_exists(board: list[list[str]], word: str) -> bool:
    """"""
    >>> word_exists([[""A"",""B"",""C"",""E""],[""S"",""F"",""C"",""S""],[""A"",""D"",""E"",""E""]], ""ABCCED"")
    True
    >>> word_exists([[""A"",""B"",""C"",""E""],[""S"",""F"",""C"",""S""],[""A"",""D"",""E"",""E""]], ""SEE"")
    True
    >>> word_exists([[""A"",""B"",""C"",""E""],[""S"",""F"",""C"",""S""],[""A"",""D"",""E"",""E""]], ""ABCB"")
    False
    >>> word_exists([[""A""]], ""A"")
    True
    >>> word_exists([[""B"", ""A"", ""A""], [""A"", ""A"", ""A""], [""A"", ""B"", ""A""]], ""ABB"")
    False
    >>> word_exists([[""A""]], 123)
    Traceback (most recent call last):
        ...
    ValueError: The word parameter should be a string of length greater than 0.
    >>> word_exists([[""A""]], """")
    Traceback (most recent call last):
        ...
    ValueError: The word parameter should be a string of length greater than 0.
    >>> word_exists([[]], ""AB"")
    Traceback (most recent call last):
        ...
    ValueError: The board should be a non empty matrix of single chars strings.
    >>> word_exists([], ""AB"")
    Traceback (most recent call last):
        ...
    ValueError: The board should be a non empty matrix of single chars strings.
    >>> word_exists([[""A""], [21]], ""AB"")
    Traceback (most recent call last):
        ...
    ValueError: The board should be a non empty matrix of single chars strings.
    """"""
    # Validate board
    board_error_message = (
        ""The board should be a non empty matrix of single chars strings.""
    )
    len_board = len(board)
    if not isinstance(board, list) or len(board) == 0:
        raise ValueError(board_error_message)
    for row in board:
        if not isinstance(row, list) or len(row) == 0:
            raise ValueError(board_error_message)
        for item in row:
            if not isinstance(item, str) or len(item) != 1:
                raise ValueError(board_error_message)
    # Validate word
    if not isinstance(word, str) or len(word) == 0:
        raise ValueError(
            ""The word parameter should be a string of length greater than 0.""
        )
    len_board_column = len(board[0])
    for i in range(len_board):
        for j in range(len_board_column):
            if exits_word(
                board, word, i, j, 0, {get_point_key(len_board, len_board_column, i, j)}
            ):
                return True
    return False


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
18,"""""""
Graph Coloring also called ""m coloring problem""
consists of coloring a given graph with at most m colors
such that no adjacent vertices are assigned the same color

Wikipedia: https://en.wikipedia.org/wiki/Graph_coloring
""""""


def valid_coloring(
    neighbours: list[int], colored_vertices: list[int], color: int
) -> bool:
    """"""
    For each neighbour check if the coloring constraint is satisfied
    If any of the neighbours fail the constraint return False
    If all neighbours validate the constraint return True

    >>> neighbours = [0,1,0,1,0]
    >>> colored_vertices = [0, 2, 1, 2, 0]

    >>> color = 1
    >>> valid_coloring(neighbours, colored_vertices, color)
    True

    >>> color = 2
    >>> valid_coloring(neighbours, colored_vertices, color)
    False
    """"""
    # Does any neighbour not satisfy the constraints
    return not any(
        neighbour == 1 and colored_vertices[i] == color
        for i, neighbour in enumerate(neighbours)
    )


def util_color(
    graph: list[list[int]], max_colors: int, colored_vertices: list[int], index: int
) -> bool:
    """"""
    Pseudo-Code

    Base Case:
    1. Check if coloring is complete
        1.1 If complete return True (meaning that we successfully colored the graph)

    Recursive Step:
    2. Iterates over each color:
        Check if the current coloring is valid:
            2.1. Color given vertex
            2.2. Do recursive call, check if this coloring leads to a solution
            2.4. if current coloring leads to a solution return
            2.5. Uncolor given vertex

    >>> graph = [[0, 1, 0, 0, 0],
    ...          [1, 0, 1, 0, 1],
    ...          [0, 1, 0, 1, 0],
    ...          [0, 1, 1, 0, 0],
    ...          [0, 1, 0, 0, 0]]
    >>> max_colors = 3
    >>> colored_vertices = [0, 1, 0, 0, 0]
    >>> index = 3

    >>> util_color(graph, max_colors, colored_vertices, index)
    True

    >>> max_colors = 2
    >>> util_color(graph, max_colors, colored_vertices, index)
    False
    """"""

    # Base Case
    if index == len(graph):
        return True

    # Recursive Step
    for i in range(max_colors):
        if valid_coloring(graph[index], colored_vertices, i):
            # Color current vertex
            colored_vertices[index] = i
            # Validate coloring
            if util_color(graph, max_colors, colored_vertices, index + 1):
                return True
            # Backtrack
            colored_vertices[index] = -1
    return False


def color(graph: list[list[int]], max_colors: int) -> list[int]:
    """"""
    Wrapper function to call subroutine called util_color
    which will either return True or False.
    If True is returned colored_vertices list is filled with correct colorings

    >>> graph = [[0, 1, 0, 0, 0],
    ...          [1, 0, 1, 0, 1],
    ...          [0, 1, 0, 1, 0],
    ...          [0, 1, 1, 0, 0],
    ...          [0, 1, 0, 0, 0]]

    >>> max_colors = 3
    >>> color(graph, max_colors)
    [0, 1, 0, 2, 0]

    >>> max_colors = 2
    >>> color(graph, max_colors)
    []
    """"""
    colored_vertices = [-1] * len(graph)

    if util_color(graph, max_colors, colored_vertices, 0):
        return colored_vertices

    return []
","""""""
Graph Coloring also called ""m coloring problem""
consists of coloring a given graph with at most m colors
such that no adjacent vertices are assigned the same color

Wikipedia: https://en.wikipedia.org/wiki/Graph_coloring
""""""


def valid_coloring(
    neighbours: list[int], colored_vertices: list[int], color: int
) -> bool:
    """"""
    For each neighbour check if the coloring constraint is satisfied
    If any of the neighbours fail the constraint return False
    If all neighbours validate the constraint return True

    >>> neighbours = [0,1,0,1,0]
    >>> colored_vertices = [0, 2, 1, 2, 0]

    >>> color = 1
    >>> valid_coloring(neighbours, colored_vertices, color)
    True

    >>> color = 2
    >>> valid_coloring(neighbours, colored_vertices, color)
    False
    """"""
    # Does any neighbour not satisfy the constraints
    return not any(
        neighbour == 1 and colored_vertices[i] == color
        for i, neighbour in enumerate(neighbours)
    )


def util_color(
    graph: list[list[int]], max_colors: int, colored_vertices: list[int], index: int
) -> bool:
    """"""
    Pseudo-Code

    Base Case:
    1. Check if coloring is complete
        1.1 If complete return True (meaning that we successfully colored the graph)

    Recursive Step:
    2. Iterates over each color:
        Check if the current coloring is valid:
            2.1. Color given vertex
            2.2. Do recursive call, check if this coloring leads to a solution
            2.4. if current coloring leads to a solution return
            2.5. Uncolor given vertex

    >>> graph = [[0, 1, 0, 0, 0],
    ...          [1, 0, 1, 0, 1],
    ...          [0, 1, 0, 1, 0],
    ...          [0, 1, 1, 0, 0],
    ...          [0, 1, 0, 0, 0]]
    >>> max_colors = 3
    >>> colored_vertices = [0, 1, 0, 0, 0]
    >>> index = 3

    >>> util_color(graph, max_colors, colored_vertices, index)
    True

    >>> max_colors = 2
    >>> util_color(graph, max_colors, colored_vertices, index)
    False
    """"""

    # Base Case
    if index == len(graph):
        return True

    # Recursive Step
    for i in range(max_colors):
        if valid_coloring(graph[index], colored_vertices, i):
            # Color current vertex
            colored_vertices[index] = i
            # Validate coloring
            if util_color(graph, max_colors, colored_vertices, index + 1):
                return True
            # Backtrack
            colored_vertices[index] = -1
    return False


def color(graph: list[list[int]], max_colors: int) -> list[int]:
    """"""
    Wrapper function to call subroutine called util_color
    which will either return True or False.
    If True is returned colored_vertices list is filled with correct colorings

    >>> graph = [[0, 1, 0, 0, 0],
    ...          [1, 0, 1, 0, 1],
    ...          [0, 1, 0, 1, 0],
    ...          [0, 1, 1, 0, 0],
    ...          [0, 1, 0, 0, 0]]

    >>> max_colors = 3
    >>> color(graph, max_colors)
    [0, 1, 0, 2, 0]

    >>> max_colors = 2
    >>> color(graph, max_colors)
    []
    """"""
    colored_vertices = [-1] * len(graph)
    if util_color(graph, max_colors, colored_vertices, 0):
        return colored_vertices
    return []","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
19,"import asyncio
import json
import logging
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, AsyncGenerator, Generator, Generic, Optional, TypeVar

from pydantic import BaseModel
from redis.asyncio.client import PubSub as AsyncPubSub
from redis.client import PubSub

from backend.data import redis

logger = logging.getLogger(__name__)


class DateTimeEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)


M = TypeVar(""M"", bound=BaseModel)


class BaseRedisEventBus(Generic[M], ABC):
    Model: type[M]

    @property
    @abstractmethod
    def event_bus_name(self) -> str:
        pass

    def _serialize_message(self, item: M, channel_key: str) -> tuple[str, str]:
        message = json.dumps(item.model_dump(), cls=DateTimeEncoder)
        channel_name = f""{self.event_bus_name}/{channel_key}""
        logger.info(f""[{channel_name}] Publishing an event to Redis {message}"")
        return message, channel_name

    def _deserialize_message(self, msg: Any, channel_key: str) -> M | None:
        message_type = ""pmessage"" if ""*"" in channel_key else ""message""
        if msg[""type""] != message_type:
            return None
        try:
            data = json.loads(msg[""data""])
            logger.info(f""Consuming an event from Redis {data}"")
            return self.Model(**data)
        except Exception as e:
            logger.error(f""Failed to parse event result from Redis {msg} {e}"")

    def _get_pubsub_channel(
        self, connection: redis.Redis | redis.AsyncRedis, channel_key: str
    ) -> tuple[PubSub | AsyncPubSub, str]:
        full_channel_name = f""{self.event_bus_name}/{channel_key}""
        pubsub = connection.pubsub()
        return pubsub, full_channel_name


class RedisEventBus(BaseRedisEventBus[M], ABC):
    Model: type[M]

    @property
    def connection(self) -> redis.Redis:
        return redis.get_redis()

    def publish_event(self, event: M, channel_key: str):
        message, full_channel_name = self._serialize_message(event, channel_key)
        self.connection.publish(full_channel_name, message)

    def listen_events(self, channel_key: str) -> Generator[M, None, None]:
        pubsub, full_channel_name = self._get_pubsub_channel(
            self.connection, channel_key
        )
        assert isinstance(pubsub, PubSub)

        if ""*"" in channel_key:
            pubsub.psubscribe(full_channel_name)
        else:
            pubsub.subscribe(full_channel_name)

        for message in pubsub.listen():
            if event := self._deserialize_message(message, channel_key):
                yield event


class AsyncRedisEventBus(BaseRedisEventBus[M], ABC):
    Model: type[M]

    @property
    async def connection(self) -> redis.AsyncRedis:
        return await redis.get_redis_async()

    async def publish_event(self, event: M, channel_key: str):
        message, full_channel_name = self._serialize_message(event, channel_key)
        connection = await self.connection
        await connection.publish(full_channel_name, message)

    async def listen_events(self, channel_key: str) -> AsyncGenerator[M, None]:
        pubsub, full_channel_name = self._get_pubsub_channel(
            await self.connection, channel_key
        )
        assert isinstance(pubsub, AsyncPubSub)

        if ""*"" in channel_key:
            await pubsub.psubscribe(full_channel_name)
        else:
            await pubsub.subscribe(full_channel_name)

        async for message in pubsub.listen():
            if event := self._deserialize_message(message, channel_key):
                yield event

    async def wait_for_event(
        self, channel_key: str, timeout: Optional[float] = None
    ) -> M | None:
        try:
            return await asyncio.wait_for(
                anext(aiter(self.listen_events(channel_key))), timeout
            )
        except TimeoutError:
            return None
","import asyncio
import json
import logging
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, AsyncGenerator, Generator, Generic, Optional, TypeVar

from pydantic import BaseModel
from redis.asyncio.client import PubSub as AsyncPubSub
from redis.client import PubSub

from backend.data import redis

logger = logging.getLogger(__name__)

class DateTimeEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)

M = TypeVar(""M"", bound=BaseModel)

class BaseRedisEventBus(Generic[M], ABC):
    Model: type[M]

    @property
    @abstractmethod
    def event_bus_name(self) -> str:
        pass

    def _serialize_message(self, item: M, channel_key: str) -> tuple[str, str]:
        message = json.dumps(item.model_dump(), cls=DateTimeEncoder)
        channel_name = f""{self.event_bus_name}/{channel_key}""
        logger.info(f""[{channel_name}] Publishing an event to Redis {message}"")
        return message, channel_name

    def _deserialize_message(self, msg: Any, channel_key: str) -> M | None:
        message_type = ""pmessage"" if ""*"" in channel_key else ""message""
        if msg[""type""] != message_type:
            return None

        try:
            data = json.loads(msg[""data""])
            logger.info(f""Consuming an event from Redis {data}"")
            return self.Model(**data)
        except Exception as e:
            logger.error(f""Failed to parse event result from Redis {msg} {e}"")

    def _get_pubsub_channel(
        self, connection: redis.Redis | redis.AsyncRedis, channel_key: str
    ) -> tuple[PubSub | AsyncPubSub, str]:
        full_channel_name = f""{self.event_bus_name}/{channel_key}""
        pubsub = connection.pubsub()
        return pubsub, full_channel_name

class RedisEventBus(BaseRedisEventBus[M], ABC):
    Model: type[M]

    @property
    def connection(self) -> redis.Redis:
        return redis.get_redis()

    def publish_event(self, event: M, channel_key: str):
        message, full_channel_name = self._serialize_message(event, channel_key)
        self.connection.publish(full_channel_name, message)

    def listen_events(self, channel_key: str) -> Generator[M, None, None]:
        pubsub, full_channel_name = self._get_pubsub_channel(
            self.connection, channel_key
        )
        assert isinstance(pubsub, PubSub)

        if ""*"" in channel_key:
            pubsub.psubscribe(full_channel_name)
        else:
            pubsub.subscribe(full_channel_name)

        for message in pubsub.listen():
            if event := self._deserialize_message(message, channel_key):
                yield event

class AsyncRedisEventBus(BaseRedisEventBus[M], ABC):
    Model: type[M]

    @property
    async def connection(self) -> redis.AsyncRedis:
        return await redis.get_redis_async()

    async def publish_event(self, event: M, channel_key: str):
        message, full_channel_name = self._serialize_message(event, channel_key)
        connection = await self.connection
        await connection.publish(full_channel_name, message)

    async def listen_events(self, channel_key: str) -> AsyncGenerator[M, None]:
        pubsub, full_channel_name = self._get_pubsub_channel(
            await self.connection, channel_key
        )
        assert isinstance(pubsub, AsyncPubSub)

        if ""*"" in channel_key:
            await pubsub.psubscribe(full_channel_name)
        else:
            await pubsub.subscribe(full_channel_name)

        async for message in pubsub.listen():
            if event := self._deserialize_message(message, channel_key):
                yield event

    async def wait_for_event(
        self, channel_key: str, timeout: Optional[float] = None
    ) -> M | None:
        try:
            return await asyncio.wait_for(
                anext(aiter(self.listen_events(channel_key))), timeout
            )
        except TimeoutError:
            return None","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
20,"""""""
Calculates the sum of two non-negative integers using bitwise operators
Wikipedia explanation: https://en.wikipedia.org/wiki/Binary_number
""""""


def bitwise_addition_recursive(number: int, other_number: int) -> int:
    """"""
    >>> bitwise_addition_recursive(4, 5)
    9
    >>> bitwise_addition_recursive(8, 9)
    17
    >>> bitwise_addition_recursive(0, 4)
    4
    >>> bitwise_addition_recursive(4.5, 9)
    Traceback (most recent call last):
        ...
    TypeError: Both arguments MUST be integers!
    >>> bitwise_addition_recursive('4', 9)
    Traceback (most recent call last):
        ...
    TypeError: Both arguments MUST be integers!
    >>> bitwise_addition_recursive('4.5', 9)
    Traceback (most recent call last):
        ...
    TypeError: Both arguments MUST be integers!
    >>> bitwise_addition_recursive(-1, 9)
    Traceback (most recent call last):
        ...
    ValueError: Both arguments MUST be non-negative!
    >>> bitwise_addition_recursive(1, -9)
    Traceback (most recent call last):
        ...
    ValueError: Both arguments MUST be non-negative!
    """"""

    if not isinstance(number, int) or not isinstance(other_number, int):
        raise TypeError(""Both arguments MUST be integers!"")

    if number < 0 or other_number < 0:
        raise ValueError(""Both arguments MUST be non-negative!"")

    bitwise_sum = number ^ other_number
    carry = number & other_number

    if carry == 0:
        return bitwise_sum

    return bitwise_addition_recursive(bitwise_sum, carry << 1)


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","""""""
Calculates the sum of two non-negative integers using bitwise operators
Wikipedia explanation: https://en.wikipedia.org/wiki/Binary_number
""""""


def bitwise_addition_recursive(number: int, other_number: int) -> int:
    """"""
    >>> bitwise_addition_recursive(4, 5)
    9
    >>> bitwise_addition_recursive(8, 9)
    17
    >>> bitwise_addition_recursive(0, 4)
    4
    >>> bitwise_addition_recursive(4.5, 9)
    Traceback (most recent call last):
        ...
    TypeError: Both arguments MUST be integers!
    >>> bitwise_addition_recursive('4', 9)
    Traceback (most recent call last):
        ...
    TypeError: Both arguments MUST be integers!
    >>> bitwise_addition_recursive('4.5', 9)
    Traceback (most recent call last):
        ...
    TypeError: Both arguments MUST be integers!
    >>> bitwise_addition_recursive(-1, 9)
    Traceback (most recent call last):
        ...
    ValueError: Both arguments MUST be non-negative!
    >>> bitwise_addition_recursive(1, -9)
    Traceback (most recent call last):
        ...
    ValueError: Both arguments MUST be non-negative!
    """"""
    if not isinstance(number, int) or not isinstance(other_number, int):
        raise TypeError(""Both arguments MUST be integers!"")
    
    if number < 0 or other_number < 0:
        raise ValueError(""Both arguments MUST be non-negative!"")
    
    bitwise_sum = number ^ other_number
    carry = number & other_number

    if carry == 0:
        return bitwise_sum
    
    return bitwise_addition_recursive(bitwise_sum, carry << 1)


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
21,"import asyncio
import logging
from collections import defaultdict
from typing import TYPE_CHECKING, Annotated, Any, Sequence

import pydantic
from autogpt_libs.auth.middleware import auth_middleware
from autogpt_libs.feature_flag.client import feature_flag
from autogpt_libs.utils.cache import thread_cached
from fastapi import APIRouter, Depends, HTTPException
from typing_extensions import Optional, TypedDict

import backend.data.block
import backend.server.integrations.router
import backend.server.routers.analytics
from backend.data import execution as execution_db
from backend.data import graph as graph_db
from backend.data.api_key import (
    APIKeyError,
    APIKeyNotFoundError,
    APIKeyPermissionError,
    APIKeyWithoutHash,
    generate_api_key,
    get_api_key_by_id,
    list_user_api_keys,
    revoke_api_key,
    suspend_api_key,
    update_api_key_permissions,
)
from backend.data.block import BlockInput, CompletedBlockOutput
from backend.data.credit import get_block_costs, get_user_credit_model
from backend.data.user import get_or_create_user
from backend.executor import ExecutionManager, ExecutionScheduler, scheduler
from backend.integrations.creds_manager import IntegrationCredentialsManager
from backend.integrations.webhooks.graph_lifecycle_hooks import (
    on_graph_activate,
    on_graph_deactivate,
)
from backend.server.model import (
    CreateAPIKeyRequest,
    CreateAPIKeyResponse,
    CreateGraph,
    SetGraphActiveVersion,
    UpdatePermissionsRequest,
)
from backend.server.utils import get_user_id
from backend.util.service import get_service_client
from backend.util.settings import Settings

if TYPE_CHECKING:
    from backend.data.model import Credentials


@thread_cached
def execution_manager_client() -> ExecutionManager:
    return get_service_client(ExecutionManager)


@thread_cached
def execution_scheduler_client() -> ExecutionScheduler:
    return get_service_client(ExecutionScheduler)


settings = Settings()
logger = logging.getLogger(__name__)
integration_creds_manager = IntegrationCredentialsManager()


_user_credit_model = get_user_credit_model()

# Define the API routes
v1_router = APIRouter()

v1_router.include_router(
    backend.server.integrations.router.router,
    prefix=""/integrations"",
    tags=[""integrations""],
)

v1_router.include_router(
    backend.server.routers.analytics.router,
    prefix=""/analytics"",
    tags=[""analytics""],
    dependencies=[Depends(auth_middleware)],
)


########################################################
##################### Auth #############################
########################################################


@v1_router.post(""/auth/user"", tags=[""auth""], dependencies=[Depends(auth_middleware)])
async def get_or_create_user_route(user_data: dict = Depends(auth_middleware)):
    user = await get_or_create_user(user_data)
    return user.model_dump()


########################################################
##################### Blocks ###########################
########################################################


@v1_router.get(path=""/blocks"", tags=[""blocks""], dependencies=[Depends(auth_middleware)])
def get_graph_blocks() -> Sequence[dict[Any, Any]]:
    blocks = [block() for block in backend.data.block.get_blocks().values()]
    costs = get_block_costs()
    return [{**b.to_dict(), ""costs"": costs.get(b.id, [])} for b in blocks]


@v1_router.post(
    path=""/blocks/{block_id}/execute"",
    tags=[""blocks""],
    dependencies=[Depends(auth_middleware)],
)
def execute_graph_block(block_id: str, data: BlockInput) -> CompletedBlockOutput:
    obj = backend.data.block.get_block(block_id)
    if not obj:
        raise HTTPException(status_code=404, detail=f""Block #{block_id} not found."")

    output = defaultdict(list)
    for name, data in obj.execute(data):
        output[name].append(data)
    return output


########################################################
##################### Credits ##########################
########################################################


@v1_router.get(path=""/credits"", dependencies=[Depends(auth_middleware)])
async def get_user_credits(
    user_id: Annotated[str, Depends(get_user_id)],
) -> dict[str, int]:
    # Credits can go negative, so ensure it's at least 0 for user to see.
    return {""credits"": max(await _user_credit_model.get_or_refill_credit(user_id), 0)}


########################################################
##################### Graphs ###########################
########################################################


class DeleteGraphResponse(TypedDict):
    version_counts: int


@v1_router.get(path=""/graphs"", tags=[""graphs""], dependencies=[Depends(auth_middleware)])
async def get_graphs(
    user_id: Annotated[str, Depends(get_user_id)]
) -> Sequence[graph_db.GraphModel]:
    return await graph_db.get_graphs(filter_by=""active"", user_id=user_id)


@v1_router.get(
    path=""/graphs/{graph_id}"", tags=[""graphs""], dependencies=[Depends(auth_middleware)]
)
@v1_router.get(
    path=""/graphs/{graph_id}/versions/{version}"",
    tags=[""graphs""],
    dependencies=[Depends(auth_middleware)],
)
async def get_graph(
    graph_id: str,
    user_id: Annotated[str, Depends(get_user_id)],
    version: int | None = None,
    hide_credentials: bool = False,
) -> graph_db.GraphModel:
    graph = await graph_db.get_graph(
        graph_id, version, user_id=user_id, for_export=hide_credentials
    )
    if not graph:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")
    return graph


@v1_router.get(
    path=""/graphs/{graph_id}/versions"",
    tags=[""graphs""],
    dependencies=[Depends(auth_middleware)],
)
@v1_router.get(
    path=""/templates/{graph_id}/versions"",
    tags=[""templates"", ""graphs""],
    dependencies=[Depends(auth_middleware)],
)
async def get_graph_all_versions(
    graph_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> Sequence[graph_db.GraphModel]:
    graphs = await graph_db.get_graph_all_versions(graph_id, user_id=user_id)
    if not graphs:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")
    return graphs


@v1_router.post(
    path=""/graphs"", tags=[""graphs""], dependencies=[Depends(auth_middleware)]
)
async def create_new_graph(
    create_graph: CreateGraph, user_id: Annotated[str, Depends(get_user_id)]
) -> graph_db.GraphModel:
    return await do_create_graph(create_graph, is_template=False, user_id=user_id)


async def do_create_graph(
    create_graph: CreateGraph,
    is_template: bool,
    # user_id doesn't have to be annotated like on other endpoints,
    # because create_graph isn't used directly as an endpoint
    user_id: str,
) -> graph_db.GraphModel:
    if create_graph.graph:
        graph = graph_db.make_graph_model(create_graph.graph, user_id)
    elif create_graph.template_id:
        # Create a new graph from a template
        graph = await graph_db.get_graph(
            create_graph.template_id,
            create_graph.template_version,
            template=True,
            user_id=user_id,
        )
        if not graph:
            raise HTTPException(
                400, detail=f""Template #{create_graph.template_id} not found""
            )
        graph.version = 1
    else:
        raise HTTPException(
            status_code=400, detail=""Either graph or template_id must be provided.""
        )

    graph.is_template = is_template
    graph.is_active = not is_template
    graph.reassign_ids(user_id=user_id, reassign_graph_id=True)

    graph = await graph_db.create_graph(graph, user_id=user_id)
    graph = await on_graph_activate(
        graph,
        get_credentials=lambda id: integration_creds_manager.get(user_id, id),
    )
    return graph


@v1_router.delete(
    path=""/graphs/{graph_id}"", tags=[""graphs""], dependencies=[Depends(auth_middleware)]
)
async def delete_graph(
    graph_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> DeleteGraphResponse:
    if active_version := await graph_db.get_graph(graph_id, user_id=user_id):

        def get_credentials(credentials_id: str) -> ""Credentials | None"":
            return integration_creds_manager.get(user_id, credentials_id)

        await on_graph_deactivate(active_version, get_credentials)

    return {""version_counts"": await graph_db.delete_graph(graph_id, user_id=user_id)}


@v1_router.put(
    path=""/graphs/{graph_id}"", tags=[""graphs""], dependencies=[Depends(auth_middleware)]
)
@v1_router.put(
    path=""/templates/{graph_id}"",
    tags=[""templates"", ""graphs""],
    dependencies=[Depends(auth_middleware)],
)
async def update_graph(
    graph_id: str,
    graph: graph_db.Graph,
    user_id: Annotated[str, Depends(get_user_id)],
) -> graph_db.GraphModel:
    # Sanity check
    if graph.id and graph.id != graph_id:
        raise HTTPException(400, detail=""Graph ID does not match ID in URI"")

    # Determine new version
    existing_versions = await graph_db.get_graph_all_versions(graph_id, user_id=user_id)
    if not existing_versions:
        raise HTTPException(404, detail=f""Graph #{graph_id} not found"")
    latest_version_number = max(g.version for g in existing_versions)
    graph.version = latest_version_number + 1

    latest_version_graph = next(
        v for v in existing_versions if v.version == latest_version_number
    )
    current_active_version = next((v for v in existing_versions if v.is_active), None)
    if latest_version_graph.is_template != graph.is_template:
        raise HTTPException(
            400, detail=""Changing is_template on an existing graph is forbidden""
        )
    graph.is_active = not graph.is_template
    graph = graph_db.make_graph_model(graph, user_id)
    graph.reassign_ids(user_id=user_id)

    new_graph_version = await graph_db.create_graph(graph, user_id=user_id)

    if new_graph_version.is_active:

        def get_credentials(credentials_id: str) -> ""Credentials | None"":
            return integration_creds_manager.get(user_id, credentials_id)

        # Handle activation of the new graph first to ensure continuity
        new_graph_version = await on_graph_activate(
            new_graph_version,
            get_credentials=get_credentials,
        )
        # Ensure new version is the only active version
        await graph_db.set_graph_active_version(
            graph_id=graph_id, version=new_graph_version.version, user_id=user_id
        )
        if current_active_version:
            # Handle deactivation of the previously active version
            await on_graph_deactivate(
                current_active_version,
                get_credentials=get_credentials,
            )

    return new_graph_version


@v1_router.put(
    path=""/graphs/{graph_id}/versions/active"",
    tags=[""graphs""],
    dependencies=[Depends(auth_middleware)],
)
async def set_graph_active_version(
    graph_id: str,
    request_body: SetGraphActiveVersion,
    user_id: Annotated[str, Depends(get_user_id)],
):
    new_active_version = request_body.active_graph_version
    new_active_graph = await graph_db.get_graph(
        graph_id, new_active_version, user_id=user_id
    )
    if not new_active_graph:
        raise HTTPException(404, f""Graph #{graph_id} v{new_active_version} not found"")

    current_active_graph = await graph_db.get_graph(graph_id, user_id=user_id)

    def get_credentials(credentials_id: str) -> ""Credentials | None"":
        return integration_creds_manager.get(user_id, credentials_id)

    # Handle activation of the new graph first to ensure continuity
    await on_graph_activate(
        new_active_graph,
        get_credentials=get_credentials,
    )
    # Ensure new version is the only active version
    await graph_db.set_graph_active_version(
        graph_id=graph_id,
        version=new_active_version,
        user_id=user_id,
    )
    if current_active_graph and current_active_graph.version != new_active_version:
        # Handle deactivation of the previously active version
        await on_graph_deactivate(
            current_active_graph,
            get_credentials=get_credentials,
        )


@v1_router.post(
    path=""/graphs/{graph_id}/execute"",
    tags=[""graphs""],
    dependencies=[Depends(auth_middleware)],
)
def execute_graph(
    graph_id: str,
    node_input: dict[Any, Any],
    user_id: Annotated[str, Depends(get_user_id)],
) -> dict[str, Any]:  # FIXME: add proper return type
    try:
        graph_exec = execution_manager_client().add_execution(
            graph_id, node_input, user_id=user_id
        )
        return {""id"": graph_exec.graph_exec_id}
    except Exception as e:
        msg = e.__str__().encode().decode(""unicode_escape"")
        raise HTTPException(status_code=400, detail=msg)


@v1_router.post(
    path=""/graphs/{graph_id}/executions/{graph_exec_id}/stop"",
    tags=[""graphs""],
    dependencies=[Depends(auth_middleware)],
)
async def stop_graph_run(
    graph_exec_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> Sequence[execution_db.ExecutionResult]:
    if not await graph_db.get_execution(user_id=user_id, execution_id=graph_exec_id):
        raise HTTPException(404, detail=f""Agent execution #{graph_exec_id} not found"")

    await asyncio.to_thread(
        lambda: execution_manager_client().cancel_execution(graph_exec_id)
    )

    # Retrieve & return canceled graph execution in its final state
    return await execution_db.get_execution_results(graph_exec_id)


@v1_router.get(
    path=""/executions"",
    tags=[""graphs""],
    dependencies=[Depends(auth_middleware)],
)
async def get_executions(
    user_id: Annotated[str, Depends(get_user_id)],
) -> list[graph_db.GraphExecution]:
    return await graph_db.get_executions(user_id=user_id)


@v1_router.get(
    path=""/graphs/{graph_id}/executions/{graph_exec_id}"",
    tags=[""graphs""],
    dependencies=[Depends(auth_middleware)],
)
async def get_graph_run_node_execution_results(
    graph_id: str,
    graph_exec_id: str,
    user_id: Annotated[str, Depends(get_user_id)],
) -> Sequence[execution_db.ExecutionResult]:
    graph = await graph_db.get_graph(graph_id, user_id=user_id)
    if not graph:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")

    return await execution_db.get_execution_results(graph_exec_id)


########################################################
##################### Templates ########################
########################################################


@v1_router.get(
    path=""/templates"",
    tags=[""graphs"", ""templates""],
    dependencies=[Depends(auth_middleware)],
)
async def get_templates(
    user_id: Annotated[str, Depends(get_user_id)]
) -> Sequence[graph_db.GraphModel]:
    return await graph_db.get_graphs(filter_by=""template"", user_id=user_id)


@v1_router.get(
    path=""/templates/{graph_id}"",
    tags=[""templates"", ""graphs""],
    dependencies=[Depends(auth_middleware)],
)
async def get_template(
    graph_id: str, version: int | None = None
) -> graph_db.GraphModel:
    graph = await graph_db.get_graph(graph_id, version, template=True)
    if not graph:
        raise HTTPException(status_code=404, detail=f""Template #{graph_id} not found."")
    return graph


@v1_router.post(
    path=""/templates"",
    tags=[""templates"", ""graphs""],
    dependencies=[Depends(auth_middleware)],
)
async def create_new_template(
    create_graph: CreateGraph, user_id: Annotated[str, Depends(get_user_id)]
) -> graph_db.GraphModel:
    return await do_create_graph(create_graph, is_template=True, user_id=user_id)


########################################################
##################### Schedules ########################
########################################################


class ScheduleCreationRequest(pydantic.BaseModel):
    cron: str
    input_data: dict[Any, Any]
    graph_id: str


@v1_router.post(
    path=""/schedules"",
    tags=[""schedules""],
    dependencies=[Depends(auth_middleware)],
)
async def create_schedule(
    user_id: Annotated[str, Depends(get_user_id)],
    schedule: ScheduleCreationRequest,
) -> scheduler.JobInfo:
    graph = await graph_db.get_graph(schedule.graph_id, user_id=user_id)
    if not graph:
        raise HTTPException(
            status_code=404, detail=f""Graph #{schedule.graph_id} not found.""
        )

    return await asyncio.to_thread(
        lambda: execution_scheduler_client().add_execution_schedule(
            graph_id=schedule.graph_id,
            graph_version=graph.version,
            cron=schedule.cron,
            input_data=schedule.input_data,
            user_id=user_id,
        )
    )


@v1_router.delete(
    path=""/schedules/{schedule_id}"",
    tags=[""schedules""],
    dependencies=[Depends(auth_middleware)],
)
def delete_schedule(
    schedule_id: str,
    user_id: Annotated[str, Depends(get_user_id)],
) -> dict[Any, Any]:
    execution_scheduler_client().delete_schedule(schedule_id, user_id=user_id)
    return {""id"": schedule_id}


@v1_router.get(
    path=""/schedules"",
    tags=[""schedules""],
    dependencies=[Depends(auth_middleware)],
)
def get_execution_schedules(
    user_id: Annotated[str, Depends(get_user_id)],
    graph_id: str | None = None,
) -> list[scheduler.JobInfo]:
    return execution_scheduler_client().get_execution_schedules(
        user_id=user_id,
        graph_id=graph_id,
    )


########################################################
#####################  API KEY ##############################
########################################################


@v1_router.post(
    ""/api-keys"",
    response_model=list[CreateAPIKeyResponse] | dict[str, str],
    tags=[""api-keys""],
    dependencies=[Depends(auth_middleware)],
)
@feature_flag(""api-keys-enabled"")
async def create_api_key(
    request: CreateAPIKeyRequest, user_id: Annotated[str, Depends(get_user_id)]
) -> CreateAPIKeyResponse:
    """"""Create a new API key""""""
    try:
        api_key, plain_text = await generate_api_key(
            name=request.name,
            user_id=user_id,
            permissions=request.permissions,
            description=request.description,
        )
        return CreateAPIKeyResponse(api_key=api_key, plain_text_key=plain_text)
    except APIKeyError as e:
        logger.error(f""Failed to create API key: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))


@v1_router.get(
    ""/api-keys"",
    response_model=list[APIKeyWithoutHash] | dict[str, str],
    tags=[""api-keys""],
    dependencies=[Depends(auth_middleware)],
)
@feature_flag(""api-keys-enabled"")
async def get_api_keys(
    user_id: Annotated[str, Depends(get_user_id)]
) -> list[APIKeyWithoutHash]:
    """"""List all API keys for the user""""""
    try:
        return await list_user_api_keys(user_id)
    except APIKeyError as e:
        logger.error(f""Failed to list API keys: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))


@v1_router.get(
    ""/api-keys/{key_id}"",
    response_model=list[APIKeyWithoutHash] | dict[str, str],
    tags=[""api-keys""],
    dependencies=[Depends(auth_middleware)],
)
@feature_flag(""api-keys-enabled"")
async def get_api_key(
    key_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> APIKeyWithoutHash:
    """"""Get a specific API key""""""
    try:
        api_key = await get_api_key_by_id(key_id, user_id)
        if not api_key:
            raise HTTPException(status_code=404, detail=""API key not found"")
        return api_key
    except APIKeyError as e:
        logger.error(f""Failed to get API key: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))


@v1_router.delete(
    ""/api-keys/{key_id}"",
    response_model=list[APIKeyWithoutHash] | dict[str, str],
    tags=[""api-keys""],
    dependencies=[Depends(auth_middleware)],
)
@feature_flag(""api-keys-enabled"")
async def delete_api_key(
    key_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> Optional[APIKeyWithoutHash]:
    """"""Revoke an API key""""""
    try:
        return await revoke_api_key(key_id, user_id)
    except APIKeyNotFoundError:
        raise HTTPException(status_code=404, detail=""API key not found"")
    except APIKeyPermissionError:
        raise HTTPException(status_code=403, detail=""Permission denied"")
    except APIKeyError as e:
        logger.error(f""Failed to revoke API key: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))


@v1_router.post(
    ""/api-keys/{key_id}/suspend"",
    response_model=list[APIKeyWithoutHash] | dict[str, str],
    tags=[""api-keys""],
    dependencies=[Depends(auth_middleware)],
)
@feature_flag(""api-keys-enabled"")
async def suspend_key(
    key_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> Optional[APIKeyWithoutHash]:
    """"""Suspend an API key""""""
    try:
        return await suspend_api_key(key_id, user_id)
    except APIKeyNotFoundError:
        raise HTTPException(status_code=404, detail=""API key not found"")
    except APIKeyPermissionError:
        raise HTTPException(status_code=403, detail=""Permission denied"")
    except APIKeyError as e:
        logger.error(f""Failed to suspend API key: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))


@v1_router.put(
    ""/api-keys/{key_id}/permissions"",
    response_model=list[APIKeyWithoutHash] | dict[str, str],
    tags=[""api-keys""],
    dependencies=[Depends(auth_middleware)],
)
@feature_flag(""api-keys-enabled"")
async def update_permissions(
    key_id: str,
    request: UpdatePermissionsRequest,
    user_id: Annotated[str, Depends(get_user_id)],
) -> Optional[APIKeyWithoutHash]:
    """"""Update API key permissions""""""
    try:
        return await update_api_key_permissions(key_id, user_id, request.permissions)
    except APIKeyNotFoundError:
        raise HTTPException(status_code=404, detail=""API key not found"")
    except APIKeyPermissionError:
        raise HTTPException(status_code=403, detail=""Permission denied"")
    except APIKeyError as e:
        logger.error(f""Failed to update API key permissions: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))
","import asyncio
import logging
from collections import defaultdict
from typing import TYPE_CHECKING, Annotated, Any, Sequence

import pydantic
from autogpt_libs.auth.middleware import auth_middleware
from autogpt_libs.feature_flag.client import feature_flag
from autogpt_libs.utils.cache import thread_cached
from fastapi import APIRouter, Depends, HTTPException
from typing_extensions import Optional, TypedDict

import backend.data.block
import backend.server.integrations.router
import backend.server.routers.analytics
from backend.data import execution as execution_db
from backend.data import graph as graph_db
from backend.data.api_key import (
    APIKeyError,
    APIKeyNotFoundError,
    APIKeyPermissionError,
    APIKeyWithoutHash,
    generate_api_key,
    get_api_key_by_id,
    list_user_api_keys,
    revoke_api_key,
    suspend_api_key,
    update_api_key_permissions,
)
from backend.data.block import BlockInput, CompletedBlockOutput
from backend.data.credit import get_block_costs, get_user_credit_model
from backend.data.user import get_or_create_user
from backend.executor import ExecutionManager, ExecutionScheduler, scheduler
from backend.integrations.creds_manager import IntegrationCredentialsManager
from backend.integrations.webhooks.graph_lifecycle_hooks import (
    on_graph_activate,
    on_graph_deactivate,
)
from backend.server.model import (
    CreateAPIKeyRequest,
    CreateAPIKeyResponse,
    CreateGraph,
    SetGraphActiveVersion,
    UpdatePermissionsRequest,
)
from backend.server.utils import get_user_id
from backend.util.service import get_service_client
from backend.util.settings import Settings

if TYPE_CHECKING:
    ...

@thread_cached
def execution_manager_client() -> ExecutionManager:
    return get_service_client(ExecutionManager)

@thread_cached
def execution_scheduler_client() -> ExecutionScheduler:
    return get_service_client(ExecutionScheduler)

settings = Settings()
logger = logging.getLogger(__name__)
integration_creds_manager = IntegrationCredentialsManager()
_user_credit_model = get_user_credit_model()
v1_router = APIRouter()

@v1_router.post(""/auth/user"", tags=[""auth""], dependencies=[Depends(auth_middleware)])
async def get_or_create_user_route(user_data: dict = Depends(auth_middleware)):
    user = await get_or_create_user(user_data)
    return user.model_dump()

@v1_router.get(path=""/blocks"", tags=[""blocks""], dependencies=[Depends(auth_middleware)])
def get_graph_blocks() -> Sequence[dict[Any, Any]]:
    blocks = [block() for block in backend.data.block.get_blocks().values()]
    costs = get_block_costs()
    return [{**b.to_dict(), ""costs"": costs.get(b.id, [])} for b in blocks]

def execute_graph_block(block_id: str, data: BlockInput) -> CompletedBlockOutput:
    obj = backend.data.block.get_block(block_id)
    if not obj:
        raise HTTPException(status_code=404, detail=f""Block #{block_id} not found."")
    output = defaultdict(list)
    for name, data in obj.execute(data):
        output[name].append(data)
    return output

@v1_router.get(path=""/credits"", dependencies=[Depends(auth_middleware)])
async def get_user_credits(
    user_id: Annotated[str, Depends(get_user_id)],
) -> dict[str, int]:
    # Credits can go negative, so ensure it's at least 0 for user to see.
    return {""credits"": max(await _user_credit_model.get_or_refill_credit(user_id), 0)}

class DeleteGraphResponse(TypedDict):
    version_counts: int

@v1_router.get(path=""/graphs"", tags=[""graphs""], dependencies=[Depends(auth_middleware)])
async def get_graphs(
    user_id: Annotated[str, Depends(get_user_id)]
) -> Sequence[graph_db.GraphModel]:
    return await graph_db.get_graphs(filter_by=""active"", user_id=user_id)

async def get_graph(
    graph_id: str,
    user_id: Annotated[str, Depends(get_user_id)],
    version: int | None = None,
    hide_credentials: bool = False,
) -> graph_db.GraphModel:
    graph = await graph_db.get_graph(
        graph_id, version, user_id=user_id, for_export=hide_credentials
    )
    if not graph:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")
    return graph

async def get_graph_all_versions(
    graph_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> Sequence[graph_db.GraphModel]:
    graphs = await graph_db.get_graph_all_versions(graph_id, user_id=user_id)
    if not graphs:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")
    return graphs

async def create_new_graph(
    create_graph: CreateGraph, user_id: Annotated[str, Depends(get_user_id)]
) -> graph_db.GraphModel:
    return await do_create_graph(create_graph, is_template=False, user_id=user_id)

async def do_create_graph(
    create_graph: CreateGraph,
    is_template: bool,
    # user_id doesn't have to be annotated like on other endpoints,
    # because create_graph isn't used directly as an endpoint
    user_id: str,
) -> graph_db.GraphModel:
    if create_graph.graph:
        graph = graph_db.make_graph_model(create_graph.graph, user_id)
    elif create_graph.template_id:
        # Create a new graph from a template
        graph = await graph_db.get_graph(
            create_graph.template_id,
            create_graph.template_version,
            template=True,
            user_id=user_id,
        )
        if not graph:
            raise HTTPException(
                400, detail=f""Template #{create_graph.template_id} not found""
            )
        graph.version = 1
    else:
        raise HTTPException(
            status_code=400, detail=""Either graph or template_id must be provided.""
        )
    graph.is_template = is_template
    graph.is_active = not is_template
    graph.reassign_ids(user_id=user_id, reassign_graph_id=True)
    graph = await graph_db.create_graph(graph, user_id=user_id)
    graph = await on_graph_activate(
        graph,
        get_credentials=lambda id: integration_creds_manager.get(user_id, id),
    )
    return graph

async def delete_graph(
    graph_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> DeleteGraphResponse:
    if active_version := await graph_db.get_graph(graph_id, user_id=user_id):
        def get_credentials(credentials_id: str) -> ""Credentials | None"":
            return integration_creds_manager.get(user_id, credentials_id)
        await on_graph_deactivate(active_version, get_credentials)
    return {""version_counts"": await graph_db.delete_graph(graph_id, user_id=user_id)}

async def update_graph(
    graph_id: str,
    graph: graph_db.Graph,
    user_id: Annotated[str, Depends(get_user_id)],
) -> graph_db.GraphModel:
    # Sanity check
    if graph.id and graph.id != graph_id:
        raise HTTPException(400, detail=""Graph ID does not match ID in URI"")
    # Determine new version
    existing_versions = await graph_db.get_graph_all_versions(graph_id, user_id=user_id)
    if not existing_versions:
        raise HTTPException(404, detail=f""Graph #{graph_id} not found"")
    latest_version_number = max(g.version for g in existing_versions)
    graph.version = latest_version_number + 1
    latest_version_graph = next(
        v for v in existing_versions if v.version == latest_version_number
    )
    current_active_version = next((v for v in existing_versions if v.is_active), None)
    if latest_version_graph.is_template != graph.is_template:
        raise HTTPException(
            400, detail=""Changing is_template on an existing graph is forbidden""
        )
    graph.is_active = not graph.is_template
    graph = graph_db.make_graph_model(graph, user_id)
    graph.reassign_ids(user_id=user_id)
    new_graph_version = await graph_db.create_graph(graph, user_id=user_id)
    if new_graph_version.is_active:
        def get_credentials(credentials_id: str) -> ""Credentials | None"":
            return integration_creds_manager.get(user_id, credentials_id)
        # Handle activation of the new graph first to ensure continuity
        new_graph_version = await on_graph_activate(
            new_graph_version,
            get_credentials=get_credentials,
        )
        # Ensure new version is the only active version
        await graph_db.set_graph_active_version(
            graph_id=graph_id, version=new_graph_version.version, user_id=user_id
        )
        if current_active_version:
            # Handle deactivation of the previously active version
            await on_graph_deactivate(
                current_active_version,
                get_credentials=get_credentials,
            )
    return new_graph_version

async def set_graph_active_version(
    graph_id: str,
    request_body: SetGraphActiveVersion,
    user_id: Annotated[str, Depends(get_user_id)],
):
    new_active_version = request_body.active_graph_version
    new_active_graph = await graph_db.get_graph(
        graph_id, new_active_version, user_id=user_id
    )
    if not new_active_graph:
        raise HTTPException(404, f""Graph #{graph_id} v{new_active_version} not found"")
    current_active_graph = await graph_db.get_graph(graph_id, user_id=user_id)
    def get_credentials(credentials_id: str) -> ""Credentials | None"":
        return integration_creds_manager.get(user_id, credentials_id)
    await on_graph_activate(
        new_active_graph,
        get_credentials=get_credentials,
    )
    await graph_db.set_graph_active_version(
        graph_id=graph_id,
        version=new_active_version,
        user_id=user_id,
    )
    if current_active_graph and current_active_graph.version != new_active_version:
        await on_graph_deactivate(
            current_active_graph,
            get_credentials=get_credentials,
        )

def execute_graph(
    graph_id: str,
    node_input: dict[Any, Any],
    user_id: Annotated[str, Depends(get_user_id)],
) -> dict[str, Any]:  # FIXME: add proper return type
    try:
        graph_exec = execution_manager_client().add_execution(
            graph_id, node_input, user_id=user_id
        )
        return {""id"": graph_exec.graph_exec_id}
    except Exception as e:
        msg = e.__str__().encode().decode(""unicode_escape"")
        raise HTTPException(status_code=400, detail=msg)

async def stop_graph_run(
    graph_exec_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> Sequence[execution_db.ExecutionResult]:
    if not await graph_db.get_execution(user_id=user_id, execution_id=graph_exec_id):
        raise HTTPException(404, detail=f""Agent execution #{graph_exec_id} not found"")
    await asyncio.to_thread(
        lambda: execution_manager_client().cancel_execution(graph_exec_id)
    )
    # Retrieve & return canceled graph execution in its final state
    return await execution_db.get_execution_results(graph_exec_id)

async def get_executions(
    user_id: Annotated[str, Depends(get_user_id)],
) -> list[graph_db.GraphExecution]:
    return await graph_db.get_executions(user_id=user_id)

async def get_graph_run_node_execution_results(
    graph_id: str,
    graph_exec_id: str,
    user_id: Annotated[str, Depends(get_user_id)],
) -> Sequence[execution_db.ExecutionResult]:
    graph = await graph_db.get_graph(graph_id, user_id=user_id)
    if not graph:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")
    return await execution_db.get_execution_results(graph_exec_id)

async def get_templates(
    user_id: Annotated[str, Depends(get_user_id)]
) -> Sequence[graph_db.GraphModel]:
    return await graph_db.get_graphs(filter_by=""template"", user_id=user_id)

async def get_template(
    graph_id: str, version: int | None = None
) -> graph_db.GraphModel:
    graph = await graph_db.get_graph(graph_id, version, template=True)
    if not graph:
        raise HTTPException(status_code=404, detail=f""Template #{graph_id} not found."")
    return graph

async def create_new_template(
    create_graph: CreateGraph, user_id: Annotated[str, Depends(get_user_id)]
) -> graph_db.GraphModel:
    return await do_create_graph(create_graph, is_template=True, user_id=user_id)

class ScheduleCreationRequest(pydantic.BaseModel):
    cron: str
    input_data: dict[Any, Any]
    graph_id: str

async def create_schedule(
    user_id: Annotated[str, Depends(get_user_id)],
    schedule: ScheduleCreationRequest,
) -> scheduler.JobInfo:
    graph = await graph_db.get_graph(schedule.graph_id, user_id=user_id)
    if not graph:
        raise HTTPException(
            status_code=404, detail=f""Graph #{schedule.graph_id} not found.""
        )
    return await asyncio.to_thread(
        lambda: execution_scheduler_client().add_execution_schedule(
            graph_id=schedule.graph_id,
            graph_version=graph.version,
            cron=schedule.cron,
            input_data=schedule.input_data,
            user_id=user_id,
        )
    )

def delete_schedule(
    schedule_id: str,
    user_id: Annotated[str, Depends(get_user_id)],
) -> dict[Any, Any]:
    execution_scheduler_client().delete_schedule(schedule_id, user_id=user_id)
    return {""id"": schedule_id}

def get_execution_schedules(
    user_id: Annotated[str, Depends(get_user_id)],
    graph_id: str | None = None,
) -> list[scheduler.JobInfo]:
    return execution_scheduler_client().get_execution_schedules(
        user_id=user_id,
        graph_id=graph_id,
    )

async def create_api_key(
    request: CreateAPIKeyRequest, user_id: Annotated[str, Depends(get_user_id)]
) -> CreateAPIKeyResponse:
    """"""Create a new API key""""""
    try:
        api_key, plain_text = await generate_api_key(
            name=request.name,
            user_id=user_id,
            permissions=request.permissions,
            description=request.description,
        )
        return CreateAPIKeyResponse(api_key=api_key, plain_text_key=plain_text)
    except APIKeyError as e:
        logger.error(f""Failed to create API key: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))

async def get_api_keys(
    user_id: Annotated[str, Depends(get_user_id)]
) -> list[APIKeyWithoutHash]:
    """"""List all API keys for the user""""""
    try:
        return await list_user_api_keys(user_id)
    except APIKeyError as e:
        logger.error(f""Failed to list API keys: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))

async def get_api_key(
    key_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> APIKeyWithoutHash:
    """"""Get a specific API key""""""
    try:
        api_key = await get_api_key_by_id(key_id, user_id)
        if not api_key:
            raise HTTPException(status_code=404, detail=""API key not found"")
        return api_key
    except APIKeyError as e:
        logger.error(f""Failed to get API key: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))

async def delete_api_key(
    key_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> Optional[APIKeyWithoutHash]:
    """"""Revoke an API key""""""
    try:
        return await revoke_api_key(key_id, user_id)
    except APIKeyNotFoundError:
        raise HTTPException(status_code=404, detail=""API key not found"")
    except APIKeyPermissionError:
        raise HTTPException(status_code=403, detail=""Permission denied"")
    except APIKeyError as e:
        logger.error(f""Failed to revoke API key: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))

async def suspend_key(
    key_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> Optional[APIKeyWithoutHash]:
    """"""Suspend an API key""""""
    try:
        return await suspend_api_key(key_id, user_id)
    except APIKeyNotFoundError:
        raise HTTPException(status_code=404, detail=""API key not found"")
    except APIKeyPermissionError:
        raise HTTPException(status_code=403, detail=""Permission denied"")
    except APIKeyError as e:
        logger.error(f""Failed to suspend API key: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))

async def update_permissions(
    key_id: str,
    request: UpdatePermissionsRequest,
    user_id: Annotated[str, Depends(get_user_id)],
) -> Optional[APIKeyWithoutHash]:
    """"""Update API key permissions""""""
    try:
        return await update_api_key_permissions(key_id, user_id, request.permissions)
    except APIKeyNotFoundError:
        raise HTTPException(status_code=404, detail=""API key not found"")
    except APIKeyPermissionError:
        raise HTTPException(status_code=403, detail=""Permission denied"")
    except APIKeyError as e:
        logger.error(f""Failed to update API key permissions: {str(e)}"")
        raise HTTPException(status_code=400, detail=str(e))","{'final_score': 87.14, 'length_similarity': 75.19, 'token_similarity': 89.69, 'structure_similarity': 69.75, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
22,"from urllib.parse import urlparse

from typing_extensions import TypedDict

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

from ._api import get_api
from ._auth import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GithubCredentials,
    GithubCredentialsField,
    GithubCredentialsInput,
)


def is_github_url(url: str) -> bool:
    return urlparse(url).netloc == ""github.com""


# --8<-- [start:GithubCommentBlockExample]
class GithubCommentBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        comment: str = SchemaField(
            description=""Comment to post on the issue or pull request"",
            placeholder=""Enter your comment"",
        )

    class Output(BlockSchema):
        id: int = SchemaField(description=""ID of the created comment"")
        url: str = SchemaField(description=""URL to the comment on GitHub"")
        error: str = SchemaField(
            description=""Error message if the comment posting failed""
        )

    def __init__(self):
        super().__init__(
            id=""a8db4d8d-db1c-4a25-a1b0-416a8c33602b"",
            description=""This block posts a comment on a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubCommentBlock.Input,
            output_schema=GithubCommentBlock.Output,
            test_input=[
                {
                    ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                    ""comment"": ""This is a test comment."",
                    ""credentials"": TEST_CREDENTIALS_INPUT,
                },
                {
                    ""issue_url"": ""https://github.com/owner/repo/pull/1"",
                    ""comment"": ""This is a test comment."",
                    ""credentials"": TEST_CREDENTIALS_INPUT,
                },
            ],
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""id"", 1337),
                (""url"", ""https://github.com/owner/repo/issues/1#issuecomment-1337""),
                (""id"", 1337),
                (
                    ""url"",
                    ""https://github.com/owner/repo/issues/1#issuecomment-1337"",
                ),
            ],
            test_mock={
                ""post_comment"": lambda *args, **kwargs: (
                    1337,
                    ""https://github.com/owner/repo/issues/1#issuecomment-1337"",
                )
            },
        )

    @staticmethod
    def post_comment(
        credentials: GithubCredentials, issue_url: str, body_text: str
    ) -> tuple[int, str]:
        api = get_api(credentials)
        data = {""body"": body_text}
        if ""pull"" in issue_url:
            issue_url = issue_url.replace(""pull"", ""issues"")
        comments_url = issue_url + ""/comments""
        response = api.post(comments_url, json=data)
        comment = response.json()
        return comment[""id""], comment[""html_url""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        id, url = self.post_comment(
            credentials,
            input_data.issue_url,
            input_data.comment,
        )
        yield ""id"", id
        yield ""url"", url


# --8<-- [end:GithubCommentBlockExample]


class GithubMakeIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        title: str = SchemaField(
            description=""Title of the issue"", placeholder=""Enter the issue title""
        )
        body: str = SchemaField(
            description=""Body of the issue"", placeholder=""Enter the issue body""
        )

    class Output(BlockSchema):
        number: int = SchemaField(description=""Number of the created issue"")
        url: str = SchemaField(description=""URL of the created issue"")
        error: str = SchemaField(
            description=""Error message if the issue creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""691dad47-f494-44c3-a1e8-05b7990f2dab"",
            description=""This block creates a new issue on a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakeIssueBlock.Input,
            output_schema=GithubMakeIssueBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""title"": ""Test Issue"",
                ""body"": ""This is a test issue."",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""number"", 1),
                (""url"", ""https://github.com/owner/repo/issues/1""),
            ],
            test_mock={
                ""create_issue"": lambda *args, **kwargs: (
                    1,
                    ""https://github.com/owner/repo/issues/1"",
                )
            },
        )

    @staticmethod
    def create_issue(
        credentials: GithubCredentials, repo_url: str, title: str, body: str
    ) -> tuple[int, str]:
        api = get_api(credentials)
        data = {""title"": title, ""body"": body}
        issues_url = repo_url + ""/issues""
        response = api.post(issues_url, json=data)
        issue = response.json()
        return issue[""number""], issue[""html_url""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        number, url = self.create_issue(
            credentials,
            input_data.repo_url,
            input_data.title,
            input_data.body,
        )
        yield ""number"", number
        yield ""url"", url


class GithubReadIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )

    class Output(BlockSchema):
        title: str = SchemaField(description=""Title of the issue"")
        body: str = SchemaField(description=""Body of the issue"")
        user: str = SchemaField(description=""User who created the issue"")
        error: str = SchemaField(
            description=""Error message if reading the issue failed""
        )

    def __init__(self):
        super().__init__(
            id=""6443c75d-032a-4772-9c08-230c707c8acc"",
            description=""This block reads the body, title, and user of a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadIssueBlock.Input,
            output_schema=GithubReadIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""title"", ""Title of the issue""),
                (""body"", ""This is the body of the issue.""),
                (""user"", ""username""),
            ],
            test_mock={
                ""read_issue"": lambda *args, **kwargs: (
                    ""Title of the issue"",
                    ""This is the body of the issue."",
                    ""username"",
                )
            },
        )

    @staticmethod
    def read_issue(
        credentials: GithubCredentials, issue_url: str
    ) -> tuple[str, str, str]:
        api = get_api(credentials)
        response = api.get(issue_url)
        data = response.json()
        title = data.get(""title"", ""No title found"")
        body = data.get(""body"", ""No body content found"")
        user = data.get(""user"", {}).get(""login"", ""No user found"")
        return title, body, user

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        title, body, user = self.read_issue(
            credentials,
            input_data.issue_url,
        )
        if title:
            yield ""title"", title
        if body:
            yield ""body"", body
        if user:
            yield ""user"", user


class GithubListIssuesBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class IssueItem(TypedDict):
            title: str
            url: str

        issue: IssueItem = SchemaField(
            title=""Issue"", description=""Issues with their title and URL""
        )
        error: str = SchemaField(description=""Error message if listing issues failed"")

    def __init__(self):
        super().__init__(
            id=""c215bfd7-0e57-4573-8f8c-f7d4963dcd74"",
            description=""This block lists all issues for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListIssuesBlock.Input,
            output_schema=GithubListIssuesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""issue"",
                    {
                        ""title"": ""Issue 1"",
                        ""url"": ""https://github.com/owner/repo/issues/1"",
                    },
                )
            ],
            test_mock={
                ""list_issues"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Issue 1"",
                        ""url"": ""https://github.com/owner/repo/issues/1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_issues(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.IssueItem]:
        api = get_api(credentials)
        issues_url = repo_url + ""/issues""
        response = api.get(issues_url)
        data = response.json()
        issues: list[GithubListIssuesBlock.Output.IssueItem] = [
            {""title"": issue[""title""], ""url"": issue[""html_url""]} for issue in data
        ]
        return issues

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        issues = self.list_issues(
            credentials,
            input_data.repo_url,
        )
        yield from ((""issue"", issue) for issue in issues)


class GithubAddLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        label: str = SchemaField(
            description=""Label to add to the issue or pull request"",
            placeholder=""Enter the label"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the label addition operation"")
        error: str = SchemaField(
            description=""Error message if the label addition failed""
        )

    def __init__(self):
        super().__init__(
            id=""98bd6b77-9506-43d5-b669-6b9733c4b1f1"",
            description=""This block adds a label to a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAddLabelBlock.Input,
            output_schema=GithubAddLabelBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""label"": ""bug"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Label added successfully"")],
            test_mock={""add_label"": lambda *args, **kwargs: ""Label added successfully""},
        )

    @staticmethod
    def add_label(credentials: GithubCredentials, issue_url: str, label: str) -> str:
        api = get_api(credentials)
        data = {""labels"": [label]}
        labels_url = issue_url + ""/labels""
        api.post(labels_url, json=data)
        return ""Label added successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.add_label(
            credentials,
            input_data.issue_url,
            input_data.label,
        )
        yield ""status"", status


class GithubRemoveLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        label: str = SchemaField(
            description=""Label to remove from the issue or pull request"",
            placeholder=""Enter the label"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the label removal operation"")
        error: str = SchemaField(
            description=""Error message if the label removal failed""
        )

    def __init__(self):
        super().__init__(
            id=""78f050c5-3e3a-48c0-9e5b-ef1ceca5589c"",
            description=""This block removes a label from a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubRemoveLabelBlock.Input,
            output_schema=GithubRemoveLabelBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""label"": ""bug"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Label removed successfully"")],
            test_mock={
                ""remove_label"": lambda *args, **kwargs: ""Label removed successfully""
            },
        )

    @staticmethod
    def remove_label(credentials: GithubCredentials, issue_url: str, label: str) -> str:
        api = get_api(credentials)
        label_url = issue_url + f""/labels/{label}""
        api.delete(label_url)
        return ""Label removed successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.remove_label(
            credentials,
            input_data.issue_url,
            input_data.label,
        )
        yield ""status"", status


class GithubAssignIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        assignee: str = SchemaField(
            description=""Username to assign to the issue"",
            placeholder=""Enter the username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the issue assignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the issue assignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""90507c72-b0ff-413a-886a-23bbbd66f542"",
            description=""This block assigns a user to a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAssignIssueBlock.Input,
            output_schema=GithubAssignIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""assignee"": ""username1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Issue assigned successfully"")],
            test_mock={
                ""assign_issue"": lambda *args, **kwargs: ""Issue assigned successfully""
            },
        )

    @staticmethod
    def assign_issue(
        credentials: GithubCredentials,
        issue_url: str,
        assignee: str,
    ) -> str:
        api = get_api(credentials)
        assignees_url = issue_url + ""/assignees""
        data = {""assignees"": [assignee]}
        api.post(assignees_url, json=data)
        return ""Issue assigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.assign_issue(
            credentials,
            input_data.issue_url,
            input_data.assignee,
        )
        yield ""status"", status


class GithubUnassignIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        assignee: str = SchemaField(
            description=""Username to unassign from the issue"",
            placeholder=""Enter the username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the issue unassignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the issue unassignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""d154002a-38f4-46c2-962d-2488f2b05ece"",
            description=""This block unassigns a user from a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubUnassignIssueBlock.Input,
            output_schema=GithubUnassignIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""assignee"": ""username1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Issue unassigned successfully"")],
            test_mock={
                ""unassign_issue"": lambda *args, **kwargs: ""Issue unassigned successfully""
            },
        )

    @staticmethod
    def unassign_issue(
        credentials: GithubCredentials,
        issue_url: str,
        assignee: str,
    ) -> str:
        api = get_api(credentials)
        assignees_url = issue_url + ""/assignees""
        data = {""assignees"": [assignee]}
        api.delete(assignees_url, json=data)
        return ""Issue unassigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.unassign_issue(
            credentials,
            input_data.issue_url,
            input_data.assignee,
        )
        yield ""status"", status
","from urllib.parse import urlparse
from typing_extensions import TypedDict
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from ._api import get_api
from ._auth import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GithubCredentials,
    GithubCredentialsField,
    GithubCredentialsInput,
)

def is_github_url(url: str) -> bool:
    return urlparse(url).netloc == ""github.com""

class GithubCommentBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        comment: str = SchemaField(
            description=""Comment to post on the issue or pull request"",
            placeholder=""Enter your comment"",
        )

    class Output(BlockSchema):
        id: int = SchemaField(description=""ID of the created comment"")
        url: str = SchemaField(description=""URL to the comment on GitHub"")
        error: str = SchemaField(
            description=""Error message if the comment posting failed""
        )

    def __init__(self):
        super().__init__(
            id=""a8db4d8d-db1c-4a25-a1b0-416a8c33602b"",
            description=""This block posts a comment on a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubCommentBlock.Input,
            output_schema=GithubCommentBlock.Output,
            test_input=[
                {
                    ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                    ""comment"": ""This is a test comment."",
                    ""credentials"": TEST_CREDENTIALS_INPUT,
                },
                {
                    ""issue_url"": ""https://github.com/owner/repo/pull/1"",
                    ""comment"": ""This is a test comment."",
                    ""credentials"": TEST_CREDENTIALS_INPUT,
                },
            ],
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""id"", 1337),
                (""url"", ""https://github.com/owner/repo/issues/1#issuecomment-1337""),
                (""id"", 1337),
                (
                    ""url"",
                    ""https://github.com/owner/repo/issues/1#issuecomment-1337"",
                ),
            ],
            test_mock={
                ""post_comment"": lambda *args, **kwargs: (
                    1337,
                    ""https://github.com/owner/repo/issues/1#issuecomment-1337"",
                )
            },
        )

    @staticmethod
    def post_comment(
        credentials: GithubCredentials, issue_url: str, body_text: str
    ) -> tuple[int, str]:
        api = get_api(credentials)
        data = {""body"": body_text}
        if ""pull"" in issue_url:
            issue_url = issue_url.replace(""pull"", ""issues"")
        comments_url = issue_url + ""/comments""
        response = api.post(comments_url, json=data)
        comment = response.json()
        return comment[""id""], comment[""html_url""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        id, url = self.post_comment(
            credentials,
            input_data.issue_url,
            input_data.comment,
        )
        yield ""id"", id
        yield ""url"", url

class GithubMakeIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        title: str = SchemaField(
            description=""Title of the issue"", placeholder=""Enter the issue title""
        )
        body: str = SchemaField(
            description=""Body of the issue"", placeholder=""Enter the issue body""
        )

    class Output(BlockSchema):
        number: int = SchemaField(description=""Number of the created issue"")
        url: str = SchemaField(description=""URL of the created issue"")
        error: str = SchemaField(
            description=""Error message if the issue creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""691dad47-f494-44c3-a1e8-05b7990f2dab"",
            description=""This block creates a new issue on a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakeIssueBlock.Input,
            output_schema=GithubMakeIssueBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""title"": ""Test Issue"",
                ""body"": ""This is a test issue."",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""number"", 1),
                (""url"", ""https://github.com/owner/repo/issues/1""),
            ],
            test_mock={
                ""create_issue"": lambda *args, **kwargs: (
                    1,
                    ""https://github.com/owner/repo/issues/1"",
                )
            },
        )

    @staticmethod
    def create_issue(
        credentials: GithubCredentials, repo_url: str, title: str, body: str
    ) -> tuple[int, str]:
        api = get_api(credentials)
        data = {""title"": title, ""body"": body}
        issues_url = repo_url + ""/issues""
        response = api.post(issues_url, json=data)
        issue = response.json()
        return issue[""number""], issue[""html_url""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        number, url = self.create_issue(
            credentials,
            input_data.repo_url,
            input_data.title,
            input_data.body,
        )
        yield ""number"", number
        yield ""url"", url

class GithubReadIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )

    class Output(BlockSchema):
        title: str = SchemaField(description=""Title of the issue"")
        body: str = SchemaField(description=""Body of the issue"")
        user: str = SchemaField(description=""User who created the issue"")
        error: str = SchemaField(
            description=""Error message if reading the issue failed""
        )

    def __init__(self):
        super().__init__(
            id=""6443c75d-032a-4772-9c08-230c707c8acc"",
            description=""This block reads the body, title, and user of a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadIssueBlock.Input,
            output_schema=GithubReadIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""title"", ""Title of the issue""),
                (""body"", ""This is the body of the issue.""),
                (""user"", ""username""),
            ],
            test_mock={
                ""read_issue"": lambda *args, **kwargs: (
                    ""Title of the issue"",
                    ""This is the body of the issue."",
                    ""username"",
                )
            },
        )

    @staticmethod
    def read_issue(
        credentials: GithubCredentials, issue_url: str
    ) -> tuple[str, str, str]:
        api = get_api(credentials)
        response = api.get(issue_url)
        data = response.json()
        title = data.get(""title"", ""No title found"")
        body = data.get(""body"", ""No body content found"")
        user = data.get(""user"", {}).get(""login"", ""No user found"")
        return title, body, user

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        title, body, user = self.read_issue(
            credentials,
            input_data.issue_url,
        )
        if title:
            yield ""title"", title
        if body:
            yield ""body"", body
        if user:
            yield ""user"", user

class GithubListIssuesBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class IssueItem(TypedDict):
            title: str
            url: str

        issue: IssueItem = SchemaField(
            title=""Issue"", description=""Issues with their title and URL""
        )
        error: str = SchemaField(description=""Error message if listing issues failed"")

    def __init__(self):
        super().__init__(
            id=""c215bfd7-0e57-4573-8f8c-f7d4963dcd74"",
            description=""This block lists all issues for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListIssuesBlock.Input,
            output_schema=GithubListIssuesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""issue"",
                    {
                        ""title"": ""Issue 1"",
                        ""url"": ""https://github.com/owner/repo/issues/1"",
                    },
                )
            ],
            test_mock={
                ""list_issues"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Issue 1"",
                        ""url"": ""https://github.com/owner/repo/issues/1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_issues(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.IssueItem]:
        api = get_api(credentials)
        issues_url = repo_url + ""/issues""
        response = api.get(issues_url)
        data = response.json()
        issues: list[GithubListIssuesBlock.Output.IssueItem] = [
            {""title"": issue[""title""], ""url"": issue[""html_url""]} for issue in data
        ]
        return issues

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        issues = self.list_issues(
            credentials,
            input_data.repo_url,
        )
        yield from ((""issue"", issue) for issue in issues)

class GithubAddLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        label: str = SchemaField(
            description=""Label to add to the issue or pull request"",
            placeholder=""Enter the label"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the label addition operation"")
        error: str = SchemaField(
            description=""Error message if the label addition failed""
        )

    def __init__(self):
        super().__init__(
            id=""98bd6b77-9506-43d5-b669-6b9733c4b1f1"",
            description=""This block adds a label to a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAddLabelBlock.Input,
            output_schema=GithubAddLabelBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""label"": ""bug"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Label added successfully"")],
            test_mock={""add_label"": lambda *args, **kwargs: ""Label added successfully""},
        )

    @staticmethod
    def add_label(credentials: GithubCredentials, issue_url: str, label: str) -> str:
        api = get_api(credentials)
        data = {""labels"": [label]}
        labels_url = issue_url + ""/labels""
        api.post(labels_url, json=data)
        return ""Label added successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.add_label(
            credentials,
            input_data.issue_url,
            input_data.label,
        )
        yield ""status"", status

class GithubRemoveLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        label: str = SchemaField(
            description=""Label to remove from the issue or pull request"",
            placeholder=""Enter the label"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the label removal operation"")
        error: str = SchemaField(
            description=""Error message if the label removal failed""
        )

    def __init__(self):
        super().__init__(
            id=""78f050c5-3e3a-48c0-9e5b-ef1ceca5589c"",
            description=""This block removes a label from a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubRemoveLabelBlock.Input,
            output_schema=GithubRemoveLabelBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""label"": ""bug"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Label removed successfully"")],
            test_mock={
                ""remove_label"": lambda *args, **kwargs: ""Label removed successfully""
            },
        )

    @staticmethod
    def remove_label(credentials: GithubCredentials, issue_url: str, label: str) -> str:
        api = get_api(credentials)
        label_url = issue_url + f""/labels/{label}""
        api.delete(label_url)
        return ""Label removed successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.remove_label(
            credentials,
            input_data.issue_url,
            input_data.label,
        )
        yield ""status"", status

class GithubAssignIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        assignee: str = SchemaField(
            description=""Username to assign to the issue"",
            placeholder=""Enter the username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the issue assignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the issue assignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""90507c72-b0ff-413a-886a-23bbbd66f542"",
            description=""This block assigns a user to a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAssignIssueBlock.Input,
            output_schema=GithubAssignIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""assignee"": ""username1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Issue assigned successfully"")],
            test_mock={
                ""assign_issue"": lambda *args, **kwargs: ""Issue assigned successfully""
            },
        )

    @staticmethod
    def assign_issue(
        credentials: GithubCredentials,
        issue_url: str,
        assignee: str,
    ) -> str:
        api = get_api(credentials)
        assignees_url = issue_url + ""/assignees""
        data = {""assignees"": [assignee]}
        api.post(assignees_url, json=data)
        return ""Issue assigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.assign_issue(
            credentials,
            input_data.issue_url,
            input_data.assignee,
        )
        yield ""status"", status

class GithubUnassignIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        assignee: str = SchemaField(
            description=""Username to unassign from the issue"",
            placeholder=""Enter the username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the issue unassignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the issue unassignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""d154002a-38f4-46c2-962d-2488f2b05ece"",
            description=""This block unassigns a user from a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubUnassignIssueBlock.Input,
            output_schema=GithubUnassignIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""assignee"": ""username1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Issue unassigned successfully"")],
            test_mock={
                ""unassign_issue"": lambda *args, **kwargs: ""Issue unassigned successfully""
            },
        )

    @staticmethod
    def unassign_issue(
        credentials: GithubCredentials,
        issue_url: str,
        assignee: str,
    ) -> str:
        api = get_api(credentials)
        assignees_url = issue_url + ""/assignees""
        data = {""assignees"": [assignee]}
        api.delete(assignees_url, json=data)
        return ""Issue unassigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.unassign_issue(
            credentials,
            input_data.issue_url,
            input_data.assignee,
        )
        yield ""status"", status","{'final_score': 99.51, 'length_similarity': 99.56, 'token_similarity': 98.84, 'structure_similarity': 99.62, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
23,"import pytest

from backend.util.request import validate_url


def test_validate_url():
    # Rejected IP ranges
    with pytest.raises(ValueError):
        validate_url(""localhost"", [])

    with pytest.raises(ValueError):
        validate_url(""192.168.1.1"", [])

    with pytest.raises(ValueError):
        validate_url(""127.0.0.1"", [])

    with pytest.raises(ValueError):
        validate_url(""0.0.0.0"", [])

    # Normal URLs
    assert validate_url(""google.com/a?b=c"", []) == ""http://google.com/a?b=c""
    assert validate_url(""github.com?key=!@!@"", []) == ""http://github.com?key=!@!@""

    # Scheme Enforcement
    with pytest.raises(ValueError):
        validate_url(""ftp://example.com"", [])
    with pytest.raises(ValueError):
        validate_url(""file://example.com"", [])

    # International domain that converts to punycode - should be allowed if public
    assert validate_url(""http://xn--exmple-cua.com"", []) == ""http://xn--exmple-cua.com""
    # If the domain fails IDNA encoding or is invalid, it should raise an error
    with pytest.raises(ValueError):
        validate_url(""http://exa◌mple.com"", [])

    # IPv6 Addresses
    with pytest.raises(ValueError):
        validate_url(""::1"", [])  # IPv6 loopback should be blocked
    with pytest.raises(ValueError):
        validate_url(""http://[::1]"", [])  # IPv6 loopback in URL form

    # Suspicious Characters in Hostname
    with pytest.raises(ValueError):
        validate_url(""http://example_underscore.com"", [])
    with pytest.raises(ValueError):
        validate_url(""http://exa mple.com"", [])  # Space in hostname

    # Malformed URLs
    with pytest.raises(ValueError):
        validate_url(""http://"", [])  # No hostname
    with pytest.raises(ValueError):
        validate_url(""://missing-scheme"", [])  # Missing proper scheme

    # Trusted Origins
    trusted = [""internal-api.company.com"", ""10.0.0.5""]
    assert (
        validate_url(""internal-api.company.com"", trusted)
        == ""http://internal-api.company.com""
    )
    assert validate_url(""10.0.0.5"", [""10.0.0.5""]) == ""http://10.0.0.5""

    # Special Characters in Path or Query
    assert (
        validate_url(""example.com/path%20with%20spaces"", [])
        == ""http://example.com/path%20with%20spaces""
    )

    # Backslashes should be replaced with forward slashes
    assert (
        validate_url(""http://example.com\\backslash"", [])
        == ""http://example.com/backslash""
    )

    # Check defaulting scheme behavior for valid domains
    assert validate_url(""example.com"", []) == ""http://example.com""
    assert validate_url(""https://secure.com"", []) == ""https://secure.com""

    # Non-ASCII Characters in Query/Fragment
    assert validate_url(""example.com?param=äöü"", []) == ""http://example.com?param=äöü""
","import pytest
from backend.util.request import validate_url

def test_validate_url():
    # Rejected IP ranges
    with pytest.raises(ValueError):
        validate_url(""localhost"", [])
    with pytest.raises(ValueError):
        validate_url(""192.168.1.1"", [])
    with pytest.raises(ValueError):
        validate_url(""127.0.0.1"", [])
    with pytest.raises(ValueError):
        validate_url(""0.0.0.0"", [])

    # Normal URLs
    assert validate_url(""google.com/a?b=c"", []) == ""http://google.com/a?b=c""
    assert validate_url(""github.com?key=!@!@"", []) == ""http://github.com?key=!@!@""

    # Scheme Enforcement
    with pytest.raises(ValueError):
        validate_url(""ftp://example.com"", [])
    with pytest.raises(ValueError):
        validate_url(""file://example.com"", [])

    # International domain that converts to punycode - should be allowed if public
    assert validate_url(""http://xn--exmple-cua.com"", []) == ""http://xn--exmple-cua.com""

    # If the domain fails IDNA encoding or is invalid, it should raise an error
    with pytest.raises(ValueError):
        validate_url(""http://exa\u25ccmple.com"", [])

    # IPv6 Addresses
    with pytest.raises(ValueError):
        validate_url(""::1"", [])  # IPv6 loopback should be blocked
    with pytest.raises(ValueError):
        validate_url(""http://[::1]"", [])  # IPv6 loopback in URL form

    # Suspicious Characters in Hostname
    with pytest.raises(ValueError):
        validate_url(""http://example_underscore.com"", [])
    with pytest.raises(ValueError):
        validate_url(""http://exa mple.com"", [])  # Space in hostname

    # Malformed URLs
    with pytest.raises(ValueError):
        validate_url(""http://"", [])  # No hostname
    with pytest.raises(ValueError):
        validate_url(""://missing-scheme"", [])  # Missing proper scheme

    # Trusted Origins
    trusted = [""internal-api.company.com"", ""10.0.0.5""]
    assert (
        validate_url(""internal-api.company.com"", trusted)
        == ""http://internal-api.company.com""
    )
    assert validate_url(""10.0.0.5"", [""10.0.0.5""]) == ""http://10.0.0.5""

    # Special Characters in Path or Query
    assert (
        validate_url(""example.com/path%20with%20spaces"", [])
        == ""http://example.com/path%20with%20spaces""
    )

    # Backslashes should be replaced with forward slashes
    assert (
        validate_url(""http://example.com\\backslash"", [])
        == ""http://example.com/backslash""
    )

    # Check defaulting scheme behavior for valid domains
    assert validate_url(""example.com"", []) == ""http://example.com""
    assert validate_url(""https://secure.com"", []) == ""https://secure.com""

    # Non-ASCII Characters in Query/Fragment
    assert validate_url(""example.com?param=\u00e4\u00f6\u00fc"", []) == ""http://example.com?param=\u00e4\u00f6\u00fc""","{'final_score': 99.29, 'length_similarity': 99.32, 'token_similarity': 97.96, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
24,"from backend.data.block import BlockOutput, BlockSchema
from backend.data.model import APIKeyCredentials, SchemaField

from ._api import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    Slant3DCredentialsField,
    Slant3DCredentialsInput,
)
from .base import Slant3DBlockBase


class Slant3DSlicerBlock(Slant3DBlockBase):
    """"""Block for slicing 3D model files""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        file_url: str = SchemaField(
            description=""URL of the 3D model file to slice (STL)""
        )

    class Output(BlockSchema):
        message: str = SchemaField(description=""Response message"")
        price: float = SchemaField(description=""Calculated price for printing"")
        error: str = SchemaField(description=""Error message if slicing failed"")

    def __init__(self):
        super().__init__(
            id=""f8a12c8d-3e4b-4d5f-b6a7-8c9d0e1f2g3h"",
            description=""Slice a 3D model file and get pricing information"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""file_url"": ""https://example.com/model.stl"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""message"", ""Slicing successful""), (""price"", 8.23)],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""message"": ""Slicing successful"",
                    ""data"": {""price"": 8.23},
                }
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            result = self._make_request(
                ""POST"",
                ""slicer"",
                credentials.api_key.get_secret_value(),
                json={""fileURL"": input_data.file_url},
            )
            yield ""message"", result[""message""]
            yield ""price"", result[""data""][""price""]
        except Exception as e:
            yield ""error"", str(e)
            raise
","from backend.data.block import BlockOutput, BlockSchema
from backend.data.model import APIKeyCredentials, SchemaField
from ._api import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    Slant3DCredentialsField,
    Slant3DCredentialsInput,
)
from .base import Slant3DBlockBase

class Slant3DSlicerBlock(Slant3DBlockBase):
    """"""Block for slicing 3D model files""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        file_url: str = SchemaField(
            description=""URL of the 3D model file to slice (STL)""
        )

    class Output(BlockSchema):
        message: str = SchemaField(description=""Response message"")
        price: float = SchemaField(description=""Calculated price for printing"")
        error: str = SchemaField(description=""Error message if slicing failed"")

    def __init__(self):
        super().__init__(
            id=""f8a12c8d-3e4b-4d5f-b6a7-8c9d0e1f2g3h"",
            description=""Slice a 3D model file and get pricing information"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""file_url"": ""https://example.com/model.stl"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""message"", ""Slicing successful""), (""price"", 8.23)],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""message"": ""Slicing successful"",
                    ""data"": {""price"": 8.23},
                }
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            result = self._make_request(
                ""POST"",
                ""slicer"",
                credentials.api_key.get_secret_value(),
                json={""fileURL"": input_data.file_url},
            )
            yield ""message"", result[""message""]
            yield ""price"", result[""data""][""price""]
        except Exception as e:
            yield ""error"", str(e)
            raise","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
25,"import logging
import time
from enum import Enum
from typing import Literal

import replicate
from pydantic import SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

logger = logging.getLogger(__name__)

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""replicate"",
    api_key=SecretStr(""mock-replicate-api-key""),
    title=""Mock Replicate API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


# Model version enum
class MusicGenModelVersion(str, Enum):
    STEREO_LARGE = ""stereo-large""
    MELODY_LARGE = ""melody-large""
    LARGE = ""large""


# Audio format enum
class AudioFormat(str, Enum):
    WAV = ""wav""
    MP3 = ""mp3""


# Normalization strategy enum
class NormalizationStrategy(str, Enum):
    LOUDNESS = ""loudness""
    CLIP = ""clip""
    PEAK = ""peak""
    RMS = ""rms""


class AIMusicGeneratorBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.REPLICATE], Literal[""api_key""]
        ] = CredentialsField(
            description=""The Replicate integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        prompt: str = SchemaField(
            description=""A description of the music you want to generate"",
            placeholder=""e.g., 'An upbeat electronic dance track with heavy bass'"",
            title=""Prompt"",
        )
        music_gen_model_version: MusicGenModelVersion = SchemaField(
            description=""Model to use for generation"",
            default=MusicGenModelVersion.STEREO_LARGE,
            title=""Model Version"",
        )
        duration: int = SchemaField(
            description=""Duration of the generated audio in seconds"",
            default=8,
            title=""Duration"",
        )
        temperature: float = SchemaField(
            description=""Controls the 'conservativeness' of the sampling process. Higher temperature means more diversity"",
            default=1.0,
            title=""Temperature"",
        )
        top_k: int = SchemaField(
            description=""Reduces sampling to the k most likely tokens"",
            default=250,
            title=""Top K"",
        )
        top_p: float = SchemaField(
            description=""Reduces sampling to tokens with cumulative probability of p. When set to 0 (default), top_k sampling is used"",
            default=0.0,
            title=""Top P"",
        )
        classifier_free_guidance: int = SchemaField(
            description=""Increases the influence of inputs on the output. Higher values produce lower-variance outputs that adhere more closely to inputs"",
            default=3,
            title=""Classifier Free Guidance"",
        )
        output_format: AudioFormat = SchemaField(
            description=""Output format for generated audio"",
            default=AudioFormat.WAV,
            title=""Output Format"",
        )
        normalization_strategy: NormalizationStrategy = SchemaField(
            description=""Strategy for normalizing audio"",
            default=NormalizationStrategy.LOUDNESS,
            title=""Normalization Strategy"",
        )

    class Output(BlockSchema):
        result: str = SchemaField(description=""URL of the generated audio file"")
        error: str = SchemaField(description=""Error message if the model run failed"")

    def __init__(self):
        super().__init__(
            id=""44f6c8ad-d75c-4ae1-8209-aad1c0326928"",
            description=""This block generates music using Meta's MusicGen model on Replicate."",
            categories={BlockCategory.AI},
            input_schema=AIMusicGeneratorBlock.Input,
            output_schema=AIMusicGeneratorBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""prompt"": ""An upbeat electronic dance track with heavy bass"",
                ""music_gen_model_version"": MusicGenModelVersion.STEREO_LARGE,
                ""duration"": 8,
                ""temperature"": 1.0,
                ""top_k"": 250,
                ""top_p"": 0.0,
                ""classifier_free_guidance"": 3,
                ""output_format"": AudioFormat.WAV,
                ""normalization_strategy"": NormalizationStrategy.LOUDNESS,
            },
            test_output=[
                (
                    ""result"",
                    ""https://replicate.com/output/generated-audio-url.wav"",
                ),
            ],
            test_mock={
                ""run_model"": lambda api_key, music_gen_model_version, prompt, duration, temperature, top_k, top_p, classifier_free_guidance, output_format, normalization_strategy: ""https://replicate.com/output/generated-audio-url.wav"",
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        max_retries = 3
        retry_delay = 5  # seconds
        last_error = None

        for attempt in range(max_retries):
            try:
                logger.debug(
                    f""[AIMusicGeneratorBlock] - Running model (attempt {attempt + 1})""
                )
                result = self.run_model(
                    api_key=credentials.api_key,
                    music_gen_model_version=input_data.music_gen_model_version,
                    prompt=input_data.prompt,
                    duration=input_data.duration,
                    temperature=input_data.temperature,
                    top_k=input_data.top_k,
                    top_p=input_data.top_p,
                    classifier_free_guidance=input_data.classifier_free_guidance,
                    output_format=input_data.output_format,
                    normalization_strategy=input_data.normalization_strategy,
                )
                if result and result != ""No output received"":
                    yield ""result"", result
                    return
                else:
                    last_error = ""Model returned empty or invalid response""
                    raise ValueError(last_error)
            except Exception as e:
                last_error = f""Unexpected error: {str(e)}""
                logger.error(f""[AIMusicGeneratorBlock] - Error: {last_error}"")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue

        # If we've exhausted all retries, yield the error
        yield ""error"", f""Failed after {max_retries} attempts. Last error: {last_error}""

    def run_model(
        self,
        api_key: SecretStr,
        music_gen_model_version: MusicGenModelVersion,
        prompt: str,
        duration: int,
        temperature: float,
        top_k: int,
        top_p: float,
        classifier_free_guidance: int,
        output_format: AudioFormat,
        normalization_strategy: NormalizationStrategy,
    ):
        # Initialize Replicate client with the API key
        client = replicate.Client(api_token=api_key.get_secret_value())

        # Run the model with parameters
        output = client.run(
            ""meta/musicgen:671ac645ce5e552cc63a54a2bbff63fcf798043055d2dac5fc9e36a837eedcfb"",
            input={
                ""prompt"": prompt,
                ""music_gen_model_version"": music_gen_model_version,
                ""duration"": duration,
                ""temperature"": temperature,
                ""top_k"": top_k,
                ""top_p"": top_p,
                ""classifier_free_guidance"": classifier_free_guidance,
                ""output_format"": output_format,
                ""normalization_strategy"": normalization_strategy,
            },
        )

        # Handle the output
        if isinstance(output, list) and len(output) > 0:
            result_url = output[0]  # If output is a list, get the first element
        elif isinstance(output, str):
            result_url = output  # If output is a string, use it directly
        else:
            result_url = (
                ""No output received""  # Fallback message if output is not as expected
            )

        return result_url
","import logging
import time
from enum import Enum
from typing import Literal
import replicate
from pydantic import SecretStr
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

logger = logging.getLogger(__name__)

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""replicate"",
    api_key=SecretStr(""mock-replicate-api-key""),
    title=""Mock Replicate API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}

# Model version enum
class MusicGenModelVersion(str, Enum):
    STEREO_LARGE = ""stereo-large""
    MELODY_LARGE = ""melody-large""
    LARGE = ""large""

class AudioFormat(str, Enum):
    WAV = ""wav""
    MP3 = ""mp3""

class NormalizationStrategy(str, Enum):
    LOUDNESS = ""loudness""
    CLIP = ""clip""
    PEAK = ""peak""
    RMS = ""rms""

class AIMusicGeneratorBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.REPLICATE], Literal[""api_key""]
        ] = CredentialsField(
            description=""The Replicate integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        prompt: str = SchemaField(
            description=""A description of the music you want to generate"",
            placeholder=""e.g., 'An upbeat electronic dance track with heavy bass'"",
            title=""Prompt"",
        )
        music_gen_model_version: MusicGenModelVersion = SchemaField(
            description=""Model to use for generation"",
            default=MusicGenModelVersion.STEREO_LARGE,
            title=""Model Version"",
        )
        duration: int = SchemaField(
            description=""Duration of the generated audio in seconds"",
            default=8,
            title=""Duration"",
        )
        temperature: float = SchemaField(
            description=""Controls the 'conservativeness' of the sampling process. Higher temperature means more diversity"",
            default=1.0,
            title=""Temperature"",
        )
        top_k: int = SchemaField(
            description=""Reduces sampling to the k most likely tokens"",
            default=250,
            title=""Top K"",
        )
        top_p: float = SchemaField(
            description=""Reduces sampling to tokens with cumulative probability of p. When set to 0 (default), top_k sampling is used"",
            default=0.0,
            title=""Top P"",
        )
        classifier_free_guidance: int = SchemaField(
            description=""Increases the influence of inputs on the output. Higher values produce lower-variance outputs that adhere more closely to inputs"",
            default=3,
            title=""Classifier Free Guidance"",
        )
        output_format: AudioFormat = SchemaField(
            description=""Output format for generated audio"",
            default=AudioFormat.WAV,
            title=""Output Format"",
        )
        normalization_strategy: NormalizationStrategy = SchemaField(
            description=""Strategy for normalizing audio"",
            default=NormalizationStrategy.LOUDNESS,
            title=""Normalization Strategy"",
        )

    class Output(BlockSchema):
        result: str = SchemaField(description=""URL of the generated audio file"")
        error: str = SchemaField(description=""Error message if the model run failed"")

    def __init__(self):
        super().__init__(
            id=""44f6c8ad-d75c-4ae1-8209-aad1c0326928"",
            description=""This block generates music using Meta's MusicGen model on Replicate."",
            categories={BlockCategory.AI},
            input_schema=AIMusicGeneratorBlock.Input,
            output_schema=AIMusicGeneratorBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""prompt"": ""An upbeat electronic dance track with heavy bass"",
                ""music_gen_model_version"": MusicGenModelVersion.STEREO_LARGE,
                ""duration"": 8,
                ""temperature"": 1.0,
                ""top_k"": 250,
                ""top_p"": 0.0,
                ""classifier_free_guidance"": 3,
                ""output_format"": AudioFormat.WAV,
                ""normalization_strategy"": NormalizationStrategy.LOUDNESS,
            },
            test_output=[
                (
                    ""result"",
                    ""https://replicate.com/output/generated-audio-url.wav"",
                ),
            ],
            test_mock={
                ""run_model"": lambda api_key, music_gen_model_version, prompt, duration, temperature, top_k, top_p, classifier_free_guidance, output_format, normalization_strategy: ""https://replicate.com/output/generated-audio-url.wav"",
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        max_retries = 3
        retry_delay = 5  # seconds
        last_error = None

        for attempt in range(max_retries):
            try:
                logger.debug(
                    f""[AIMusicGeneratorBlock] - Running model (attempt {attempt + 1})""
                )
                result = self.run_model(
                    api_key=credentials.api_key,
                    music_gen_model_version=input_data.music_gen_model_version,
                    prompt=input_data.prompt,
                    duration=input_data.duration,
                    temperature=input_data.temperature,
                    top_k=input_data.top_k,
                    top_p=input_data.top_p,
                    classifier_free_guidance=input_data.classifier_free_guidance,
                    output_format=input_data.output_format,
                    normalization_strategy=input_data.normalization_strategy,
                )
                if result and result != ""No output received"":
                    yield ""result"", result
                    return
                else:
                    last_error = ""Model returned empty or invalid response""
                    raise ValueError(last_error)
            except Exception as e:
                last_error = f""Unexpected error: {str(e)}""
                logger.error(f""[AIMusicGeneratorBlock] - Error: {last_error}"")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue

        yield ""error"", f""Failed after {max_retries} attempts. Last error: {last_error}""

    def run_model(
        self,
        api_key: SecretStr,
        music_gen_model_version: MusicGenModelVersion,
        prompt: str,
        duration: int,
        temperature: float,
        top_k: int,
        top_p: float,
        classifier_free_guidance: int,
        output_format: AudioFormat,
        normalization_strategy: NormalizationStrategy,
    ):
        # Initialize Replicate client with the API key
        client = replicate.Client(api_token=api_key.get_secret_value())

        # Run the model with parameters
        output = client.run(
            ""meta/musicgen:671ac645ce5e552cc63a54a2bbff63fcf798043055d2dac5fc9e36a837eedcfb"",
            input={
                ""prompt"": prompt,
                ""music_gen_model_version"": music_gen_model_version,
                ""duration"": duration,
                ""temperature"": temperature,
                ""top_k"": top_k,
                ""top_p"": top_p,
                ""classifier_free_guidance"": classifier_free_guidance,
                ""output_format"": output_format,
                ""normalization_strategy"": normalization_strategy,
            },
        )

        # Handle the output
        if isinstance(output, list) and len(output) > 0:
            result_url = output[0]  # If output is a list, get the first element
        elif isinstance(output, str):
            result_url = output  # If output is a string, use it directly
        else:
            result_url = (
                ""No output received""  # Fallback message if output is not as expected
            )

        return result_url","{'final_score': 98.9, 'length_similarity': 98.71, 'token_similarity': 97.94, 'structure_similarity': 98.54, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
26,"from enum import Enum
from typing import Literal

import replicate
from pydantic import SecretStr
from replicate.helpers import FileOutput

from backend.data.block import Block, BlockCategory, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName


class ImageSize(str, Enum):
    """"""
    Semantic sizes that map reliably across all models
    """"""

    SQUARE = ""square""  # For profile pictures, icons, etc.
    LANDSCAPE = ""landscape""  # For traditional photos, scenes
    PORTRAIT = ""portrait""  # For vertical photos, portraits
    WIDE = ""wide""  # For cinematic, desktop wallpapers
    TALL = ""tall""  # For mobile wallpapers, stories


# Mapping semantic sizes to model-specific formats
SIZE_TO_SD_RATIO = {
    ImageSize.SQUARE: ""1:1"",
    ImageSize.LANDSCAPE: ""4:3"",
    ImageSize.PORTRAIT: ""3:4"",
    ImageSize.WIDE: ""16:9"",
    ImageSize.TALL: ""9:16"",
}

SIZE_TO_FLUX_RATIO = {
    ImageSize.SQUARE: ""1:1"",
    ImageSize.LANDSCAPE: ""4:3"",
    ImageSize.PORTRAIT: ""3:4"",
    ImageSize.WIDE: ""16:9"",
    ImageSize.TALL: ""9:16"",
}

SIZE_TO_FLUX_DIMENSIONS = {
    ImageSize.SQUARE: (1024, 1024),
    ImageSize.LANDSCAPE: (1365, 1024),
    ImageSize.PORTRAIT: (1024, 1365),
    ImageSize.WIDE: (1440, 810),  # Adjusted to maintain 16:9 within 1440 limit
    ImageSize.TALL: (810, 1440),  # Adjusted to maintain 9:16 within 1440 limit
}

SIZE_TO_RECRAFT_DIMENSIONS = {
    ImageSize.SQUARE: ""1024x1024"",
    ImageSize.LANDSCAPE: ""1365x1024"",
    ImageSize.PORTRAIT: ""1024x1365"",
    ImageSize.WIDE: ""1536x1024"",
    ImageSize.TALL: ""1024x1536"",
}


class ImageStyle(str, Enum):
    """"""
    Complete set of supported styles
    """"""

    ANY = ""any""
    # Realistic image styles
    REALISTIC = ""realistic_image""
    REALISTIC_BW = ""realistic_image/b_and_w""
    REALISTIC_HDR = ""realistic_image/hdr""
    REALISTIC_NATURAL = ""realistic_image/natural_light""
    REALISTIC_STUDIO = ""realistic_image/studio_portrait""
    REALISTIC_ENTERPRISE = ""realistic_image/enterprise""
    REALISTIC_HARD_FLASH = ""realistic_image/hard_flash""
    REALISTIC_MOTION_BLUR = ""realistic_image/motion_blur""
    # Digital illustration styles
    DIGITAL_ART = ""digital_illustration""
    PIXEL_ART = ""digital_illustration/pixel_art""
    HAND_DRAWN = ""digital_illustration/hand_drawn""
    GRAIN = ""digital_illustration/grain""
    SKETCH = ""digital_illustration/infantile_sketch""
    POSTER = ""digital_illustration/2d_art_poster""
    POSTER_2 = ""digital_illustration/2d_art_poster_2""
    HANDMADE_3D = ""digital_illustration/handmade_3d""
    HAND_DRAWN_OUTLINE = ""digital_illustration/hand_drawn_outline""
    ENGRAVING_COLOR = ""digital_illustration/engraving_color""


class ImageGenModel(str, Enum):
    """"""
    Available model providers
    """"""

    FLUX = ""Flux 1.1 Pro""
    FLUX_ULTRA = ""Flux 1.1 Pro Ultra""
    RECRAFT = ""Recraft v3""
    SD3_5 = ""Stable Diffusion 3.5 Medium""


class AIImageGeneratorBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.REPLICATE], Literal[""api_key""]
        ] = CredentialsField(
            description=""Enter your Replicate API key to access the image generation API. You can obtain an API key from https://replicate.com/account/api-tokens."",
        )
        prompt: str = SchemaField(
            description=""Text prompt for image generation"",
            placeholder=""e.g., 'A red panda using a laptop in a snowy forest'"",
            title=""Prompt"",
        )
        model: ImageGenModel = SchemaField(
            description=""The AI model to use for image generation"",
            default=ImageGenModel.SD3_5,
            title=""Model"",
        )
        size: ImageSize = SchemaField(
            description=(
                ""Format of the generated image:\n""
                ""- Square: Perfect for profile pictures, icons\n""
                ""- Landscape: Traditional photo format\n""
                ""- Portrait: Vertical photos, portraits\n""
                ""- Wide: Cinematic format, desktop wallpapers\n""
                ""- Tall: Mobile wallpapers, social media stories""
            ),
            default=ImageSize.SQUARE,
            title=""Image Format"",
        )
        style: ImageStyle = SchemaField(
            description=""Visual style for the generated image"",
            default=ImageStyle.ANY,
            title=""Image Style"",
        )

    class Output(BlockSchema):
        image_url: str = SchemaField(description=""URL of the generated image"")
        error: str = SchemaField(description=""Error message if generation failed"")

    def __init__(self):
        super().__init__(
            id=""ed1ae7a0-b770-4089-b520-1f0005fad19a"",
            description=""Generate images using various AI models through a unified interface"",
            categories={BlockCategory.AI},
            input_schema=AIImageGeneratorBlock.Input,
            output_schema=AIImageGeneratorBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""prompt"": ""An octopus using a laptop in a snowy forest with 'AutoGPT' clearly visible on the screen"",
                ""model"": ImageGenModel.RECRAFT,
                ""size"": ImageSize.SQUARE,
                ""style"": ImageStyle.REALISTIC,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""image_url"",
                    ""https://replicate.delivery/generated-image.webp"",
                ),
            ],
            test_mock={
                ""_run_client"": lambda *args, **kwargs: ""https://replicate.delivery/generated-image.webp""
            },
        )

    def _run_client(
        self, credentials: APIKeyCredentials, model_name: str, input_params: dict
    ):
        try:
            # Initialize Replicate client
            client = replicate.Client(api_token=credentials.api_key.get_secret_value())

            # Run the model with input parameters
            output = client.run(model_name, input=input_params, wait=False)

            # Process output
            if isinstance(output, list) and len(output) > 0:
                if isinstance(output[0], FileOutput):
                    result_url = output[0].url
                else:
                    result_url = output[0]
            elif isinstance(output, FileOutput):
                result_url = output.url
            elif isinstance(output, str):
                result_url = output
            else:
                result_url = None

            return result_url

        except TypeError as e:
            raise TypeError(f""Error during model execution: {e}"")
        except Exception as e:
            raise RuntimeError(f""Unexpected error during model execution: {e}"")

    def generate_image(self, input_data: Input, credentials: APIKeyCredentials):
        try:
            # Handle style-based prompt modification for models without native style support
            modified_prompt = input_data.prompt
            if input_data.model not in [ImageGenModel.RECRAFT]:
                style_prefix = self._style_to_prompt_prefix(input_data.style)
                modified_prompt = f""{style_prefix} {modified_prompt}"".strip()

            if input_data.model == ImageGenModel.SD3_5:
                # Use Stable Diffusion 3.5 with aspect ratio
                input_params = {
                    ""prompt"": modified_prompt,
                    ""aspect_ratio"": SIZE_TO_SD_RATIO[input_data.size],
                    ""output_format"": ""webp"",
                    ""output_quality"": 90,
                    ""steps"": 40,
                    ""cfg_scale"": 7.0,
                }
                output = self._run_client(
                    credentials,
                    ""stability-ai/stable-diffusion-3.5-medium"",
                    input_params,
                )
                return output

            elif input_data.model == ImageGenModel.FLUX:
                # Use Flux-specific dimensions with 'jpg' format to avoid ReplicateError
                width, height = SIZE_TO_FLUX_DIMENSIONS[input_data.size]
                input_params = {
                    ""prompt"": modified_prompt,
                    ""width"": width,
                    ""height"": height,
                    ""aspect_ratio"": SIZE_TO_FLUX_RATIO[input_data.size],
                    ""output_format"": ""jpg"",  # Set to jpg for Flux models
                    ""output_quality"": 90,
                }
                output = self._run_client(
                    credentials, ""black-forest-labs/flux-1.1-pro"", input_params
                )
                return output

            elif input_data.model == ImageGenModel.FLUX_ULTRA:
                width, height = SIZE_TO_FLUX_DIMENSIONS[input_data.size]
                input_params = {
                    ""prompt"": modified_prompt,
                    ""width"": width,
                    ""height"": height,
                    ""aspect_ratio"": SIZE_TO_FLUX_RATIO[input_data.size],
                    ""output_format"": ""jpg"",
                    ""output_quality"": 90,
                }
                output = self._run_client(
                    credentials, ""black-forest-labs/flux-1.1-pro-ultra"", input_params
                )
                return output

            elif input_data.model == ImageGenModel.RECRAFT:
                input_params = {
                    ""prompt"": input_data.prompt,
                    ""size"": SIZE_TO_RECRAFT_DIMENSIONS[input_data.size],
                    ""style"": input_data.style.value,
                }
                output = self._run_client(
                    credentials, ""recraft-ai/recraft-v3"", input_params
                )
                return output

        except Exception as e:
            raise RuntimeError(f""Failed to generate image: {str(e)}"")

    def _style_to_prompt_prefix(self, style: ImageStyle) -> str:
        """"""
        Convert a style enum to a prompt prefix for models without native style support.
        """"""
        if style == ImageStyle.ANY:
            return """"

        style_map = {
            ImageStyle.REALISTIC: ""photorealistic"",
            ImageStyle.REALISTIC_BW: ""black and white photograph"",
            ImageStyle.REALISTIC_HDR: ""HDR photograph"",
            ImageStyle.REALISTIC_NATURAL: ""natural light photograph"",
            ImageStyle.REALISTIC_STUDIO: ""studio portrait photograph"",
            ImageStyle.REALISTIC_ENTERPRISE: ""enterprise photograph"",
            ImageStyle.REALISTIC_HARD_FLASH: ""hard flash photograph"",
            ImageStyle.REALISTIC_MOTION_BLUR: ""motion blur photograph"",
            ImageStyle.DIGITAL_ART: ""digital art"",
            ImageStyle.PIXEL_ART: ""pixel art"",
            ImageStyle.HAND_DRAWN: ""hand drawn illustration"",
            ImageStyle.GRAIN: ""grainy digital illustration"",
            ImageStyle.SKETCH: ""sketchy illustration"",
            ImageStyle.POSTER: ""2D art poster"",
            ImageStyle.POSTER_2: ""alternate 2D art poster"",
            ImageStyle.HANDMADE_3D: ""handmade 3D illustration"",
            ImageStyle.HAND_DRAWN_OUTLINE: ""hand drawn outline illustration"",
            ImageStyle.ENGRAVING_COLOR: ""color engraving illustration"",
        }

        style_text = style_map.get(style, """")
        return f""{style_text} of"" if style_text else """"

    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs):
        try:
            url = self.generate_image(input_data, credentials)
            if url:
                yield ""image_url"", url
            else:
                yield ""error"", ""Image generation returned an empty result.""
        except Exception as e:
            # Capture and return only the message of the exception, avoiding serialization of non-serializable objects
            yield ""error"", str(e)


# Test credentials stay the same
TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""replicate"",
    api_key=SecretStr(""mock-replicate-api-key""),
    title=""Mock Replicate API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.title,
}
","from enum import Enum
from typing import Literal
import replicate
from pydantic import SecretStr
from replicate.helpers import FileOutput
from backend.data.block import Block, BlockCategory, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName


class ImageSize(str, Enum):
    """"""
    Semantic sizes that map reliably across all models
    """"""
    SQUARE = ""square""  # For profile pictures, icons, etc.
    LANDSCAPE = ""landscape""  # For traditional photos, scenes
    PORTRAIT = ""portrait""  # For vertical photos, portraits
    WIDE = ""wide""  # For cinematic, desktop wallpapers
    TALL = ""tall""  # For mobile wallpapers, stories


# Mapping semantic sizes to model-specific formats
SIZE_TO_SD_RATIO = {
    ImageSize.SQUARE: ""1:1"",
    ImageSize.LANDSCAPE: ""4:3"",
    ImageSize.PORTRAIT: ""3:4"",
    ImageSize.WIDE: ""16:9"",
    ImageSize.TALL: ""9:16"",
}

SIZE_TO_FLUX_RATIO = {
    ImageSize.SQUARE: ""1:1"",
    ImageSize.LANDSCAPE: ""4:3"",
    ImageSize.PORTRAIT: ""3:4"",
    ImageSize.WIDE: ""16:9"",
    ImageSize.TALL: ""9:16"",
}

SIZE_TO_FLUX_DIMENSIONS = {
    ImageSize.SQUARE: (1024, 1024),
    ImageSize.LANDSCAPE: (1365, 1024),
    ImageSize.PORTRAIT: (1024, 1365),
    ImageSize.WIDE: (1440, 810),  # Adjusted to maintain 16:9 within 1440 limit
    ImageSize.TALL: (810, 1440),  # Adjusted to maintain 9:16 within 1440 limit
}

SIZE_TO_RECRAFT_DIMENSIONS = {
    ImageSize.SQUARE: ""1024x1024"",
    ImageSize.LANDSCAPE: ""1365x1024"",
    ImageSize.PORTRAIT: ""1024x1365"",
    ImageSize.WIDE: ""1536x1024"",
    ImageSize.TALL: ""1024x1536"",
}


class ImageStyle(str, Enum):
    """"""
    Complete set of supported styles
    """"""
    ANY = ""any""
    REALISTIC = ""realistic_image""
    REALISTIC_BW = ""realistic_image/b_and_w""
    REALISTIC_HDR = ""realistic_image/hdr""
    REALISTIC_NATURAL = ""realistic_image/natural_light""
    REALISTIC_STUDIO = ""realistic_image/studio_portrait""
    REALISTIC_ENTERPRISE = ""realistic_image/enterprise""
    REALISTIC_HARD_FLASH = ""realistic_image/hard_flash""
    REALISTIC_MOTION_BLUR = ""realistic_image/motion_blur""
    DIGITAL_ART = ""digital_illustration""
    PIXEL_ART = ""digital_illustration/pixel_art""
    HAND_DRAWN = ""digital_illustration/hand_drawn""
    GRAIN = ""digital_illustration/grain""
    SKETCH = ""digital_illustration/infantile_sketch""
    POSTER = ""digital_illustration/2d_art_poster""
    POSTER_2 = ""digital_illustration/2d_art_poster_2""
    HANDMADE_3D = ""digital_illustration/handmade_3d""
    HAND_DRAWN_OUTLINE = ""digital_illustration/hand_drawn_outline""
    ENGRAVING_COLOR = ""digital_illustration/engraving_color""


class ImageGenModel(str, Enum):
    """"""
    Available model providers
    """"""
    FLUX = ""Flux 1.1 Pro""
    FLUX_ULTRA = ""Flux 1.1 Pro Ultra""
    RECRAFT = ""Recraft v3""
    SD3_5 = ""Stable Diffusion 3.5 Medium""


class AIImageGeneratorBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.REPLICATE], Literal[""api_key""]
        ] = CredentialsField(
            description=""Enter your Replicate API key to access the image generation API. You can obtain an API key from https://replicate.com/account/api-tokens."",
        )
        prompt: str = SchemaField(
            description=""Text prompt for image generation"",
            placeholder=""e.g., 'A red panda using a laptop in a snowy forest'"",
            title=""Prompt"",
        )
        model: ImageGenModel = SchemaField(
            description=""The AI model to use for image generation"",
            default=ImageGenModel.SD3_5,
            title=""Model"",
        )
        size: ImageSize = SchemaField(
            description=(
                ""Format of the generated image:\n""
                ""- Square: Perfect for profile pictures, icons\n""
                ""- Landscape: Traditional photo format\n""
                ""- Portrait: Vertical photos, portraits\n""
                ""- Wide: Cinematic format, desktop wallpapers\n""
                ""- Tall: Mobile wallpapers, social media stories""
            ),
            default=ImageSize.SQUARE,
            title=""Image Format"",
        )
        style: ImageStyle = SchemaField(
            description=""Visual style for the generated image"",
            default=ImageStyle.ANY,
            title=""Image Style"",
        )

    class Output(BlockSchema):
        image_url: str = SchemaField(description=""URL of the generated image"")
        error: str = SchemaField(description=""Error message if generation failed"")

    def __init__(self):
        super().__init__(
            id=""ed1ae7a0-b770-4089-b520-1f0005fad19a"",
            description=""Generate images using various AI models through a unified interface"",
            categories={BlockCategory.AI},
            input_schema=AIImageGeneratorBlock.Input,
            output_schema=AIImageGeneratorBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""prompt"": ""An octopus using a laptop in a snowy forest with 'AutoGPT' clearly visible on the screen"",
                ""model"": ImageGenModel.RECRAFT,
                ""size"": ImageSize.SQUARE,
                ""style"": ImageStyle.REALISTIC,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""image_url"",
                    ""https://replicate.delivery/generated-image.webp"",
                ),
            ],
            test_mock={
                ""_run_client"": lambda *args, **kwargs: ""https://replicate.delivery/generated-image.webp""
            },
        )

    def _run_client(
        self, credentials: APIKeyCredentials, model_name: str, input_params: dict
    ):
        try:
            # Initialize Replicate client
            client = replicate.Client(api_token=credentials.api_key.get_secret_value())
            # Run the model with input parameters
            output = client.run(model_name, input=input_params, wait=False)
            # Process output
            if isinstance(output, list) and len(output) > 0:
                if isinstance(output[0], FileOutput):
                    result_url = output[0].url
                else:
                    result_url = output[0]
            elif isinstance(output, FileOutput):
                result_url = output.url
            elif isinstance(output, str):
                result_url = output
            else:
                result_url = None

            return result_url

        except TypeError as e:
            raise TypeError(f""Error during model execution: {e}"")
        except Exception as e:
            raise RuntimeError(f""Unexpected error during model execution: {e}"")

    def generate_image(self, input_data: Input, credentials: APIKeyCredentials):
        try:
            # Handle style-based prompt modification for models without native style support
            modified_prompt = input_data.prompt
            if input_data.model not in [ImageGenModel.RECRAFT]:
                style_prefix = self._style_to_prompt_prefix(input_data.style)
                modified_prompt = f""{style_prefix} {modified_prompt}"".strip()

            if input_data.model == ImageGenModel.SD3_5:
                # Use Stable Diffusion 3.5 with aspect ratio
                input_params = {
                    ""prompt"": modified_prompt,
                    ""aspect_ratio"": SIZE_TO_SD_RATIO[input_data.size],
                    ""output_format"": ""webp"",
                    ""output_quality"": 90,
                    ""steps"": 40,
                    ""cfg_scale"": 7.0,
                }
                output = self._run_client(
                    credentials,
                    ""stability-ai/stable-diffusion-3.5-medium"",
                    input_params,
                )
                return output

            elif input_data.model == ImageGenModel.FLUX:
                # Use Flux-specific dimensions with 'jpg' format to avoid ReplicateError
                width, height = SIZE_TO_FLUX_DIMENSIONS[input_data.size]
                input_params = {
                    ""prompt"": modified_prompt,
                    ""width"": width,
                    ""height"": height,
                    ""aspect_ratio"": SIZE_TO_FLUX_RATIO[input_data.size],
                    ""output_format"": ""jpg"",  # Set to jpg for Flux models
                    ""output_quality"": 90,
                }
                output = self._run_client(
                    credentials, ""black-forest-labs/flux-1.1-pro"", input_params
                )
                return output

            elif input_data.model == ImageGenModel.FLUX_ULTRA:
                width, height = SIZE_TO_FLUX_DIMENSIONS[input_data.size]
                input_params = {
                    ""prompt"": modified_prompt,
                    ""width"": width,
                    ""height"": height,
                    ""aspect_ratio"": SIZE_TO_FLUX_RATIO[input_data.size],
                    ""output_format"": ""jpg"",
                    ""output_quality"": 90,
                }
                output = self._run_client(
                    credentials, ""black-forest-labs/flux-1.1-pro-ultra"", input_params
                )
                return output

            elif input_data.model == ImageGenModel.RECRAFT:
                input_params = {
                    ""prompt"": input_data.prompt,
                    ""size"": SIZE_TO_RECRAFT_DIMENSIONS[input_data.size],
                    ""style"": input_data.style.value,
                }
                output = self._run_client(
                    credentials, ""recraft-ai/recraft-v3"", input_params
                )
                return output

        except Exception as e:
            raise RuntimeError(f""Failed to generate image: {str(e)}"")

    def _style_to_prompt_prefix(self, style: ImageStyle) -> str:
        """"""
        Convert a style enum to a prompt prefix for models without native style support.
        """"""
        if style == ImageStyle.ANY:
            return """"
        style_map = {
            ImageStyle.REALISTIC: ""photorealistic"",
            ImageStyle.REALISTIC_BW: ""black and white photograph"",
            ImageStyle.REALISTIC_HDR: ""HDR photograph"",
            ImageStyle.REALISTIC_NATURAL: ""natural light photograph"",
            ImageStyle.REALISTIC_STUDIO: ""studio portrait photograph"",
            ImageStyle.REALISTIC_ENTERPRISE: ""enterprise photograph"",
            ImageStyle.REALISTIC_HARD_FLASH: ""hard flash photograph"",
            ImageStyle.REALISTIC_MOTION_BLUR: ""motion blur photograph"",
            ImageStyle.DIGITAL_ART: ""digital art"",
            ImageStyle.PIXEL_ART: ""pixel art"",
            ImageStyle.HAND_DRAWN: ""hand drawn illustration"",
            ImageStyle.GRAIN: ""grainy digital illustration"",
            ImageStyle.SKETCH: ""sketchy illustration"",
            ImageStyle.POSTER: ""2D art poster"",
            ImageStyle.POSTER_2: ""alternate 2D art poster"",
            ImageStyle.HANDMADE_3D: ""handmade 3D illustration"",
            ImageStyle.HAND_DRAWN_OUTLINE: ""hand drawn outline illustration"",
            ImageStyle.ENGRAVING_COLOR: ""color engraving illustration"",
        }

        style_text = style_map.get(style, """")
        return f""{style_text} of"" if style_text else """"

    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs):
        try:
            url = self.generate_image(input_data, credentials)
            if url:
                yield ""image_url"", url
            else:
                yield ""error"", ""Image generation returned an empty result.""
        except Exception as e:
            # Capture and return only the message of the exception, avoiding serialization of non-serializable objects
            yield ""error"", str(e)


# Test credentials stay the same
TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""replicate"",
    api_key=SecretStr(""mock-replicate-api-key""),
    title=""Mock Replicate API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.title,
}","{'final_score': 99.61, 'length_similarity': 99.48, 'token_similarity': 99.44, 'structure_similarity': 99.3, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
27,"from typing import cast
from unittest.mock import AsyncMock

import pytest
from fastapi import WebSocket, WebSocketDisconnect

from backend.server.conn_manager import ConnectionManager
from backend.server.ws_api import (
    Methods,
    WsMessage,
    handle_subscribe,
    handle_unsubscribe,
    websocket_router,
)


@pytest.fixture
def mock_websocket() -> AsyncMock:
    return AsyncMock(spec=WebSocket)


@pytest.fixture
def mock_manager() -> AsyncMock:
    return AsyncMock(spec=ConnectionManager)


@pytest.mark.asyncio
async def test_websocket_router_subscribe(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    mock_websocket.receive_text.side_effect = [
        WsMessage(
            method=Methods.SUBSCRIBE, data={""graph_id"": ""test_graph""}
        ).model_dump_json(),
        WebSocketDisconnect(),
    ]

    await websocket_router(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager)
    )

    mock_manager.connect.assert_called_once_with(mock_websocket)
    mock_manager.subscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""subscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]
    mock_manager.disconnect.assert_called_once_with(mock_websocket)


@pytest.mark.asyncio
async def test_websocket_router_unsubscribe(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    mock_websocket.receive_text.side_effect = [
        WsMessage(
            method=Methods.UNSUBSCRIBE, data={""graph_id"": ""test_graph""}
        ).model_dump_json(),
        WebSocketDisconnect(),
    ]

    await websocket_router(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager)
    )

    mock_manager.connect.assert_called_once_with(mock_websocket)
    mock_manager.unsubscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""unsubscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]
    mock_manager.disconnect.assert_called_once_with(mock_websocket)


@pytest.mark.asyncio
async def test_websocket_router_invalid_method(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    mock_websocket.receive_text.side_effect = [
        WsMessage(method=Methods.EXECUTION_EVENT).model_dump_json(),
        WebSocketDisconnect(),
    ]

    await websocket_router(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager)
    )

    mock_manager.connect.assert_called_once_with(mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""error""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":false' in mock_websocket.send_text.call_args[0][0]
    mock_manager.disconnect.assert_called_once_with(mock_websocket)


@pytest.mark.asyncio
async def test_handle_subscribe_success(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.SUBSCRIBE, data={""graph_id"": ""test_graph""})

    await handle_subscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )

    mock_manager.subscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""subscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]


@pytest.mark.asyncio
async def test_handle_subscribe_missing_data(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.SUBSCRIBE)

    await handle_subscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )

    mock_manager.subscribe.assert_not_called()
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""error""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":false' in mock_websocket.send_text.call_args[0][0]


@pytest.mark.asyncio
async def test_handle_unsubscribe_success(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.UNSUBSCRIBE, data={""graph_id"": ""test_graph""})

    await handle_unsubscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )

    mock_manager.unsubscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""unsubscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]


@pytest.mark.asyncio
async def test_handle_unsubscribe_missing_data(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.UNSUBSCRIBE)

    await handle_unsubscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )

    mock_manager.unsubscribe.assert_not_called()
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""error""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":false' in mock_websocket.send_text.call_args[0][0]
","from typing import cast
from unittest.mock import AsyncMock
import pytest
from fastapi import WebSocket, WebSocketDisconnect
from backend.server.conn_manager import ConnectionManager
from backend.server.ws_api import (
    Methods,
    WsMessage,
    handle_subscribe,
    handle_unsubscribe,
    websocket_router,
)

@pytest.fixture
def mock_websocket() -> AsyncMock:
    return AsyncMock(spec=WebSocket)

@pytest.fixture
def mock_manager() -> AsyncMock:
    return AsyncMock(spec=ConnectionManager)

@pytest.mark.asyncio
async def test_websocket_router_subscribe(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    mock_websocket.receive_text.side_effect = [
        WsMessage(
            method=Methods.SUBSCRIBE, data={""graph_id"": ""test_graph""}
        ).model_dump_json(),
        WebSocketDisconnect(),
    ]

    await websocket_router(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager)
    )
    mock_manager.connect.assert_called_once_with(mock_websocket)
    mock_manager.subscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""subscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]
    mock_manager.disconnect.assert_called_once_with(mock_websocket)

@pytest.mark.asyncio
async def test_websocket_router_unsubscribe(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    mock_websocket.receive_text.side_effect = [
        WsMessage(
            method=Methods.UNSUBSCRIBE, data={""graph_id"": ""test_graph""}
        ).model_dump_json(),
        WebSocketDisconnect(),
    ]

    await websocket_router(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager)
    )
    mock_manager.connect.assert_called_once_with(mock_websocket)
    mock_manager.unsubscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""unsubscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]
    mock_manager.disconnect.assert_called_once_with(mock_websocket)

@pytest.mark.asyncio
async def test_websocket_router_invalid_method(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    mock_websocket.receive_text.side_effect = [
        WsMessage(method=Methods.EXECUTION_EVENT).model_dump_json(),
        WebSocketDisconnect(),
    ]

    await websocket_router(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager)
    )
    mock_manager.connect.assert_called_once_with(mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""error""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":false' in mock_websocket.send_text.call_args[0][0]
    mock_manager.disconnect.assert_called_once_with(mock_websocket)

@pytest.mark.asyncio
async def test_handle_subscribe_success(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.SUBSCRIBE, data={""graph_id"": ""test_graph""})
    await handle_subscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )
    mock_manager.subscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""subscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]

@pytest.mark.asyncio
async def test_handle_subscribe_missing_data(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.SUBSCRIBE)
    await handle_subscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )
    mock_manager.subscribe.assert_not_called()
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""error""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":false' in mock_websocket.send_text.call_args[0][0]

@pytest.mark.asyncio
async def test_handle_unsubscribe_success(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.UNSUBSCRIBE, data={""graph_id"": ""test_graph""})
    await handle_unsubscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )
    mock_manager.unsubscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""unsubscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]

@pytest.mark.asyncio
async def test_handle_unsubscribe_missing_data(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.UNSUBSCRIBE)
    await handle_unsubscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )
    mock_manager.unsubscribe.assert_not_called()
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""error""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":false' in mock_websocket.send_text.call_args[0][0]","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
28,"from enum import Enum
from typing import Any, Optional

from pydantic import BaseModel

from backend.data.block import BlockInput


class BlockCostType(str, Enum):
    RUN = ""run""  # cost X credits per run
    BYTE = ""byte""  # cost X credits per byte
    SECOND = ""second""  # cost X credits per second
    DOLLAR = ""dollar""  # cost X dollars per run


class BlockCost(BaseModel):
    cost_amount: int
    cost_filter: BlockInput
    cost_type: BlockCostType

    def __init__(
        self,
        cost_amount: int,
        cost_type: BlockCostType = BlockCostType.RUN,
        cost_filter: Optional[BlockInput] = None,
        **data: Any,
    ) -> None:
        super().__init__(
            cost_amount=cost_amount,
            cost_filter=cost_filter or {},
            cost_type=cost_type,
            **data,
        )
","from enum import Enum
from typing import Any, Optional
from pydantic import BaseModel
from backend.data.block import BlockInput

class BlockCostType(str, Enum):
    RUN = ""run""  # cost X credits per run
    BYTE = ""byte""  # cost X credits per byte
    SECOND = ""second""  # cost X credits per second
    DOLLAR = ""dollar""  # cost X dollars per run

class BlockCost(BaseModel):
    cost_amount: int
    cost_filter: BlockInput
    cost_type: BlockCostType

    def __init__(
        self,
        cost_amount: int,
        cost_type: BlockCostType = BlockCostType.RUN,
        cost_filter: Optional[BlockInput] = None,
        **data: Any,
    ) -> None:
        super().__init__(
            cost_amount=cost_amount,
            cost_filter=cost_filter or {},
            cost_type=cost_type,
            **data,
        )","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
29,"from backend.app import run_processes
from backend.server.ws_api import WebsocketServer


def main():
    """"""
    Run all the processes required for the AutoGPT-server WebSocket API.
    """"""
    run_processes(WebsocketServer())


if __name__ == ""__main__"":
    main()
","from backend.app import run_processes
from backend.server.ws_api import WebsocketServer

def main():
    """"""
    Run all the processes required for the AutoGPT-server WebSocket API.
    """"""
    run_processes(WebsocketServer())

if __name__ == ""__main__"":
    main()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
30,"# -*- coding: utf-8 -*-

import re
import sys
import random
from typing import List, Tuple

import requests
from requests.models import Response


def find_links_in_text(text: str) -> List[str]:
    """"""Find links in a text and return a list of URLs.""""""

    link_pattern = re.compile(r'((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:\'\"".,<>?«»“”‘’]))')

    raw_links = re.findall(link_pattern, text)

    links = [
        str(raw_link[0]) for raw_link in raw_links
    ]

    return links


def find_links_in_file(filename: str) -> List[str]:
    """"""Find links in a file and return a list of URLs from text file.""""""

    with open(filename, mode='r', encoding='utf-8') as file:
        readme = file.read()
        index_section = readme.find('## Index')
        if index_section == -1:
            index_section = 0
        content = readme[index_section:]

    links = find_links_in_text(content)

    return links


def check_duplicate_links(links: List[str]) -> Tuple[bool, List]:
    """"""Check for duplicated links.

    Returns a tuple with True or False and duplicate list.
    """"""

    seen = {}
    duplicates = []
    has_duplicate = False

    for link in links:
        link = link.rstrip('/')
        if link not in seen:
            seen[link] = 1
        else:
            if seen[link] == 1:
                duplicates.append(link)

    if duplicates:
        has_duplicate = True

    return (has_duplicate, duplicates)


def fake_user_agent() -> str:
    """"""Faking user agent as some hosting services block not-whitelisted UA.""""""

    user_agents = [
        'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1467.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko)',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36',
    ]

    return random.choice(user_agents)


def get_host_from_link(link: str) -> str:

    host = link.split('://', 1)[1] if '://' in link else link

    # Remove routes, arguments and anchors
    if '/' in host:
        host = host.split('/', 1)[0]

    elif '?' in host:
        host = host.split('?', 1)[0]

    elif '#' in host:
        host = host.split('#', 1)[0]

    return host


def has_cloudflare_protection(resp: Response) -> bool:
    """"""Checks if there is any cloudflare protection in the response.

    Cloudflare implements multiple network protections on a given link,
    this script tries to detect if any of them exist in the response from request.

    Common protections have the following HTTP code as a response:
        - 403: When host header is missing or incorrect (and more)
        - 503: When DDOS protection exists

    See more about it at:
        - https://support.cloudflare.com/hc/en-us/articles/115003014512-4xx-Client-Error
        - https://support.cloudflare.com/hc/en-us/articles/115003011431-Troubleshooting-Cloudflare-5XX-errors
        - https://www.cloudflare.com/ddos/
        - https://superuser.com/a/888526

    Discussions in issues and pull requests:
        - https://github.com/public-apis/public-apis/pull/2409
        - https://github.com/public-apis/public-apis/issues/2960 
    """"""

    code = resp.status_code
    server = resp.headers.get('Server') or resp.headers.get('server')
    cloudflare_flags = [
        '403 Forbidden',
        'cloudflare',
        'Cloudflare',
        'Security check',
        'Please Wait... | Cloudflare',
        'We are checking your browser...',
        'Please stand by, while we are checking your browser...',
        'Checking your browser before accessing',
        'This process is automatic.',
        'Your browser will redirect to your requested content shortly.',
        'Please allow up to 5 seconds',
        'DDoS protection by',
        'Ray ID:',
        'Cloudflare Ray ID:',
        '_cf_chl',
        '_cf_chl_opt',
        '__cf_chl_rt_tk',
        'cf-spinner-please-wait',
        'cf-spinner-redirecting'
    ]

    if code in [403, 503] and server == 'cloudflare':
        html = resp.text

        flags_found = [flag in html for flag in cloudflare_flags]
        any_flag_found = any(flags_found)

        if any_flag_found:
            return True

    return False


def check_if_link_is_working(link: str) -> Tuple[bool, str]:
    """"""Checks if a link is working.

    If an error is identified when the request for the link occurs,
    the return will be a tuple with the first value True and the second
    value a string containing the error message.

    If no errors are identified, the return will be a tuple with the
    first value False and the second an empty string.
    """"""

    has_error = False
    error_message = ''

    try:
        resp = requests.get(link, timeout=25, headers={
            'User-Agent': fake_user_agent(),
            'host': get_host_from_link(link)
        })

        code = resp.status_code

        if code >= 400 and not has_cloudflare_protection(resp):
            has_error = True
            error_message = f'ERR:CLT: {code} : {link}'

    except requests.exceptions.SSLError as error:
        has_error = True
        error_message = f'ERR:SSL: {error} : {link}'

    except requests.exceptions.ConnectionError as error:
        has_error = True
        error_message = f'ERR:CNT: {error} : {link}'

    except (TimeoutError, requests.exceptions.ConnectTimeout):
        has_error = True
        error_message = f'ERR:TMO: {link}'

    except requests.exceptions.TooManyRedirects as error:
        has_error = True
        error_message = f'ERR:TMR: {error} : {link}'

    except (Exception, requests.exceptions.RequestException) as error:
        has_error = True
        error_message = f'ERR:UKN: {error} : {link}'

    return (has_error, error_message)


def check_if_list_of_links_are_working(list_of_links: List[str]) -> List[str]:
    error_messages = []
    for link in list_of_links:
        has_error, error_message = check_if_link_is_working(link)

        if has_error:
            error_messages.append(error_message)

    return error_messages


def start_duplicate_links_checker(links: List[str]) -> None:

    print('Checking for duplicate links...')

    has_duplicate_link, duplicates_links = check_duplicate_links(links)

    if has_duplicate_link:
        print(f'Found duplicate links:')

        for duplicate_link in duplicates_links:
            print(duplicate_link)

        sys.exit(1)
    else:
        print('No duplicate links.')


def start_links_working_checker(links: List[str]) -> None:

    print(f'Checking if {len(links)} links are working...')

    errors = check_if_list_of_links_are_working(links)
    if errors:

        num_errors = len(errors)
        print(f'Apparently {num_errors} links are not working properly. See in:')

        for error_message in errors:
            print(error_message)

        sys.exit(1)


def main(filename: str, only_duplicate_links_checker: bool) -> None:

    links = find_links_in_file(filename)

    start_duplicate_links_checker(links)

    if not only_duplicate_links_checker:
        start_links_working_checker(links)


if __name__ == '__main__':
    num_args = len(sys.argv)
    only_duplicate_links_checker = False

    if num_args < 2:
        print('No .md file passed')
        sys.exit(1)
    elif num_args == 3:
        third_arg = sys.argv[2].lower()

        if third_arg == '-odlc' or third_arg == '--only_duplicate_links_checker':
            only_duplicate_links_checker = True
        else:
            print(f'Third invalid argument. Usage: python {__file__} [-odlc | --only_duplicate_links_checker]')
            sys.exit(1)

    filename = sys.argv[1]

    main(filename, only_duplicate_links_checker)
","# -*- coding: utf-8 -*-

import re
import sys
import random
from typing import List, Tuple

import requests
from requests.models import Response


def find_links_in_text(text: str) -> List[str]:
    """"""Find links in a text and return a list of URLs.""""""
    link_pattern = re.compile(r'((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\\\"".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))')

    raw_links = re.findall(link_pattern, text)

    links = [
        str(raw_link[0]) for raw_link in raw_links
    ]
    return links


def find_links_in_file(filename: str) -> List[str]:
    """"""Find links in a file and return a list of URLs from text file.""""""
    with open(filename, mode='r', encoding='utf-8') as file:
        readme = file.read()
        index_section = readme.find('## Index')

        if index_section == -1:
            index_section = 0
        content = readme[index_section:]

    links = find_links_in_text(content)

    return links


def check_duplicate_links(links: List[str]) -> Tuple[bool, List]:
    """"""Check for duplicated links.

    Returns a tuple with True or False and duplicate list.
    """"""
    seen = {}
    duplicates = []
    has_duplicate = False

    for link in links:
        link = link.rstrip('/')

        if link not in seen:
            seen[link] = 1
        else:
            if seen[link] == 1:
                duplicates.append(link)
    if duplicates:
        has_duplicate = True
    return (has_duplicate, duplicates)


def fake_user_agent() -> str:
    """"""Faking user agent as some hosting services block not-whitelisted UA.""""""
    user_agents = [
        'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1467.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko)',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36',
    ]

    return random.choice(user_agents)


def get_host_from_link(link: str) -> str:
    host = link.split('://', 1)[1] if '://' in link else link

    # Remove routes, arguments and anchors
    if '/' in host:
        host = host.split('/', 1)[0]
    elif '?' in host:
        host = host.split('?', 1)[0]
    elif '#' in host:
        host = host.split('#', 1)[0]
    return host


def has_cloudflare_protection(resp: Response) -> bool:
    """"""Checks if there is any cloudflare protection in the response.

    Cloudflare implements multiple network protections on a given link,
    this script tries to detect if any of them exist in the response from request.

    Common protections have the following HTTP code as a response:
        - 403: When host header is missing or incorrect (and more)
        - 503: When DDOS protection exists

    See more about it at:
        - https://support.cloudflare.com/hc/en-us/articles/115003014512-4xx-Client-Error
        - https://support.cloudflare.com/hc/en-us/articles/115003011431-Troubleshooting-Cloudflare-5XX-errors
        - https://www.cloudflare.com/ddos/
        - https://superuser.com/a/888526

    Discussions in issues and pull requests:
        - https://github.com/public-apis/public-apis/pull/2409
        - https://github.com/public-apis/public-apis/issues/2960 
    """"""
    code = resp.status_code
    server = resp.headers.get('Server') or resp.headers.get('server')

    cloudflare_flags = [
        '403 Forbidden',
        'cloudflare',
        'Cloudflare',
        'Security check',
        'Please Wait... | Cloudflare',
        'We are checking your browser...',
        'Please stand by, while we are checking your browser...',
        'Checking your browser before accessing',
        'This process is automatic.',
        'Your browser will redirect to your requested content shortly.',
        'Please allow up to 5 seconds',
        'DDoS protection by',
        'Ray ID:',
        'Cloudflare Ray ID:',
        '_cf_chl',
        '_cf_chl_opt',
        '__cf_chl_rt_tk',
        'cf-spinner-please-wait',
        'cf-spinner-redirecting'
    ]

    if code in [403, 503] and server == 'cloudflare':
        html = resp.text
        flags_found = [flag in html for flag in cloudflare_flags]
        any_flag_found = any(flags_found)
        if any_flag_found:
            return True
    return False


def check_if_link_is_working(link: str) -> Tuple[bool, str]:
    """"""Checks if a link is working.

    If an error is identified when the request for the link occurs,
    the return will be a tuple with the first value True and the second
    value a string containing the error message.

    If no errors are identified, the return will be a tuple with the
    first value False and the second an empty string.
    """"""
    has_error = False
    error_message = ''

    try:
        resp = requests.get(link, timeout=25, headers={
            'User-Agent': fake_user_agent(),
            'host': get_host_from_link(link)
        })
        code = resp.status_code

        if code >= 400 and not has_cloudflare_protection(resp):
            has_error = True
            error_message = f'ERR:CLT: {code} : {link}'
    except requests.exceptions.SSLError as error:
        has_error = True
        error_message = f'ERR:SSL: {error} : {link}'
    except requests.exceptions.ConnectionError as error:
        has_error = True
        error_message = f'ERR:CNT: {error} : {link}'
    except (TimeoutError, requests.exceptions.ConnectTimeout):
        has_error = True
        error_message = f'ERR:TMO: {link}'
    except requests.exceptions.TooManyRedirects as error:
        has_error = True
        error_message = f'ERR:TMR: {error} : {link}'
    except (Exception, requests.exceptions.RequestException) as error:
        has_error = True
        error_message = f'ERR:UKN: {error} : {link}'
    return (has_error, error_message)


def check_if_list_of_links_are_working(list_of_links: List[str]) -> List[str]:
    error_messages = []
    for link in list_of_links:
        has_error, error_message = check_if_link_is_working(link)
        if has_error:
            error_messages.append(error_message)
    return error_messages


def start_duplicate_links_checker(links: List[str]) -> None:
    print('Checking for duplicate links...')
    has_duplicate_link, duplicates_links = check_duplicate_links(links)
    if has_duplicate_link:
        print(f'Found duplicate links:')
        for duplicate_link in duplicates_links:
            print(duplicate_link)
        sys.exit(1)
    print('No duplicate links.')


def start_links_working_checker(links: List[str]) -> None:
    print(f'Checking if {len(links)} links are working...')
    errors = check_if_list_of_links_are_working(links)
    if errors:
        num_errors = len(errors)
        print(f'Apparently {num_errors} links are not working properly. See in:')
        for error_message in errors:
            print(error_message)
        sys.exit(1)


def main(filename: str, only_duplicate_links_checker: bool) -> None:
    links = find_links_in_file(filename)
    start_duplicate_links_checker(links)
    if not only_duplicate_links_checker:
        start_links_working_checker(links)


if __name__ == '__main__':
    num_args = len(sys.argv)
    only_duplicate_links_checker = False

    if num_args < 2:
        print('No .md file passed')
        sys.exit(1)
    elif num_args == 3:
        third_arg = sys.argv[2].lower()

        if third_arg == '-odlc' or third_arg == '--only_duplicate_links_checker':
            only_duplicate_links_checker = True
        else:
            print(f'Third invalid argument. Usage: python {__file__} [-odlc | --only_duplicate_links_checker]')
            sys.exit(1)

    filename = sys.argv[1]

    main(filename, only_duplicate_links_checker)","{'final_score': 99.87, 'length_similarity': 99.81, 'token_similarity': 100.0, 'structure_similarity': 99.47, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
31,"def excess_3_code(number: int) -> str:
    """"""
    Find excess-3 code of integer base 10.
    Add 3 to all digits in a decimal number then convert to a binary-coded decimal.
    https://en.wikipedia.org/wiki/Excess-3

    >>> excess_3_code(0)
    '0b0011'
    >>> excess_3_code(3)
    '0b0110'
    >>> excess_3_code(2)
    '0b0101'
    >>> excess_3_code(20)
    '0b01010011'
    >>> excess_3_code(120)
    '0b010001010011'
    """"""
    num = """"
    for digit in str(max(0, number)):
        num += str(bin(int(digit) + 3))[2:].zfill(4)
    return ""0b"" + num


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","def excess_3_code(number: int) -> str:
    """"""
    Find excess-3 code of integer base 10.
    Add 3 to all digits in a decimal number then convert to a binary-coded decimal.
    https://en.wikipedia.org/wiki/Excess-3

    >>> excess_3_code(0)
    '0b0011'
    >>> excess_3_code(3)
    '0b0110'
    >>> excess_3_code(2)
    '0b0101'
    >>> excess_3_code(20)
    '0b01010011'
    >>> excess_3_code(120)
    '0b010001010011'
    """"""
    num = """"
    for digit in str(max(0, number)):
        num += str(bin(int(digit) + 3))[2:].zfill(4)
    return ""0b"" + num

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
32,"import datetime

import autogpt_libs.auth.depends
import autogpt_libs.auth.middleware
import fastapi
import fastapi.testclient
import prisma.enums
import pytest_mock

import backend.server.v2.store.model
import backend.server.v2.store.routes

app = fastapi.FastAPI()
app.include_router(backend.server.v2.store.routes.router)

client = fastapi.testclient.TestClient(app)


def override_auth_middleware():
    """"""Override auth middleware for testing""""""
    return {""sub"": ""test-user-id""}


def override_get_user_id():
    """"""Override get_user_id for testing""""""
    return ""test-user-id""


app.dependency_overrides[autogpt_libs.auth.middleware.auth_middleware] = (
    override_auth_middleware
)
app.dependency_overrides[autogpt_libs.auth.depends.get_user_id] = override_get_user_id


def test_get_agents_defaults(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=0,
            total_items=0,
            total_pages=0,
            page_size=10,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value
    response = client.get(""/agents"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert data.pagination.total_pages == 0
    assert data.agents == []
    mock_db_call.assert_called_once_with(
        featured=False,
        creator=None,
        sorted_by=None,
        search_query=None,
        category=None,
        page=1,
        page_size=20,
    )


def test_get_agents_featured(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""featured-agent"",
                agent_name=""Featured Agent"",
                agent_image=""featured.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar1.jpg"",
                sub_heading=""Featured agent subheading"",
                description=""Featured agent description"",
                runs=100,
                rating=4.5,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value
    response = client.get(""/agents?featured=true"")
    assert response.status_code == 200
    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 1
    assert data.agents[0].slug == ""featured-agent""
    mock_db_call.assert_called_once_with(
        featured=True,
        creator=None,
        sorted_by=None,
        search_query=None,
        category=None,
        page=1,
        page_size=20,
    )


def test_get_agents_by_creator(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""creator-agent"",
                agent_name=""Creator Agent"",
                agent_image=""agent.jpg"",
                creator=""specific-creator"",
                creator_avatar=""avatar.jpg"",
                sub_heading=""Creator agent subheading"",
                description=""Creator agent description"",
                runs=50,
                rating=4.0,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value
    response = client.get(""/agents?creator=specific-creator"")
    assert response.status_code == 200
    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 1
    assert data.agents[0].creator == ""specific-creator""
    mock_db_call.assert_called_once_with(
        featured=False,
        creator=""specific-creator"",
        sorted_by=None,
        search_query=None,
        category=None,
        page=1,
        page_size=20,
    )


def test_get_agents_sorted(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""top-agent"",
                agent_name=""Top Agent"",
                agent_image=""top.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar1.jpg"",
                sub_heading=""Top agent subheading"",
                description=""Top agent description"",
                runs=1000,
                rating=5.0,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value
    response = client.get(""/agents?sorted_by=runs"")
    assert response.status_code == 200
    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 1
    assert data.agents[0].runs == 1000
    mock_db_call.assert_called_once_with(
        featured=False,
        creator=None,
        sorted_by=""runs"",
        search_query=None,
        category=None,
        page=1,
        page_size=20,
    )


def test_get_agents_search(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""search-agent"",
                agent_name=""Search Agent"",
                agent_image=""search.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar1.jpg"",
                sub_heading=""Search agent subheading"",
                description=""Specific search term description"",
                runs=75,
                rating=4.2,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value
    response = client.get(""/agents?search_query=specific"")
    assert response.status_code == 200
    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 1
    assert ""specific"" in data.agents[0].description.lower()
    mock_db_call.assert_called_once_with(
        featured=False,
        creator=None,
        sorted_by=None,
        search_query=""specific"",
        category=None,
        page=1,
        page_size=20,
    )


def test_get_agents_category(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""category-agent"",
                agent_name=""Category Agent"",
                agent_image=""category.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar1.jpg"",
                sub_heading=""Category agent subheading"",
                description=""Category agent description"",
                runs=60,
                rating=4.1,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value
    response = client.get(""/agents?category=test-category"")
    assert response.status_code == 200
    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 1
    mock_db_call.assert_called_once_with(
        featured=False,
        creator=None,
        sorted_by=None,
        search_query=None,
        category=""test-category"",
        page=1,
        page_size=20,
    )


def test_get_agents_pagination(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=f""agent-{i}"",
                agent_name=f""Agent {i}"",
                agent_image=f""agent{i}.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar1.jpg"",
                sub_heading=f""Agent {i} subheading"",
                description=f""Agent {i} description"",
                runs=i * 10,
                rating=4.0,
            )
            for i in range(5)
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=2,
            total_items=15,
            total_pages=3,
            page_size=5,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value
    response = client.get(""/agents?page=2&page_size=5"")
    assert response.status_code == 200
    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 5
    assert data.pagination.current_page == 2
    assert data.pagination.page_size == 5
    mock_db_call.assert_called_once_with(
        featured=False,
        creator=None,
        sorted_by=None,
        search_query=None,
        category=None,
        page=2,
        page_size=5,
    )


def test_get_agents_malformed_request(mocker: pytest_mock.MockFixture):
    # Test with invalid page number
    response = client.get(""/agents?page=-1"")
    assert response.status_code == 422

    # Test with invalid page size
    response = client.get(""/agents?page_size=0"")
    assert response.status_code == 422

    # Test with non-numeric values
    response = client.get(""/agents?page=abc&page_size=def"")
    assert response.status_code == 422

    # Verify no DB calls were made
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.assert_not_called()


def test_get_agent_details(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentDetails(
        store_listing_version_id=""test-version-id"",
        slug=""test-agent"",
        agent_name=""Test Agent"",
        agent_video=""video.mp4"",
        agent_image=[""image1.jpg"", ""image2.jpg""],
        creator=""creator1"",
        creator_avatar=""avatar1.jpg"",
        sub_heading=""Test agent subheading"",
        description=""Test agent description"",
        categories=[""category1"", ""category2""],
        runs=100,
        rating=4.5,
        versions=[""1.0.0"", ""1.1.0""],
        last_updated=datetime.datetime.now(),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agent_details"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/agents/creator1/test-agent"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreAgentDetails.model_validate(
        response.json()
    )
    assert data.agent_name == ""Test Agent""
    assert data.creator == ""creator1""
    mock_db_call.assert_called_once_with(username=""creator1"", agent_name=""test-agent"")


def test_get_creators_defaults(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.CreatorsResponse(
        creators=[],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=0,
            total_items=0,
            total_pages=0,
            page_size=10,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_creators"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/creators"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.CreatorsResponse.model_validate(
        response.json()
    )
    assert data.pagination.total_pages == 0
    assert data.creators == []
    mock_db_call.assert_called_once_with(
        featured=False, search_query=None, sorted_by=None, page=1, page_size=20
    )


def test_get_creators_pagination(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.CreatorsResponse(
        creators=[
            backend.server.v2.store.model.Creator(
                name=f""Creator {i}"",
                username=f""creator{i}"",
                description=f""Creator {i} description"",
                avatar_url=f""avatar{i}.jpg"",
                num_agents=1,
                agent_rating=4.5,
                agent_runs=100,
                is_featured=False,
            )
            for i in range(5)
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=2,
            total_items=15,
            total_pages=3,
            page_size=5,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_creators"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/creators?page=2&page_size=5"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.CreatorsResponse.model_validate(
        response.json()
    )
    assert len(data.creators) == 5
    assert data.pagination.current_page == 2
    assert data.pagination.page_size == 5
    mock_db_call.assert_called_once_with(
        featured=False, search_query=None, sorted_by=None, page=2, page_size=5
    )


def test_get_creators_malformed_request(mocker: pytest_mock.MockFixture):
    # Test with invalid page number
    response = client.get(""/creators?page=-1"")
    assert response.status_code == 422

    # Test with invalid page size
    response = client.get(""/creators?page_size=0"")
    assert response.status_code == 422

    # Test with non-numeric values
    response = client.get(""/creators?page=abc&page_size=def"")
    assert response.status_code == 422

    # Verify no DB calls were made
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_creators"")
    mock_db_call.assert_not_called()


def test_get_creator_details(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.CreatorDetails(
        name=""Test User"",
        username=""creator1"",
        description=""Test creator description"",
        links=[""link1.com"", ""link2.com""],
        avatar_url=""avatar.jpg"",
        agent_rating=4.8,
        agent_runs=1000,
        top_categories=[""category1"", ""category2""],
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_creator_details"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/creator/creator1"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.CreatorDetails.model_validate(response.json())
    assert data.username == ""creator1""
    assert data.name == ""Test User""
    mock_db_call.assert_called_once_with(username=""creator1"")


def test_get_submissions_success(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreSubmissionsResponse(
        submissions=[
            backend.server.v2.store.model.StoreSubmission(
                name=""Test Agent"",
                description=""Test agent description"",
                image_urls=[""test.jpg""],
                date_submitted=datetime.datetime.now(),
                status=prisma.enums.SubmissionStatus.APPROVED,
                runs=50,
                rating=4.2,
                agent_id=""test-agent-id"",
                agent_version=1,
                sub_heading=""Test agent subheading"",
                slug=""test-agent"",
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_submissions"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/submissions"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreSubmissionsResponse.model_validate(
        response.json()
    )
    assert len(data.submissions) == 1
    assert data.submissions[0].name == ""Test Agent""
    assert data.pagination.current_page == 1
    mock_db_call.assert_called_once_with(user_id=""test-user-id"", page=1, page_size=20)


def test_get_submissions_pagination(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreSubmissionsResponse(
        submissions=[],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=2,
            total_items=10,
            total_pages=2,
            page_size=5,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_submissions"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/submissions?page=2&page_size=5"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreSubmissionsResponse.model_validate(
        response.json()
    )
    assert data.pagination.current_page == 2
    assert data.pagination.page_size == 5
    mock_db_call.assert_called_once_with(user_id=""test-user-id"", page=2, page_size=5)


def test_get_submissions_malformed_request(mocker: pytest_mock.MockFixture):
    # Test with invalid page number
    response = client.get(""/submissions?page=-1"")
    assert response.status_code == 422

    # Test with invalid page size
    response = client.get(""/submissions?page_size=0"")
    assert response.status_code == 422

    # Test with non-numeric values
    response = client.get(""/submissions?page=abc&page_size=def"")
    assert response.status_code == 422

    # Verify no DB calls were made
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_submissions"")
    mock_db_call.assert_not_called()
","import datetime
import autogpt_libs.auth.depends
import autogpt_libs.auth.middleware
import fastapi
import fastapi.testclient
import prisma.enums
import pytest_mock
import backend.server.v2.store.model
import backend.server.v2.store.routes

app = fastapi.FastAPI()
app.include_router(backend.server.v2.store.routes.router)

client = fastapi.testclient.TestClient(app)

def override_auth_middleware():
    """"""Override auth middleware for testing""""""
    return {""sub"": ""test-user-id""}

def override_get_user_id():
    """"""Override get_user_id for testing""""""
    return ""test-user-id""

app.dependency_overrides[autogpt_libs.auth.middleware.auth_middleware] = (
    override_auth_middleware
)

app.dependency_overrides[autogpt_libs.auth.depends.get_user_id] = override_get_user_id

def test_get_agents_defaults(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=0,
            total_items=0,
            total_pages=0,
            page_size=10,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/agents"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert data.pagination.total_pages == 0
    assert data.agents == []

    mock_db_call.assert_called_once_with(
        featured=False,
        creator=None,
        sorted_by=None,
        search_query=None,
        category=None,
        page=1,
        page_size=20,
    )

def test_get_agents_featured(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""featured-agent"",
                agent_name=""Featured Agent"",
                agent_image=""featured.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar1.jpg"",
                sub_heading=""Featured agent subheading"",
                description=""Featured agent description"",
                runs=100,
                rating=4.5,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/agents?featured=true"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 1
    assert data.agents[0].slug == ""featured-agent""

    mock_db_call.assert_called_once_with(
        featured=True,
        creator=None,
        sorted_by=None,
        search_query=None,
        category=None,
        page=1,
        page_size=20,
    )

def test_get_agents_by_creator(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""creator-agent"",
                agent_name=""Creator Agent"",
                agent_image=""agent.jpg"",
                creator=""specific-creator"",
                creator_avatar=""avatar.jpg"",
                sub_heading=""Creator agent subheading"",
                description=""Creator agent description"",
                runs=50,
                rating=4.0,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/agents?creator=specific-creator"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 1
    assert data.agents[0].creator == ""specific-creator""

    mock_db_call.assert_called_once_with(
        featured=False,
        creator=""specific-creator"",
        sorted_by=None,
        search_query=None,
        category=None,
        page=1,
        page_size=20,
    )

def test_get_agents_sorted(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""top-agent"",
                agent_name=""Top Agent"",
                agent_image=""top.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar1.jpg"",
                sub_heading=""Top agent subheading"",
                description=""Top agent description"",
                runs=1000,
                rating=5.0,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/agents?sorted_by=runs"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 1
    assert data.agents[0].runs == 1000

    mock_db_call.assert_called_once_with(
        featured=False,
        creator=None,
        sorted_by=""runs"",
        search_query=None,
        category=None,
        page=1,
        page_size=20,
    )

def test_get_agents_search(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""search-agent"",
                agent_name=""Search Agent"",
                agent_image=""search.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar1.jpg"",
                sub_heading=""Search agent subheading"",
                description=""Specific search term description"",
                runs=75,
                rating=4.2,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/agents?search_query=specific"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 1
    assert ""specific"" in data.agents[0].description.lower()

    mock_db_call.assert_called_once_with(
        featured=False,
        creator=None,
        sorted_by=None,
        search_query=""specific"",
        category=None,
        page=1,
        page_size=20,
    )

def test_get_agents_category(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""category-agent"",
                agent_name=""Category Agent"",
                agent_image=""category.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar1.jpg"",
                sub_heading=""Category agent subheading"",
                description=""Category agent description"",
                runs=60,
                rating=4.1,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/agents?category=test-category"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 1

    mock_db_call.assert_called_once_with(
        featured=False,
        creator=None,
        sorted_by=None,
        search_query=None,
        category=""test-category"",
        page=1,
        page_size=20,
    )

def test_get_agents_pagination(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=f""agent-{i}"",
                agent_name=f""Agent {i}"",
                agent_image=f""agent{i}.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar1.jpg"",
                sub_heading=f""Agent {i} subheading"",
                description=f""Agent {i} description"",
                runs=i * 10,
                rating=4.0,
            )
            for i in range(5)
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=2,
            total_items=15,
            total_pages=3,
            page_size=5,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/agents?page=2&page_size=5"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreAgentsResponse.model_validate(
        response.json()
    )
    assert len(data.agents) == 5
    assert data.pagination.current_page == 2
    assert data.pagination.page_size == 5

    mock_db_call.assert_called_once_with(
        featured=False,
        creator=None,
        sorted_by=None,
        search_query=None,
        category=None,
        page=2,
        page_size=5,
    )

def test_get_agents_malformed_request(mocker: pytest_mock.MockFixture):
    # Test with invalid page number
    response = client.get(""/agents?page=-1"")
    assert response.status_code == 422

    # Test with invalid page size
    response = client.get(""/agents?page_size=0"")
    assert response.status_code == 422

    # Test with non-numeric values
    response = client.get(""/agents?page=abc&page_size=def"")
    assert response.status_code == 422

    # Verify no DB calls were made
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agents"")
    mock_db_call.assert_not_called()

def test_get_agent_details(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreAgentDetails(
        store_listing_version_id=""test-version-id"",
        slug=""test-agent"",
        agent_name=""Test Agent"",
        agent_video=""video.mp4"",
        agent_image=[""image1.jpg"", ""image2.jpg""],
        creator=""creator1"",
        creator_avatar=""avatar1.jpg"",
        sub_heading=""Test agent subheading"",
        description=""Test agent description"",
        categories=[""category1"", ""category2""],
        runs=100,
        rating=4.5,
        versions=[""1.0.0"", ""1.1.0""],
        last_updated=datetime.datetime.now(),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_agent_details"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/agents/creator1/test-agent"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreAgentDetails.model_validate(
        response.json()
    )
    assert data.agent_name == ""Test Agent""
    assert data.creator == ""creator1""

    mock_db_call.assert_called_once_with(username=""creator1"", agent_name=""test-agent"")

def test_get_creators_defaults(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.CreatorsResponse(
        creators=[],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=0,
            total_items=0,
            total_pages=0,
            page_size=10,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_creators"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/creators"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.CreatorsResponse.model_validate(
        response.json()
    )
    assert data.pagination.total_pages == 0
    assert data.creators == []

    mock_db_call.assert_called_once_with(
        featured=False, search_query=None, sorted_by=None, page=1, page_size=20
    )

def test_get_creators_pagination(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.CreatorsResponse(
        creators=[
            backend.server.v2.store.model.Creator(
                name=f""Creator {i}"",
                username=f""creator{i}"",
                description=f""Creator {i} description"",
                avatar_url=f""avatar{i}.jpg"",
                num_agents=1,
                agent_rating=4.5,
                agent_runs=100,
                is_featured=False,
            )
            for i in range(5)
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=2,
            total_items=15,
            total_pages=3,
            page_size=5,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_creators"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/creators?page=2&page_size=5"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.CreatorsResponse.model_validate(
        response.json()
    )
    assert len(data.creators) == 5
    assert data.pagination.current_page == 2
    assert data.pagination.page_size == 5

    mock_db_call.assert_called_once_with(
        featured=False, search_query=None, sorted_by=None, page=2, page_size=5
    )

def test_get_creators_malformed_request(mocker: pytest_mock.MockFixture):
    # Test with invalid page number
    response = client.get(""/creators?page=-1"")
    assert response.status_code == 422

    # Test with invalid page size
    response = client.get(""/creators?page_size=0"")
    assert response.status_code == 422

    # Test with non-numeric values
    response = client.get(""/creators?page=abc&page_size=def"")
    assert response.status_code == 422

    # Verify no DB calls were made
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_creators"")
    mock_db_call.assert_not_called()

def test_get_creator_details(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.CreatorDetails(
        name=""Test User"",
        username=""creator1"",
        description=""Test creator description"",
        links=[""link1.com"", ""link2.com""],
        avatar_url=""avatar.jpg"",
        agent_rating=4.8,
        agent_runs=1000,
        top_categories=[""category1"", ""category2""],
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_creator_details"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/creator/creator1"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.CreatorDetails.model_validate(response.json())
    assert data.username == ""creator1""
    assert data.name == ""Test User""

    mock_db_call.assert_called_once_with(username=""creator1"")

def test_get_submissions_success(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreSubmissionsResponse(
        submissions=[
            backend.server.v2.store.model.StoreSubmission(
                name=""Test Agent"",
                description=""Test agent description"",
                image_urls=[""test.jpg""],
                date_submitted=datetime.datetime.now(),
                status=prisma.enums.SubmissionStatus.APPROVED,
                runs=50,
                rating=4.2,
                agent_id=""test-agent-id"",
                agent_version=1,
                sub_heading=""Test agent subheading"",
                slug=""test-agent"",
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=1,
            total_items=1,
            total_pages=1,
            page_size=20,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_submissions"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/submissions"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreSubmissionsResponse.model_validate(
        response.json()
    )
    assert len(data.submissions) == 1
    assert data.submissions[0].name == ""Test Agent""
    assert data.pagination.current_page == 1
    mock_db_call.assert_called_once_with(user_id=""test-user-id"", page=1, page_size=20)

def test_get_submissions_pagination(mocker: pytest_mock.MockFixture):
    mocked_value = backend.server.v2.store.model.StoreSubmissionsResponse(
        submissions=[],
        pagination=backend.server.v2.store.model.Pagination(
            current_page=2,
            total_items=10,
            total_pages=2,
            page_size=5,
        ),
    )
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_submissions"")
    mock_db_call.return_value = mocked_value

    response = client.get(""/submissions?page=2&page_size=5"")
    assert response.status_code == 200

    data = backend.server.v2.store.model.StoreSubmissionsResponse.model_validate(
        response.json()
    )
    assert data.pagination.current_page == 2
    assert data.pagination.page_size == 5
    mock_db_call.assert_called_once_with(user_id=""test-user-id"", page=2, page_size=5)

def test_get_submissions_malformed_request(mocker: pytest_mock.MockFixture):
    # Test with invalid page number
    response = client.get(""/submissions?page=-1"")
    assert response.status_code == 422

    # Test with invalid page size
    response = client.get(""/submissions?page_size=0"")
    assert response.status_code == 422

    # Test with non-numeric values
    response = client.get(""/submissions?page=abc&page_size=def"")
    assert response.status_code == 422

    # Verify no DB calls were made
    mock_db_call = mocker.patch(""backend.server.v2.store.db.get_store_submissions"")
    mock_db_call.assert_not_called()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
33,"import codecs

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class TextDecoderBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""A string containing escaped characters to be decoded"",
            placeholder='Your entire text block with \\n and \\"" escaped characters',
        )

    class Output(BlockSchema):
        decoded_text: str = SchemaField(
            description=""The decoded text with escape sequences processed""
        )

    def __init__(self):
        super().__init__(
            id=""2570e8fe-8447-43ed-84c7-70d657923231"",
            description=""Decodes a string containing escape sequences into actual text"",
            categories={BlockCategory.TEXT},
            input_schema=TextDecoderBlock.Input,
            output_schema=TextDecoderBlock.Output,
            test_input={""text"": """"""Hello\nWorld!\nThis is a \""quoted\"" string.""""""},
            test_output=[
                (
                    ""decoded_text"",
                    """"""Hello
World!
This is a ""quoted"" string."""""",
                )
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        decoded_text = codecs.decode(input_data.text, ""unicode_escape"")
        yield ""decoded_text"", decoded_text
","import codecs
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

class TextDecoderBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""A string containing escaped characters to be decoded"",
            placeholder='Your entire text block with \\n and \\"" escaped characters'
        )

    class Output(BlockSchema):
        decoded_text: str = SchemaField(
            description=""The decoded text with escape sequences processed""
        )

    def __init__(self):
        super().__init__(
            id=""2570e8fe-8447-43ed-84c7-70d657923231"",
            description=""Decodes a string containing escape sequences into actual text"",
            categories={BlockCategory.TEXT},
            input_schema=TextDecoderBlock.Input,
            output_schema=TextDecoderBlock.Output,
            test_input={""text"": """"""Hello\nWorld!\nThis is a \""quoted\"" string.""""""},
            test_output=[
                (
                    ""decoded_text"",
                    """"""Hello
World!
This is a ""quoted"" string.""""""
                )
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        decoded_text = codecs.decode(input_data.text, ""unicode_escape"")
        yield ""decoded_text"", decoded_text","{'final_score': 98.97, 'length_similarity': 99.85, 'token_similarity': 96.63, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
34,"from functools import wraps
from typing import Any, Callable, Concatenate, Coroutine, ParamSpec, TypeVar, cast

from backend.data.credit import get_user_credit_model
from backend.data.execution import (
    ExecutionResult,
    RedisExecutionEventBus,
    create_graph_execution,
    get_execution_results,
    get_incomplete_executions,
    get_latest_execution,
    update_execution_status,
    update_graph_execution_stats,
    update_node_execution_stats,
    upsert_execution_input,
    upsert_execution_output,
)
from backend.data.graph import get_graph, get_node
from backend.data.user import (
    get_user_integrations,
    get_user_metadata,
    update_user_integrations,
    update_user_metadata,
)
from backend.util.service import AppService, expose, register_pydantic_serializers
from backend.util.settings import Config

P = ParamSpec(""P"")
R = TypeVar(""R"")
config = Config()


class DatabaseManager(AppService):
    def __init__(self):
        super().__init__()
        self.use_db = True
        self.use_redis = True
        self.event_queue = RedisExecutionEventBus()

    @classmethod
    def get_port(cls) -> int:
        return config.database_api_port

    @expose
    def send_execution_update(self, execution_result: ExecutionResult):
        self.event_queue.publish(execution_result)

    @staticmethod
    def exposed_run_and_wait(
        f: Callable[P, Coroutine[None, None, R]]
    ) -> Callable[Concatenate[object, P], R]:
        @expose
        @wraps(f)
        def wrapper(self, *args: P.args, **kwargs: P.kwargs) -> R:
            coroutine = f(*args, **kwargs)
            res = self.run_and_wait(coroutine)
            return res

        # Register serializers for annotations on bare function
        register_pydantic_serializers(f)

        return wrapper

    # Executions
    create_graph_execution = exposed_run_and_wait(create_graph_execution)
    get_execution_results = exposed_run_and_wait(get_execution_results)
    get_incomplete_executions = exposed_run_and_wait(get_incomplete_executions)
    get_latest_execution = exposed_run_and_wait(get_latest_execution)
    update_execution_status = exposed_run_and_wait(update_execution_status)
    update_graph_execution_stats = exposed_run_and_wait(update_graph_execution_stats)
    update_node_execution_stats = exposed_run_and_wait(update_node_execution_stats)
    upsert_execution_input = exposed_run_and_wait(upsert_execution_input)
    upsert_execution_output = exposed_run_and_wait(upsert_execution_output)

    # Graphs
    get_node = exposed_run_and_wait(get_node)
    get_graph = exposed_run_and_wait(get_graph)

    # Credits
    user_credit_model = get_user_credit_model()
    get_or_refill_credit = cast(
        Callable[[Any, str], int],
        exposed_run_and_wait(user_credit_model.get_or_refill_credit),
    )
    spend_credits = cast(
        Callable[[Any, str, int, str, dict[str, str], float, float], int],
        exposed_run_and_wait(user_credit_model.spend_credits),
    )

    # User + User Metadata + User Integrations
    get_user_metadata = exposed_run_and_wait(get_user_metadata)
    update_user_metadata = exposed_run_and_wait(update_user_metadata)
    get_user_integrations = exposed_run_and_wait(get_user_integrations)
    update_user_integrations = exposed_run_and_wait(update_user_integrations)
","from functools import wraps
from typing import Any, Callable, Concatenate, Coroutine, ParamSpec, TypeVar, cast

from backend.data.credit import get_user_credit_model
from backend.data.execution import (
    ExecutionResult,
    RedisExecutionEventBus,
    create_graph_execution,
    get_execution_results,
    get_incomplete_executions,
    get_latest_execution,
    update_execution_status,
    update_graph_execution_stats,
    update_node_execution_stats,
    upsert_execution_input,
    upsert_execution_output,
)

from backend.data.graph import get_graph, get_node
from backend.data.user import (
    get_user_integrations,
    get_user_metadata,
    update_user_integrations,
    update_user_metadata,
)

from backend.util.service import AppService, expose, register_pydantic_serializers
from backend.util.settings import Config

P = ParamSpec(""P"")
R = TypeVar(""R"")

config = Config()

class DatabaseManager(AppService):
    def __init__(self):
        super().__init__()
        self.use_db = True
        self.use_redis = True
        self.event_queue = RedisExecutionEventBus()

    @classmethod
    def get_port(cls) -> int:
        return config.database_api_port

    @expose
    def send_execution_update(self, execution_result: ExecutionResult):
        self.event_queue.publish(execution_result)

    @staticmethod
    def exposed_run_and_wait(
        f: Callable[P, Coroutine[None, None, R]]
    ) -> Callable[Concatenate[object, P], R]:
        @expose
        @wraps(f)
        def wrapper(self, *args: P.args, **kwargs: P.kwargs) -> R:
            coroutine = f(*args, **kwargs)
            res = self.run_and_wait(coroutine)
            return res

        # Register serializers for annotations on bare function
        register_pydantic_serializers(f)

        return wrapper

    # Executions
    create_graph_execution = exposed_run_and_wait(create_graph_execution)
    get_execution_results = exposed_run_and_wait(get_execution_results)
    get_incomplete_executions = exposed_run_and_wait(get_incomplete_executions)
    get_latest_execution = exposed_run_and_wait(get_latest_execution)
    update_execution_status = exposed_run_and_wait(update_execution_status)
    update_graph_execution_stats = exposed_run_and_wait(update_graph_execution_stats)
    update_node_execution_stats = exposed_run_and_wait(update_node_execution_stats)
    upsert_execution_input = exposed_run_and_wait(upsert_execution_input)
    upsert_execution_output = exposed_run_and_wait(upsert_execution_output)

    # Graphs
    get_node = exposed_run_and_wait(get_node)
    get_graph = exposed_run_and_wait(get_graph)

    # Credits
    user_credit_model = get_user_credit_model()
    get_or_refill_credit = cast(
        Callable[[Any, str], int],
        exposed_run_and_wait(user_credit_model.get_or_refill_credit),
    )
    spend_credits = cast(
        Callable[[Any, str, int, str, dict[str, str], float, float], int],
        exposed_run_and_wait(user_credit_model.spend_credits),
    )

    # User + User Metadata + User Integrations
    get_user_metadata = exposed_run_and_wait(get_user_metadata)
    update_user_metadata = exposed_run_and_wait(update_user_metadata)
    get_user_integrations = exposed_run_and_wait(get_user_integrations)
    update_user_integrations = exposed_run_and_wait(update_user_integrations)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
35,"from backend.util.type import convert


def test_type_conversion():
    assert convert(5.5, int) == 5
    assert convert(""5.5"", int) == 5
    assert convert([1, 2, 3], int) == 3

    assert convert(""5.5"", float) == 5.5
    assert convert(5, float) == 5.0

    assert convert(""True"", bool) is True
    assert convert(""False"", bool) is False

    assert convert(5, str) == ""5""
    assert convert({""a"": 1, ""b"": 2}, str) == '{""a"": 1, ""b"": 2}'
    assert convert([1, 2, 3], str) == ""[1, 2, 3]""

    assert convert(""5"", list) == [""5""]
    assert convert((1, 2, 3), list) == [1, 2, 3]
    assert convert({1, 2, 3}, list) == [1, 2, 3]

    assert convert(""5"", dict) == {""value"": 5}
    assert convert('{""a"": 1, ""b"": 2}', dict) == {""a"": 1, ""b"": 2}
    assert convert([1, 2, 3], dict) == {0: 1, 1: 2, 2: 3}
    assert convert((1, 2, 3), dict) == {0: 1, 1: 2, 2: 3}

    from typing import List

    assert convert(""5"", List[int]) == [5]
    assert convert(""[5,4,2]"", List[int]) == [5, 4, 2]
    assert convert([5, 4, 2], List[str]) == [""5"", ""4"", ""2""]
","from backend.util.type import convert

def test_type_conversion():
    assert convert(5.5, int) == 5
    assert convert(""5.5"", int) == 5
    assert convert([1, 2, 3], int) == 3
    assert convert(""5.5"", float) == 5.5
    assert convert(5, float) == 5.0
    assert convert(""True"", bool) is True
    assert convert(""False"", bool) is False
    assert convert(5, str) == ""5""
    assert convert({""a"": 1, ""b"": 2}, str) == '{""a"": 1, ""b"": 2}'
    assert convert([1, 2, 3], str) == ""[1, 2, 3]""
    assert convert(""5"", list) == [""5""]
    assert convert((1, 2, 3), list) == [1, 2, 3]
    assert convert({1, 2, 3}, list) == [1, 2, 3]
    assert convert(""5"", dict) == {""value"": 5}
    assert convert('{""a"": 1, ""b"": 2}', dict) == {""a"": 1, ""b"": 2}
    assert convert([1, 2, 3], dict) == {0: 1, 1: 2, 2: 3}
    assert convert((1, 2, 3), dict) == {0: 1, 1: 2, 2: 3}
    from typing import List
    assert convert(""5"", List[int]) == [5]
    assert convert(""[5,4,2]"", List[int]) == [5, 4, 2]
    assert convert([5, 4, 2], List[str]) == [""5"", ""4"", ""2""]","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
36,"from math import log2


def binary_count_trailing_zeros(a: int) -> int:
    """"""
    Take in 1 integer, return a number that is
    the number of trailing zeros in binary representation of that number.

    >>> binary_count_trailing_zeros(25)
    0
    >>> binary_count_trailing_zeros(36)
    2
    >>> binary_count_trailing_zeros(16)
    4
    >>> binary_count_trailing_zeros(58)
    1
    >>> binary_count_trailing_zeros(4294967296)
    32
    >>> binary_count_trailing_zeros(0)
    0
    >>> binary_count_trailing_zeros(-10)
    Traceback (most recent call last):
        ...
    ValueError: Input value must be a positive integer
    >>> binary_count_trailing_zeros(0.8)
    Traceback (most recent call last):
        ...
    TypeError: Input value must be a 'int' type
    >>> binary_count_trailing_zeros(""0"")
    Traceback (most recent call last):
        ...
    TypeError: '<' not supported between instances of 'str' and 'int'
    """"""
    if a < 0:
        raise ValueError(""Input value must be a positive integer"")
    elif isinstance(a, float):
        raise TypeError(""Input value must be a 'int' type"")
    return 0 if (a == 0) else int(log2(a & -a))


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","from math import log2

def binary_count_trailing_zeros(a: int) -> int:
    """"""
    Take in 1 integer, return a number that is
    the number of trailing zeros in binary representation of that number.

    >>> binary_count_trailing_zeros(25)
    0
    >>> binary_count_trailing_zeros(36)
    2
    >>> binary_count_trailing_zeros(16)
    4
    >>> binary_count_trailing_zeros(58)
    1
    >>> binary_count_trailing_zeros(4294967296)
    32
    >>> binary_count_trailing_zeros(0)
    0
    >>> binary_count_trailing_zeros(-10)
    Traceback (most recent call last):
        ...
    ValueError: Input value must be a positive integer
    >>> binary_count_trailing_zeros(0.8)
    Traceback (most recent call last):
        ...
    TypeError: Input value must be a 'int' type
    >>> binary_count_trailing_zeros(""0"")
    Traceback (most recent call last):
        ...
    TypeError: '<' not supported between instances of 'str' and 'int'
    """"""
    if a < 0:
        raise ValueError(""Input value must be a positive integer"")
    elif isinstance(a, float):
        raise TypeError(""Input value must be a 'int' type"")
    return 0 if (a == 0) else int(log2(a & -a))

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
37,"from datetime import datetime, timedelta

from backend.blocks.hubspot._auth import (
    HubSpotCredentials,
    HubSpotCredentialsField,
    HubSpotCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class HubSpotEngagementBlock(Block):
    class Input(BlockSchema):
        credentials: HubSpotCredentialsInput = HubSpotCredentialsField()
        operation: str = SchemaField(
            description=""Operation to perform (send_email, track_engagement)"",
            default=""send_email"",
        )
        email_data: dict = SchemaField(
            description=""Email data including recipient, subject, content"",
            default={},
        )
        contact_id: str = SchemaField(
            description=""Contact ID for engagement tracking"", default=""""
        )
        timeframe_days: int = SchemaField(
            description=""Number of days to look back for engagement"",
            default=30,
            optional=True,
        )

    class Output(BlockSchema):
        result: dict = SchemaField(description=""Operation result"")
        status: str = SchemaField(description=""Operation status"")

    def __init__(self):
        super().__init__(
            id=""c6524385-7d87-49d6-a470-248bd29ca765"",
            description=""Manages HubSpot engagements - sends emails and tracks engagement metrics"",
            categories={BlockCategory.CRM, BlockCategory.COMMUNICATION},
            input_schema=HubSpotEngagementBlock.Input,
            output_schema=HubSpotEngagementBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: HubSpotCredentials, **kwargs
    ) -> BlockOutput:
        base_url = ""https://api.hubapi.com""
        headers = {
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
        }

        if input_data.operation == ""send_email"":
            # Using the email send API
            email_url = f""{base_url}/crm/v3/objects/emails""
            email_data = {
                ""properties"": {
                    ""hs_timestamp"": datetime.now().isoformat(),
                    ""hubspot_owner_id"": ""1"",  # This should be configurable
                    ""hs_email_direction"": ""OUTBOUND"",
                    ""hs_email_status"": ""SEND"",
                    ""hs_email_subject"": input_data.email_data.get(""subject""),
                    ""hs_email_text"": input_data.email_data.get(""content""),
                    ""hs_email_to_email"": input_data.email_data.get(""recipient""),
                }
            }

            response = requests.post(email_url, headers=headers, json=email_data)
            result = response.json()
            yield ""result"", result
            yield ""status"", ""email_sent""

        elif input_data.operation == ""track_engagement"":
            # Get engagement events for the contact
            from_date = datetime.now() - timedelta(days=input_data.timeframe_days)
            engagement_url = (
                f""{base_url}/crm/v3/objects/contacts/{input_data.contact_id}/engagement""
            )

            params = {""limit"": 100, ""after"": from_date.isoformat()}

            response = requests.get(engagement_url, headers=headers, params=params)
            engagements = response.json()

            # Process engagement metrics
            metrics = {
                ""email_opens"": 0,
                ""email_clicks"": 0,
                ""email_replies"": 0,
                ""last_engagement"": None,
                ""engagement_score"": 0,
            }

            for engagement in engagements.get(""results"", []):
                eng_type = engagement.get(""properties"", {}).get(""hs_engagement_type"")
                if eng_type == ""EMAIL"":
                    metrics[""email_opens""] += 1
                elif eng_type == ""EMAIL_CLICK"":
                    metrics[""email_clicks""] += 1
                elif eng_type == ""EMAIL_REPLY"":
                    metrics[""email_replies""] += 1

                # Update last engagement time
                eng_time = engagement.get(""properties"", {}).get(""hs_timestamp"")
                if eng_time and (
                    not metrics[""last_engagement""]
                    or eng_time > metrics[""last_engagement""]
                ):
                    metrics[""last_engagement""] = eng_time

            # Calculate simple engagement score
            metrics[""engagement_score""] = (
                metrics[""email_opens""]
                + metrics[""email_clicks""] * 2
                + metrics[""email_replies""] * 3
            )

            yield ""result"", metrics
            yield ""status"", ""engagement_tracked""
","from datetime import datetime, timedelta
from backend.blocks.hubspot._auth import (
    HubSpotCredentials,
    HubSpotCredentialsField,
    HubSpotCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class HubSpotEngagementBlock(Block):
    class Input(BlockSchema):
        credentials: HubSpotCredentialsInput = HubSpotCredentialsField()
        operation: str = SchemaField(
            description=""Operation to perform (send_email, track_engagement)"",
            default=""send_email"",
        )
        email_data: dict = SchemaField(
            description=""Email data including recipient, subject, content"",
            default={},
        )
        contact_id: str = SchemaField(
            description=""Contact ID for engagement tracking"", default=""""
        )
        timeframe_days: int = SchemaField(
            description=""Number of days to look back for engagement"",
            default=30,
            optional=True,
        )

    class Output(BlockSchema):
        result: dict = SchemaField(description=""Operation result"")
        status: str = SchemaField(description=""Operation status"")

    def __init__(self):
        super().__init__(
            id=""c6524385-7d87-49d6-a470-248bd29ca765"",
            description=""Manages HubSpot engagements - sends emails and tracks engagement metrics"",
            categories={BlockCategory.CRM, BlockCategory.COMMUNICATION},
            input_schema=HubSpotEngagementBlock.Input,
            output_schema=HubSpotEngagementBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: HubSpotCredentials, **kwargs
    ) -> BlockOutput:
        base_url = ""https://api.hubapi.com""
        headers = {
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
        }

        if input_data.operation == ""send_email"":
            # Using the email send API
            email_url = f""{base_url}/crm/v3/objects/emails""
            email_data = {
                ""properties"": {
                    ""hs_timestamp"": datetime.now().isoformat(),
                    ""hubspot_owner_id"": ""1"",  # This should be configurable
                    ""hs_email_direction"": ""OUTBOUND"",
                    ""hs_email_status"": ""SEND"",
                    ""hs_email_subject"": input_data.email_data.get(""subject""),
                    ""hs_email_text"": input_data.email_data.get(""content""),
                    ""hs_email_to_email"": input_data.email_data.get(""recipient""),
                }
            }

            response = requests.post(email_url, headers=headers, json=email_data)
            result = response.json()
            yield ""result"", result
            yield ""status"", ""email_sent""

        elif input_data.operation == ""track_engagement"":
            # Get engagement events for the contact
            from_date = datetime.now() - timedelta(days=input_data.timeframe_days)
            engagement_url = (
                f""{base_url}/crm/v3/objects/contacts/{input_data.contact_id}/engagement""
            )

            params = {""limit"": 100, ""after"": from_date.isoformat()}

            response = requests.get(engagement_url, headers=headers, params=params)
            engagements = response.json()

            # Process engagement metrics
            metrics = {
                ""email_opens"": 0,
                ""email_clicks"": 0,
                ""email_replies"": 0,
                ""last_engagement"": None,
                ""engagement_score"": 0,
            }

            for engagement in engagements.get(""results"", []):
                eng_type = engagement.get(""properties"", {}).get(""hs_engagement_type"")
                if eng_type == ""EMAIL"":
                    metrics[""email_opens""] += 1
                elif eng_type == ""EMAIL_CLICK"":
                    metrics[""email_clicks""] += 1
                elif eng_type == ""EMAIL_REPLY"":
                    metrics[""email_replies""] += 1

                # Update last engagement time
                eng_time = engagement.get(""properties"", {}).get(""hs_timestamp"")
                if eng_time and (
                    not metrics[""last_engagement""]
                    or eng_time > metrics[""last_engagement""]
                ):
                    metrics[""last_engagement""] = eng_time

            # Calculate simple engagement score
            metrics[""engagement_score""] = (
                metrics[""email_opens""]
                + metrics[""email_clicks""] * 2
                + metrics[""email_replies""] * 3
            )

            yield ""result"", metrics
            yield ""status"", ""engagement_tracked""","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
38,"import asyncio
import logging
import uuid
from collections import defaultdict
from datetime import datetime, timezone
from typing import Any, Literal, Optional, Type

import prisma
from prisma.models import AgentGraph, AgentGraphExecution, AgentNode, AgentNodeLink
from prisma.types import AgentGraphWhereInput
from pydantic.fields import computed_field

from backend.blocks.agent import AgentExecutorBlock
from backend.blocks.basic import AgentInputBlock, AgentOutputBlock
from backend.util import json

from .block import BlockInput, BlockType, get_block, get_blocks
from .db import BaseDbModel, transaction
from .execution import ExecutionStatus
from .includes import AGENT_GRAPH_INCLUDE, AGENT_NODE_INCLUDE
from .integrations import Webhook

logger = logging.getLogger(__name__)


class Link(BaseDbModel):
    source_id: str
    sink_id: str
    source_name: str
    sink_name: str
    is_static: bool = False

    @staticmethod
    def from_db(link: AgentNodeLink):
        return Link(
            id=link.id,
            source_name=link.sourceName,
            source_id=link.agentNodeSourceId,
            sink_name=link.sinkName,
            sink_id=link.agentNodeSinkId,
            is_static=link.isStatic,
        )

    def __hash__(self):
        return hash((self.source_id, self.sink_id, self.source_name, self.sink_name))


class Node(BaseDbModel):
    block_id: str
    input_default: BlockInput = {}  # dict[input_name, default_value]
    metadata: dict[str, Any] = {}
    input_links: list[Link] = []
    output_links: list[Link] = []

    webhook_id: Optional[str] = None


class NodeModel(Node):
    graph_id: str
    graph_version: int

    webhook: Optional[Webhook] = None

    @staticmethod
    def from_db(node: AgentNode):
        if not node.AgentBlock:
            raise ValueError(f""Invalid node {node.id}, invalid AgentBlock."")
        obj = NodeModel(
            id=node.id,
            block_id=node.AgentBlock.id,
            input_default=json.loads(node.constantInput, target_type=dict[str, Any]),
            metadata=json.loads(node.metadata, target_type=dict[str, Any]),
            graph_id=node.agentGraphId,
            graph_version=node.agentGraphVersion,
            webhook_id=node.webhookId,
            webhook=Webhook.from_db(node.Webhook) if node.Webhook else None,
        )
        obj.input_links = [Link.from_db(link) for link in node.Input or []]
        obj.output_links = [Link.from_db(link) for link in node.Output or []]
        return obj

    def is_triggered_by_event_type(self, event_type: str) -> bool:
        if not (block := get_block(self.block_id)):
            raise ValueError(f""Block #{self.block_id} not found for node #{self.id}"")
        if not block.webhook_config:
            raise TypeError(""This method can't be used on non-webhook blocks"")
        if not block.webhook_config.event_filter_input:
            return True
        event_filter = self.input_default.get(block.webhook_config.event_filter_input)
        if not event_filter:
            raise ValueError(f""Event filter is not configured on node #{self.id}"")
        return event_type in [
            block.webhook_config.event_format.format(event=k)
            for k in event_filter
            if event_filter[k] is True
        ]


# Fix 2-way reference Node <-> Webhook
Webhook.model_rebuild()


class GraphExecution(BaseDbModel):
    execution_id: str
    started_at: datetime
    ended_at: datetime
    duration: float
    total_run_time: float
    status: ExecutionStatus
    graph_id: str
    graph_version: int

    @staticmethod
    def from_db(execution: AgentGraphExecution):
        now = datetime.now(timezone.utc)
        start_time = execution.startedAt or execution.createdAt
        end_time = execution.updatedAt or now
        duration = (end_time - start_time).total_seconds()
        total_run_time = duration

        try:
            stats = json.loads(execution.stats or ""{}"", target_type=dict[str, Any])
        except ValueError:
            stats = {}

        duration = stats.get(""walltime"", duration)
        total_run_time = stats.get(""nodes_walltime"", total_run_time)

        return GraphExecution(
            id=execution.id,
            execution_id=execution.id,
            started_at=start_time,
            ended_at=end_time,
            duration=duration,
            total_run_time=total_run_time,
            status=ExecutionStatus(execution.executionStatus),
            graph_id=execution.agentGraphId,
            graph_version=execution.agentGraphVersion,
        )


class Graph(BaseDbModel):
    version: int = 1
    is_active: bool = True
    is_template: bool = False
    name: str
    description: str
    nodes: list[Node] = []
    links: list[Link] = []

    @computed_field
    @property
    def input_schema(self) -> dict[str, Any]:
        return self._generate_schema(
            AgentInputBlock.Input,
            [
                node.input_default
                for node in self.nodes
                if (b := get_block(node.block_id))
                and b.block_type == BlockType.INPUT
                and ""name"" in node.input_default
            ],
        )

    @computed_field
    @property
    def output_schema(self) -> dict[str, Any]:
        return self._generate_schema(
            AgentOutputBlock.Input,
            [
                node.input_default
                for node in self.nodes
                if (b := get_block(node.block_id))
                and b.block_type == BlockType.OUTPUT
                and ""name"" in node.input_default
            ],
        )

    @staticmethod
    def _generate_schema(
        type_class: Type[AgentInputBlock.Input] | Type[AgentOutputBlock.Input],
        data: list[dict],
    ) -> dict[str, Any]:
        props = []
        for p in data:
            try:
                props.append(type_class(**p))
            except Exception as e:
                logger.warning(f""Invalid {type_class}: {p}, {e}"")

        return {
            ""type"": ""object"",
            ""properties"": {
                p.name: {
                    ""secret"": p.secret,
                    ""advanced"": p.advanced,
                    ""title"": p.title or p.name,
                    **({""description"": p.description} if p.description else {}),
                    **({""default"": p.value} if p.value is not None else {}),
                }
                for p in props
            },
            ""required"": [p.name for p in props if p.value is None],
        }


class GraphModel(Graph):
    user_id: str
    nodes: list[NodeModel] = []  # type: ignore

    @property
    def starting_nodes(self) -> list[Node]:
        outbound_nodes = {link.sink_id for link in self.links}
        input_nodes = {
            v.id
            for v in self.nodes
            if (b := get_block(v.block_id)) and b.block_type == BlockType.INPUT
        }
        return [
            node
            for node in self.nodes
            if node.id not in outbound_nodes or node.id in input_nodes
        ]

    def reassign_ids(self, user_id: str, reassign_graph_id: bool = False):
        """"""
        Reassigns all IDs in the graph to new UUIDs.
        This method can be used before storing a new graph to the database.
        """"""

        # Reassign Graph ID
        id_map = {node.id: str(uuid.uuid4()) for node in self.nodes}
        if reassign_graph_id:
            self.id = str(uuid.uuid4())

        # Reassign Node IDs
        for node in self.nodes:
            node.id = id_map[node.id]

        # Reassign Link IDs
        for link in self.links:
            link.source_id = id_map[link.source_id]
            link.sink_id = id_map[link.sink_id]

        # Reassign User IDs for agent blocks
        for node in self.nodes:
            if node.block_id != AgentExecutorBlock().id:
                continue
            node.input_default[""user_id""] = user_id
            node.input_default.setdefault(""data"", {})

        self.validate_graph()

    def validate_graph(self, for_run: bool = False):
        def sanitize(name):
            return name.split(""_#_"")[0].split(""_@_"")[0].split(""_$_"")[0]

        input_links = defaultdict(list)
        for link in self.links:
            input_links[link.sink_id].append(link)

        # Nodes: required fields are filled or connected and dependencies are satisfied
        for node in self.nodes:
            block = get_block(node.block_id)
            if block is None:
                raise ValueError(f""Invalid block {node.block_id} for node #{node.id}"")

            provided_inputs = set(
                [sanitize(name) for name in node.input_default]
                + [sanitize(link.sink_name) for link in input_links.get(node.id, [])]
            )
            for name in block.input_schema.get_required_fields():
                if (
                    name not in provided_inputs
                    and not (
                        name == ""payload""
                        and block.block_type
                        in (BlockType.WEBHOOK, BlockType.WEBHOOK_MANUAL)
                    )
                    and (
                        for_run  # Skip input completion validation, unless when executing.
                        or block.block_type == BlockType.INPUT
                        or block.block_type == BlockType.OUTPUT
                        or block.block_type == BlockType.AGENT
                    )
                ):
                    raise ValueError(
                        f""Node {block.name} #{node.id} required input missing: `{name}`""
                    )

            # Get input schema properties and check dependencies
            input_schema = block.input_schema.model_fields
            required_fields = block.input_schema.get_required_fields()

            def has_value(name):
                return (
                    node is not None
                    and name in node.input_default
                    and node.input_default[name] is not None
                    and str(node.input_default[name]).strip() != """"
                ) or (name in input_schema and input_schema[name].default is not None)

            # Validate dependencies between fields
            for field_name, field_info in input_schema.items():
                # Apply input dependency validation only on run & field with depends_on
                json_schema_extra = field_info.json_schema_extra or {}
                dependencies = json_schema_extra.get(""depends_on"", [])
                if not for_run or not dependencies:
                    continue

                # Check if dependent field has value in input_default
                field_has_value = has_value(field_name)
                field_is_required = field_name in required_fields

                # Check for missing dependencies when dependent field is present
                missing_deps = [dep for dep in dependencies if not has_value(dep)]
                if missing_deps and (field_has_value or field_is_required):
                    raise ValueError(
                        f""Node {block.name} #{node.id}: Field `{field_name}` requires [{', '.join(missing_deps)}] to be set""
                    )

        node_map = {v.id: v for v in self.nodes}

        def is_static_output_block(nid: str) -> bool:
            bid = node_map[nid].block_id
            b = get_block(bid)
            return b.static_output if b else False

        # Links: links are connected and the connected pin data type are compatible.
        for link in self.links:
            source = (link.source_id, link.source_name)
            sink = (link.sink_id, link.sink_name)
            suffix = f""Link {source} <-> {sink}""

            for i, (node_id, name) in enumerate([source, sink]):
                node = node_map.get(node_id)
                if not node:
                    raise ValueError(
                        f""{suffix}, {node_id} is invalid node id, available nodes: {node_map.keys()}""
                    )

                block = get_block(node.block_id)
                if not block:
                    blocks = {v().id: v().name for v in get_blocks().values()}
                    raise ValueError(
                        f""{suffix}, {node.block_id} is invalid block id, available blocks: {blocks}""
                    )

                sanitized_name = sanitize(name)
                vals = node.input_default
                if i == 0:
                    fields = (
                        block.output_schema.get_fields()
                        if block.block_type != BlockType.AGENT
                        else vals.get(""output_schema"", {}).get(""properties"", {}).keys()
                    )
                else:
                    fields = (
                        block.input_schema.get_fields()
                        if block.block_type != BlockType.AGENT
                        else vals.get(""input_schema"", {}).get(""properties"", {}).keys()
                    )
                if sanitized_name not in fields:
                    fields_msg = f""Allowed fields: {fields}""
                    raise ValueError(f""{suffix}, `{name}` invalid, {fields_msg}"")

            if is_static_output_block(link.source_id):
                link.is_static = True  # Each value block output should be static.

    @staticmethod
    def from_db(graph: AgentGraph, for_export: bool = False):
        return GraphModel(
            id=graph.id,
            user_id=graph.userId,
            version=graph.version,
            is_active=graph.isActive,
            is_template=graph.isTemplate,
            name=graph.name or """",
            description=graph.description or """",
            nodes=[
                NodeModel.from_db(GraphModel._process_node(node, for_export))
                for node in graph.AgentNodes or []
            ],
            links=list(
                {
                    Link.from_db(link)
                    for node in graph.AgentNodes or []
                    for link in (node.Input or []) + (node.Output or [])
                }
            ),
        )

    @staticmethod
    def _process_node(node: AgentNode, for_export: bool) -> AgentNode:
        if for_export:
            # Remove credentials from node input
            if node.constantInput:
                constant_input = json.loads(
                    node.constantInput, target_type=dict[str, Any]
                )
                constant_input = GraphModel._hide_node_input_credentials(constant_input)
                node.constantInput = json.dumps(constant_input)

            # Remove webhook info
            node.webhookId = None
            node.Webhook = None

        return node

    @staticmethod
    def _hide_node_input_credentials(input_data: dict[str, Any]) -> dict[str, Any]:
        sensitive_keys = [""credentials"", ""api_key"", ""password"", ""token"", ""secret""]
        result = {}
        for key, value in input_data.items():
            if isinstance(value, dict):
                result[key] = GraphModel._hide_node_input_credentials(value)
            elif isinstance(value, str) and any(
                sensitive_key in key.lower() for sensitive_key in sensitive_keys
            ):
                # Skip this key-value pair in the result
                continue
            else:
                result[key] = value
        return result


# --------------------- CRUD functions --------------------- #


async def get_node(node_id: str) -> NodeModel:
    node = await AgentNode.prisma().find_unique_or_raise(
        where={""id"": node_id},
        include=AGENT_NODE_INCLUDE,
    )
    return NodeModel.from_db(node)


async def set_node_webhook(node_id: str, webhook_id: str | None) -> NodeModel:
    node = await AgentNode.prisma().update(
        where={""id"": node_id},
        data=(
            {""Webhook"": {""connect"": {""id"": webhook_id}}}
            if webhook_id
            else {""Webhook"": {""disconnect"": True}}
        ),
        include=AGENT_NODE_INCLUDE,
    )
    if not node:
        raise ValueError(f""Node #{node_id} not found"")
    return NodeModel.from_db(node)


async def get_graphs(
    user_id: str,
    filter_by: Literal[""active"", ""template""] | None = ""active"",
) -> list[GraphModel]:
    """"""
    Retrieves graph metadata objects.
    Default behaviour is to get all currently active graphs.

    Args:
        filter_by: An optional filter to either select templates or active graphs.
        user_id: The ID of the user that owns the graph.

    Returns:
        list[GraphModel]: A list of objects representing the retrieved graphs.
    """"""
    where_clause: AgentGraphWhereInput = {""userId"": user_id}

    if filter_by == ""active"":
        where_clause[""isActive""] = True
    elif filter_by == ""template"":
        where_clause[""isTemplate""] = True

    graphs = await AgentGraph.prisma().find_many(
        where=where_clause,
        distinct=[""id""],
        order={""version"": ""desc""},
        include=AGENT_GRAPH_INCLUDE,
    )

    graph_models = []
    for graph in graphs:
        try:
            graph_models.append(GraphModel.from_db(graph))
        except Exception as e:
            logger.error(f""Error processing graph {graph.id}: {e}"")
            continue

    return graph_models


async def get_executions(user_id: str) -> list[GraphExecution]:
    executions = await AgentGraphExecution.prisma().find_many(
        where={""userId"": user_id},
        order={""createdAt"": ""desc""},
    )
    return [GraphExecution.from_db(execution) for execution in executions]


async def get_execution(user_id: str, execution_id: str) -> GraphExecution | None:
    execution = await AgentGraphExecution.prisma().find_first(
        where={""id"": execution_id, ""userId"": user_id}
    )
    return GraphExecution.from_db(execution) if execution else None


async def get_graph(
    graph_id: str,
    version: int | None = None,
    template: bool = False,
    user_id: str | None = None,
    for_export: bool = False,
) -> GraphModel | None:
    """"""
    Retrieves a graph from the DB.
    Defaults to the version with `is_active` if `version` is not passed,
    or the latest version with `is_template` if `template=True`.

    Returns `None` if the record is not found.
    """"""
    where_clause: AgentGraphWhereInput = {
        ""id"": graph_id,
    }
    if version is not None:
        where_clause[""version""] = version
    elif not template:
        where_clause[""isActive""] = True

    # TODO: Fix hack workaround to get adding store agents to work
    if user_id is not None and not template:
        where_clause[""userId""] = user_id

    graph = await AgentGraph.prisma().find_first(
        where=where_clause,
        include=AGENT_GRAPH_INCLUDE,
        order={""version"": ""desc""},
    )
    return GraphModel.from_db(graph, for_export) if graph else None


async def set_graph_active_version(graph_id: str, version: int, user_id: str) -> None:
    # Activate the requested version if it exists and is owned by the user.
    updated_count = await AgentGraph.prisma().update_many(
        data={""isActive"": True},
        where={
            ""id"": graph_id,
            ""version"": version,
            ""userId"": user_id,
        },
    )
    if updated_count == 0:
        raise Exception(f""Graph #{graph_id} v{version} not found or not owned by user"")

    # Deactivate all other versions.
    await AgentGraph.prisma().update_many(
        data={""isActive"": False},
        where={
            ""id"": graph_id,
            ""version"": {""not"": version},
            ""userId"": user_id,
            ""isActive"": True,
        },
    )


async def get_graph_all_versions(graph_id: str, user_id: str) -> list[GraphModel]:
    graph_versions = await AgentGraph.prisma().find_many(
        where={""id"": graph_id, ""userId"": user_id},
        order={""version"": ""desc""},
        include=AGENT_GRAPH_INCLUDE,
    )

    if not graph_versions:
        return []

    return [GraphModel.from_db(graph) for graph in graph_versions]


async def delete_graph(graph_id: str, user_id: str) -> int:
    entries_count = await AgentGraph.prisma().delete_many(
        where={""id"": graph_id, ""userId"": user_id}
    )
    if entries_count:
        logger.info(f""Deleted {entries_count} graph entries for Graph #{graph_id}"")
    return entries_count


async def create_graph(graph: Graph, user_id: str) -> GraphModel:
    async with transaction() as tx:
        await __create_graph(tx, graph, user_id)

    if created_graph := await get_graph(
        graph.id, graph.version, graph.is_template, user_id=user_id
    ):
        return created_graph

    raise ValueError(f""Created graph {graph.id} v{graph.version} is not in DB"")


async def __create_graph(tx, graph: Graph, user_id: str):
    await AgentGraph.prisma(tx).create(
        data={
            ""id"": graph.id,
            ""version"": graph.version,
            ""name"": graph.name,
            ""description"": graph.description,
            ""isTemplate"": graph.is_template,
            ""isActive"": graph.is_active,
            ""userId"": user_id,
        }
    )

    await asyncio.gather(
        *[
            AgentNode.prisma(tx).create(
                {
                    ""id"": node.id,
                    ""agentBlockId"": node.block_id,
                    ""agentGraphId"": graph.id,
                    ""agentGraphVersion"": graph.version,
                    ""constantInput"": json.dumps(node.input_default),
                    ""metadata"": json.dumps(node.metadata),
                }
            )
            for node in graph.nodes
        ]
    )

    await asyncio.gather(
        *[
            AgentNodeLink.prisma(tx).create(
                {
                    ""id"": str(uuid.uuid4()),
                    ""sourceName"": link.source_name,
                    ""sinkName"": link.sink_name,
                    ""agentNodeSourceId"": link.source_id,
                    ""agentNodeSinkId"": link.sink_id,
                    ""isStatic"": link.is_static,
                }
            )
            for link in graph.links
        ]
    )


# ------------------------ UTILITIES ------------------------ #


def make_graph_model(creatable_graph: Graph, user_id: str) -> GraphModel:
    """"""
    Convert a Graph to a GraphModel, setting graph_id and graph_version on all nodes.

    Args:
        creatable_graph (Graph): The creatable graph to convert.
        user_id (str): The ID of the user creating the graph.

    Returns:
        GraphModel: The converted Graph object.
    """"""
    # Create a new Graph object, inheriting properties from CreatableGraph
    return GraphModel(
        **creatable_graph.model_dump(exclude={""nodes""}),
        user_id=user_id,
        nodes=[
            NodeModel(
                **creatable_node.model_dump(),
                graph_id=creatable_graph.id,
                graph_version=creatable_graph.version,
            )
            for creatable_node in creatable_graph.nodes
        ],
    )


async def fix_llm_provider_credentials():
    """"""Fix node credentials with provider `llm`""""""
    from backend.integrations.credentials_store import IntegrationCredentialsStore

    from .user import get_user_integrations

    store = IntegrationCredentialsStore()

    broken_nodes = await prisma.get_client().query_raw(
        """"""
        SELECT    graph.""userId""       user_id,
                  node.id              node_id,
                  node.""constantInput"" node_preset_input
        FROM      platform.""AgentNode""  node
        LEFT JOIN platform.""AgentGraph"" graph
        ON        node.""agentGraphId"" = graph.id
        WHERE     node.""constantInput""::jsonb->'credentials'->>'provider' = 'llm'
        ORDER BY  graph.""userId"";
        """"""
    )
    logger.info(f""Fixing LLM credential inputs on {len(broken_nodes)} nodes"")

    user_id: str = """"
    user_integrations = None
    for node in broken_nodes:
        if node[""user_id""] != user_id:
            # Save queries by only fetching once per user
            user_id = node[""user_id""]
            user_integrations = await get_user_integrations(user_id)
        elif not user_integrations:
            raise RuntimeError(f""Impossible state while processing node {node}"")

        node_id: str = node[""node_id""]
        node_preset_input: dict = json.loads(node[""node_preset_input""])
        credentials_meta: dict = node_preset_input[""credentials""]

        credentials = next(
            (
                c
                for c in user_integrations.credentials
                if c.id == credentials_meta[""id""]
            ),
            None,
        )
        if not credentials:
            continue
        if credentials.type != ""api_key"":
            logger.warning(
                f""User {user_id} credentials {credentials.id} with provider 'llm' ""
                f""has invalid type '{credentials.type}'""
            )
            continue

        api_key = credentials.api_key.get_secret_value()
        if api_key.startswith(""sk-ant-api03-""):
            credentials.provider = credentials_meta[""provider""] = ""anthropic""
        elif api_key.startswith(""sk-""):
            credentials.provider = credentials_meta[""provider""] = ""openai""
        elif api_key.startswith(""gsk_""):
            credentials.provider = credentials_meta[""provider""] = ""groq""
        else:
            logger.warning(
                f""Could not identify provider from key prefix {api_key[:13]}*****""
            )
            continue

        store.update_creds(user_id, credentials)
        await AgentNode.prisma().update(
            where={""id"": node_id},
            data={""constantInput"": json.dumps(node_preset_input)},
        )
","import asyncio
import logging
import uuid
from collections import defaultdict
from datetime import datetime, timezone
from typing import Any, Literal, Optional, Type

import prisma
from prisma.models import AgentGraph, AgentGraphExecution, AgentNode, AgentNodeLink
from prisma.types import AgentGraphWhereInput
from pydantic.fields import computed_field

from backend.blocks.agent import AgentExecutorBlock
from backend.blocks.basic import AgentInputBlock, AgentOutputBlock
from backend.util import json

from .block import BlockInput, BlockType, get_block, get_blocks
from .db import BaseDbModel, transaction
from .execution import ExecutionStatus
from .includes import AGENT_GRAPH_INCLUDE, AGENT_NODE_INCLUDE
from .integrations import Webhook

logger = logging.getLogger(__name__)

class Link(BaseDbModel):
    source_id: str
    sink_id: str
    source_name: str
    sink_name: str
    is_static: bool = False

    @staticmethod
    def from_db(link: AgentNodeLink):
        return Link(
            id=link.id,
            source_name=link.sourceName,
            source_id=link.agentNodeSourceId,
            sink_name=link.sinkName,
            sink_id=link.agentNodeSinkId,
            is_static=link.isStatic,
        )

    def __hash__(self):
        return hash((self.source_id, self.sink_id, self.source_name, self.sink_name))

class Node(BaseDbModel):
    block_id: str
    input_default: BlockInput = {}  # dict[input_name, default_value]
    metadata: dict[str, Any] = {}
    input_links: list[Link] = []
    output_links: list[Link] = []

    webhook_id: Optional[str] = None

class NodeModel(Node):
    graph_id: str
    graph_version: int

    webhook: Optional[Webhook] = None

    @staticmethod
    def from_db(node: AgentNode):
        if not node.AgentBlock:
            raise ValueError(f""Invalid node {node.id}, invalid AgentBlock."")
        
        obj = NodeModel(
            id=node.id,
            block_id=node.AgentBlock.id,
            input_default=json.loads(node.constantInput, target_type=dict[str, Any]),
            metadata=json.loads(node.metadata, target_type=dict[str, Any]),
            graph_id=node.agentGraphId,
            graph_version=node.agentGraphVersion,
            webhook_id=node.webhookId,
            webhook=Webhook.from_db(node.Webhook) if node.Webhook else None,
        )

        obj.input_links = [Link.from_db(link) for link in node.Input or []]
        obj.output_links = [Link.from_db(link) for link in node.Output or []]
        return obj

    def is_triggered_by_event_type(self, event_type: str) -> bool:
        if not (block := get_block(self.block_id)):
            raise ValueError(f""Block #{self.block_id} not found for node #{self.id}"")

        if not block.webhook_config:
            raise TypeError(""This method can't be used on non-webhook blocks"")

        if not block.webhook_config.event_filter_input:
            return True

        event_filter = self.input_default.get(block.webhook_config.event_filter_input)
        if not event_filter:
            raise ValueError(f""Event filter is not configured on node #{self.id}"")

        return event_type in [
            block.webhook_config.event_format.format(event=k)
            for k in event_filter
            if event_filter[k] is True
        ]

# Fix 2-way reference Node <-> Webhook
Webhook.model_rebuild()

class GraphExecution(BaseDbModel):
    execution_id: str
    started_at: datetime
    ended_at: datetime
    duration: float
    total_run_time: float
    status: ExecutionStatus
    graph_id: str
    graph_version: int

    @staticmethod
    def from_db(execution: AgentGraphExecution):
        now = datetime.now(timezone.utc)
        start_time = execution.startedAt or execution.createdAt
        end_time = execution.updatedAt or now
        duration = (end_time - start_time).total_seconds()
        total_run_time = duration

        try:
            stats = json.loads(execution.stats or ""{}"", target_type=dict[str, Any])
        except ValueError:
            stats = {}

        duration = stats.get(""walltime"", duration)
        total_run_time = stats.get(""nodes_walltime"", total_run_time)

        return GraphExecution(
            id=execution.id,
            execution_id=execution.id,
            started_at=start_time,
            ended_at=end_time,
            duration=duration,
            total_run_time=total_run_time,
            status=ExecutionStatus(execution.executionStatus),
            graph_id=execution.agentGraphId,
            graph_version=execution.agentGraphVersion,
        )

class Graph(BaseDbModel):
    version: int = 1
    is_active: bool = True
    is_template: bool = False
    name: str
    description: str
    nodes: list[Node] = []
    links: list[Link] = []

    @computed_field
    @property
    def input_schema(self) -> dict[str, Any]:
        return self._generate_schema(
            AgentInputBlock.Input,
            [
                node.input_default
                for node in self.nodes
                if (b := get_block(node.block_id))
                and b.block_type == BlockType.INPUT
                and ""name"" in node.input_default
            ],
        )

    @computed_field
    @property
    def output_schema(self) -> dict[str, Any]:
        return self._generate_schema(
            AgentOutputBlock.Input,
            [
                node.input_default
                for node in self.nodes
                if (b := get_block(node.block_id))
                and b.block_type == BlockType.OUTPUT
                and ""name"" in node.input_default
            ],
        )

    @staticmethod
    def _generate_schema(
        type_class: Type[AgentInputBlock.Input] | Type[AgentOutputBlock.Input],
        data: list[dict],
    ) -> dict[str, Any]:
        props = []
        for p in data:
            try:
                props.append(type_class(**p))
            except Exception as e:
                logger.warning(f""Invalid {type_class}: {p}, {e}"")

        return {
            ""type"": ""object"",
            ""properties"": {
                p.name: {
                    ""secret"": p.secret,
                    ""advanced"": p.advanced,
                    ""title"": p.title or p.name,
                    **({""description"": p.description} if p.description else {}),
                    **({""default"": p.value} if p.value is not None else {}),
                }
                for p in props
            },
            ""required"": [p.name for p in props if p.value is None],
        }

class GraphModel(Graph):
    user_id: str
    nodes: list[NodeModel] = []  # type: ignore

    @property
    def starting_nodes(self) -> list[Node]:
        outbound_nodes = {link.sink_id for link in self.links}
        input_nodes = {
            v.id
            for v in self.nodes
            if (b := get_block(v.block_id)) and b.block_type == BlockType.INPUT
        }
        return [
            node
            for node in self.nodes
            if node.id not in outbound_nodes or node.id in input_nodes
        ]

    def reassign_ids(self, user_id: str, reassign_graph_id: bool = False):
        """"""
        Reassigns all IDs in the graph to new UUIDs.
        This method can be used before storing a new graph to the database.
        """"""
        # Reassign Graph ID
        id_map = {node.id: str(uuid.uuid4()) for node in self.nodes}
        if reassign_graph_id:
            self.id = str(uuid.uuid4())

        # Reassign Node IDs
        for node in self.nodes:
            node.id = id_map[node.id]

        # Reassign Link IDs
        for link in self.links:
            link.source_id = id_map[link.source_id]
            link.sink_id = id_map[link.sink_id]

        # Reassign User IDs for agent blocks
        for node in self.nodes:
            if node.block_id != AgentExecutorBlock().id:
                continue
            node.input_default[""user_id""] = user_id
            node.input_default.setdefault(""data"", {})

        self.validate_graph()

    def validate_graph(self, for_run: bool = False):
        def sanitize(name):
            return name.split(""_#_"")[0].split(""_@_"")[0].split(""_$_"")[0]

        input_links = defaultdict(list)
        for link in self.links:
            input_links[link.sink_id].append(link)

        # Nodes: required fields are filled or connected and dependencies are satisfied
        for node in self.nodes:
            block = get_block(node.block_id)
            if block is None:
                raise ValueError(f""Invalid block {node.block_id} for node #{node.id}"")

            provided_inputs = set(
                [sanitize(name) for name in node.input_default]
                + [sanitize(link.sink_name) for link in input_links.get(node.id, [])]
            )
            for name in block.input_schema.get_required_fields():
                if (
                    name not in provided_inputs
                    and not (
                        name == ""payload""
                        and block.block_type
                        in (BlockType.WEBHOOK, BlockType.WEBHOOK_MANUAL)
                    )
                    and (
                        for_run  # Skip input completion validation, unless when executing.
                        or block.block_type == BlockType.INPUT
                        or block.block_type == BlockType.OUTPUT
                        or block.block_type == BlockType.AGENT
                    )
                ):
                    raise ValueError(
                        f""Node {block.name} #{node.id} required input missing: `{name}`""
                    )

            # Get input schema properties and check dependencies
            input_schema = block.input_schema.model_fields
            required_fields = block.input_schema.get_required_fields()

            def has_value(name):
                return (
                    node is not None
                    and name in node.input_default
                    and node.input_default[name] is not None
                    and str(node.input_default[name]).strip() != """"
                ) or (name in input_schema and input_schema[name].default is not None)

            # Validate dependencies between fields
            for field_name, field_info in input_schema.items():
                # Apply input dependency validation only on run & field with depends_on
                json_schema_extra = field_info.json_schema_extra or {}
                dependencies = json_schema_extra.get(""depends_on"", [])
                if not for_run or not dependencies:
                    continue

                # Check if dependent field has value in input_default
                field_has_value = has_value(field_name)
                field_is_required = field_name in required_fields

                # Check for missing dependencies when dependent field is present
                missing_deps = [dep for dep in dependencies if not has_value(dep)]
                if missing_deps and (field_has_value or field_is_required):
                    raise ValueError(
                        f""Node {block.name} #{node.id}: Field `{field_name}` requires [{', '.join(missing_deps)}] to be set""
                    )

        node_map = {v.id: v for v in self.nodes}

        def is_static_output_block(nid: str) -> bool:
            bid = node_map[nid].block_id
            b = get_block(bid)
            return b.static_output if b else False

        # Links: links are connected and the connected pin data type are compatible.
        for link in self.links:
            source = (link.source_id, link.source_name)
            sink = (link.sink_id, link.sink_name)
            suffix = f""Link {source} <-> {sink}""

            for i, (node_id, name) in enumerate([source, sink]):
                node = node_map.get(node_id)
                if not node:
                    raise ValueError(
                        f""{suffix}, {node_id} is invalid node id, available nodes: {node_map.keys()}""
                    )

                block = get_block(node.block_id)
                if not block:
                    blocks = {v().id: v().name for v in get_blocks().values()}
                    raise ValueError(
                        f""{suffix}, {node.block_id} is invalid block id, available blocks: {blocks}""
                    )

                sanitized_name = sanitize(name)
                vals = node.input_default
                if i == 0:
                    fields = (
                        block.output_schema.get_fields()
                        if block.block_type != BlockType.AGENT
                        else vals.get(""output_schema"", {}).get(""properties"", {}).keys()
                    )
                else:
                    fields = (
                        block.input_schema.get_fields()
                        if block.block_type != BlockType.AGENT
                        else vals.get(""input_schema"", {}).get(""properties"", {}).keys()
                    )
                if sanitized_name not in fields:
                    fields_msg = f""Allowed fields: {fields}""
                    raise ValueError(f""{suffix}, `{name}` invalid, {fields_msg}"")

            if is_static_output_block(link.source_id):
                link.is_static = True  # Each value block output should be static.

    @staticmethod
    def from_db(graph: AgentGraph, for_export: bool = False):
        return GraphModel(
            id=graph.id,
            user_id=graph.userId,
            version=graph.version,
            is_active=graph.isActive,
            is_template=graph.isTemplate,
            name=graph.name or """",
            description=graph.description or """",
            nodes=[
                NodeModel.from_db(GraphModel._process_node(node, for_export))
                for node in graph.AgentNodes or []
            ],
            links=list(
                {
                    Link.from_db(link)
                    for node in graph.AgentNodes or []
                    for link in (node.Input or []) + (node.Output or [])
                }
            ),
        )

    @staticmethod
    def _process_node(node: AgentNode, for_export: bool) -> AgentNode:
        if for_export:
            # Remove credentials from node input
            if node.constantInput:
                constant_input = json.loads(
                    node.constantInput, target_type=dict[str, Any]
                )
                constant_input = GraphModel._hide_node_input_credentials(constant_input)
                node.constantInput = json.dumps(constant_input)

            # Remove webhook info
            node.webhookId = None
            node.Webhook = None

        return node

    @staticmethod
    def _hide_node_input_credentials(input_data: dict[str, Any]) -> dict[str, Any]:
        sensitive_keys = [""credentials"", ""api_key"", ""password"", ""token"", ""secret""]
        result = {}
        for key, value in input_data.items():
            if isinstance(value, dict):
                result[key] = GraphModel._hide_node_input_credentials(value)
            elif isinstance(value, str) and any(
                sensitive_key in key.lower() for sensitive_key in sensitive_keys
            ):
                # Skip this key-value pair in the result
                continue
            else:
                result[key] = value
        return result

# --------------------- CRUD functions --------------------- #

async def get_node(node_id: str) -> NodeModel:
    node = await AgentNode.prisma().find_unique_or_raise(
        where={""id"": node_id},
        include=AGENT_NODE_INCLUDE,
    )
    return NodeModel.from_db(node)

async def set_node_webhook(node_id: str, webhook_id: str | None) -> NodeModel:
    node = await AgentNode.prisma().update(
        where={""id"": node_id},
        data=(
            {""Webhook"": {""connect"": {""id"": webhook_id}}}
            if webhook_id
            else {""Webhook"": {""disconnect"": True}}
        ),
        include=AGENT_NODE_INCLUDE,
    )
    if not node:
        raise ValueError(f""Node #{node_id} not found"")
    return NodeModel.from_db(node)

async def get_graphs(
    user_id: str,
    filter_by: Literal[""active"", ""template""] | None = ""active"",
) -> list[GraphModel]:
    """"""
    Retrieves graph metadata objects.
    Default behaviour is to get all currently active graphs.

    Args:
        filter_by: An optional filter to either select templates or active graphs.
        user_id: The ID of the user that owns the graph.

    Returns:
        list[GraphModel]: A list of objects representing the retrieved graphs.
    """"""
    where_clause: AgentGraphWhereInput = {""userId"": user_id}
    if filter_by == ""active"":
        where_clause[""isActive""] = True
    elif filter_by == ""template"":
        where_clause[""isTemplate""] = True
    graphs = await AgentGraph.prisma().find_many(
        where=where_clause,
        distinct=[""id""],
        order={""version"": ""desc""},
        include=AGENT_GRAPH_INCLUDE,
    )
    graph_models = []
    for graph in graphs:
        try:
            graph_models.append(GraphModel.from_db(graph))
        except Exception as e:
            logger.error(f""Error processing graph {graph.id}: {e}"")
            continue
    return graph_models

async def get_executions(user_id: str) -> list[GraphExecution]:
    executions = await AgentGraphExecution.prisma().find_many(
        where={""userId"": user_id},
        order={""createdAt"": ""desc""},
    )
    return [GraphExecution.from_db(execution) for execution in executions]

async def get_execution(user_id: str, execution_id: str) -> GraphExecution | None:
    execution = await AgentGraphExecution.prisma().find_first(
        where={""id"": execution_id, ""userId"": user_id}
    )
    return GraphExecution.from_db(execution) if execution else None

async def get_graph(
    graph_id: str,
    version: int | None = None,
    template: bool = False,
    user_id: str | None = None,
    for_export: bool = False,
) -> GraphModel | None:
    """"""
    Retrieves a graph from the DB.
    Defaults to the version with `is_active` if `version` is not passed,
    or the latest version with `is_template` if `template=True`.

    Returns `None` if the record is not found.
    """"""
    where_clause: AgentGraphWhereInput = {
        ""id"": graph_id,
    }
    if version is not None:
        where_clause[""version""] = version
    elif not template:
        where_clause[""isActive""] = True
    # TODO: Fix hack workaround to get adding store agents to work
    if user_id is not None and not template:
        where_clause[""userId""] = user_id
    graph = await AgentGraph.prisma().find_first(
        where=where_clause,
        include=AGENT_GRAPH_INCLUDE,
        order={""version"": ""desc""},
    )
    return GraphModel.from_db(graph, for_export) if graph else None

async def set_graph_active_version(graph_id: str, version: int, user_id: str) -> None:
    # Activate the requested version if it exists and is owned by the user.
    updated_count = await AgentGraph.prisma().update_many(
        data={""isActive"": True},
        where={
            ""id"": graph_id,
            ""version"": version,
            ""userId"": user_id,
        },
    )
    if updated_count == 0:
        raise Exception(f""Graph #{graph_id} v{version} not found or not owned by user"")

    # Deactivate all other versions.
    await AgentGraph.prisma().update_many(
        data={""isActive"": False},
        where={
            ""id"": graph_id,
            ""version"": {""not"": version},
            ""userId"": user_id,
            ""isActive"": True,
        },
    )

async def get_graph_all_versions(graph_id: str, user_id: str) -> list[GraphModel]:
    graph_versions = await AgentGraph.prisma().find_many(
        where={""id"": graph_id, ""userId"": user_id},
        order={""version"": ""desc""},
        include=AGENT_GRAPH_INCLUDE,
    )
    if not graph_versions:
        return []
    return [GraphModel.from_db(graph) for graph in graph_versions]

async def delete_graph(graph_id: str, user_id: str) -> int:
    entries_count = await AgentGraph.prisma().delete_many(
        where={""id"": graph_id, ""userId"": user_id}
    )
    if entries_count:
        logger.info(f""Deleted {entries_count} graph entries for Graph #{graph_id}"")
    return entries_count

async def create_graph(graph: Graph, user_id: str) -> GraphModel:
    async with transaction() as tx:
        await __create_graph(tx, graph, user_id)
    if created_graph := await get_graph(
        graph.id, graph.version, graph.is_template, user_id=user_id
    ):
        return created_graph
    raise ValueError(f""Created graph {graph.id} v{graph.version} is not in DB"")

async def __create_graph(tx, graph: Graph, user_id: str):
    await AgentGraph.prisma(tx).create(
        data={
            ""id"": graph.id,
            ""version"": graph.version,
            ""name"": graph.name,
            ""description"": graph.description,
            ""isTemplate"": graph.is_template,
            ""isActive"": graph.is_active,
            ""userId"": user_id,
        }
    )
    await asyncio.gather(
        *[
            AgentNode.prisma(tx).create(
                {
                    ""id"": node.id,
                    ""agentBlockId"": node.block_id,
                    ""agentGraphId"": graph.id,
                    ""agentGraphVersion"": graph.version,
                    ""constantInput"": json.dumps(node.input_default),
                    ""metadata"": json.dumps(node.metadata),
                }
            )
            for node in graph.nodes
        ]
    )
    await asyncio.gather(
        *[
            AgentNodeLink.prisma(tx).create(
                {
                    ""id"": str(uuid.uuid4()),
                    ""sourceName"": link.source_name,
                    ""sinkName"": link.sink_name,
                    ""agentNodeSourceId"": link.source_id,
                    ""agentNodeSinkId"": link.sink_id,
                    ""isStatic"": link.is_static,
                }
            )
            for link in graph.links
        ]
    )

# ------------------------ UTILITIES ------------------------ #

def make_graph_model(creatable_graph: Graph, user_id: str) -> GraphModel:
    """"""
    Convert a Graph to a GraphModel, setting graph_id and graph_version on all nodes.

    Args:
        creatable_graph (Graph): The creatable graph to convert.
        user_id (str): The ID of the user creating the graph.

    Returns:
        GraphModel: The converted Graph object.
    """"""
    # Create a new Graph object, inheriting properties from CreatableGraph
    return GraphModel(
        **creatable_graph.model_dump(exclude={""nodes""}),
        user_id=user_id,
        nodes=[
            NodeModel(
                **creatable_node.model_dump(),
                graph_id=creatable_graph.id,
                graph_version=creatable_graph.version,
            )
            for creatable_node in creatable_graph.nodes
        ],
    )

async def fix_llm_provider_credentials():
    """"""Fix node credentials with provider `llm`""""""
    from backend.integrations.credentials_store import IntegrationCredentialsStore
    from .user import get_user_integrations

    store = IntegrationCredentialsStore()
    broken_nodes = await prisma.get_client().query_raw(
        """"""
        SELECT    graph.""userId""       user_id,
                  node.id              node_id,
                  node.""constantInput"" node_preset_input
        FROM      platform.""AgentNode""  node
        LEFT JOIN platform.""AgentGraph"" graph
        ON        node.""agentGraphId"" = graph.id
        WHERE     node.""constantInput""::jsonb->'credentials'->>'provider' = 'llm'
        ORDER BY  graph.""userId"";
        """"""
    )
    logger.info(f""Fixing LLM credential inputs on {len(broken_nodes)} nodes"")
    user_id: str = """"
    user_integrations = None
    for node in broken_nodes:
        if node[""user_id""] != user_id:
            user_id = node[""user_id""]
            user_integrations = await get_user_integrations(user_id)
        elif not user_integrations:
            raise RuntimeError(f""Impossible state while processing node {node}"")
        node_id: str = node[""node_id""]
        node_preset_input: dict = json.loads(node[""node_preset_input""])
        credentials_meta: dict = node_preset_input[""credentials""]
        credentials = next(
            (
                c
                for c in user_integrations.credentials
                if c.id == credentials_meta[""id""]
            ),
            None,
        )
        if not credentials:
            continue
        if credentials.type != ""api_key"":
            logger.warning(
                f""User {user_id} credentials {credentials.id} with provider 'llm' ""
                f""has invalid type '{credentials.type}'""
            )
            continue
        api_key = credentials.api_key.get_secret_value()
        if api_key.startswith(""sk-ant-api03-""):
            credentials.provider = credentials_meta[""provider""] = ""anthropic""
        elif api_key.startswith(""sk-""):
            credentials.provider = credentials_meta[""provider""] = ""openai""
        elif api_key.startswith(""gsk_""):
            credentials.provider = credentials_meta[""provider""] = ""groq""
        else:
            logger.warning(
                f""Could not identify provider from key prefix {api_key[:13]}*****""
            )
            continue
        store.update_creds(user_id, credentials)
        await AgentNode.prisma().update(
            where={""id"": node_id},
            data={""constantInput"": json.dumps(node_preset_input)},
        )","{'final_score': 99.76, 'length_similarity': 99.77, 'token_similarity': 99.42, 'structure_similarity': 99.84, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
39,"import asyncio
from typing import Literal

import aiohttp
import discord
from pydantic import SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

DiscordCredentials = CredentialsMetaInput[
    Literal[ProviderName.DISCORD], Literal[""api_key""]
]


def DiscordCredentialsField() -> DiscordCredentials:
    return CredentialsField(description=""Discord bot token"")


TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""discord"",
    api_key=SecretStr(""test_api_key""),
    title=""Mock Discord API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class ReadDiscordMessagesBlock(Block):
    class Input(BlockSchema):
        credentials: DiscordCredentials = DiscordCredentialsField()

    class Output(BlockSchema):
        message_content: str = SchemaField(
            description=""The content of the message received""
        )
        channel_name: str = SchemaField(
            description=""The name of the channel the message was received from""
        )
        username: str = SchemaField(
            description=""The username of the user who sent the message""
        )

    def __init__(self):
        super().__init__(
            id=""df06086a-d5ac-4abb-9996-2ad0acb2eff7"",
            input_schema=ReadDiscordMessagesBlock.Input,  # Assign input schema
            output_schema=ReadDiscordMessagesBlock.Output,  # Assign output schema
            description=""Reads messages from a Discord channel using a bot token."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""continuous_read"": False,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""message_content"",
                    ""Hello!\n\nFile from user: example.txt\nContent: This is the content of the file."",
                ),
                (""channel_name"", ""general""),
                (""username"", ""test_user""),
            ],
            test_mock={
                ""run_bot"": lambda token: asyncio.Future()  # Create a Future object for mocking
            },
        )

    async def run_bot(self, token: SecretStr):
        intents = discord.Intents.default()
        intents.message_content = True

        client = discord.Client(intents=intents)

        self.output_data = None
        self.channel_name = None
        self.username = None

        @client.event
        async def on_ready():
            print(f""Logged in as {client.user}"")

        @client.event
        async def on_message(message):
            if message.author == client.user:
                return

            self.output_data = message.content
            self.channel_name = message.channel.name
            self.username = message.author.name

            if message.attachments:
                attachment = message.attachments[0]  # Process the first attachment
                if attachment.filename.endswith(("".txt"", "".py"")):
                    async with aiohttp.ClientSession() as session:
                        async with session.get(attachment.url) as response:
                            file_content = await response.text()
                            self.output_data += f""\n\nFile from user: {attachment.filename}\nContent: {file_content}""

            await client.close()

        await client.start(token.get_secret_value())

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        while True:
            for output_name, output_value in self.__run(input_data, credentials):
                yield output_name, output_value
            break

    def __run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:
        try:
            loop = asyncio.get_event_loop()
            future = self.run_bot(credentials.api_key)

            # If it's a Future (mock), set the result
            if isinstance(future, asyncio.Future):
                future.set_result(
                    {
                        ""output_data"": ""Hello!\n\nFile from user: example.txt\nContent: This is the content of the file."",
                        ""channel_name"": ""general"",
                        ""username"": ""test_user"",
                    }
                )

            result = loop.run_until_complete(future)

            # For testing purposes, use the mocked result
            if isinstance(result, dict):
                self.output_data = result.get(""output_data"")
                self.channel_name = result.get(""channel_name"")
                self.username = result.get(""username"")

            if (
                self.output_data is None
                or self.channel_name is None
                or self.username is None
            ):
                raise ValueError(""No message, channel name, or username received."")

            yield ""message_content"", self.output_data
            yield ""channel_name"", self.channel_name
            yield ""username"", self.username

        except discord.errors.LoginFailure as login_err:
            raise ValueError(f""Login error occurred: {login_err}"")
        except Exception as e:
            raise ValueError(f""An error occurred: {e}"")


class SendDiscordMessageBlock(Block):
    class Input(BlockSchema):
        credentials: DiscordCredentials = DiscordCredentialsField()
        message_content: str = SchemaField(
            description=""The content of the message received""
        )
        channel_name: str = SchemaField(
            description=""The name of the channel the message was received from""
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""The status of the operation (e.g., 'Message sent', 'Error')""
        )

    def __init__(self):
        super().__init__(
            id=""d0822ab5-9f8a-44a3-8971-531dd0178b6b"",
            input_schema=SendDiscordMessageBlock.Input,  # Assign input schema
            output_schema=SendDiscordMessageBlock.Output,  # Assign output schema
            description=""Sends a message to a Discord channel using a bot token."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""channel_name"": ""general"",
                ""message_content"": ""Hello, Discord!"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[(""status"", ""Message sent"")],
            test_mock={
                ""send_message"": lambda token, channel_name, message_content: asyncio.Future()
            },
            test_credentials=TEST_CREDENTIALS,
        )

    async def send_message(self, token: str, channel_name: str, message_content: str):
        intents = discord.Intents.default()
        intents.guilds = True  # Required for fetching guild/channel information
        client = discord.Client(intents=intents)

        @client.event
        async def on_ready():
            print(f""Logged in as {client.user}"")
            for guild in client.guilds:
                for channel in guild.text_channels:
                    if channel.name == channel_name:
                        # Split message into chunks if it exceeds 2000 characters
                        for chunk in self.chunk_message(message_content):
                            await channel.send(chunk)
                        self.output_data = ""Message sent""
                        await client.close()
                        return

            self.output_data = ""Channel not found""
            await client.close()

        await client.start(token)

    def chunk_message(self, message: str, limit: int = 2000) -> list:
        """"""Splits a message into chunks not exceeding the Discord limit.""""""
        return [message[i : i + limit] for i in range(0, len(message), limit)]

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            loop = asyncio.get_event_loop()
            future = self.send_message(
                credentials.api_key.get_secret_value(),
                input_data.channel_name,
                input_data.message_content,
            )

            # If it's a Future (mock), set the result
            if isinstance(future, asyncio.Future):
                future.set_result(""Message sent"")

            result = loop.run_until_complete(future)

            # For testing purposes, use the mocked result
            if isinstance(result, str):
                self.output_data = result

            if self.output_data is None:
                raise ValueError(""No status message received."")

            yield ""status"", self.output_data

        except discord.errors.LoginFailure as login_err:
            raise ValueError(f""Login error occurred: {login_err}"")
        except Exception as e:
            raise ValueError(f""An error occurred: {e}"")
","import asyncio
from typing import Literal

import aiohttp
import discord
from pydantic import SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

DiscordCredentials = CredentialsMetaInput[
    Literal[ProviderName.DISCORD], Literal[""api_key""]
]

def DiscordCredentialsField() -> DiscordCredentials:
    return CredentialsField(description=""Discord bot token"")

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""discord"",
    api_key=SecretStr(""test_api_key""),
    title=""Mock Discord API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}

class ReadDiscordMessagesBlock(Block):
    class Input(BlockSchema):
        credentials: DiscordCredentials = DiscordCredentialsField()

    class Output(BlockSchema):
        message_content: str = SchemaField(
            description=""The content of the message received""
        )
        channel_name: str = SchemaField(
            description=""The name of the channel the message was received from""
        )
        username: str = SchemaField(
            description=""The username of the user who sent the message""
        )

    def __init__(self):
        super().__init__(
            id=""df06086a-d5ac-4abb-9996-2ad0acb2eff7"",
            input_schema=ReadDiscordMessagesBlock.Input,
            output_schema=ReadDiscordMessagesBlock.Output,
            description=""Reads messages from a Discord channel using a bot token."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""continuous_read"": False,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""message_content"",
                    ""Hello!\n\nFile from user: example.txt\nContent: This is the content of the file."",
                ),
                (""channel_name"", ""general""),
                (""username"", ""test_user""),
            ],
            test_mock={
                ""run_bot"": lambda token: asyncio.Future()
            },
        )

    async def run_bot(self, token: SecretStr):
        intents = discord.Intents.default()
        intents.message_content = True
        client = discord.Client(intents=intents)

        self.output_data = None
        self.channel_name = None
        self.username = None

        @client.event
        async def on_ready():
            print(f""Logged in as {client.user}"")

        @client.event
        async def on_message(message):
            if message.author == client.user:
                return

            self.output_data = message.content
            self.channel_name = message.channel.name
            self.username = message.author.name

            if message.attachments:
                attachment = message.attachments[0]  
                if attachment.filename.endswith(("".txt"", "".py"")):
                    async with aiohttp.ClientSession() as session:
                        async with session.get(attachment.url) as response:
                            file_content = await response.text()
                            self.output_data += f""\n\nFile from user: {attachment.filename}\nContent: {file_content}""
            await client.close()

        await client.start(token.get_secret_value())
    
    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:
        while True:
            for output_name, output_value in self.__run(input_data, credentials):
                yield output_name, output_value
            break
    
    def __run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:
        try:
            loop = asyncio.get_event_loop()
            future = self.run_bot(credentials.api_key)
            if isinstance(future, asyncio.Future):
                future.set_result(
                    {
                        ""output_data"": ""Hello!\n\nFile from user: example.txt\nContent: This is the content of the file."",
                        ""channel_name"": ""general"",
                        ""username"": ""test_user"",
                    }
                )
            result = loop.run_until_complete(future)
            if isinstance(result, dict):
                self.output_data = result.get(""output_data"")
                self.channel_name = result.get(""channel_name"")
                self.username = result.get(""username"")
            if (
                self.output_data is None
                or self.channel_name is None
                or self.username is None
            ):
                raise ValueError(""No message, channel name, or username received."")
            yield ""message_content"", self.output_data
            yield ""channel_name"", self.channel_name
            yield ""username"", self.username
        except discord.errors.LoginFailure as login_err:
            raise ValueError(f""Login error occurred: {login_err}"")
        except Exception as e:
            raise ValueError(f""An error occurred: {e}"")

class SendDiscordMessageBlock(Block):
    class Input(BlockSchema):
        credentials: DiscordCredentials = DiscordCredentialsField()
        message_content: str = SchemaField(
            description=""The content of the message received""
        )
        channel_name: str = SchemaField(
            description=""The name of the channel the message was received from""
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""The status of the operation (e.g., 'Message sent', 'Error')""
        )

    def __init__(self):
        super().__init__(
            id=""d0822ab5-9f8a-44a3-8971-531dd0178b6b"",
            input_schema=SendDiscordMessageBlock.Input,
            output_schema=SendDiscordMessageBlock.Output,
            description=""Sends a message to a Discord channel using a bot token."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""channel_name"": ""general"",
                ""message_content"": ""Hello, Discord!"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[(""status"", ""Message sent"")],
            test_mock={
                ""send_message"": lambda token, channel_name, message_content: asyncio.Future()
            },
            test_credentials=TEST_CREDENTIALS,
        )

    async def send_message(self, token: str, channel_name: str, message_content: str):
        intents = discord.Intents.default()
        intents.guilds = True
        client = discord.Client(intents=intents)

        @client.event
        async def on_ready():
            print(f""Logged in as {client.user}"")
            for guild in client.guilds:
                for channel in guild.text_channels:
                    if channel.name == channel_name:
                        for chunk in self.chunk_message(message_content):
                            await channel.send(chunk)
                        self.output_data = ""Message sent""
                        await client.close()
                        return
            self.output_data = ""Channel not found""
            await client.close()

        await client.start(token)

    def chunk_message(self, message: str, limit: int = 2000) -> list:
        return [message[i : i + limit] for i in range(0, len(message), limit)]
    
    def run(self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs) -> BlockOutput:
        try:
            loop = asyncio.get_event_loop()
            future = self.send_message(
                credentials.api_key.get_secret_value(),
                input_data.channel_name,
                input_data.message_content,
            )
            if isinstance(future, asyncio.Future):
                future.set_result(""Message sent"")
            result = loop.run_until_complete(future)
            if isinstance(result, str):
                self.output_data = result
            if self.output_data is None:
                raise ValueError(""No status message received."")
            yield ""status"", self.output_data
        except discord.errors.LoginFailure as login_err:
            raise ValueError(f""Login error occurred: {login_err}"")
        except Exception as e:
            raise ValueError(f""An error occurred: {e}"")","{'final_score': 93.48, 'length_similarity': 93.27, 'token_similarity': 89.81, 'structure_similarity': 95.24, 'function_similarity': 100, 'complexity_similarity': 85.0, 'variable_similarity': 100}",
40,"""""""
Word Break Problem is a well-known problem in computer science.
Given a string and a dictionary of words, the task is to determine if
the string can be segmented into a sequence of one or more dictionary words.

Wikipedia: https://en.wikipedia.org/wiki/Word_break_problem
""""""


def backtrack(input_string: str, word_dict: set[str], start: int) -> bool:
    """"""
    Helper function that uses backtracking to determine if a valid
    word segmentation is possible starting from index 'start'.

    Parameters:
    input_string (str): The input string to be segmented.
    word_dict (set[str]): A set of valid dictionary words.
    start (int): The starting index of the substring to be checked.

    Returns:
    bool: True if a valid segmentation is possible, otherwise False.

    Example:
    >>> backtrack(""leetcode"", {""leet"", ""code""}, 0)
    True

    >>> backtrack(""applepenapple"", {""apple"", ""pen""}, 0)
    True

    >>> backtrack(""catsandog"", {""cats"", ""dog"", ""sand"", ""and"", ""cat""}, 0)
    False
    """"""

    # Base case: if the starting index has reached the end of the string
    if start == len(input_string):
        return True

    # Try every possible substring from 'start' to 'end'
    for end in range(start + 1, len(input_string) + 1):
        if input_string[start:end] in word_dict and backtrack(
            input_string, word_dict, end
        ):
            return True

    return False


def word_break(input_string: str, word_dict: set[str]) -> bool:
    """"""
    Determines if the input string can be segmented into a sequence of
    valid dictionary words using backtracking.

    Parameters:
    input_string (str): The input string to segment.
    word_dict (set[str]): The set of valid words.

    Returns:
    bool: True if the string can be segmented into valid words, otherwise False.

    Example:
    >>> word_break(""leetcode"", {""leet"", ""code""})
    True

    >>> word_break(""applepenapple"", {""apple"", ""pen""})
    True

    >>> word_break(""catsandog"", {""cats"", ""dog"", ""sand"", ""and"", ""cat""})
    False
    """"""

    return backtrack(input_string, word_dict, 0)
","""""""
Word Break Problem is a well-known problem in computer science.
Given a string and a dictionary of words, the task is to determine if
the string can be segmented into a sequence of one or more dictionary words.

Wikipedia: https://en.wikipedia.org/wiki/Word_break_problem
""""""

def backtrack(input_string: str, word_dict: set[str], start: int) -> bool:
    """"""
    Helper function that uses backtracking to determine if a valid
    word segmentation is possible starting from index 'start'.

    Parameters:
    input_string (str): The input string to be segmented.
    word_dict (set[str]): A set of valid dictionary words.
    start (int): The starting index of the substring to be checked.

    Returns:
    bool: True if a valid segmentation is possible, otherwise False.

    Example:
    >>> backtrack(""leetcode"", {""leet"", ""code""}, 0)
    True

    >>> backtrack(""applepenapple"", {""apple"", ""pen""}, 0)
    True

    >>> backtrack(""catsandog"", {""cats"", ""dog"", ""sand"", ""and"", ""cat""}, 0)
    False
    """"""
    # Base case: if the starting index has reached the end of the string
    if start == len(input_string):
        return True

    # Try every possible substring from 'start' to 'end'
    for end in range(start + 1, len(input_string) + 1):
        if input_string[start:end] in word_dict and backtrack(
            input_string, word_dict, end
        ):
            return True

    return False

def word_break(input_string: str, word_dict: set[str]) -> bool:
    """"""
    Determines if the input string can be segmented into a sequence of
    valid dictionary words using backtracking.

    Parameters:
    input_string (str): The input string to segment.
    word_dict (set[str]): The set of valid words.

    Returns:
    bool: True if the string can be segmented into valid words, otherwise False.

    Example:
    >>> word_break(""leetcode"", {""leet"", ""code""})
    True

    >>> word_break(""applepenapple"", {""apple"", ""pen""})
    True

    >>> word_break(""catsandog"", {""cats"", ""dog"", ""sand"", ""and"", ""cat""})
    False
    """"""
    return backtrack(input_string, word_dict, 0)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
41,"#!/usr/bin/env python
# coding: utf-8

""""""
    The approach taken is explained below. I decided to do it simply.
    Initially I was considering parsing the data into some sort of
    structure and then generating an appropriate README. I am still
    considering doing it - but for now this should work. The only issue
    I see is that it only sorts the entries at the lowest level, and that
    the order of the top-level contents do not match the order of the actual
    entries.

    This could be extended by having nested blocks, sorting them recursively
    and flattening the end structure into a list of lines. Revision 2 maybe ^.^.
""""""

def sort_blocks():
    # First, we load the current README into memory
    with open('README.md', 'r') as read_me_file:
        read_me = read_me_file.read()

    # Separating the 'table of contents' from the contents (blocks)
    table_of_contents = ''.join(read_me.split('- - -')[0])
    blocks = ''.join(read_me.split('- - -')[1]).split('\n# ')
    for i in range(len(blocks)):
        if i == 0:
            blocks[i] = blocks[i] + '\n'
        else:
            blocks[i] = '# ' + blocks[i] + '\n'

    # Sorting the libraries
    inner_blocks = sorted(blocks[0].split('##'))
    for i in range(1, len(inner_blocks)):
        if inner_blocks[i][0] != '#':
            inner_blocks[i] = '##' + inner_blocks[i]
    inner_blocks = ''.join(inner_blocks)

    # Replacing the non-sorted libraries by the sorted ones and gathering all at the final_README file
    blocks[0] = inner_blocks
    final_README = table_of_contents + '- - -' + ''.join(blocks)

    with open('README.md', 'w+') as sorted_file:
        sorted_file.write(final_README)

def main():
    # First, we load the current README into memory as an array of lines
    with open('README.md', 'r') as read_me_file:
        read_me = read_me_file.readlines()

    # Then we cluster the lines together as blocks
    # Each block represents a collection of lines that should be sorted
    # This was done by assuming only links ([...](...)) are meant to be sorted
    # Clustering is done by indentation
    blocks = []
    last_indent = None
    for line in read_me:
        s_line = line.lstrip()
        indent = len(line) - len(s_line)

        if any([s_line.startswith(s) for s in ['* [', '- [']]):
            if indent == last_indent:
                blocks[-1].append(line)
            else:
                blocks.append([line])
            last_indent = indent
        else:
            blocks.append([line])
            last_indent = None

    with open('README.md', 'w+') as sorted_file:
        # Then all of the blocks are sorted individually
        blocks = [
            ''.join(sorted(block, key=str.lower)) for block in blocks
        ]
        # And the result is written back to README.md
        sorted_file.write(''.join(blocks))

    # Then we call the sorting method
    sort_blocks()


if __name__ == ""__main__"":
    main()
","#!/usr/bin/env python
# coding: utf-8

""""""
    The approach taken is explained below. I decided to do it simply.
    Initially I was considering parsing the data into some sort of
    structure and then generating an appropriate README. I am still
    considering doing it - but for now this should work. The only issue
    I see is that it only sorts the entries at the lowest level, and that
    the order of the top-level contents do not match the order of the actual
    entries.

    This could be extended by having nested blocks, sorting them recursively
    and flattening the end structure into a list of lines. Revision 2 maybe ^.^.
""""""


def sort_blocks():
    # First, we load the current README into memory
    with open('README.md', 'r') as read_me_file:
        read_me = read_me_file.read()

    # Separating the 'table of contents' from the contents (blocks)
    table_of_contents = ''.join(read_me.split('- - -')[0])
    blocks = ''.join(read_me.split('- - -')[1]).split('\n# ')
    for i in range(len(blocks)):
        if i == 0:
            blocks[i] = blocks[i] + '\n'
        else:
            blocks[i] = '# ' + blocks[i] + '\n'

    # Sorting the libraries
    inner_blocks = sorted(blocks[0].split('##'))
    for i in range(1, len(inner_blocks)):
        if inner_blocks[i][0] != '#':
            inner_blocks[i] = '##' + inner_blocks[i]
    inner_blocks = ''.join(inner_blocks)

    # Replacing the non-sorted libraries by the sorted ones and gathering all at the final_README file
    blocks[0] = inner_blocks
    final_README = table_of_contents + '- - -' + ''.join(blocks)

    with open('README.md', 'w+') as sorted_file:
        sorted_file.write(final_README)


def main():
    # First, we load the current README into memory as an array of lines
    with open('README.md', 'r') as read_me_file:
        read_me = read_me_file.readlines()

    # Then we cluster the lines together as blocks
    # Each block represents a collection of lines that should be sorted
    # This was done by assuming only links ([...](...)) are meant to be sorted
    # Clustering is done by indentation
    blocks = []
    last_indent = None
    for line in read_me:
        s_line = line.lstrip()
        indent = len(line) - len(s_line)

        if any([s_line.startswith(s) for s in ['* [', '- [']]):
            if indent == last_indent:
                blocks[-1].append(line)
            else:
                blocks.append([line])
            last_indent = indent
        else:
            blocks.append([line])
            last_indent = None

    with open('README.md', 'w+') as sorted_file:
        # Then all of the blocks are sorted individually
        blocks = [
            ''.join(sorted(block, key=str.lower)) for block in blocks
        ]
        # And the result is written back to README.md
        sorted_file.write(''.join(blocks))

    # Then we call the sorting method
    sort_blocks()


if __name__ == ""__main__"":
    main()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
42,"import logging

from backend.data import integrations
from backend.data.model import APIKeyCredentials, Credentials, OAuth2Credentials

from ._base import WT, BaseWebhooksManager

logger = logging.getLogger(__name__)


class ManualWebhookManagerBase(BaseWebhooksManager[WT]):
    async def _register_webhook(
        self,
        credentials: Credentials,
        webhook_type: WT,
        resource: str,
        events: list[str],
        ingress_url: str,
        secret: str,
    ) -> tuple[str, dict]:
        print(ingress_url)  # FIXME: pass URL to user in front end

        return """", {}

    async def _deregister_webhook(
        self,
        webhook: integrations.Webhook,
        credentials: OAuth2Credentials | APIKeyCredentials,
    ) -> None:
        pass
","import logging
from backend.data import integrations
from backend.data.model import APIKeyCredentials, Credentials, OAuth2Credentials
from ._base import WT, BaseWebhooksManager

logger = logging.getLogger(__name__)

class ManualWebhookManagerBase(BaseWebhooksManager[WT]):
    async def _register_webhook(
        self,
        credentials: Credentials,
        webhook_type: WT,
        resource: str,
        events: list[str],
        ingress_url: str,
        secret: str,
    ) -> tuple[str, dict]:
        print(ingress_url)  # FIXME: pass URL to user in front end
        return """", {}

    async def _deregister_webhook(
        self,
        webhook: integrations.Webhook,
        credentials: OAuth2Credentials | APIKeyCredentials,
    ) -> None:
        pass","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
43,"from autogpt_libs.auth.depends import requires_user
from autogpt_libs.auth.models import User
from fastapi import Depends

from backend.util.settings import Settings

settings = Settings()


def get_user_id(user: User = Depends(requires_user)) -> str:
    return user.user_id
","from autogpt_libs.auth.depends import requires_user
from autogpt_libs.auth.models import User
from fastapi import Depends

from backend.util.settings import Settings

settings = Settings()

def get_user_id(user: User = Depends(requires_user)) -> str:
    return user.user_id","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
44,"from enum import Enum
from typing import List, Literal

from pydantic import SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    BlockSecret,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
    SecretField,
)
from backend.integrations.providers import ProviderName
from backend.util.request import requests

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""medium"",
    api_key=SecretStr(""mock-medium-api-key""),
    title=""Mock Medium API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class PublishToMediumStatus(str, Enum):
    PUBLIC = ""public""
    DRAFT = ""draft""
    UNLISTED = ""unlisted""


class PublishToMediumBlock(Block):
    class Input(BlockSchema):
        author_id: BlockSecret = SecretField(
            key=""medium_author_id"",
            description=""""""The Medium AuthorID of the user. You can get this by calling the /me endpoint of the Medium API.\n\ncurl -H ""Authorization: Bearer YOUR_ACCESS_TOKEN"" https://api.medium.com/v1/me"" the response will contain the authorId field."""""",
            placeholder=""Enter the author's Medium AuthorID"",
        )
        title: str = SchemaField(
            description=""The title of your Medium post"",
            placeholder=""Enter your post title"",
        )
        content: str = SchemaField(
            description=""The main content of your Medium post"",
            placeholder=""Enter your post content"",
        )
        content_format: str = SchemaField(
            description=""The format of the content: 'html' or 'markdown'"",
            placeholder=""html"",
        )
        tags: List[str] = SchemaField(
            description=""List of tags for your Medium post (up to 5)"",
            placeholder=""['technology', 'AI', 'blogging']"",
        )
        canonical_url: str | None = SchemaField(
            default=None,
            description=""The original home of this content, if it was originally published elsewhere"",
            placeholder=""https://yourblog.com/original-post"",
        )
        publish_status: PublishToMediumStatus = SchemaField(
            description=""The publish status"",
            placeholder=PublishToMediumStatus.DRAFT,
        )
        license: str = SchemaField(
            default=""all-rights-reserved"",
            description=""The license of the post: 'all-rights-reserved', 'cc-40-by', 'cc-40-by-sa', 'cc-40-by-nd', 'cc-40-by-nc', 'cc-40-by-nc-nd', 'cc-40-by-nc-sa', 'cc-40-zero', 'public-domain'"",
            placeholder=""all-rights-reserved"",
        )
        notify_followers: bool = SchemaField(
            default=False,
            description=""Whether to notify followers that the user has published"",
            placeholder=""False"",
        )
        credentials: CredentialsMetaInput[
            Literal[ProviderName.MEDIUM], Literal[""api_key""]
        ] = CredentialsField(
            description=""The Medium integration can be used with any API key with sufficient permissions for the blocks it is used on."",
        )

    class Output(BlockSchema):
        post_id: str = SchemaField(description=""The ID of the created Medium post"")
        post_url: str = SchemaField(description=""The URL of the created Medium post"")
        published_at: int = SchemaField(
            description=""The timestamp when the post was published""
        )
        error: str = SchemaField(
            description=""Error message if the post creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""3f7b2dcb-4a78-4e3f-b0f1-88132e1b89df"",
            input_schema=PublishToMediumBlock.Input,
            output_schema=PublishToMediumBlock.Output,
            description=""Publishes a post to Medium."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""author_id"": ""1234567890abcdef"",
                ""title"": ""Test Post"",
                ""content"": ""<h1>Test Content</h1><p>This is a test post.</p>"",
                ""content_format"": ""html"",
                ""tags"": [""test"", ""automation""],
                ""license"": ""all-rights-reserved"",
                ""notify_followers"": False,
                ""publish_status"": PublishToMediumStatus.DRAFT.value,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (""post_id"", ""e6f36a""),
                (""post_url"", ""https://medium.com/@username/test-post-e6f36a""),
                (""published_at"", 1626282600),
            ],
            test_mock={
                ""create_post"": lambda *args, **kwargs: {
                    ""data"": {
                        ""id"": ""e6f36a"",
                        ""url"": ""https://medium.com/@username/test-post-e6f36a"",
                        ""authorId"": ""1234567890abcdef"",
                        ""publishedAt"": 1626282600,
                    }
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def create_post(
        self,
        api_key: SecretStr,
        author_id,
        title,
        content,
        content_format,
        tags,
        canonical_url,
        publish_status,
        license,
        notify_followers,
    ):
        headers = {
            ""Authorization"": f""Bearer {api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json"",
        }

        data = {
            ""title"": title,
            ""content"": content,
            ""contentFormat"": content_format,
            ""tags"": tags,
            ""canonicalUrl"": canonical_url,
            ""publishStatus"": publish_status,
            ""license"": license,
            ""notifyFollowers"": notify_followers,
        }

        response = requests.post(
            f""https://api.medium.com/v1/users/{author_id}/posts"",
            headers=headers,
            json=data,
        )

        return response.json()

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        response = self.create_post(
            credentials.api_key,
            input_data.author_id.get_secret_value(),
            input_data.title,
            input_data.content,
            input_data.content_format,
            input_data.tags,
            input_data.canonical_url,
            input_data.publish_status,
            input_data.license,
            input_data.notify_followers,
        )

        if ""data"" in response:
            yield ""post_id"", response[""data""][""id""]
            yield ""post_url"", response[""data""][""url""]
            yield ""published_at"", response[""data""][""publishedAt""]
        else:
            error_message = response.get(""errors"", [{}])[0].get(
                ""message"", ""Unknown error occurred""
            )
            raise RuntimeError(f""Failed to create Medium post: {error_message}"")
","from enum import Enum
from typing import List, Literal
from pydantic import SecretStr
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    BlockSecret,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
    SecretField,
)
from backend.integrations.providers import ProviderName
from backend.util.request import requests


TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""medium"",
    api_key=SecretStr(""mock-medium-api-key""),
    title=""Mock Medium API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class PublishToMediumStatus(str, Enum):
    PUBLIC = ""public""
    DRAFT = ""draft""
    UNLISTED = ""unlisted""


class PublishToMediumBlock(Block):

    class Input(BlockSchema):
        author_id: BlockSecret = SecretField(
            key=""medium_author_id"",
            description=""""""The Medium AuthorID of the user. You can get this by calling the /me endpoint of the Medium API.

curl -H \""Authorization: Bearer YOUR_ACCESS_TOKEN\"" https://api.medium.com/v1/me"" the response will contain the authorId field."""""",
            placeholder=""Enter the author's Medium AuthorID"",
        )
        title: str = SchemaField(
            description=""The title of your Medium post"",
            placeholder=""Enter your post title"",
        )
        content: str = SchemaField(
            description=""The main content of your Medium post"",
            placeholder=""Enter your post content"",
        )
        content_format: str = SchemaField(
            description=""The format of the content: 'html' or 'markdown'"",
            placeholder=""html"",
        )
        tags: List[str] = SchemaField(
            description=""List of tags for your Medium post (up to 5)"",
            placeholder=""['technology', 'AI', 'blogging']"",
        )
        canonical_url: str | None = SchemaField(
            default=None,
            description=""The original home of this content, if it was originally published elsewhere"",
            placeholder=""https://yourblog.com/original-post"",
        )
        publish_status: PublishToMediumStatus = SchemaField(
            description=""The publish status"",
            placeholder=PublishToMediumStatus.DRAFT,
        )
        license: str = SchemaField(
            default=""all-rights-reserved"",
            description=""The license of the post: 'all-rights-reserved', 'cc-40-by', 'cc-40-by-sa', 'cc-40-by-nd', 'cc-40-by-nc', 'cc-40-by-nc-nd', 'cc-40-by-nc-sa', 'cc-40-zero', 'public-domain'"",
            placeholder=""all-rights-reserved"",
        )
        notify_followers: bool = SchemaField(
            default=False,
            description=""Whether to notify followers that the user has published"",
            placeholder=""False"",
        )
        credentials: CredentialsMetaInput[
            Literal[ProviderName.MEDIUM], Literal[""api_key""]
        ] = CredentialsField(
            description=""The Medium integration can be used with any API key with sufficient permissions for the blocks it is used on."",
        )

    class Output(BlockSchema):
        post_id: str = SchemaField(description=""The ID of the created Medium post"")
        post_url: str = SchemaField(description=""The URL of the created Medium post"")
        published_at: int = SchemaField(
            description=""The timestamp when the post was published""
        )
        error: str = SchemaField(
            description=""Error message if the post creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""3f7b2dcb-4a78-4e3f-b0f1-88132e1b89df"",
            input_schema=PublishToMediumBlock.Input,
            output_schema=PublishToMediumBlock.Output,
            description=""Publishes a post to Medium."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""author_id"": ""1234567890abcdef"",
                ""title"": ""Test Post"",
                ""content"": ""<h1>Test Content</h1><p>This is a test post.</p>"",
                ""content_format"": ""html"",
                ""tags"": [""test"", ""automation""],
                ""license"": ""all-rights-reserved"",
                ""notify_followers"": False,
                ""publish_status"": PublishToMediumStatus.DRAFT.value,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (""post_id"", ""e6f36a""),
                (""post_url"", ""https://medium.com/@username/test-post-e6f36a""),
                (""published_at"", 1626282600),
            ],
            test_mock={
                ""create_post"": lambda *args, **kwargs: {
                    ""data"": {
                        ""id"": ""e6f36a"",
                        ""url"": ""https://medium.com/@username/test-post-e6f36a"",
                        ""authorId"": ""1234567890abcdef"",
                        ""publishedAt"": 1626282600,
                    }
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def create_post(
        self,
        api_key: SecretStr,
        author_id,
        title,
        content,
        content_format,
        tags,
        canonical_url,
        publish_status,
        license,
        notify_followers,
    ):
        headers = {
            ""Authorization"": f""Bearer {api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json"",
        }
        data = {
            ""title"": title,
            ""content"": content,
            ""contentFormat"": content_format,
            ""tags"": tags,
            ""canonicalUrl"": canonical_url,
            ""publishStatus"": publish_status,
            ""license"": license,
            ""notifyFollowers"": notify_followers,
        }
        response = requests.post(
            f""https://api.medium.com/v1/users/{author_id}/posts"",
            headers=headers,
            json=data,
        )
        return response.json()

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        response = self.create_post(
            credentials.api_key,
            input_data.author_id.get_secret_value(),
            input_data.title,
            input_data.content,
            input_data.content_format,
            input_data.tags,
            input_data.canonical_url,
            input_data.publish_status,
            input_data.license,
            input_data.notify_followers,
        )

        if ""data"" in response:
            yield ""post_id"", response[""data""][""id""]
            yield ""post_url"", response[""data""][""url""]
            yield ""published_at"", response[""data""][""publishedAt""]

        else:
            error_message = response.get(""errors"", [{}])[0].get(
                ""message"", ""Unknown error occurred""
            )
            raise RuntimeError(f""Failed to create Medium post: {error_message}"")","{'final_score': 99.25, 'length_similarity': 99.99, 'token_similarity': 97.87, 'structure_similarity': 99.45, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
45,"import logging
import uuid
from datetime import datetime, timezone
from typing import List, Optional

from autogpt_libs.api_key.key_manager import APIKeyManager
from prisma.enums import APIKeyPermission, APIKeyStatus
from prisma.errors import PrismaError
from prisma.models import APIKey as PrismaAPIKey
from prisma.types import (
    APIKeyCreateInput,
    APIKeyUpdateInput,
    APIKeyWhereInput,
    APIKeyWhereUniqueInput,
)
from pydantic import BaseModel

from backend.data.db import BaseDbModel

logger = logging.getLogger(__name__)


# Some basic exceptions
class APIKeyError(Exception):
    """"""Base exception for API key operations""""""

    pass


class APIKeyNotFoundError(APIKeyError):
    """"""Raised when an API key is not found""""""

    pass


class APIKeyPermissionError(APIKeyError):
    """"""Raised when there are permission issues with API key operations""""""

    pass


class APIKeyValidationError(APIKeyError):
    """"""Raised when API key validation fails""""""

    pass


class APIKey(BaseDbModel):
    name: str
    prefix: str
    key: str
    status: APIKeyStatus = APIKeyStatus.ACTIVE
    permissions: List[APIKeyPermission]
    postfix: str
    created_at: datetime
    last_used_at: Optional[datetime] = None
    revoked_at: Optional[datetime] = None
    description: Optional[str] = None
    user_id: str

    @staticmethod
    def from_db(api_key: PrismaAPIKey):
        try:
            return APIKey(
                id=api_key.id,
                name=api_key.name,
                prefix=api_key.prefix,
                postfix=api_key.postfix,
                key=api_key.key,
                status=APIKeyStatus(api_key.status),
                permissions=[APIKeyPermission(p) for p in api_key.permissions],
                created_at=api_key.createdAt,
                last_used_at=api_key.lastUsedAt,
                revoked_at=api_key.revokedAt,
                description=api_key.description,
                user_id=api_key.userId,
            )
        except Exception as e:
            logger.error(f""Error creating APIKey from db: {str(e)}"")
            raise APIKeyError(f""Failed to create API key object: {str(e)}"")


class APIKeyWithoutHash(BaseModel):
    id: str
    name: str
    prefix: str
    postfix: str
    status: APIKeyStatus
    permissions: List[APIKeyPermission]
    created_at: datetime
    last_used_at: Optional[datetime]
    revoked_at: Optional[datetime]
    description: Optional[str]
    user_id: str

    @staticmethod
    def from_db(api_key: PrismaAPIKey):
        try:
            return APIKeyWithoutHash(
                id=api_key.id,
                name=api_key.name,
                prefix=api_key.prefix,
                postfix=api_key.postfix,
                status=APIKeyStatus(api_key.status),
                permissions=[APIKeyPermission(p) for p in api_key.permissions],
                created_at=api_key.createdAt,
                last_used_at=api_key.lastUsedAt,
                revoked_at=api_key.revokedAt,
                description=api_key.description,
                user_id=api_key.userId,
            )
        except Exception as e:
            logger.error(f""Error creating APIKeyWithoutHash from db: {str(e)}"")
            raise APIKeyError(f""Failed to create API key object: {str(e)}"")


async def generate_api_key(
    name: str,
    user_id: str,
    permissions: List[APIKeyPermission],
    description: Optional[str] = None,
) -> tuple[APIKeyWithoutHash, str]:
    """"""
    Generate a new API key and store it in the database.
    Returns the API key object (without hash) and the plain text key.
    """"""
    try:
        api_manager = APIKeyManager()
        key = api_manager.generate_api_key()

        api_key = await PrismaAPIKey.prisma().create(
            data=APIKeyCreateInput(
                id=str(uuid.uuid4()),
                name=name,
                prefix=key.prefix,
                postfix=key.postfix,
                key=key.hash,
                permissions=[p for p in permissions],
                description=description,
                userId=user_id,
            )
        )

        api_key_without_hash = APIKeyWithoutHash.from_db(api_key)
        return api_key_without_hash, key.raw
    except PrismaError as e:
        logger.error(f""Database error while generating API key: {str(e)}"")
        raise APIKeyError(f""Failed to generate API key: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while generating API key: {str(e)}"")
        raise APIKeyError(f""Failed to generate API key: {str(e)}"")


async def validate_api_key(plain_text_key: str) -> Optional[APIKey]:
    """"""
    Validate an API key and return the API key object if valid.
    """"""
    try:
        if not plain_text_key.startswith(APIKeyManager.PREFIX):
            logger.warning(""Invalid API key format"")
            return None

        prefix = plain_text_key[: APIKeyManager.PREFIX_LENGTH]
        api_manager = APIKeyManager()

        api_key = await PrismaAPIKey.prisma().find_first(
            where=APIKeyWhereInput(prefix=prefix, status=(APIKeyStatus.ACTIVE))
        )

        if not api_key:
            logger.warning(f""No active API key found with prefix {prefix}"")
            return None

        is_valid = api_manager.verify_api_key(plain_text_key, api_key.key)
        if not is_valid:
            logger.warning(""API key verification failed"")
            return None

        return APIKey.from_db(api_key)
    except Exception as e:
        logger.error(f""Error validating API key: {str(e)}"")
        raise APIKeyValidationError(f""Failed to validate API key: {str(e)}"")


async def revoke_api_key(key_id: str, user_id: str) -> Optional[APIKeyWithoutHash]:
    try:
        api_key = await PrismaAPIKey.prisma().find_unique(where={""id"": key_id})

        if not api_key:
            raise APIKeyNotFoundError(f""API key with id {key_id} not found"")

        if api_key.userId != user_id:
            raise APIKeyPermissionError(
                ""You do not have permission to revoke this API key.""
            )

        where_clause: APIKeyWhereUniqueInput = {""id"": key_id}
        updated_api_key = await PrismaAPIKey.prisma().update(
            where=where_clause,
            data=APIKeyUpdateInput(
                status=APIKeyStatus.REVOKED, revokedAt=datetime.now(timezone.utc)
            ),
        )

        if updated_api_key:
            return APIKeyWithoutHash.from_db(updated_api_key)
        return None
    except (APIKeyNotFoundError, APIKeyPermissionError) as e:
        raise e
    except PrismaError as e:
        logger.error(f""Database error while revoking API key: {str(e)}"")
        raise APIKeyError(f""Failed to revoke API key: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while revoking API key: {str(e)}"")
        raise APIKeyError(f""Failed to revoke API key: {str(e)}"")


async def list_user_api_keys(user_id: str) -> List[APIKeyWithoutHash]:
    try:
        where_clause: APIKeyWhereInput = {""userId"": user_id}

        api_keys = await PrismaAPIKey.prisma().find_many(
            where=where_clause, order={""createdAt"": ""desc""}
        )

        return [APIKeyWithoutHash.from_db(key) for key in api_keys]
    except PrismaError as e:
        logger.error(f""Database error while listing API keys: {str(e)}"")
        raise APIKeyError(f""Failed to list API keys: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while listing API keys: {str(e)}"")
        raise APIKeyError(f""Failed to list API keys: {str(e)}"")


async def suspend_api_key(key_id: str, user_id: str) -> Optional[APIKeyWithoutHash]:
    try:
        api_key = await PrismaAPIKey.prisma().find_unique(where={""id"": key_id})

        if not api_key:
            raise APIKeyNotFoundError(f""API key with id {key_id} not found"")

        if api_key.userId != user_id:
            raise APIKeyPermissionError(
                ""You do not have permission to suspend this API key.""
            )

        where_clause: APIKeyWhereUniqueInput = {""id"": key_id}
        updated_api_key = await PrismaAPIKey.prisma().update(
            where=where_clause,
            data=APIKeyUpdateInput(status=APIKeyStatus.SUSPENDED),
        )

        if updated_api_key:
            return APIKeyWithoutHash.from_db(updated_api_key)
        return None
    except (APIKeyNotFoundError, APIKeyPermissionError) as e:
        raise e
    except PrismaError as e:
        logger.error(f""Database error while suspending API key: {str(e)}"")
        raise APIKeyError(f""Failed to suspend API key: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while suspending API key: {str(e)}"")
        raise APIKeyError(f""Failed to suspend API key: {str(e)}"")


def has_permission(api_key: APIKey, required_permission: APIKeyPermission) -> bool:
    try:
        return required_permission in api_key.permissions
    except Exception as e:
        logger.error(f""Error checking API key permissions: {str(e)}"")
        return False


async def get_api_key_by_id(key_id: str, user_id: str) -> Optional[APIKeyWithoutHash]:
    try:
        api_key = await PrismaAPIKey.prisma().find_first(
            where=APIKeyWhereInput(id=key_id, userId=user_id)
        )

        if not api_key:
            return None

        return APIKeyWithoutHash.from_db(api_key)
    except PrismaError as e:
        logger.error(f""Database error while getting API key: {str(e)}"")
        raise APIKeyError(f""Failed to get API key: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while getting API key: {str(e)}"")
        raise APIKeyError(f""Failed to get API key: {str(e)}"")


async def update_api_key_permissions(
    key_id: str, user_id: str, permissions: List[APIKeyPermission]
) -> Optional[APIKeyWithoutHash]:
    """"""
    Update the permissions of an API key.
    """"""
    try:
        api_key = await PrismaAPIKey.prisma().find_unique(where={""id"": key_id})

        if api_key is None:
            raise APIKeyNotFoundError(""No such API key found."")

        if api_key.userId != user_id:
            raise APIKeyPermissionError(
                ""You do not have permission to update this API key.""
            )

        where_clause: APIKeyWhereUniqueInput = {""id"": key_id}
        updated_api_key = await PrismaAPIKey.prisma().update(
            where=where_clause,
            data=APIKeyUpdateInput(permissions=permissions),
        )

        if updated_api_key:
            return APIKeyWithoutHash.from_db(updated_api_key)
        return None
    except (APIKeyNotFoundError, APIKeyPermissionError) as e:
        raise e
    except PrismaError as e:
        logger.error(f""Database error while updating API key permissions: {str(e)}"")
        raise APIKeyError(f""Failed to update API key permissions: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while updating API key permissions: {str(e)}"")
        raise APIKeyError(f""Failed to update API key permissions: {str(e)}"")
","import logging
import uuid
from datetime import datetime, timezone
from typing import List, Optional

from autogpt_libs.api_key.key_manager import APIKeyManager
from prisma.enums import APIKeyPermission, APIKeyStatus
from prisma.errors import PrismaError
from prisma.models import APIKey as PrismaAPIKey
from prisma.types import (
    APIKeyCreateInput,
    APIKeyUpdateInput,
    APIKeyWhereInput,
    APIKeyWhereUniqueInput,
)
from pydantic import BaseModel

from backend.data.db import BaseDbModel

logger = logging.getLogger(__name__)

# Some basic exceptions

class APIKeyError(Exception):
    """"""Base exception for API key operations""""""
    pass

class APIKeyNotFoundError(APIKeyError):
    """"""Raised when an API key is not found""""""
    pass

class APIKeyPermissionError(APIKeyError):
    """"""Raised when there are permission issues with API key operations""""""
    pass

class APIKeyValidationError(APIKeyError):
    """"""Raised when API key validation fails""""""

    pass

class APIKey(BaseDbModel):
    name: str
    prefix: str
    key: str
    status: APIKeyStatus = APIKeyStatus.ACTIVE
    permissions: List[APIKeyPermission]
    postfix: str
    created_at: datetime
    last_used_at: Optional[datetime] = None
    revoked_at: Optional[datetime] = None
    description: Optional[str] = None
    user_id: str

    @staticmethod
    def from_db(api_key: PrismaAPIKey):
        try:
            return APIKey(
                id=api_key.id,
                name=api_key.name,
                prefix=api_key.prefix,
                postfix=api_key.postfix,
                key=api_key.key,
                status=APIKeyStatus(api_key.status),
                permissions=[APIKeyPermission(p) for p in api_key.permissions],
                created_at=api_key.createdAt,
                last_used_at=api_key.lastUsedAt,
                revoked_at=api_key.revokedAt,
                description=api_key.description,
                user_id=api_key.userId,
            )
        except Exception as e:
            logger.error(f""Error creating APIKey from db: {str(e)}"")
            raise APIKeyError(f""Failed to create API key object: {str(e)}"")

    def from_db(api_key: PrismaAPIKey):
        try:
            return APIKey(
                id=api_key.id,
                name=api_key.name,
                prefix=api_key.prefix,
                postfix=api_key.postfix,
                key=api_key.key,
                status=APIKeyStatus(api_key.status),
                permissions=[APIKeyPermission(p) for p in api_key.permissions],
                created_at=api_key.createdAt,
                last_used_at=api_key.lastUsedAt,
                revoked_at=api_key.revokedAt,
                description=api_key.description,
                user_id=api_key.userId,
            )
        except Exception as e:
            logger.error(f""Error creating APIKey from db: {str(e)}"")
            raise APIKeyError(f""Failed to create API key object: {str(e)}"")

class APIKeyWithoutHash(BaseModel):
    id: str
    name: str
    prefix: str
    postfix: str
    status: APIKeyStatus
    permissions: List[APIKeyPermission]
    created_at: datetime
    last_used_at: Optional[datetime]
    revoked_at: Optional[datetime]
    description: Optional[str]
    user_id: str

    @staticmethod
    def from_db(api_key: PrismaAPIKey):
        try:
            return APIKeyWithoutHash(
                id=api_key.id,
                name=api_key.name,
                prefix=api_key.prefix,
                postfix=api_key.postfix,
                status=APIKeyStatus(api_key.status),
                permissions=[APIKeyPermission(p) for p in api_key.permissions],
                created_at=api_key.createdAt,
                last_used_at=api_key.lastUsedAt,
                revoked_at=api_key.revokedAt,
                description=api_key.description,
                user_id=api_key.userId,
            )
        except Exception as e:
            logger.error(f""Error creating APIKeyWithoutHash from db: {str(e)}"")
            raise APIKeyError(f""Failed to create API key object: {str(e)}"")

    def from_db(api_key: PrismaAPIKey):
        try:
            return APIKeyWithoutHash(
                id=api_key.id,
                name=api_key.name,
                prefix=api_key.prefix,
                postfix=api_key.postfix,
                status=APIKeyStatus(api_key.status),
                permissions=[APIKeyPermission(p) for p in api_key.permissions],
                created_at=api_key.createdAt,
                last_used_at=api_key.lastUsedAt,
                revoked_at=api_key.revokedAt,
                description=api_key.description,
                user_id=api_key.userId,
            )
        except Exception as e:
            logger.error(f""Error creating APIKeyWithoutHash from db: {str(e)}"")
            raise APIKeyError(f""Failed to create API key object: {str(e)}"")

async def generate_api_key(
    name: str,
    user_id: str,
    permissions: List[APIKeyPermission],
    description: Optional[str] = None,
) -> tuple[APIKeyWithoutHash, str]:
    """"""
    Generate a new API key and store it in the database.
    Returns the API key object (without hash) and the plain text key.
    """"""
    try:
        api_manager = APIKeyManager()
        key = api_manager.generate_api_key()

        api_key = await PrismaAPIKey.prisma().create(
            data=APIKeyCreateInput(
                id=str(uuid.uuid4()),
                name=name,
                prefix=key.prefix,
                postfix=key.postfix,
                key=key.hash,
                permissions=[p for p in permissions],
                description=description,
                userId=user_id,
            )
        )
        api_key_without_hash = APIKeyWithoutHash.from_db(api_key)
        return api_key_without_hash, key.raw
    except PrismaError as e:
        logger.error(f""Database error while generating API key: {str(e)}"")
        raise APIKeyError(f""Failed to generate API key: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while generating API key: {str(e)}"")
        raise APIKeyError(f""Failed to generate API key: {str(e)}"")

async def validate_api_key(plain_text_key: str) -> Optional[APIKey]:
    """"""
    Validate an API key and return the API key object if valid.
    """"""
    try:
        if not plain_text_key.startswith(APIKeyManager.PREFIX):
            logger.warning(""Invalid API key format"")
            return None
        prefix = plain_text_key[: APIKeyManager.PREFIX_LENGTH]
        api_manager = APIKeyManager()

        api_key = await PrismaAPIKey.prisma().find_first(
            where=APIKeyWhereInput(prefix=prefix, status=(APIKeyStatus.ACTIVE))
        )
        if not api_key:
            logger.warning(f""No active API key found with prefix {prefix}"")
            return None
        is_valid = api_manager.verify_api_key(plain_text_key, api_key.key)
        if not is_valid:
            logger.warning(""API key verification failed"")
            return None
        return APIKey.from_db(api_key)
    except Exception as e:
        logger.error(f""Error validating API key: {str(e)}"")
        raise APIKeyValidationError(f""Failed to validate API key: {str(e)}"")

async def revoke_api_key(key_id: str, user_id: str) -> Optional[APIKeyWithoutHash]:
    try:
        api_key = await PrismaAPIKey.prisma().find_unique(where={""id"": key_id})
        if not api_key:
            raise APIKeyNotFoundError(f""API key with id {key_id} not found"")
        if api_key.userId != user_id:
            raise APIKeyPermissionError(
                ""You do not have permission to revoke this API key.""
            )
        where_clause: APIKeyWhereUniqueInput = {""id"": key_id}
        updated_api_key = await PrismaAPIKey.prisma().update(
            where=where_clause,
            data=APIKeyUpdateInput(
                status=APIKeyStatus.REVOKED, revokedAt=datetime.now(timezone.utc)
            ),
        )
        if updated_api_key:
            return APIKeyWithoutHash.from_db(updated_api_key)
        return None
    except (APIKeyNotFoundError, APIKeyPermissionError) as e:
        raise e
    except PrismaError as e:
        logger.error(f""Database error while revoking API key: {str(e)}"")
        raise APIKeyError(f""Failed to revoke API key: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while revoking API key: {str(e)}"")
        raise APIKeyError(f""Failed to revoke API key: {str(e)}"")

async def list_user_api_keys(user_id: str) -> List[APIKeyWithoutHash]:
    try:
        where_clause: APIKeyWhereInput = {""userId"": user_id}

        api_keys = await PrismaAPIKey.prisma().find_many(
            where=where_clause, order={""createdAt"": ""desc""}
        )

        return [APIKeyWithoutHash.from_db(key) for key in api_keys]
    except PrismaError as e:
        logger.error(f""Database error while listing API keys: {str(e)}"")
        raise APIKeyError(f""Failed to list API keys: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while listing API keys: {str(e)}"")
        raise APIKeyError(f""Failed to list API keys: {str(e)}"")

async def suspend_api_key(key_id: str, user_id: str) -> Optional[APIKeyWithoutHash]:
    try:
        api_key = await PrismaAPIKey.prisma().find_unique(where={""id"": key_id})
        if not api_key:
            raise APIKeyNotFoundError(f""API key with id {key_id} not found"")
        if api_key.userId != user_id:
            raise APIKeyPermissionError(
                ""You do not have permission to suspend this API key.""
            )
        where_clause: APIKeyWhereUniqueInput = {""id"": key_id}
        updated_api_key = await PrismaAPIKey.prisma().update(
            where=where_clause,
            data=APIKeyUpdateInput(status=APIKeyStatus.SUSPENDED),
        )
        if updated_api_key:
            return APIKeyWithoutHash.from_db(updated_api_key)
        return None
    except (APIKeyNotFoundError, APIKeyPermissionError) as e:
        raise e
    except PrismaError as e:
        logger.error(f""Database error while suspending API key: {str(e)}"")
        raise APIKeyError(f""Failed to suspend API key: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while suspending API key: {str(e)}"")
        raise APIKeyError(f""Failed to suspend API key: {str(e)}"")

def has_permission(api_key: APIKey, required_permission: APIKeyPermission) -> bool:
    try:
        return required_permission in api_key.permissions
    except Exception as e:
        logger.error(f""Error checking API key permissions: {str(e)}"")
        return False

async def get_api_key_by_id(key_id: str, user_id: str) -> Optional[APIKeyWithoutHash]:
    try:
        api_key = await PrismaAPIKey.prisma().find_first(
            where=APIKeyWhereInput(id=key_id, userId=user_id)
        )
        if not api_key:
            return None
        return APIKeyWithoutHash.from_db(api_key)
    except PrismaError as e:
        logger.error(f""Database error while getting API key: {str(e)}"")
        raise APIKeyError(f""Failed to get API key: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while getting API key: {str(e)}"")
        raise APIKeyError(f""Failed to get API key: {str(e)}"")

async def update_api_key_permissions(
    key_id: str, user_id: str, permissions: List[APIKeyPermission]
) -> Optional[APIKeyWithoutHash]:
    """"""
    Update the permissions of an API key.
    """"""
    try:
        api_key = await PrismaAPIKey.prisma().find_unique(where={""id"": key_id})
        if api_key is None:
            raise APIKeyNotFoundError(""No such API key found."")
        if api_key.userId != user_id:
            raise APIKeyPermissionError(
                ""You do not have permission to update this API key.""
            )
        where_clause: APIKeyWhereUniqueInput = {""id"": key_id}
        updated_api_key = await PrismaAPIKey.prisma().update(
            where=where_clause,
            data=APIKeyUpdateInput(permissions=permissions),
        )
        if updated_api_key:
            return APIKeyWithoutHash.from_db(updated_api_key)
        return None
    except (APIKeyNotFoundError, APIKeyPermissionError) as e:
        raise e
    except PrismaError as e:
        logger.error(f""Database error while updating API key permissions: {str(e)}"")
        raise APIKeyError(f""Failed to update API key permissions: {str(e)}"")
    except Exception as e:
        logger.error(f""Unexpected error while updating API key permissions: {str(e)}"")
        raise APIKeyError(f""Failed to update API key permissions: {str(e)}"")","{'final_score': 95.04, 'length_similarity': 87.18, 'token_similarity': 100.0, 'structure_similarity': 87.75, 'function_similarity': 100, 'complexity_similarity': 94.12, 'variable_similarity': 100}",
46,"from datetime import datetime
from typing import Any, List

from backend.blocks.exa._auth import (
    ExaCredentials,
    ExaCredentialsField,
    ExaCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests

from .helpers import ContentSettings


class ExaFindSimilarBlock(Block):
    class Input(BlockSchema):
        credentials: ExaCredentialsInput = ExaCredentialsField()
        url: str = SchemaField(
            description=""The url for which you would like to find similar links""
        )
        number_of_results: int = SchemaField(
            description=""Number of results to return"",
            default=10,
            advanced=True,
        )
        include_domains: List[str] = SchemaField(
            description=""Domains to include in search"",
            default=[],
            advanced=True,
        )
        exclude_domains: List[str] = SchemaField(
            description=""Domains to exclude from search"",
            default=[],
            advanced=True,
        )
        start_crawl_date: datetime = SchemaField(
            description=""Start date for crawled content"",
        )
        end_crawl_date: datetime = SchemaField(
            description=""End date for crawled content"",
        )
        start_published_date: datetime = SchemaField(
            description=""Start date for published content"",
        )
        end_published_date: datetime = SchemaField(
            description=""End date for published content"",
        )
        include_text: List[str] = SchemaField(
            description=""Text patterns to include (max 1 string, up to 5 words)"",
            default=[],
            advanced=True,
        )
        exclude_text: List[str] = SchemaField(
            description=""Text patterns to exclude (max 1 string, up to 5 words)"",
            default=[],
            advanced=True,
        )
        contents: ContentSettings = SchemaField(
            description=""Content retrieval settings"",
            default=ContentSettings(),
            advanced=True,
        )

    class Output(BlockSchema):
        results: List[Any] = SchemaField(
            description=""List of similar documents with title, URL, published date, author, and score"",
            default=[],
        )

    def __init__(self):
        super().__init__(
            id=""5e7315d1-af61-4a0c-9350-7c868fa7438a"",
            description=""Finds similar links using Exa's findSimilar API"",
            categories={BlockCategory.SEARCH},
            input_schema=ExaFindSimilarBlock.Input,
            output_schema=ExaFindSimilarBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: ExaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://api.exa.ai/findSimilar""
        headers = {
            ""Content-Type"": ""application/json"",
            ""x-api-key"": credentials.api_key.get_secret_value(),
        }

        payload = {
            ""url"": input_data.url,
            ""numResults"": input_data.number_of_results,
            ""contents"": input_data.contents.dict(),
        }

        optional_field_mapping = {
            ""include_domains"": ""includeDomains"",
            ""exclude_domains"": ""excludeDomains"",
            ""include_text"": ""includeText"",
            ""exclude_text"": ""excludeText"",
        }

        # Add optional fields if they have values
        for input_field, api_field in optional_field_mapping.items():
            value = getattr(input_data, input_field)
            if value:  # Only add non-empty values
                payload[api_field] = value

        date_field_mapping = {
            ""start_crawl_date"": ""startCrawlDate"",
            ""end_crawl_date"": ""endCrawlDate"",
            ""start_published_date"": ""startPublishedDate"",
            ""end_published_date"": ""endPublishedDate"",
        }

        # Add dates if they exist
        for input_field, api_field in date_field_mapping.items():
            value = getattr(input_data, input_field, None)
            if value:
                payload[api_field] = value.strftime(""%Y-%m-%dT%H:%M:%S.000Z"")

        try:
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            yield ""results"", data.get(""results"", [])
        except Exception as e:
            yield ""error"", str(e)
            yield ""results"", []
","from datetime import datetime
from typing import Any, List

from backend.blocks.exa._auth import (
    ExaCredentials,
    ExaCredentialsField,
    ExaCredentialsInput,
)

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests

from .helpers import ContentSettings


class ExaFindSimilarBlock(Block):
    class Input(BlockSchema):
        credentials: ExaCredentialsInput = ExaCredentialsField()
        url: str = SchemaField(
            description=""The url for which you would like to find similar links""
        )
        number_of_results: int = SchemaField(
            description=""Number of results to return"",
            default=10,
            advanced=True,
        )
        include_domains: List[str] = SchemaField(
            description=""Domains to include in search"",
            default=[],
            advanced=True,
        )
        exclude_domains: List[str] = SchemaField(
            description=""Domains to exclude from search"",
            default=[],
            advanced=True,
        )
        start_crawl_date: datetime = SchemaField(
            description=""Start date for crawled content"",
        )
        end_crawl_date: datetime = SchemaField(
            description=""End date for crawled content"",
        )
        start_published_date: datetime = SchemaField(
            description=""Start date for published content"",
        )
        end_published_date: datetime = SchemaField(
            description=""End date for published content"",
        )
        include_text: List[str] = SchemaField(
            description=""Text patterns to include (max 1 string, up to 5 words)"",
            default=[],
            advanced=True,
        )
        exclude_text: List[str] = SchemaField(
            description=""Text patterns to exclude (max 1 string, up to 5 words)"",
            default=[],
            advanced=True,
        )
        contents: ContentSettings = SchemaField(
            description=""Content retrieval settings"",
            default=ContentSettings(),
            advanced=True,
        )

    class Output(BlockSchema):
        results: List[Any] = SchemaField(
            description=""List of similar documents with title, URL, published date, author, and score"",
            default=[],
        )

    def __init__(self):
        super().__init__(
            id=""5e7315d1-af61-4a0c-9350-7c868fa7438a"",
            description=""Finds similar links using Exa's findSimilar API"",
            categories={BlockCategory.SEARCH},
            input_schema=ExaFindSimilarBlock.Input,
            output_schema=ExaFindSimilarBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: ExaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://api.exa.ai/findSimilar""
        headers = {
            ""Content-Type"": ""application/json"",
            ""x-api-key"": credentials.api_key.get_secret_value(),
        }

        payload = {
            ""url"": input_data.url,
            ""numResults"": input_data.number_of_results,
            ""contents"": input_data.contents.dict(),
        }

        optional_field_mapping = {
            ""include_domains"": ""includeDomains"",
            ""exclude_domains"": ""excludeDomains"",
            ""include_text"": ""includeText"",
            ""exclude_text"": ""excludeText"",
        }

        # Add optional fields if they have values
        for input_field, api_field in optional_field_mapping.items():
            value = getattr(input_data, input_field)
            if value:  # Only add non-empty values
                payload[api_field] = value

        date_field_mapping = {
            ""start_crawl_date"": ""startCrawlDate"",
            ""end_crawl_date"": ""endCrawlDate"",
            ""start_published_date"": ""startPublishedDate"",
            ""end_published_date"": ""endPublishedDate"",
        }

        # Add dates if they exist
        for input_field, api_field in date_field_mapping.items():
            value = getattr(input_data, input_field, None)
            if value:
                payload[api_field] = value.strftime(""%Y-%m-%dT%H:%M:%S.000Z"")

        try:
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            yield ""results"", data.get(""results"", [])

        except Exception as e:
            yield ""error"", str(e)
            yield ""results"", []","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
47,"import asyncio
import logging
import os
import threading
from functools import wraps
from uuid import uuid4

from tenacity import retry, stop_after_attempt, wait_exponential

from backend.util.process import get_service_name

logger = logging.getLogger(__name__)


def _log_prefix(resource_name: str, conn_id: str):
    """"""
    Returns a prefix string for logging purposes.
    This needs to be called on the fly to get the current process ID & service name,
    not the parent process ID & service name.
    """"""
    return f""[PID-{os.getpid()}|THREAD-{threading.get_native_id()}|{get_service_name()}|{resource_name}-{conn_id}]""


def conn_retry(
    resource_name: str,
    action_name: str,
    max_retry: int = 5,
    multiplier: int = 1,
    min_wait: float = 1,
    max_wait: float = 30,
):
    conn_id = str(uuid4())

    def on_retry(retry_state):
        prefix = _log_prefix(resource_name, conn_id)
        exception = retry_state.outcome.exception()
        logger.error(f""{prefix} {action_name} failed: {exception}. Retrying now..."")

    def decorator(func):
        is_coroutine = asyncio.iscoroutinefunction(func)
        retry_decorator = retry(
            stop=stop_after_attempt(max_retry + 1),
            wait=wait_exponential(multiplier=multiplier, min=min_wait, max=max_wait),
            before_sleep=on_retry,
            reraise=True,
        )
        wrapped_func = retry_decorator(func)

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            prefix = _log_prefix(resource_name, conn_id)
            logger.info(f""{prefix} {action_name} started..."")
            try:
                result = wrapped_func(*args, **kwargs)
                logger.info(f""{prefix} {action_name} completed successfully."")
                return result
            except Exception as e:
                logger.error(f""{prefix} {action_name} failed after retries: {e}"")
                raise

        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            prefix = _log_prefix(resource_name, conn_id)
            logger.info(f""{prefix} {action_name} started..."")
            try:
                result = await wrapped_func(*args, **kwargs)
                logger.info(f""{prefix} {action_name} completed successfully."")
                return result
            except Exception as e:
                logger.error(f""{prefix} {action_name} failed after retries: {e}"")
                raise

        return async_wrapper if is_coroutine else sync_wrapper

    return decorator
","import asyncio
import logging
import os
import threading
from functools import wraps
from uuid import uuid4

from tenacity import retry, stop_after_attempt, wait_exponential

from backend.util.process import get_service_name

logger = logging.getLogger(__name__)

def _log_prefix(resource_name: str, conn_id: str):
    """"""
    Returns a prefix string for logging purposes.
    This needs to be called on the fly to get the current process ID & service name,
    not the parent process ID & service name.
    """"""
    return f""[PID-{os.getpid()}|THREAD-{threading.get_native_id()}|{get_service_name()}|{resource_name}-{conn_id}]""

def conn_retry(
    resource_name: str,
    action_name: str,
    max_retry: int = 5,
    multiplier: int = 1,
    min_wait: float = 1,
    max_wait: float = 30,
):
    conn_id = str(uuid4())
    
    def on_retry(retry_state):
        prefix = _log_prefix(resource_name, conn_id)
        exception = retry_state.outcome.exception()
        logger.error(f""{prefix} {action_name} failed: {exception}. Retrying now..."")

    def decorator(func):
        is_coroutine = asyncio.iscoroutinefunction(func)
        retry_decorator = retry(
            stop=stop_after_attempt(max_retry + 1),
            wait=wait_exponential(multiplier=multiplier, min=min_wait, max=max_wait),
            before_sleep=on_retry,
            reraise=True,
        )

        wrapped_func = retry_decorator(func)

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            prefix = _log_prefix(resource_name, conn_id)
            logger.info(f""{prefix} {action_name} started..."")
            try:
                result = wrapped_func(*args, **kwargs)
                logger.info(f""{prefix} {action_name} completed successfully."")
                return result
            except Exception as e:
                logger.error(f""{prefix} {action_name} failed after retries: {e}"")
                raise

        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            prefix = _log_prefix(resource_name, conn_id)
            logger.info(f""{prefix} {action_name} started..."")
            try:
                result = await wrapped_func(*args, **kwargs)
                logger.info(f""{prefix} {action_name} completed successfully."")
                return result
            except Exception as e:
                logger.error(f""{prefix} {action_name} failed after retries: {e}"")
                raise

        return async_wrapper if is_coroutine else sync_wrapper

    return decorator","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
48,"def match_word_pattern(pattern: str, input_string: str) -> bool:
    """"""
    Determine if a given pattern matches a string using backtracking.

    pattern: The pattern to match.
    input_string: The string to match against the pattern.
    return: True if the pattern matches the string, False otherwise.

    >>> match_word_pattern(""aba"", ""GraphTreesGraph"")
    True

    >>> match_word_pattern(""xyx"", ""PythonRubyPython"")
    True

    >>> match_word_pattern(""GG"", ""PythonJavaPython"")
    False
    """"""

    def backtrack(pattern_index: int, str_index: int) -> bool:
        """"""
        >>> backtrack(0, 0)
        True

        >>> backtrack(0, 1)
        True

        >>> backtrack(0, 4)
        False
        """"""
        if pattern_index == len(pattern) and str_index == len(input_string):
            return True
        if pattern_index == len(pattern) or str_index == len(input_string):
            return False
        char = pattern[pattern_index]
        if char in pattern_map:
            mapped_str = pattern_map[char]
            if input_string.startswith(mapped_str, str_index):
                return backtrack(pattern_index + 1, str_index + len(mapped_str))
            else:
                return False
        for end in range(str_index + 1, len(input_string) + 1):
            substr = input_string[str_index:end]
            if substr in str_map:
                continue
            pattern_map[char] = substr
            str_map[substr] = char
            if backtrack(pattern_index + 1, end):
                return True
            del pattern_map[char]
            del str_map[substr]
        return False

    pattern_map: dict[str, str] = {}
    str_map: dict[str, str] = {}
    return backtrack(0, 0)


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","def match_word_pattern(pattern: str, input_string: str) -> bool:
    """"""
    Determine if a given pattern matches a string using backtracking.

    pattern: The pattern to match.
    input_string: The string to match against the pattern.
    return: True if the pattern matches the string, False otherwise.

    >>> match_word_pattern(""aba"", ""GraphTreesGraph"")
    True

    >>> match_word_pattern(""xyx"", ""PythonRubyPython"")
    True

    >>> match_word_pattern(""GG"", ""PythonJavaPython"")
    False
    """"""

    def backtrack(pattern_index: int, str_index: int) -> bool:
        """"""
        >>> backtrack(0, 0)
        True

        >>> backtrack(0, 1)
        True

        >>> backtrack(0, 4)
        False
        """"""
        if pattern_index == len(pattern) and str_index == len(input_string):
            return True

        if pattern_index == len(pattern) or str_index == len(input_string):
            return False

        char = pattern[pattern_index]
        if char in pattern_map:
            mapped_str = pattern_map[char]
            if input_string.startswith(mapped_str, str_index):
                return backtrack(pattern_index + 1, str_index + len(mapped_str))
            else:
                return False

        for end in range(str_index + 1, len(input_string) + 1):
            substr = input_string[str_index:end]
            if substr in str_map:
                continue

            pattern_map[char] = substr
            str_map[substr] = char
            if backtrack(pattern_index + 1, end):
                return True

            del pattern_map[char]
            del str_map[substr]

        return False

    pattern_map: dict[str, str] = {}
    str_map: dict[str, str] = {}

    return backtrack(0, 0)

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
49,"from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import ContributorDetails, SchemaField


class ReadCsvBlock(Block):
    class Input(BlockSchema):
        contents: str = SchemaField(
            description=""The contents of the CSV file to read"",
            placeholder=""a, b, c\n1,2,3\n4,5,6"",
        )
        delimiter: str = SchemaField(
            description=""The delimiter used in the CSV file"",
            default="","",
        )
        quotechar: str = SchemaField(
            description=""The character used to quote fields"",
            default='""',
        )
        escapechar: str = SchemaField(
            description=""The character used to escape the delimiter"",
            default=""\\"",
        )
        has_header: bool = SchemaField(
            description=""Whether the CSV file has a header row"",
            default=True,
        )
        skip_rows: int = SchemaField(
            description=""The number of rows to skip from the start of the file"",
            default=0,
        )
        strip: bool = SchemaField(
            description=""Whether to strip whitespace from the values"",
            default=True,
        )
        skip_columns: list[str] = SchemaField(
            description=""The columns to skip from the start of the row"",
            default=[],
        )

    class Output(BlockSchema):
        row: dict[str, str] = SchemaField(
            description=""The data produced from each row in the CSV file""
        )
        all_data: list[dict[str, str]] = SchemaField(
            description=""All the data in the CSV file as a list of rows""
        )

    def __init__(self):
        super().__init__(
            id=""acf7625e-d2cb-4941-bfeb-2819fc6fc015"",
            input_schema=ReadCsvBlock.Input,
            output_schema=ReadCsvBlock.Output,
            description=""Reads a CSV file and outputs the data as a list of dictionaries and individual rows via rows."",
            contributors=[ContributorDetails(name=""Nicholas Tindle"")],
            categories={BlockCategory.TEXT, BlockCategory.DATA},
            test_input={
                ""contents"": ""a, b, c\n1,2,3\n4,5,6"",
            },
            test_output=[
                (""row"", {""a"": ""1"", ""b"": ""2"", ""c"": ""3""}),
                (""row"", {""a"": ""4"", ""b"": ""5"", ""c"": ""6""}),
                (
                    ""all_data"",
                    [
                        {""a"": ""1"", ""b"": ""2"", ""c"": ""3""},
                        {""a"": ""4"", ""b"": ""5"", ""c"": ""6""},
                    ],
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        import csv
        from io import StringIO

        csv_file = StringIO(input_data.contents)
        reader = csv.reader(
            csv_file,
            delimiter=input_data.delimiter,
            quotechar=input_data.quotechar,
            escapechar=input_data.escapechar,
        )

        header = None
        if input_data.has_header:
            header = next(reader)
            if input_data.strip:
                header = [h.strip() for h in header]

        for _ in range(input_data.skip_rows):
            next(reader)

        def process_row(row):
            data = {}
            for i, value in enumerate(row):
                if i not in input_data.skip_columns:
                    if input_data.has_header and header:
                        data[header[i]] = value.strip() if input_data.strip else value
                    else:
                        data[str(i)] = value.strip() if input_data.strip else value
            return data

        all_data = []
        for row in reader:
            processed_row = process_row(row)
            all_data.append(processed_row)
            yield ""row"", processed_row

        yield ""all_data"", all_data
","from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import ContributorDetails, SchemaField


class ReadCsvBlock(Block):
    class Input(BlockSchema):
        contents: str = SchemaField(
            description=""The contents of the CSV file to read"",
            placeholder=""a, b, c\n1,2,3\n4,5,6"",
        )
        delimiter: str = SchemaField(
            description=""The delimiter used in the CSV file"",
            default="","",
        )
        quotechar: str = SchemaField(
            description=""The character used to quote fields"",
            default='""',
        )
        escapechar: str = SchemaField(
            description=""The character used to escape the delimiter"",
            default=""\\"",
        )
        has_header: bool = SchemaField(
            description=""Whether the CSV file has a header row"",
            default=True,
        )
        skip_rows: int = SchemaField(
            description=""The number of rows to skip from the start of the file"",
            default=0,
        )
        strip: bool = SchemaField(
            description=""Whether to strip whitespace from the values"",
            default=True,
        )
        skip_columns: list[str] = SchemaField(
            description=""The columns to skip from the start of the row"",
            default=[],
        )

    class Output(BlockSchema):
        row: dict[str, str] = SchemaField(
            description=""The data produced from each row in the CSV file""
        )
        all_data: list[dict[str, str]] = SchemaField(
            description=""All the data in the CSV file as a list of rows""
        )

    def __init__(self):
        super().__init__(
            id=""acf7625e-d2cb-4941-bfeb-2819fc6fc015"",
            input_schema=ReadCsvBlock.Input,
            output_schema=ReadCsvBlock.Output,
            description=""Reads a CSV file and outputs the data as a list of dictionaries and individual rows via rows."",
            contributors=[ContributorDetails(name=""Nicholas Tindle"")],
            categories={BlockCategory.TEXT, BlockCategory.DATA},
            test_input={
                ""contents"": ""a, b, c\n1,2,3\n4,5,6"",
            },
            test_output=[
                (""row"", {""a"": ""1"", ""b"": ""2"", ""c"": ""3""}),
                (""row"", {""a"": ""4"", ""b"": ""5"", ""c"": ""6""}),
                (
                    ""all_data"",
                    [
                        {""a"": ""1"", ""b"": ""2"", ""c"": ""3""},
                        {""a"": ""4"", ""b"": ""5"", ""c"": ""6""},
                    ],
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        import csv
        from io import StringIO

        csv_file = StringIO(input_data.contents)
        reader = csv.reader(
            csv_file,
            delimiter=input_data.delimiter,
            quotechar=input_data.quotechar,
            escapechar=input_data.escapechar,
        )

        header = None
        if input_data.has_header:
            header = next(reader)
            if input_data.strip:
                header = [h.strip() for h in header]

        for _ in range(input_data.skip_rows):
            next(reader)

        def process_row(row):
            data = {}
            for i, value in enumerate(row):
                if i not in input_data.skip_columns:
                    if input_data.has_header and header:
                        data[header[i]] = value.strip() if input_data.strip else value
                    else:
                        data[str(i)] = value.strip() if input_data.strip else value
            return data

        all_data = []
        for row in reader:
            processed_row = process_row(row)
            all_data.append(processed_row)
            yield ""row"", processed_row

        yield ""all_data"", all_data","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
50,"from enum import Enum
from typing import Literal

from pydantic import BaseModel, SecretStr

from backend.data.model import APIKeyCredentials, CredentialsField, CredentialsMetaInput
from backend.integrations.providers import ProviderName

Slant3DCredentialsInput = CredentialsMetaInput[
    Literal[ProviderName.SLANT3D], Literal[""api_key""]
]


def Slant3DCredentialsField() -> Slant3DCredentialsInput:
    return CredentialsField(description=""Slant3D API key for authentication"")


TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""slant3d"",
    api_key=SecretStr(""mock-slant3d-api-key""),
    title=""Mock Slant3D API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.title,
}


class CustomerDetails(BaseModel):
    name: str
    email: str
    phone: str
    address: str
    city: str
    state: str
    zip: str
    country_iso: str = ""US""
    is_residential: bool = True


class Color(Enum):
    WHITE = ""white""
    BLACK = ""black""


class Profile(Enum):
    PLA = ""PLA""
    PETG = ""PETG""


class OrderItem(BaseModel):
    # filename: str
    file_url: str
    quantity: str  # String as per API spec
    color: Color = Color.WHITE
    profile: Profile = Profile.PLA
    # image_url: str = """"
    # sku: str = """"


class Filament(BaseModel):
    filament: str
    hexColor: str
    colorTag: str
    profile: str
","from enum import Enum
from typing import Literal

from pydantic import BaseModel, SecretStr

from backend.data.model import APIKeyCredentials, CredentialsField, CredentialsMetaInput
from backend.integrations.providers import ProviderName

Slant3DCredentialsInput = CredentialsMetaInput[
    Literal[ProviderName.SLANT3D], Literal[""api_key""]
]

def Slant3DCredentialsField() -> Slant3DCredentialsInput:
    return CredentialsField(description=""Slant3D API key for authentication"")

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""slant3d"",
    api_key=SecretStr(""mock-slant3d-api-key""),
    title=""Mock Slant3D API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.title,
}

class CustomerDetails(BaseModel):
    name: str
    email: str
    phone: str
    address: str
    city: str
    state: str
    zip: str
    country_iso: str = ""US""
    is_residential: bool = True

class Color(Enum):
    WHITE = ""white""
    BLACK = ""black""

class Profile(Enum):
    PLA = ""PLA""
    PETG = ""PETG""

class OrderItem(BaseModel):
    # filename: str
    file_url: str
    quantity: str  # String as per API spec
    color: Color = Color.WHITE
    profile: Profile = Profile.PLA
    # image_url: str = """"
    # sku: str = """"

class Filament(BaseModel):
    filament: str
    hexColor: str
    colorTag: str
    profile: str","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
51,"""""""
In the Combination Sum problem, we are given a list consisting of distinct integers.
We need to find all the combinations whose sum equals to target given.
We can use an element more than one.

Time complexity(Average Case): O(n!)

Constraints:
1 <= candidates.length <= 30
2 <= candidates[i] <= 40
All elements of candidates are distinct.
1 <= target <= 40
""""""


def backtrack(
    candidates: list, path: list, answer: list, target: int, previous_index: int
) -> None:
    """"""
    A recursive function that searches for possible combinations. Backtracks in case
    of a bigger current combination value than the target value.

    Parameters
    ----------
    previous_index: Last index from the previous search
    target: The value we need to obtain by summing our integers in the path list.
    answer: A list of possible combinations
    path: Current combination
    candidates: A list of integers we can use.
    """"""
    if target == 0:
        answer.append(path.copy())
    else:
        for index in range(previous_index, len(candidates)):
            if target >= candidates[index]:
                path.append(candidates[index])
                backtrack(candidates, path, answer, target - candidates[index], index)
                path.pop(len(path) - 1)


def combination_sum(candidates: list, target: int) -> list:
    """"""
    >>> combination_sum([2, 3, 5], 8)
    [[2, 2, 2, 2], [2, 3, 3], [3, 5]]
    >>> combination_sum([2, 3, 6, 7], 7)
    [[2, 2, 3], [7]]
    >>> combination_sum([-8, 2.3, 0], 1)
    Traceback (most recent call last):
        ...
    RecursionError: maximum recursion depth exceeded
    """"""
    path = []  # type: list[int]
    answer = []  # type: list[int]
    backtrack(candidates, path, answer, target, 0)
    return answer


def main() -> None:
    print(combination_sum([-8, 2.3, 0], 1))


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
    main()
","""""""
In the Combination Sum problem, we are given a list consisting of distinct integers.
We need to find all the combinations whose sum equals to target given.
We can use an element more than one.

Time complexity(Average Case): O(n!)

Constraints:
1 <= candidates.length <= 30
2 <= candidates[i] <= 40
All elements of candidates are distinct.
1 <= target <= 40
""""""

def backtrack(
    candidates: list, path: list, answer: list, target: int, previous_index: int
) -> None:
    """"""
    A recursive function that searches for possible combinations. Backtracks in case
    of a bigger current combination value than the target value.

    Parameters
    ----------
    previous_index: Last index from the previous search
    target: The value we need to obtain by summing our integers in the path list.
    answer: A list of possible combinations
    path: Current combination
    candidates: A list of integers we can use.
    """"""
    if target == 0:
        answer.append(path.copy())
    else:
        for index in range(previous_index, len(candidates)):
            if target >= candidates[index]:
                path.append(candidates[index])
                backtrack(candidates, path, answer, target - candidates[index], index)
                path.pop(len(path) - 1)

def combination_sum(candidates: list, target: int) -> list:
    """"""
    >>> combination_sum([2, 3, 5], 8)
    [[2, 2, 2, 2], [2, 3, 3], [3, 5]]
    >>> combination_sum([2, 3, 6, 7], 7)
    [[2, 2, 3], [7]]
    >>> combination_sum([-8, 2.3, 0], 1)
    Traceback (most recent call last):
        ...
    RecursionError: maximum recursion depth exceeded
    """"""
    path = []  # type: list[int]
    answer = []  # type: list[int]
    backtrack(candidates, path, answer, target, 0)
    return answer

def main() -> None:
    print(combination_sum([-8, 2.3, 0], 1))

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
    main()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
52,"import hashlib
import secrets
from typing import NamedTuple


class APIKeyContainer(NamedTuple):
    """"""Container for API key parts.""""""

    raw: str
    prefix: str
    postfix: str
    hash: str


class APIKeyManager:
    PREFIX: str = ""agpt_""
    PREFIX_LENGTH: int = 8
    POSTFIX_LENGTH: int = 8

    def generate_api_key(self) -> APIKeyContainer:
        """"""Generate a new API key with all its parts.""""""
        raw_key = f""{self.PREFIX}{secrets.token_urlsafe(32)}""
        return APIKeyContainer(
            raw=raw_key,
            prefix=raw_key[: self.PREFIX_LENGTH],
            postfix=raw_key[-self.POSTFIX_LENGTH :],
            hash=hashlib.sha256(raw_key.encode()).hexdigest(),
        )

    def verify_api_key(self, provided_key: str, stored_hash: str) -> bool:
        """"""Verify if a provided API key matches the stored hash.""""""
        if not provided_key.startswith(self.PREFIX):
            return False
        return hashlib.sha256(provided_key.encode()).hexdigest() == stored_hash
","import hashlib
import secrets
from typing import NamedTuple

class APIKeyContainer(NamedTuple):
    """"""Container for API key parts.""""""
    raw: str
    prefix: str
    postfix: str
    hash: str

class APIKeyManager:
    PREFIX: str = ""agpt_""
    PREFIX_LENGTH: int = 8
    POSTFIX_LENGTH: int = 8

    def generate_api_key(self) -> APIKeyContainer:
        """"""Generate a new API key with all its parts.""""""
        raw_key = f""{self.PREFIX}{secrets.token_urlsafe(32)}""
        return APIKeyContainer(
            raw=raw_key,
            prefix=raw_key[: self.PREFIX_LENGTH],
            postfix=raw_key[-self.POSTFIX_LENGTH :],
            hash=hashlib.sha256(raw_key.encode()).hexdigest(),
        )

    def verify_api_key(self, provided_key: str, stored_hash: str) -> bool:
        """"""Verify if a provided API key matches the stored hash.""""""
        if not provided_key.startswith(self.PREFIX):
            return False
        return hashlib.sha256(provided_key.encode()).hexdigest() == stored_hash","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
53,"import logging

import pytest
from prisma.models import User

from backend.blocks.basic import FindInDictionaryBlock, StoreValueBlock
from backend.blocks.maths import CalculatorBlock, Operation
from backend.data import execution, graph
from backend.server.model import CreateGraph
from backend.server.rest_api import AgentServer
from backend.usecases.sample import create_test_graph, create_test_user
from backend.util.test import SpinTestServer, wait_execution

logger = logging.getLogger(__name__)


async def create_graph(s: SpinTestServer, g: graph.Graph, u: User) -> graph.Graph:
    logger.info(f""Creating graph for user {u.id}"")
    return await s.agent_server.test_create_graph(CreateGraph(graph=g), u.id)


async def execute_graph(
    agent_server: AgentServer,
    test_graph: graph.Graph,
    test_user: User,
    input_data: dict,
    num_execs: int = 4,
) -> str:
    logger.info(f""Executing graph {test_graph.id} for user {test_user.id}"")
    logger.info(f""Input data: {input_data}"")

    # --- Test adding new executions --- #
    response = await agent_server.test_execute_graph(
        test_graph.id, input_data, test_user.id
    )
    graph_exec_id = response[""id""]
    logger.info(f""Created execution with ID: {graph_exec_id}"")

    # Execution queue should be empty
    logger.info(""Waiting for execution to complete..."")
    result = await wait_execution(test_user.id, test_graph.id, graph_exec_id)
    logger.info(f""Execution completed with {len(result)} results"")
    assert result and len(result) == num_execs
    return graph_exec_id


async def assert_sample_graph_executions(
    agent_server: AgentServer,
    test_graph: graph.Graph,
    test_user: User,
    graph_exec_id: str,
):
    logger.info(f""Checking execution results for graph {test_graph.id}"")
    executions = await agent_server.test_get_graph_run_node_execution_results(
        test_graph.id,
        graph_exec_id,
        test_user.id,
    )

    output_list = [{""result"": [""Hello""]}, {""result"": [""World""]}]
    input_list = [
        {
            ""name"": ""input_1"",
            ""value"": ""Hello"",
        },
        {
            ""name"": ""input_2"",
            ""value"": ""World"",
        },
    ]

    # Executing StoreValueBlock
    exec = executions[0]
    logger.info(f""Checking first StoreValueBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert (
        exec.output_data in output_list
    ), f""Output data: {exec.output_data} and {output_list}""
    assert (
        exec.input_data in input_list
    ), f""Input data: {exec.input_data} and {input_list}""
    assert exec.node_id in [test_graph.nodes[0].id, test_graph.nodes[1].id]

    # Executing StoreValueBlock
    exec = executions[1]
    logger.info(f""Checking second StoreValueBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert (
        exec.output_data in output_list
    ), f""Output data: {exec.output_data} and {output_list}""
    assert (
        exec.input_data in input_list
    ), f""Input data: {exec.input_data} and {input_list}""
    assert exec.node_id in [test_graph.nodes[0].id, test_graph.nodes[1].id]

    # Executing FillTextTemplateBlock
    exec = executions[2]
    logger.info(f""Checking FillTextTemplateBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert exec.output_data == {""output"": [""Hello, World!!!""]}
    assert exec.input_data == {
        ""format"": ""{a}, {b}{c}"",
        ""values"": {""a"": ""Hello"", ""b"": ""World"", ""c"": ""!!!""},
        ""values_#_a"": ""Hello"",
        ""values_#_b"": ""World"",
        ""values_#_c"": ""!!!"",
    }
    assert exec.node_id == test_graph.nodes[2].id

    # Executing PrintToConsoleBlock
    exec = executions[3]
    logger.info(f""Checking PrintToConsoleBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert exec.output_data == {""status"": [""printed""]}
    assert exec.input_data == {""text"": ""Hello, World!!!""}
    assert exec.node_id == test_graph.nodes[3].id


@pytest.mark.asyncio(scope=""session"")
async def test_agent_execution(server: SpinTestServer):
    logger.info(""Starting test_agent_execution"")
    test_user = await create_test_user()
    test_graph = await create_graph(server, create_test_graph(), test_user)
    data = {""input_1"": ""Hello"", ""input_2"": ""World""}
    graph_exec_id = await execute_graph(
        server.agent_server,
        test_graph,
        test_user,
        data,
        4,
    )
    await assert_sample_graph_executions(
        server.agent_server, test_graph, test_user, graph_exec_id
    )
    logger.info(""Completed test_agent_execution"")


@pytest.mark.asyncio(scope=""session"")
async def test_input_pin_always_waited(server: SpinTestServer):
    """"""
    This test is asserting that the input pin should always be waited for the execution,
    even when default value on that pin is defined, the value has to be ignored.

    Test scenario:
    StoreValueBlock1
                \\ input
                     >------- FindInDictionaryBlock | input_default: key: """", input: {}
                // key
    StoreValueBlock2
    """"""
    logger.info(""Starting test_input_pin_always_waited"")
    nodes = [
        graph.Node(
            block_id=StoreValueBlock().id,
            input_default={""input"": {""key1"": ""value1"", ""key2"": ""value2""}},
        ),
        graph.Node(
            block_id=StoreValueBlock().id,
            input_default={""input"": ""key2""},
        ),
        graph.Node(
            block_id=FindInDictionaryBlock().id,
            input_default={""key"": """", ""input"": {}},
        ),
    ]
    links = [
        graph.Link(
            source_id=nodes[0].id,
            sink_id=nodes[2].id,
            source_name=""output"",
            sink_name=""input"",
        ),
        graph.Link(
            source_id=nodes[1].id,
            sink_id=nodes[2].id,
            source_name=""output"",
            sink_name=""key"",
        ),
    ]
    test_graph = graph.Graph(
        name=""TestGraph"",
        description=""Test graph"",
        nodes=nodes,
        links=links,
    )
    test_user = await create_test_user()
    test_graph = await create_graph(server, test_graph, test_user)
    graph_exec_id = await execute_graph(
        server.agent_server, test_graph, test_user, {}, 3
    )

    logger.info(""Checking execution results"")
    executions = await server.agent_server.test_get_graph_run_node_execution_results(
        test_graph.id, graph_exec_id, test_user.id
    )
    assert len(executions) == 3
    # FindInDictionaryBlock should wait for the input pin to be provided,
    # Hence executing extraction of ""key"" from {""key1"": ""value1"", ""key2"": ""value2""}
    assert executions[2].status == execution.ExecutionStatus.COMPLETED
    assert executions[2].output_data == {""output"": [""value2""]}
    logger.info(""Completed test_input_pin_always_waited"")


@pytest.mark.asyncio(scope=""session"")
async def test_static_input_link_on_graph(server: SpinTestServer):
    """"""
    This test is asserting the behaviour of static input link, e.g: reusable input link.

    Test scenario:
    *StoreValueBlock1*===a=========\\
    *StoreValueBlock2*===a=====\\  ||
    *StoreValueBlock3*===a===*MathBlock*====b / static====*StoreValueBlock5*
    *StoreValueBlock4*=========================================//

    In this test, there will be three input waiting in the MathBlock input pin `a`.
    And later, another output is produced on input pin `b`, which is a static link,
    this input will complete the input of those three incomplete executions.
    """"""
    logger.info(""Starting test_static_input_link_on_graph"")
    nodes = [
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 4}),  # a
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 4}),  # a
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 4}),  # a
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 5}),  # b
        graph.Node(block_id=StoreValueBlock().id),
        graph.Node(
            block_id=CalculatorBlock().id,
            input_default={""operation"": Operation.ADD.value},
        ),
    ]
    links = [
        graph.Link(
            source_id=nodes[0].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""a"",
        ),
        graph.Link(
            source_id=nodes[1].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""a"",
        ),
        graph.Link(
            source_id=nodes[2].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""a"",
        ),
        graph.Link(
            source_id=nodes[3].id,
            sink_id=nodes[4].id,
            source_name=""output"",
            sink_name=""input"",
        ),
        graph.Link(
            source_id=nodes[4].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""b"",
            is_static=True,  # This is the static link to test.
        ),
    ]
    test_graph = graph.Graph(
        name=""TestGraph"",
        description=""Test graph"",
        nodes=nodes,
        links=links,
    )
    test_user = await create_test_user()
    test_graph = await create_graph(server, test_graph, test_user)
    graph_exec_id = await execute_graph(
        server.agent_server, test_graph, test_user, {}, 8
    )
    logger.info(""Checking execution results"")
    executions = await server.agent_server.test_get_graph_run_node_execution_results(
        test_graph.id, graph_exec_id, test_user.id
    )
    assert len(executions) == 8
    # The last 3 executions will be a+b=4+5=9
    for i, exec_data in enumerate(executions[-3:]):
        logger.info(f""Checking execution {i+1} of last 3: {exec_data}"")
        assert exec_data.status == execution.ExecutionStatus.COMPLETED
        assert exec_data.output_data == {""result"": [9]}
    logger.info(""Completed test_static_input_link_on_graph"")
","import logging
import pytest
from prisma.models import User
from backend.blocks.basic import FindInDictionaryBlock, StoreValueBlock
from backend.blocks.maths import CalculatorBlock, Operation
from backend.data import execution, graph
from backend.server.model import CreateGraph
from backend.server.rest_api import AgentServer
from backend.usecases.sample import create_test_graph, create_test_user
from backend.util.test import SpinTestServer, wait_execution

logger = logging.getLogger(__name__)

async def create_graph(s: SpinTestServer, g: graph.Graph, u: User) -> graph.Graph:
    logger.info(f""Creating graph for user {u.id}"")
    return await s.agent_server.test_create_graph(CreateGraph(graph=g), u.id)

async def execute_graph(
    agent_server: AgentServer,
    test_graph: graph.Graph,
    test_user: User,
    input_data: dict,
    num_execs: int = 4,
) -> str:
    logger.info(f""Executing graph {test_graph.id} for user {test_user.id}"")
    logger.info(f""Input data: {input_data}"")
    # --- Test adding new executions --- #
    response = await agent_server.test_execute_graph(
        test_graph.id, input_data, test_user.id
    )
    graph_exec_id = response[""id""]
    logger.info(f""Created execution with ID: {graph_exec_id}"")
    # Execution queue should be empty
    logger.info(""Waiting for execution to complete..."")
    result = await wait_execution(test_user.id, test_graph.id, graph_exec_id)
    logger.info(f""Execution completed with {len(result)} results"")
    assert result and len(result) == num_execs
    return graph_exec_id

async def assert_sample_graph_executions(
    agent_server: AgentServer,
    test_graph: graph.Graph,
    test_user: User,
    graph_exec_id: str,
):
    logger.info(f""Checking execution results for graph {test_graph.id}"")
    executions = await agent_server.test_get_graph_run_node_execution_results(
        test_graph.id,
        graph_exec_id,
        test_user.id,
    )
    output_list = [{""result"": [""Hello""]}, {""result"": [""World""]}]
    input_list = [
        {
            ""name"": ""input_1"",
            ""value"": ""Hello"",
        },
        {
            ""name"": ""input_2"",
            ""value"": ""World"",
        },
    ]
    # Executing StoreValueBlock
    exec = executions[0]
    logger.info(f""Checking first StoreValueBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert (
        exec.output_data in output_list
    ), f""Output data: {exec.output_data} and {output_list}""
    assert (
        exec.input_data in input_list
    ), f""Input data: {exec.input_data} and {input_list}""
    assert exec.node_id in [test_graph.nodes[0].id, test_graph.nodes[1].id]
    # Executing StoreValueBlock
    exec = executions[1]
    logger.info(f""Checking second StoreValueBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert (
        exec.output_data in output_list
    ), f""Output data: {exec.output_data} and {output_list}""
    assert (
        exec.input_data in input_list
    ), f""Input data: {exec.input_data} and {input_list}""
    assert exec.node_id in [test_graph.nodes[0].id, test_graph.nodes[1].id]
    # Executing FillTextTemplateBlock
    exec = executions[2]
    logger.info(f""Checking FillTextTemplateBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert exec.output_data == {""output"": [""Hello, World!!!""]}
    assert exec.input_data == {
        ""format"": ""{a}, {b}{c}"",
        ""values"": {""a"": ""Hello"", ""b"": ""World"", ""c"": ""!!!""},
        ""values_#_a"": ""Hello"",
        ""values_#_b"": ""World"",
        ""values_#_c"": ""!!!"",
    }
    assert exec.node_id == test_graph.nodes[2].id
    # Executing PrintToConsoleBlock
    exec = executions[3]
    logger.info(f""Checking PrintToConsoleBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert exec.output_data == {""status"": [""printed""]}
    assert exec.input_data == {""text"": ""Hello, World!!!""}
    assert exec.node_id == test_graph.nodes[3].id

@pytest.mark.asyncio(scope=""session"")
async def test_agent_execution(server: SpinTestServer):
    logger.info(""Starting test_agent_execution"")
    test_user = await create_test_user()
    test_graph = await create_graph(server, create_test_graph(), test_user)
    data = {""input_1"": ""Hello"", ""input_2"": ""World""}
    graph_exec_id = await execute_graph(
        server.agent_server,
        test_graph,
        test_user,
        data,
        4,
    )
    await assert_sample_graph_executions(
        server.agent_server, test_graph, test_user, graph_exec_id
    )
    logger.info(""Completed test_agent_execution"")

@pytest.mark.asyncio(scope=""session"")
async def test_input_pin_always_waited(server: SpinTestServer):
    """"""
    This test is asserting that the input pin should always be waited for the execution,
    even when default value on that pin is defined, the value has to be ignored.

    Test scenario:
    StoreValueBlock1
                \\ input
                     >------- FindInDictionaryBlock | input_default: key: """", input: {}
                // key
    StoreValueBlock2
    """"""
    logger.info(""Starting test_input_pin_always_waited"")
    nodes = [
        graph.Node(
            block_id=StoreValueBlock().id,
            input_default={""input"": {""key1"": ""value1"", ""key2"": ""value2""}},
        ),
        graph.Node(
            block_id=StoreValueBlock().id,
            input_default={""input"": ""key2""},
        ),
        graph.Node(
            block_id=FindInDictionaryBlock().id,
            input_default={""key"": """", ""input"": {}},
        ),
    ]
    links = [
        graph.Link(
            source_id=nodes[0].id,
            sink_id=nodes[2].id,
            source_name=""output"",
            sink_name=""input"",
        ),
        graph.Link(
            source_id=nodes[1].id,
            sink_id=nodes[2].id,
            source_name=""output"",
            sink_name=""key"",
        ),
    ]
    test_graph = graph.Graph(
        name=""TestGraph"",
        description=""Test graph"",
        nodes=nodes,
        links=links,
    )
    test_user = await create_test_user()
    test_graph = await create_graph(server, test_graph, test_user)
    graph_exec_id = await execute_graph(
        server.agent_server, test_graph, test_user, {}, 3
    )

    logger.info(""Checking execution results"")
    executions = await server.agent_server.test_get_graph_run_node_execution_results(
        test_graph.id, graph_exec_id, test_user.id
    )
    assert len(executions) == 3
    # FindInDictionaryBlock should wait for the input pin to be provided,
    # Hence executing extraction of ""key"" from {""key1"": ""value1"", ""key2"": ""value2""}
    assert executions[2].status == execution.ExecutionStatus.COMPLETED
    assert executions[2].output_data == {""output"": [""value2""]}
    logger.info(""Completed test_input_pin_always_waited"")

@pytest.mark.asyncio(scope=""session"")
async def test_static_input_link_on_graph(server: SpinTestServer):
    """"""
    This test is asserting the behaviour of static input link, e.g: reusable input link.

    Test scenario:
    *StoreValueBlock1*===a=========\\
    *StoreValueBlock2*===a=====\\  ||
    *StoreValueBlock3*===a===*MathBlock*====b / static====*StoreValueBlock5*
    *StoreValueBlock4*=========================================//

    In this test, there will be three input waiting in the MathBlock input pin `a`.
    And later, another output is produced on input pin `b`, which is a static link,
    this input will complete the input of those three incomplete executions.
    """"""
    logger.info(""Starting test_static_input_link_on_graph"")
    nodes = [
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 4}),  # a
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 4}),  # a
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 4}),  # a
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 5}),  # b
        graph.Node(block_id=StoreValueBlock().id),
        graph.Node(
            block_id=CalculatorBlock().id,
            input_default={""operation"": Operation.ADD.value},
        ),
    ]
    links = [
        graph.Link(
            source_id=nodes[0].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""a"",
        ),
        graph.Link(
            source_id=nodes[1].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""a"",
        ),
        graph.Link(
            source_id=nodes[2].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""a"",
        ),
        graph.Link(
            source_id=nodes[3].id,
            sink_id=nodes[4].id,
            source_name=""output"",
            sink_name=""input"",
        ),
        graph.Link(
            source_id=nodes[4].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""b"",
            is_static=True,  # This is the static link to test.
        ),
    ]
    test_graph = graph.Graph(
        name=""TestGraph"",
        description=""Test graph"",
        nodes=nodes,
        links=links,
    )
    test_user = await create_test_user()
    test_graph = await create_graph(server, test_graph, test_user)
    graph_exec_id = await execute_graph(
        server.agent_server, test_graph, test_user, {}, 8
    )
    logger.info(""Checking execution results"")
    executions = await server.agent_server.test_get_graph_run_node_execution_results(
        test_graph.id, graph_exec_id, test_user.id
    )
    assert len(executions) == 8
    # The last 3 executions will be a+b=4+5=9
    for i, exec_data in enumerate(executions[-3:]):
        logger.info(f""Checking execution {i+1} of last 3: {exec_data}"")
        assert exec_data.status == execution.ExecutionStatus.COMPLETED
        assert exec_data.output_data == {""result"": [9]}
    logger.info(""Completed test_static_input_link_on_graph"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
54,"# -*- coding: utf-8 -*-

import re
import sys
from string import punctuation
from typing import List, Tuple, Dict

# Temporary replacement
# The descriptions that contain () at the end must adapt to the new policy later
punctuation = punctuation.replace('()', '')

anchor = '###'
auth_keys = ['apiKey', 'OAuth', 'X-Mashape-Key', 'User-Agent', 'No']
https_keys = ['Yes', 'No']
cors_keys = ['Yes', 'No', 'Unknown']

index_title = 0
index_desc = 1
index_auth = 2
index_https = 3
index_cors = 4

num_segments = 5
min_entries_per_category = 3
max_description_length = 100

anchor_re = re.compile(anchor + '\s(.+)')
category_title_in_index_re = re.compile('\*\s\[(.*)\]')
link_re = re.compile('\[(.+)\]\((http.*)\)')

# Type aliases
APIList = List[str]
Categories = Dict[str, APIList]
CategoriesLineNumber = Dict[str, int]


def error_message(line_number: int, message: str) -> str:
    line = line_number + 1
    return f'(L{line:03d}) {message}'


def get_categories_content(contents: List[str]) -> Tuple[Categories, CategoriesLineNumber]:

    categories = {}
    category_line_num = {}

    for line_num, line_content in enumerate(contents):

        if line_content.startswith(anchor):
            category = line_content.split(anchor)[1].strip()
            categories[category] = []
            category_line_num[category] = line_num
            continue

        if not line_content.startswith('|') or line_content.startswith('|---'):
            continue

        raw_title = [
            raw_content.strip() for raw_content in line_content.split('|')[1:-1]
        ][0]

        title_match = link_re.match(raw_title)
        if title_match:
                title = title_match.group(1).upper()
                categories[category].append(title)

    return (categories, category_line_num)


def check_alphabetical_order(lines: List[str]) -> List[str]:

    err_msgs = []

    categories, category_line_num = get_categories_content(contents=lines)

    for category, api_list in categories.items():
        if sorted(api_list) != api_list:
            err_msg = error_message(
                category_line_num[category], 
                f'{category} category is not alphabetical order'
            )
            err_msgs.append(err_msg)
    
    return err_msgs


def check_title(line_num: int, raw_title: str) -> List[str]:

    err_msgs = []

    title_match = link_re.match(raw_title)

    # url should be wrapped in ""[TITLE](LINK)"" Markdown syntax
    if not title_match:
        err_msg = error_message(line_num, 'Title syntax should be ""[TITLE](LINK)""')
        err_msgs.append(err_msg)
    else:
        # do not allow ""... API"" in the entry title
        title = title_match.group(1)
        if title.upper().endswith(' API'):
            err_msg = error_message(line_num, 'Title should not end with ""... API"". Every entry is an API here!')
            err_msgs.append(err_msg)

    return err_msgs


def check_description(line_num: int, description: str) -> List[str]:

    err_msgs = []

    first_char = description[0]
    if first_char.upper() != first_char:
        err_msg = error_message(line_num, 'first character of description is not capitalized')
        err_msgs.append(err_msg)

    last_char = description[-1]
    if last_char in punctuation:
        err_msg = error_message(line_num, f'description should not end with {last_char}')
        err_msgs.append(err_msg)

    desc_length = len(description)
    if desc_length > max_description_length:
        err_msg = error_message(line_num, f'description should not exceed {max_description_length} characters (currently {desc_length})')
        err_msgs.append(err_msg)
    
    return err_msgs


def check_auth(line_num: int, auth: str) -> List[str]:

    err_msgs = []

    backtick = '`'
    if auth != 'No' and (not auth.startswith(backtick) or not auth.endswith(backtick)):
        err_msg = error_message(line_num, 'auth value is not enclosed with `backticks`')
        err_msgs.append(err_msg)

    if auth.replace(backtick, '') not in auth_keys:
        err_msg = error_message(line_num, f'{auth} is not a valid Auth option')
        err_msgs.append(err_msg)
    
    return err_msgs


def check_https(line_num: int, https: str) -> List[str]:

    err_msgs = []

    if https not in https_keys:
        err_msg = error_message(line_num, f'{https} is not a valid HTTPS option')
        err_msgs.append(err_msg)

    return err_msgs


def check_cors(line_num: int, cors: str) -> List[str]:

    err_msgs = []

    if cors not in cors_keys:
        err_msg = error_message(line_num, f'{cors} is not a valid CORS option')
        err_msgs.append(err_msg)
    
    return err_msgs


def check_entry(line_num: int, segments: List[str]) -> List[str]:

    raw_title = segments[index_title]
    description = segments[index_desc]
    auth = segments[index_auth]
    https = segments[index_https]
    cors = segments[index_cors]

    title_err_msgs = check_title(line_num, raw_title)
    desc_err_msgs = check_description(line_num, description)
    auth_err_msgs = check_auth(line_num, auth)
    https_err_msgs = check_https(line_num, https)
    cors_err_msgs = check_cors(line_num, cors)

    err_msgs = [
        *title_err_msgs,
        *desc_err_msgs,
        *auth_err_msgs,
        *https_err_msgs,
        *cors_err_msgs
    ]

    return err_msgs


def check_file_format(lines: List[str]) -> List[str]:

    err_msgs = []
    category_title_in_index = []

    alphabetical_err_msgs = check_alphabetical_order(lines)
    err_msgs.extend(alphabetical_err_msgs)

    num_in_category = min_entries_per_category + 1
    category = ''
    category_line = 0

    for line_num, line_content in enumerate(lines):

        category_title_match = category_title_in_index_re.match(line_content)
        if category_title_match:
            category_title_in_index.append(category_title_match.group(1))

        # check each category for the minimum number of entries
        if line_content.startswith(anchor):
            category_match = anchor_re.match(line_content)
            if category_match:
                if category_match.group(1) not in category_title_in_index:
                    err_msg = error_message(line_num, f'category header ({category_match.group(1)}) not added to Index section')
                    err_msgs.append(err_msg)
            else:
                err_msg = error_message(line_num, 'category header is not formatted correctly')
                err_msgs.append(err_msg)

            if num_in_category < min_entries_per_category:
                err_msg = error_message(category_line, f'{category} category does not have the minimum {min_entries_per_category} entries (only has {num_in_category})')
                err_msgs.append(err_msg)

            category = line_content.split(' ')[1]
            category_line = line_num
            num_in_category = 0
            continue

        # skips lines that we do not care about
        if not line_content.startswith('|') or line_content.startswith('|---'):
            continue

        num_in_category += 1
        segments = line_content.split('|')[1:-1]
        if len(segments) < num_segments:
            err_msg = error_message(line_num, f'entry does not have all the required columns (have {len(segments)}, need {num_segments})')
            err_msgs.append(err_msg)
            continue
    
        for segment in segments:
            # every line segment should start and end with exactly 1 space
            if len(segment) - len(segment.lstrip()) != 1 or len(segment) - len(segment.rstrip()) != 1:
                err_msg = error_message(line_num, 'each segment must start and end with exactly 1 space')
                err_msgs.append(err_msg)
        
        segments = [segment.strip() for segment in segments]
        entry_err_msgs = check_entry(line_num, segments)
        err_msgs.extend(entry_err_msgs)
    
    return err_msgs


def main(filename: str) -> None:

    with open(filename, mode='r', encoding='utf-8') as file:
        lines = list(line.rstrip() for line in file)

    file_format_err_msgs = check_file_format(lines)

    if file_format_err_msgs:
        for err_msg in file_format_err_msgs:
            print(err_msg)
        sys.exit(1)


if __name__ == '__main__':

    num_args = len(sys.argv)

    if num_args < 2:
        print('No .md file passed (file should contain Markdown table syntax)')
        sys.exit(1)

    filename = sys.argv[1]

    main(filename)
","# -*- coding: utf-8 -*-

import re
import sys
from string import punctuation
from typing import List, Tuple, Dict

# Temporary replacement
# The descriptions that contain () at the end must adapt to the new policy later
punctuation = punctuation.replace('()', '')

anchor = '###'
auth_keys = ['apiKey', 'OAuth', 'X-Mashape-Key', 'User-Agent', 'No']
https_keys = ['Yes', 'No']
cors_keys = ['Yes', 'No', 'Unknown']

index_title = 0
index_desc = 1
index_auth = 2
index_https = 3
index_cors = 4

num_segments = 5
min_entries_per_category = 3
max_description_length = 100

anchor_re = re.compile(anchor + '\\s(.+)')
category_title_in_index_re = re.compile('\\*\\s\\[(.*)\\]')
link_re = re.compile('\\[(.+)\\]\\((http.*)\\)')

# Type aliases
APIList = List[str]
Categories = Dict[str, APIList]
CategoriesLineNumber = Dict[str, int]


def error_message(line_number: int, message: str) -> str:
    line = line_number + 1
    return f'(L{line:03d}) {message}'


def get_categories_content(contents: List[str]) -> Tuple[Categories, CategoriesLineNumber]:

    categories = {}
    category_line_num = {}

    for line_num, line_content in enumerate(contents):

        if line_content.startswith(anchor):
            category = line_content.split(anchor)[1].strip()
            categories[category] = []
            category_line_num[category] = line_num
            continue

        if not line_content.startswith('|') or line_content.startswith('|---'):
            continue

        raw_title = [
            raw_content.strip() for raw_content in line_content.split('|')[1:-1]
        ][0]

        title_match = link_re.match(raw_title)
        if title_match:
            title = title_match.group(1).upper()
            categories[category].append(title)

    return (categories, category_line_num)


def check_alphabetical_order(lines: List[str]) -> List[str]:

    err_msgs = []

    categories, category_line_num = get_categories_content(contents=lines)

    for category, api_list in categories.items():
        if sorted(api_list) != api_list:
            err_msg = error_message(
                category_line_num[category], 
                f'{category} category is not alphabetical order'
            )
            err_msgs.append(err_msg)
    
    return err_msgs


def check_title(line_num: int, raw_title: str) -> List[str]:

    err_msgs = []

    title_match = link_re.match(raw_title)

    if not title_match:
        err_msg = error_message(line_num, 'Title syntax should be ""[TITLE](LINK)""')
        err_msgs.append(err_msg)
    else:
        # do not allow ""... API"" in the entry title
        title = title_match.group(1)

        if title.upper().endswith(' API'):
            err_msg = error_message(line_num, 'Title should not end with ""... API"". Every entry is an API here!')
            err_msgs.append(err_msg)

    return err_msgs


def check_description(line_num: int, description: str) -> List[str]:

    err_msgs = []

    first_char = description[0]
    if first_char.upper() != first_char:
        err_msg = error_message(line_num, 'first character of description is not capitalized')
        err_msgs.append(err_msg)

    last_char = description[-1]
    if last_char in punctuation:
        err_msg = error_message(line_num, f'description should not end with {last_char}')
        err_msgs.append(err_msg)

    desc_length = len(description)
    if desc_length > max_description_length:
        err_msg = error_message(line_num, f'description should not exceed {max_description_length} characters (currently {desc_length})')
        err_msgs.append(err_msg)
    
    return err_msgs


def check_auth(line_num: int, auth: str) -> List[str]:

    err_msgs = []

    backtick = '`'

    if auth != 'No' and (not auth.startswith(backtick) or not auth.endswith(backtick)):
        err_msg = error_message(line_num, 'auth value is not enclosed with `backticks`')
        err_msgs.append(err_msg)

    if auth.replace(backtick, '') not in auth_keys:
        err_msg = error_message(line_num, f'{auth} is not a valid Auth option')
        err_msgs.append(err_msg)

    return err_msgs


def check_https(line_num: int, https: str) -> List[str]:

    err_msgs = []

    if https not in https_keys:
        err_msg = error_message(line_num, f'{https} is not a valid HTTPS option')
        err_msgs.append(err_msg)

    return err_msgs


def check_cors(line_num: int, cors: str) -> List[str]:

    err_msgs = []

    if cors not in cors_keys:
        err_msg = error_message(line_num, f'{cors} is not a valid CORS option')
        err_msgs.append(err_msg)

    return err_msgs


def check_entry(line_num: int, segments: List[str]) -> List[str]:

    raw_title = segments[index_title]
    description = segments[index_desc]
    auth = segments[index_auth]
    https = segments[index_https]
    cors = segments[index_cors]

    title_err_msgs = check_title(line_num, raw_title)
    desc_err_msgs = check_description(line_num, description)
    auth_err_msgs = check_auth(line_num, auth)
    https_err_msgs = check_https(line_num, https)
    cors_err_msgs = check_cors(line_num, cors)

    err_msgs = [
        *title_err_msgs,
        *desc_err_msgs,
        *auth_err_msgs,
        *https_err_msgs,
        *cors_err_msgs
    ]

    return err_msgs


def check_file_format(lines: List[str]) -> List[str]:

    err_msgs = []
    category_title_in_index = []

    alphabetical_err_msgs = check_alphabetical_order(lines)
    err_msgs.extend(alphabetical_err_msgs)

    num_in_category = min_entries_per_category + 1
    category = ''
    category_line = 0

    for line_num, line_content in enumerate(lines):

        category_title_match = category_title_in_index_re.match(line_content)
        if category_title_match:
            category_title_in_index.append(category_title_match.group(1))

        # check each category for the minimum number of entries
        if line_content.startswith(anchor):
            category_match = anchor_re.match(line_content)
            if category_match:
                if category_match.group(1) not in category_title_in_index:
                    err_msg = error_message(line_num, f'category header ({category_match.group(1)}) not added to Index section')
                    err_msgs.append(err_msg)
            else:
                err_msg = error_message(line_num, 'category header is not formatted correctly')
                err_msgs.append(err_msg)

            if num_in_category < min_entries_per_category:
                err_msg = error_message(category_line, f'{category} category does not have the minimum {min_entries_per_category} entries (only has {num_in_category})')
                err_msgs.append(err_msg)

            category = line_content.split(' ')[1]
            category_line = line_num
            num_in_category = 0
            continue

        # skips lines that we do not care about
        if not line_content.startswith('|') or line_content.startswith('|---'):
            continue

        num_in_category += 1
        segments = line_content.split('|')[1:-1]
        if len(segments) < num_segments:
            err_msg = error_message(line_num, f'entry does not have all the required columns (have {len(segments)}, need {num_segments})')
            err_msgs.append(err_msg)
            continue
    
        for segment in segments:
            # every line segment should start and end with exactly 1 space
            if len(segment) - len(segment.lstrip()) != 1 or len(segment) - len(segment.rstrip()) != 1:
                err_msg = error_message(line_num, 'each segment must start and end with exactly 1 space')
                err_msgs.append(err_msg)
        
        segments = [segment.strip() for segment in segments]
        entry_err_msgs = check_entry(line_num, segments)
        err_msgs.extend(entry_err_msgs)
    
    return err_msgs


def main(filename: str) -> None:

    with open(filename, mode='r', encoding='utf-8') as file:
        lines = list(line.rstrip() for line in file)

    file_format_err_msgs = check_file_format(lines)

    if file_format_err_msgs:
        for err_msg in file_format_err_msgs:
            print(err_msg)
        sys.exit(1)


if __name__ == '__main__':
    num_args = len(sys.argv)
    if num_args < 2:
        print('No .md file passed (file should contain Markdown table syntax)')
        sys.exit(1)
    
    filename = sys.argv[1]
    main(filename)","{'final_score': 98.33, 'length_similarity': 99.26, 'token_similarity': 95.83, 'structure_similarity': 98.44, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
55,"import time
from datetime import datetime, timedelta, timezone
from typing import Any

import feedparser
import pydantic

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class RSSEntry(pydantic.BaseModel):
    title: str
    link: str
    description: str
    pub_date: datetime
    author: str
    categories: list[str]


class ReadRSSFeedBlock(Block):
    class Input(BlockSchema):
        rss_url: str = SchemaField(
            description=""The URL of the RSS feed to read"",
            placeholder=""https://example.com/rss"",
        )
        time_period: int = SchemaField(
            description=""The time period to check in minutes relative to the run block runtime, e.g. 60 would check for new entries in the last hour."",
            placeholder=""1440"",
            default=1440,
        )
        polling_rate: int = SchemaField(
            description=""The number of seconds to wait between polling attempts."",
            placeholder=""300"",
        )
        run_continuously: bool = SchemaField(
            description=""Whether to run the block continuously or just once."",
            default=True,
        )

    class Output(BlockSchema):
        entry: RSSEntry = SchemaField(description=""The RSS item"")

    def __init__(self):
        super().__init__(
            id=""5ebe6768-8e5d-41e3-9134-1c7bd89a8d52"",
            input_schema=ReadRSSFeedBlock.Input,
            output_schema=ReadRSSFeedBlock.Output,
            description=""Reads RSS feed entries from a given URL."",
            categories={BlockCategory.INPUT},
            test_input={
                ""rss_url"": ""https://example.com/rss"",
                ""time_period"": 10_000_000,
                ""polling_rate"": 1,
                ""run_continuously"": False,
            },
            test_output=[
                (
                    ""entry"",
                    RSSEntry(
                        title=""Example RSS Item"",
                        link=""https://example.com/article"",
                        description=""This is an example RSS item description."",
                        pub_date=datetime(2023, 6, 23, 12, 30, 0, tzinfo=timezone.utc),
                        author=""John Doe"",
                        categories=[""Technology"", ""News""],
                    ),
                ),
            ],
            test_mock={
                ""parse_feed"": lambda *args, **kwargs: {
                    ""entries"": [
                        {
                            ""title"": ""Example RSS Item"",
                            ""link"": ""https://example.com/article"",
                            ""summary"": ""This is an example RSS item description."",
                            ""published_parsed"": (2023, 6, 23, 12, 30, 0, 4, 174, 0),
                            ""author"": ""John Doe"",
                            ""tags"": [{""term"": ""Technology""}, {""term"": ""News""}],
                        }
                    ]
                }
            },
        )

    @staticmethod
    def parse_feed(url: str) -> dict[str, Any]:
        return feedparser.parse(url)  # type: ignore

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        keep_going = True
        start_time = datetime.now(timezone.utc) - timedelta(
            minutes=input_data.time_period
        )
        while keep_going:
            keep_going = input_data.run_continuously

            feed = self.parse_feed(input_data.rss_url)

            for entry in feed[""entries""]:
                pub_date = datetime(*entry[""published_parsed""][:6], tzinfo=timezone.utc)

                if pub_date > start_time:
                    yield (
                        ""entry"",
                        RSSEntry(
                            title=entry[""title""],
                            link=entry[""link""],
                            description=entry.get(""summary"", """"),
                            pub_date=pub_date,
                            author=entry.get(""author"", """"),
                            categories=[tag[""term""] for tag in entry.get(""tags"", [])],
                        ),
                    )

            time.sleep(input_data.polling_rate)
","import time
from datetime import datetime, timedelta, timezone
from typing import Any

import feedparser
import pydantic

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

class RSSEntry(pydantic.BaseModel):
    title: str
    link: str
    description: str
    pub_date: datetime
    author: str
    categories: list[str]

class ReadRSSFeedBlock(Block):
    class Input(BlockSchema):
        rss_url: str = SchemaField(
            description=""The URL of the RSS feed to read"",
            placeholder=""https://example.com/rss"",
        )
        time_period: int = SchemaField(
            description=""The time period to check in minutes relative to the run block runtime, e.g. 60 would check for new entries in the last hour."",
            placeholder=""1440"",
            default=1440,
        )
        polling_rate: int = SchemaField(
            description=""The number of seconds to wait between polling attempts."",
            placeholder=""300"",
        )
        run_continuously: bool = SchemaField(
            description=""Whether to run the block continuously or just once."",
            default=True,
        )

    class Output(BlockSchema):
        entry: RSSEntry = SchemaField(description=""The RSS item"")

    def __init__(self):
        super().__init__(
            id=""5ebe6768-8e5d-41e3-9134-1c7bd89a8d52"",
            input_schema=ReadRSSFeedBlock.Input,
            output_schema=ReadRSSFeedBlock.Output,
            description=""Reads RSS feed entries from a given URL."",
            categories={BlockCategory.INPUT},
            test_input={
                ""rss_url"": ""https://example.com/rss"",
                ""time_period"": 10_000_000,
                ""polling_rate"": 1,
                ""run_continuously"": False,
            },
            test_output=[
                (
                    ""entry"",
                    RSSEntry(
                        title=""Example RSS Item"",
                        link=""https://example.com/article"",
                        description=""This is an example RSS item description."",
                        pub_date=datetime(2023, 6, 23, 12, 30, 0, tzinfo=timezone.utc),
                        author=""John Doe"",
                        categories=[""Technology"", ""News""],
                    ),
                ),
            ],
            test_mock={
                ""parse_feed"": lambda *args, **kwargs: {
                    ""entries"": [
                        {
                            ""title"": ""Example RSS Item"",
                            ""link"": ""https://example.com/article"",
                            ""summary"": ""This is an example RSS item description."",
                            ""published_parsed"": (2023, 6, 23, 12, 30, 0, 4, 174, 0),
                            ""author"": ""John Doe"",
                            ""tags"": [{""term"": ""Technology""}, {""term"": ""News""}],
                        }
                    ]
                }
            },
        )

    @staticmethod
    def parse_feed(url: str) -> dict[str, Any]:
        return feedparser.parse(url)  # type: ignore

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        keep_going = True
        start_time = datetime.now(timezone.utc) - timedelta(
            minutes=input_data.time_period
        )
        while keep_going:
            keep_going = input_data.run_continuously
            feed = self.parse_feed(input_data.rss_url)
            for entry in feed[""entries""]:
                pub_date = datetime(*entry[""published_parsed""][:6], tzinfo=timezone.utc)
                if pub_date > start_time:
                    yield (
                        ""entry"",
                        RSSEntry(
                            title=entry[""title""],
                            link=entry[""link""],
                            description=entry.get(""summary"", """"),
                            pub_date=pub_date,
                            author=entry.get(""author"", """"),
                            categories=[tag[""term""] for tag in entry.get(""tags"", [])],
                        ),
                    )
            time.sleep(input_data.polling_rate)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
56,"""""""

The nqueens problem is of placing N queens on a N * N
chess board such that no queen can attack any other queens placed
on that chess board.
This means that one queen cannot have any other queen on its horizontal, vertical and
diagonal lines.

""""""

from __future__ import annotations

solution = []


def is_safe(board: list[list[int]], row: int, column: int) -> bool:
    """"""
    This function returns a boolean value True if it is safe to place a queen there
    considering the current state of the board.

    Parameters:
    board (2D matrix): The chessboard
    row, column: Coordinates of the cell on the board

    Returns:
    Boolean Value

    >>> is_safe([[0, 0, 0], [0, 0, 0], [0, 0, 0]], 1, 1)
    True
    >>> is_safe([[1, 0, 0], [0, 0, 0], [0, 0, 0]], 1, 1)
    False
    """"""

    n = len(board)  # Size of the board

    # Check if there is any queen in the same row, column,
    # left upper diagonal, and right upper diagonal
    return (
        all(board[i][j] != 1 for i, j in zip(range(row, -1, -1), range(column, n)))
        and all(
            board[i][j] != 1 for i, j in zip(range(row, -1, -1), range(column, -1, -1))
        )
        and all(board[i][j] != 1 for i, j in zip(range(row, n), range(column, n)))
        and all(board[i][j] != 1 for i, j in zip(range(row, n), range(column, -1, -1)))
    )


def solve(board: list[list[int]], row: int) -> bool:
    """"""
    This function creates a state space tree and calls the safe function until it
    receives a False Boolean and terminates that branch and backtracks to the next
    possible solution branch.
    """"""
    if row >= len(board):
        """"""
        If the row number exceeds N, we have a board with a successful combination
        and that combination is appended to the solution list and the board is printed.
        """"""
        solution.append(board)
        printboard(board)
        print()
        return True
    for i in range(len(board)):
        """"""
        For every row, it iterates through each column to check if it is feasible to
        place a queen there.
        If all the combinations for that particular branch are successful, the board is
        reinitialized for the next possible combination.
        """"""
        if is_safe(board, row, i):
            board[row][i] = 1
            solve(board, row + 1)
            board[row][i] = 0
    return False


def printboard(board: list[list[int]]) -> None:
    """"""
    Prints the boards that have a successful combination.
    """"""
    for i in range(len(board)):
        for j in range(len(board)):
            if board[i][j] == 1:
                print(""Q"", end="" "")  # Queen is present
            else:
                print(""."", end="" "")  # Empty cell
        print()


# Number of queens (e.g., n=8 for an 8x8 board)
n = 8
board = [[0 for i in range(n)] for j in range(n)]
solve(board, 0)
print(""The total number of solutions are:"", len(solution))
","
""""""
The nqueens problem is of placing N queens on a N * N
chess board such that no queen can attack any other queens placed
on that chess board.
This means that one queen cannot have any other queen on its horizontal, vertical and
diagonal lines.
""""""

from __future__ import annotations

solution = []

def is_safe(board: list[list[int]], row: int, column: int) -> bool:
    """"""
    This function returns a boolean value True if it is safe to place a queen there
    considering the current state of the board.

    Parameters:
    board (2D matrix): The chessboard
    row, column: Coordinates of the cell on the board

    Returns:
    Boolean Value

    >>> is_safe([[0, 0, 0], [0, 0, 0], [0, 0, 0]], 1, 1)
    True
    >>> is_safe([[1, 0, 0], [0, 0, 0], [0, 0, 0]], 1, 1)
    False
    """"""
    n = len(board)  # Size of the board
    return (
        all(board[i][j] != 1 for i, j in zip(range(row, -1, -1), range(column, n)))
        and all(
            board[i][j] != 1 for i, j in zip(range(row, -1, -1), range(column, -1, -1))
        )
        and all(board[i][j] != 1 for i, j in zip(range(row, n), range(column, n)))
        and all(board[i][j] != 1 for i, j in zip(range(row, n), range(column, -1, -1)))
    )

def solve(board: list[list[int]], row: int) -> bool:
    """"""
    This function creates a state space tree and calls the safe function until it
    receives a False Boolean and terminates that branch and backtracks to the next
    possible solution branch.
    """"""
    if row >= len(board):
        """"""
        If the row number exceeds N, we have a board with a successful combination
        and that combination is appended to the solution list and the board is printed.
        """"""
        solution.append(board)
        printboard(board)
        print()
        return True

    for i in range(len(board)):
        """"""
        For every row, it iterates through each column to check if it is feasible to
        place a queen there.
        If all the combinations for that particular branch are successful, the board is
        reinitialized for the next possible combination.
        """"""
        if is_safe(board, row, i):
            board[row][i] = 1
            solve(board, row + 1)
            board[row][i] = 0
    return False

def printboard(board: list[list[int]]) -> None:
    """"""
    Prints the boards that have a successful combination.
    """"""
    for i in range(len(board)):
        for j in range(len(board)):
            if board[i][j] == 1:
                print(""Q"", end="" "")  # Queen is present
            else:
                print(""."", end="" "")  # Empty cell
        print()

# Number of queens (e.g., n=8 for an 8x8 board)
n = 8
board = [[0 for i in range(n)] for j in range(n)]
solve(board, 0)
print(""The total number of solutions are:"", len(solution))","{'final_score': 97.43, 'length_similarity': 96.18, 'token_similarity': 96.83, 'structure_similarity': 97.4, 'function_similarity': 100, 'complexity_similarity': 94.74, 'variable_similarity': 100}",
57,"import logging
from contextlib import contextmanager
from datetime import datetime
from typing import TYPE_CHECKING

from autogpt_libs.utils.synchronize import RedisKeyedMutex
from redis.lock import Lock as RedisLock

from backend.data import redis
from backend.data.model import Credentials
from backend.integrations.credentials_store import IntegrationCredentialsStore
from backend.integrations.oauth import HANDLERS_BY_NAME
from backend.util.exceptions import MissingConfigError
from backend.util.settings import Settings

if TYPE_CHECKING:
    from backend.integrations.oauth import BaseOAuthHandler

logger = logging.getLogger(__name__)
settings = Settings()


class IntegrationCredentialsManager:
    """"""
    Handles the lifecycle of integration credentials.
    - Automatically refreshes requested credentials if needed.
    - Uses locking mechanisms to ensure system-wide consistency and
      prevent invalidation of in-use tokens.

    ### ⚠️ Gotcha
    With `acquire(..)`, credentials can only be in use in one place at a time (e.g. one
    block execution).

    ### Locking mechanism
    - Because *getting* credentials can result in a refresh (= *invalidation* +
      *replacement*) of the stored credentials, *getting* is an operation that
      potentially requires read/write access.
    - Checking whether a token has to be refreshed is subject to an additional `refresh`
      scoped lock to prevent unnecessary sequential refreshes when multiple executions
      try to access the same credentials simultaneously.
    - We MUST lock credentials while in use to prevent them from being invalidated while
      they are in use, e.g. because they are being refreshed by a different part
      of the system.
    - The `!time_sensitive` lock in `acquire(..)` is part of a two-tier locking
      mechanism in which *updating* gets priority over *getting* credentials.
      This is to prevent a long queue of waiting *get* requests from blocking essential
      credential refreshes or user-initiated updates.

    It is possible to implement a reader/writer locking system where either multiple
    readers or a single writer can have simultaneous access, but this would add a lot of
    complexity to the mechanism. I don't expect the current (""simple"") mechanism to
    cause so much latency that it's worth implementing.
    """"""

    def __init__(self):
        redis_conn = redis.get_redis()
        self._locks = RedisKeyedMutex(redis_conn)
        self.store = IntegrationCredentialsStore()

    def create(self, user_id: str, credentials: Credentials) -> None:
        return self.store.add_creds(user_id, credentials)

    def exists(self, user_id: str, credentials_id: str) -> bool:
        return self.store.get_creds_by_id(user_id, credentials_id) is not None

    def get(
        self, user_id: str, credentials_id: str, lock: bool = True
    ) -> Credentials | None:
        credentials = self.store.get_creds_by_id(user_id, credentials_id)
        if not credentials:
            return None

        # Refresh OAuth credentials if needed
        if credentials.type == ""oauth2"" and credentials.access_token_expires_at:
            logger.debug(
                f""Credentials #{credentials.id} expire at ""
                f""{datetime.fromtimestamp(credentials.access_token_expires_at)}; ""
                f""current time is {datetime.now()}""
            )

            with self._locked(user_id, credentials_id, ""refresh""):
                oauth_handler = _get_provider_oauth_handler(credentials.provider)
                if oauth_handler.needs_refresh(credentials):
                    logger.debug(
                        f""Refreshing '{credentials.provider}' ""
                        f""credentials #{credentials.id}""
                    )
                    _lock = None
                    if lock:
                        # Wait until the credentials are no longer in use anywhere
                        _lock = self._acquire_lock(user_id, credentials_id)

                    fresh_credentials = oauth_handler.refresh_tokens(credentials)
                    self.store.update_creds(user_id, fresh_credentials)
                    if _lock and _lock.locked():
                        _lock.release()

                    credentials = fresh_credentials
        else:
            logger.debug(f""Credentials #{credentials.id} never expire"")

        return credentials

    def acquire(
        self, user_id: str, credentials_id: str
    ) -> tuple[Credentials, RedisLock]:
        """"""
        ⚠️ WARNING: this locks credentials system-wide and blocks both acquiring
        and updating them elsewhere until the lock is released.
        See the class docstring for more info.
        """"""
        # Use a low-priority (!time_sensitive) locking queue on top of the general lock
        # to allow priority access for refreshing/updating the tokens.
        with self._locked(user_id, credentials_id, ""!time_sensitive""):
            lock = self._acquire_lock(user_id, credentials_id)
        credentials = self.get(user_id, credentials_id, lock=False)
        if not credentials:
            raise ValueError(
                f""Credentials #{credentials_id} for user #{user_id} not found""
            )
        return credentials, lock

    def update(self, user_id: str, updated: Credentials) -> None:
        with self._locked(user_id, updated.id):
            self.store.update_creds(user_id, updated)

    def delete(self, user_id: str, credentials_id: str) -> None:
        with self._locked(user_id, credentials_id):
            self.store.delete_creds_by_id(user_id, credentials_id)

    # -- Locking utilities -- #

    def _acquire_lock(self, user_id: str, credentials_id: str, *args: str) -> RedisLock:
        key = (
            f""user:{user_id}"",
            f""credentials:{credentials_id}"",
            *args,
        )
        return self._locks.acquire(key)

    @contextmanager
    def _locked(self, user_id: str, credentials_id: str, *args: str):
        lock = self._acquire_lock(user_id, credentials_id, *args)
        try:
            yield
        finally:
            if lock.locked():
                lock.release()

    def release_all_locks(self):
        """"""Call this on process termination to ensure all locks are released""""""
        self._locks.release_all_locks()
        self.store.locks.release_all_locks()


def _get_provider_oauth_handler(provider_name: str) -> ""BaseOAuthHandler"":
    if provider_name not in HANDLERS_BY_NAME:
        raise KeyError(f""Unknown provider '{provider_name}'"")

    client_id = getattr(settings.secrets, f""{provider_name}_client_id"")
    client_secret = getattr(settings.secrets, f""{provider_name}_client_secret"")
    if not (client_id and client_secret):
        raise MissingConfigError(
            f""Integration with provider '{provider_name}' is not configured"",
        )

    handler_class = HANDLERS_BY_NAME[provider_name]
    frontend_base_url = (
        settings.config.frontend_base_url or settings.config.platform_base_url
    )
    return handler_class(
        client_id=client_id,
        client_secret=client_secret,
        redirect_uri=f""{frontend_base_url}/auth/integrations/oauth_callback"",
    )
","import logging
from contextlib import contextmanager
from datetime import datetime
from typing import TYPE_CHECKING

from autogpt_libs.utils.synchronize import RedisKeyedMutex
from redis.lock import Lock as RedisLock

from backend.data import redis
from backend.data.model import Credentials
from backend.integrations.credentials_store import IntegrationCredentialsStore
from backend.integrations.oauth import HANDLERS_BY_NAME
from backend.util.exceptions import MissingConfigError
from backend.util.settings import Settings

if TYPE_CHECKING:
    from backend.integrations.oauth import BaseOAuthHandler

logger = logging.getLogger(__name__)
settings = Settings()


class IntegrationCredentialsManager:
    """"""
    Handles the lifecycle of integration credentials.
    - Automatically refreshes requested credentials if needed.
    - Uses locking mechanisms to ensure system-wide consistency and
      prevent invalidation of in-use tokens.

    ### ⚠️ Gotcha
    With `acquire(..)`, credentials can only be in use in one place at a time (e.g. one
    block execution).

    ### Locking mechanism
    - Because *getting* credentials can result in a refresh (= *invalidation* +
      *replacement*) of the stored credentials, *getting* is an operation that
      potentially requires read/write access.
    - Checking whether a token has to be refreshed is subject to an additional `refresh`
      scoped lock to prevent unnecessary sequential refreshes when multiple executions
      try to access the same credentials simultaneously.
    - We MUST lock credentials while in use to prevent them from being invalidated while
      they are in use, e.g. because they are being refreshed by a different part
      of the system.
    - The `!time_sensitive` lock in `acquire(..)` is part of a two-tier locking
      mechanism in which *updating* gets priority over *getting* credentials.
      This is to prevent a long queue of waiting *get* requests from blocking essential
      credential refreshes or user-initiated updates.

    It is possible to implement a reader/writer locking system where either multiple
    readers or a single writer can have simultaneous access, but this would add a lot of
    complexity to the mechanism. I don't expect the current (""simple"") mechanism to
    cause so much latency that it's worth implementing.
    """"""

    def __init__(self):
        redis_conn = redis.get_redis()
        self._locks = RedisKeyedMutex(redis_conn)
        self.store = IntegrationCredentialsStore()

    def create(self, user_id: str, credentials: Credentials) -> None:
        return self.store.add_creds(user_id, credentials)

    def exists(self, user_id: str, credentials_id: str) -> bool:
        return self.store.get_creds_by_id(user_id, credentials_id) is not None

    def get(
        self, user_id: str, credentials_id: str, lock: bool = True
    ) -> Credentials | None:
        credentials = self.store.get_creds_by_id(user_id, credentials_id)
        if not credentials:
            return None

        # Refresh OAuth credentials if needed
        if credentials.type == ""oauth2"" and credentials.access_token_expires_at:
            logger.debug(
                f""Credentials #{credentials.id} expire at ""
                f""{datetime.fromtimestamp(credentials.access_token_expires_at)}; ""
                f""current time is {datetime.now()}""
            )

            with self._locked(user_id, credentials_id, ""refresh""):
                oauth_handler = _get_provider_oauth_handler(credentials.provider)
                if oauth_handler.needs_refresh(credentials):
                    logger.debug(
                        f""Refreshing '{credentials.provider}' ""
                        f""credentials #{credentials.id}""
                    )
                    _lock = None
                    if lock:
                        # Wait until the credentials are no longer in use anywhere
                        _lock = self._acquire_lock(user_id, credentials_id)

                    fresh_credentials = oauth_handler.refresh_tokens(credentials)
                    self.store.update_creds(user_id, fresh_credentials)
                    if _lock and _lock.locked():
                        _lock.release()

        else:
            logger.debug(f""Credentials #{credentials.id} never expire"")

        return credentials

    def acquire(
        self, user_id: str, credentials_id: str
    ) -> tuple[Credentials, RedisLock]:
        """"""
        ⚠️ WARNING: this locks credentials system-wide and blocks both acquiring
        and updating them elsewhere until the lock is released.
        See the class docstring for more info.
        """"""
        # Use a low-priority (!time_sensitive) locking queue on top of the general lock
        # to allow priority access for refreshing/updating the tokens.
        with self._locked(user_id, credentials_id, ""!time_sensitive""):
            lock = self._acquire_lock(user_id, credentials_id)
        credentials = self.get(user_id, credentials_id, lock=False)
        if not credentials:
            raise ValueError(
                f""Credentials #{credentials_id} for user #{user_id} not found""
            )
        return credentials, lock

    def update(self, user_id: str, updated: Credentials) -> None:
        with self._locked(user_id, updated.id):
            self.store.update_creds(user_id, updated)

    def delete(self, user_id: str, credentials_id: str) -> None:
        with self._locked(user_id, credentials_id):
            self.store.delete_creds_by_id(user_id, credentials_id)

    # -- Locking utilities -- #

    def _acquire_lock(self, user_id: str, credentials_id: str, *args: str) -> RedisLock:
        key = (
            f""user:{user_id}"",
            f""credentials:{credentials_id}"",
            *args,
        )
        return self._locks.acquire(key)

    @contextmanager
    def _locked(self, user_id: str, credentials_id: str, *args: str):
        lock = self._acquire_lock(user_id, credentials_id, *args)
        try:
            yield
        finally:
            if lock.locked():
                lock.release()

    def release_all_locks(self):
        """"""Call this on process termination to ensure all locks are released""""""
        self._locks.release_all_locks()
        self.store.locks.release_all_locks()


def _get_provider_oauth_handler(provider_name: str) -> ""BaseOAuthHandler"":
    if provider_name not in HANDLERS_BY_NAME:
        raise KeyError(f""Unknown provider '{provider_name}'"")

    client_id = getattr(settings.secrets, f""{provider_name}_client_id"")
    client_secret = getattr(settings.secrets, f""{provider_name}_client_secret"")

    if not (client_id and client_secret):
        raise MissingConfigError(
            f""Integration with provider '{provider_name}' is not configured"",
        )

    handler_class = HANDLERS_BY_NAME[provider_name]
    frontend_base_url = (
        settings.config.frontend_base_url or settings.config.platform_base_url
    )

    return handler_class(
        client_id=client_id,
        client_secret=client_secret,
        redirect_uri=f""{frontend_base_url}/auth/integrations/oauth_callback"",
    )","{'final_score': 99.75, 'length_similarity': 99.28, 'token_similarity': 100.0, 'structure_similarity': 99.32, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
58,"from enum import Enum
from typing import Any

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class ComparisonOperator(Enum):
    EQUAL = ""==""
    NOT_EQUAL = ""!=""
    GREATER_THAN = "">""
    LESS_THAN = ""<""
    GREATER_THAN_OR_EQUAL = "">=""
    LESS_THAN_OR_EQUAL = ""<=""


class ConditionBlock(Block):
    class Input(BlockSchema):
        value1: Any = SchemaField(
            description=""Enter the first value for comparison"",
            placeholder=""For example: 10 or 'hello' or True"",
        )
        operator: ComparisonOperator = SchemaField(
            description=""Choose the comparison operator"",
            placeholder=""Select an operator"",
        )
        value2: Any = SchemaField(
            description=""Enter the second value for comparison"",
            placeholder=""For example: 20 or 'world' or False"",
        )
        yes_value: Any = SchemaField(
            description=""(Optional) Value to output if the condition is true. If not provided, value1 will be used."",
            placeholder=""Leave empty to use value1, or enter a specific value"",
            default=None,
        )
        no_value: Any = SchemaField(
            description=""(Optional) Value to output if the condition is false. If not provided, value1 will be used."",
            placeholder=""Leave empty to use value1, or enter a specific value"",
            default=None,
        )

    class Output(BlockSchema):
        result: bool = SchemaField(
            description=""The result of the condition evaluation (True or False)""
        )
        yes_output: Any = SchemaField(
            description=""The output value if the condition is true""
        )
        no_output: Any = SchemaField(
            description=""The output value if the condition is false""
        )

    def __init__(self):
        super().__init__(
            id=""715696a0-e1da-45c8-b209-c2fa9c3b0be6"",
            input_schema=ConditionBlock.Input,
            output_schema=ConditionBlock.Output,
            description=""Handles conditional logic based on comparison operators"",
            categories={BlockCategory.LOGIC},
            test_input={
                ""value1"": 10,
                ""operator"": ComparisonOperator.GREATER_THAN.value,
                ""value2"": 5,
                ""yes_value"": ""Greater"",
                ""no_value"": ""Not greater"",
            },
            test_output=[
                (""result"", True),
                (""yes_output"", ""Greater""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        operator = input_data.operator

        value1 = input_data.value1
        if isinstance(value1, str):
            try:
                value1 = float(value1.strip())
            except ValueError:
                value1 = value1.strip()

        value2 = input_data.value2
        if isinstance(value2, str):
            try:
                value2 = float(value2.strip())
            except ValueError:
                value2 = value2.strip()

        yes_value = input_data.yes_value if input_data.yes_value is not None else value1
        no_value = input_data.no_value if input_data.no_value is not None else value2

        comparison_funcs = {
            ComparisonOperator.EQUAL: lambda a, b: a == b,
            ComparisonOperator.NOT_EQUAL: lambda a, b: a != b,
            ComparisonOperator.GREATER_THAN: lambda a, b: a > b,
            ComparisonOperator.LESS_THAN: lambda a, b: a < b,
            ComparisonOperator.GREATER_THAN_OR_EQUAL: lambda a, b: a >= b,
            ComparisonOperator.LESS_THAN_OR_EQUAL: lambda a, b: a <= b,
        }

        result = comparison_funcs[operator](value1, value2)

        yield ""result"", result

        if result:
            yield ""yes_output"", yes_value
        else:
            yield ""no_output"", no_value
","from enum import Enum
from typing import Any

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

class ComparisonOperator(Enum):
    EQUAL = ""==""
    NOT_EQUAL = ""!=""
    GREATER_THAN = "">""
    LESS_THAN = ""<""
    GREATER_THAN_OR_EQUAL = "">=""
    LESS_THAN_OR_EQUAL = ""<=""

class ConditionBlock(Block):
    class Input(BlockSchema):
        value1: Any = SchemaField(
            description=""Enter the first value for comparison"",
            placeholder=""For example: 10 or 'hello' or True"",
        )
        operator: ComparisonOperator = SchemaField(
            description=""Choose the comparison operator"",
            placeholder=""Select an operator"",
        )
        value2: Any = SchemaField(
            description=""Enter the second value for comparison"",
            placeholder=""For example: 20 or 'world' or False"",
        )
        yes_value: Any = SchemaField(
            description=""(Optional) Value to output if the condition is true. If not provided, value1 will be used."",
            placeholder=""Leave empty to use value1, or enter a specific value"",
            default=None,
        )
        no_value: Any = SchemaField(
            description=""(Optional) Value to output if the condition is false. If not provided, value1 will be used."",
            placeholder=""Leave empty to use value1, or enter a specific value"",
            default=None,
        )

    class Output(BlockSchema):
        result: bool = SchemaField(
            description=""The result of the condition evaluation (True or False)""
        )
        yes_output: Any = SchemaField(
            description=""The output value if the condition is true""
        )
        no_output: Any = SchemaField(
            description=""The output value if the condition is false""
        )

    def __init__(self):
        super().__init__(
            id=""715696a0-e1da-45c8-b209-c2fa9c3b0be6"",
            input_schema=ConditionBlock.Input,
            output_schema=ConditionBlock.Output,
            description=""Handles conditional logic based on comparison operators"",
            categories={BlockCategory.LOGIC},
            test_input={
                ""value1"": 10,
                ""operator"": ComparisonOperator.GREATER_THAN.value,
                ""value2"": 5,
                ""yes_value"": ""Greater"",
                ""no_value"": ""Not greater"",
            },
            test_output=[
                (""result"", True),
                (""yes_output"", ""Greater""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        operator = input_data.operator

        value1 = input_data.value1
        if isinstance(value1, str):
            try:
                value1 = float(value1.strip())
            except ValueError:
                value1 = value1.strip()

        value2 = input_data.value2
        if isinstance(value2, str):
            try:
                value2 = float(value2.strip())
            except ValueError:
                value2 = value2.strip()

        yes_value = input_data.yes_value if input_data.yes_value is not None else value1
        no_value = input_data.no_value if input_data.no_value is not None else value2

        comparison_funcs = {
            ComparisonOperator.EQUAL: lambda a, b: a == b,
            ComparisonOperator.NOT_EQUAL: lambda a, b: a != b,
            ComparisonOperator.GREATER_THAN: lambda a, b: a > b,
            ComparisonOperator.LESS_THAN: lambda a, b: a < b,
            ComparisonOperator.GREATER_THAN_OR_EQUAL: lambda a, b: a >= b,
            ComparisonOperator.LESS_THAN_OR_EQUAL: lambda a, b: a <= b,
        }

        result = comparison_funcs[operator](value1, value2)

        yield ""result"", result

        if result:
            yield ""yes_output"", yes_value
        else:
            yield ""no_output"", no_value","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
59,"import logging
import time
from typing import Sequence

from backend.data import db
from backend.data.block import Block, initialize_blocks
from backend.data.execution import ExecutionResult, ExecutionStatus
from backend.data.model import CREDENTIALS_FIELD_NAME
from backend.data.user import create_default_user
from backend.executor import DatabaseManager, ExecutionManager, ExecutionScheduler
from backend.server.rest_api import AgentServer
from backend.server.utils import get_user_id

log = logging.getLogger(__name__)


class SpinTestServer:
    def __init__(self):
        self.db_api = DatabaseManager()
        self.exec_manager = ExecutionManager()
        self.agent_server = AgentServer()
        self.scheduler = ExecutionScheduler()

    @staticmethod
    def test_get_user_id():
        return ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""

    async def __aenter__(self):
        self.setup_dependency_overrides()
        self.db_api.__enter__()
        self.agent_server.__enter__()
        self.exec_manager.__enter__()
        self.scheduler.__enter__()

        await db.connect()
        await initialize_blocks()
        await create_default_user()

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await db.disconnect()

        self.scheduler.__exit__(exc_type, exc_val, exc_tb)
        self.exec_manager.__exit__(exc_type, exc_val, exc_tb)
        self.agent_server.__exit__(exc_type, exc_val, exc_tb)
        self.db_api.__exit__(exc_type, exc_val, exc_tb)

    def setup_dependency_overrides(self):
        # Override get_user_id for testing
        self.agent_server.set_test_dependency_overrides(
            {get_user_id: self.test_get_user_id}
        )


async def wait_execution(
    user_id: str,
    graph_id: str,
    graph_exec_id: str,
    timeout: int = 20,
) -> Sequence[ExecutionResult]:
    async def is_execution_completed():
        status = await AgentServer().test_get_graph_run_status(graph_exec_id, user_id)
        log.info(f""Execution status: {status}"")
        if status == ExecutionStatus.FAILED:
            log.info(""Execution failed"")
            raise Exception(""Execution failed"")
        return status == ExecutionStatus.COMPLETED

    # Wait for the executions to complete
    for i in range(timeout):
        if await is_execution_completed():
            return await AgentServer().test_get_graph_run_node_execution_results(
                graph_id, graph_exec_id, user_id
            )
        time.sleep(1)

    assert False, ""Execution did not complete in time.""


def execute_block_test(block: Block):
    prefix = f""[Test-{block.name}]""

    if not block.test_input or not block.test_output:
        log.info(f""{prefix} No test data provided"")
        return
    if not isinstance(block.test_input, list):
        block.test_input = [block.test_input]
    if not isinstance(block.test_output, list):
        block.test_output = [block.test_output]

    output_index = 0
    log.info(f""{prefix} Executing {len(block.test_input)} tests..."")
    prefix = "" "" * 4 + prefix

    for mock_name, mock_obj in (block.test_mock or {}).items():
        log.info(f""{prefix} mocking {mock_name}..."")
        if hasattr(block, mock_name):
            setattr(block, mock_name, mock_obj)
        else:
            log.info(f""{prefix} mock {mock_name} not found in block"")

    extra_exec_kwargs = {}

    if CREDENTIALS_FIELD_NAME in block.input_schema.model_fields:
        if not block.test_credentials:
            raise ValueError(
                f""{prefix} requires credentials but has no test_credentials""
            )
        extra_exec_kwargs[CREDENTIALS_FIELD_NAME] = block.test_credentials

    for input_data in block.test_input:
        log.info(f""{prefix} in: {input_data}"")

        for output_name, output_data in block.execute(input_data, **extra_exec_kwargs):
            if output_index >= len(block.test_output):
                raise ValueError(f""{prefix} produced output more than expected"")
            ex_output_name, ex_output_data = block.test_output[output_index]

            def compare(data, expected_data):
                if data == expected_data:
                    is_matching = True
                elif isinstance(expected_data, type):
                    is_matching = isinstance(data, expected_data)
                elif callable(expected_data):
                    is_matching = expected_data(data)
                else:
                    is_matching = False

                mark = ""✅"" if is_matching else ""❌""
                log.info(f""{prefix} {mark} comparing `{data}` vs `{expected_data}`"")
                if not is_matching:
                    raise ValueError(
                        f""{prefix}: wrong output {data} vs {expected_data}""
                    )

            compare(output_data, ex_output_data)
            compare(output_name, ex_output_name)
            output_index += 1

    if output_index < len(block.test_output):
        raise ValueError(
            f""{prefix} produced output less than expected. output_index={output_index}, len(block.test_output)={len(block.test_output)}""
        )
","import logging
import time
from typing import Sequence

from backend.data import db
from backend.data.block import Block, initialize_blocks
from backend.data.execution import ExecutionResult, ExecutionStatus
from backend.data.model import CREDENTIALS_FIELD_NAME
from backend.data.user import create_default_user
from backend.executor import DatabaseManager, ExecutionManager, ExecutionScheduler
from backend.server.rest_api import AgentServer
from backend.server.utils import get_user_id

log = logging.getLogger(__name__)


class SpinTestServer:
    def __init__(self):
        self.db_api = DatabaseManager()
        self.exec_manager = ExecutionManager()
        self.agent_server = AgentServer()
        self.scheduler = ExecutionScheduler()

    @staticmethod
    def test_get_user_id():
        return ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""

    async def __aenter__(self):
        self.setup_dependency_overrides()
        self.db_api.__enter__()
        self.agent_server.__enter__()
        self.exec_manager.__enter__()
        self.scheduler.__enter__()

        await db.connect()
        await initialize_blocks()
        await create_default_user()

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await db.disconnect()
        self.scheduler.__exit__(exc_type, exc_val, exc_tb)
        self.exec_manager.__exit__(exc_type, exc_val, exc_tb)
        self.agent_server.__exit__(exc_type, exc_val, exc_tb)
        self.db_api.__exit__(exc_type, exc_val, exc_tb)

    def setup_dependency_overrides(self):
        # Override get_user_id for testing
        self.agent_server.set_test_dependency_overrides(
            {get_user_id: self.test_get_user_id}
        )


async def wait_execution(
    user_id: str,
    graph_id: str,
    graph_exec_id: str,
    timeout: int = 20,
) -> Sequence[ExecutionResult]:
    async def is_execution_completed():
        status = await AgentServer().test_get_graph_run_status(graph_exec_id, user_id)
        log.info(f""Execution status: {status}"")
        if status == ExecutionStatus.FAILED:
            log.info(""Execution failed"")
            raise Exception(""Execution failed"")
        return status == ExecutionStatus.COMPLETED

    # Wait for the executions to complete
    for i in range(timeout):
        if await is_execution_completed():
            return await AgentServer().test_get_graph_run_node_execution_results(
                graph_id, graph_exec_id, user_id
            )
        time.sleep(1)
    assert False, ""Execution did not complete in time.""


def execute_block_test(block: Block):
    prefix = f""[Test-{block.name}]""
    if not block.test_input or not block.test_output:
        log.info(f""{prefix} No test data provided"")
        return

    if not isinstance(block.test_input, list):
        block.test_input = [block.test_input]

    if not isinstance(block.test_output, list):
        block.test_output = [block.test_output]

    output_index = 0
    log.info(f""{prefix} Executing {len(block.test_input)} tests..."")
    prefix = "" "" * 4 + prefix

    for mock_name, mock_obj in (block.test_mock or {}).items():
        log.info(f""{prefix} mocking {mock_name}..."")
        if hasattr(block, mock_name):
            setattr(block, mock_name, mock_obj)
        else:
            log.info(f""{prefix} mock {mock_name} not found in block"")

    extra_exec_kwargs = {}
    if CREDENTIALS_FIELD_NAME in block.input_schema.model_fields:
        if not block.test_credentials:
            raise ValueError(
                f""{prefix} requires credentials but has no test_credentials""
            )
        extra_exec_kwargs[CREDENTIALS_FIELD_NAME] = block.test_credentials

    for input_data in block.test_input:
        log.info(f""{prefix} in: {input_data}"")
        for output_name, output_data in block.execute(input_data, **extra_exec_kwargs):
            if output_index >= len(block.test_output):
                raise ValueError(f""{prefix} produced output more than expected"")
            ex_output_name, ex_output_data = block.test_output[output_index]

            def compare(data, expected_data):
                if data == expected_data:
                    is_matching = True
                elif isinstance(expected_data, type):
                    is_matching = isinstance(data, expected_data)
                elif callable(expected_data):
                    is_matching = expected_data(data)
                else:
                    is_matching = False

                mark = ""✅"" if is_matching else ""❌""
                log.info(f""{prefix} {mark} comparing `{data}` vs `{expected_data}`"")
                if not is_matching:
                    raise ValueError(
                        f""{prefix}: wrong output {data} vs {expected_data}""
                    )

            compare(output_data, ex_output_data)
            compare(output_name, ex_output_name)
            output_index += 1

    if output_index < len(block.test_output):
        raise ValueError(
            f""{prefix} produced output less than expected. output_index={output_index}, len(block.test_output)={len(block.test_output)}""
        )","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
60,"import json
import os
from enum import Enum
from typing import Any, Dict, Generic, List, Set, Tuple, Type, TypeVar

from pydantic import BaseModel, Field, PrivateAttr, ValidationInfo, field_validator
from pydantic_settings import (
    BaseSettings,
    JsonConfigSettingsSource,
    PydanticBaseSettingsSource,
    SettingsConfigDict,
)

from backend.util.data import get_data_path

T = TypeVar(""T"", bound=BaseSettings)


class AppEnvironment(str, Enum):
    LOCAL = ""local""
    DEVELOPMENT = ""dev""
    PRODUCTION = ""prod""


class BehaveAs(str, Enum):
    LOCAL = ""local""
    CLOUD = ""cloud""


class UpdateTrackingModel(BaseModel, Generic[T]):
    _updated_fields: Set[str] = PrivateAttr(default_factory=set)

    def __setattr__(self, name: str, value) -> None:
        if name in self.model_fields:
            self._updated_fields.add(name)
        super().__setattr__(name, value)

    def mark_updated(self, field_name: str) -> None:
        if field_name in self.model_fields:
            self._updated_fields.add(field_name)

    def clear_updates(self) -> None:
        self._updated_fields.clear()

    def get_updates(self) -> Dict[str, Any]:
        return {field: getattr(self, field) for field in self._updated_fields}

    @property
    def updated_fields(self):
        return self._updated_fields


class Config(UpdateTrackingModel[""Config""], BaseSettings):
    """"""Config for the server.""""""

    num_graph_workers: int = Field(
        default=10,
        ge=1,
        le=1000,
        description=""Maximum number of workers to use for graph execution."",
    )
    num_node_workers: int = Field(
        default=5,
        ge=1,
        le=1000,
        description=""Maximum number of workers to use for node execution within a single graph."",
    )
    pyro_host: str = Field(
        default=""localhost"",
        description=""The default hostname of the Pyro server."",
    )
    pyro_client_comm_timeout: float = Field(
        default=15,
        description=""The default timeout in seconds, for Pyro client connections."",
    )
    pyro_client_comm_retry: int = Field(
        default=3,
        description=""The default number of retries for Pyro client connections."",
    )
    enable_auth: bool = Field(
        default=True,
        description=""If authentication is enabled or not"",
    )
    enable_credit: str = Field(
        default=""false"",
        description=""If user credit system is enabled or not"",
    )
    num_user_credits_refill: int = Field(
        default=1500,
        description=""Number of credits to refill for each user"",
    )
    # Add more configuration fields as needed

    model_config = SettingsConfigDict(
        env_file="".env"",
        extra=""allow"",
    )

    websocket_server_host: str = Field(
        default=""0.0.0.0"",
        description=""The host for the websocket server to run on"",
    )

    websocket_server_port: int = Field(
        default=8001,
        description=""The port for the websocket server to run on"",
    )

    execution_manager_port: int = Field(
        default=8002,
        description=""The port for execution manager daemon to run on"",
    )

    execution_scheduler_port: int = Field(
        default=8003,
        description=""The port for execution scheduler daemon to run on"",
    )

    agent_server_port: int = Field(
        default=8004,
        description=""The port for agent server daemon to run on"",
    )

    database_api_port: int = Field(
        default=8005,
        description=""The port for database server API to run on"",
    )

    agent_api_host: str = Field(
        default=""0.0.0.0"",
        description=""The host for agent server API to run on"",
    )

    agent_api_port: int = Field(
        default=8006,
        description=""The port for agent server API to run on"",
    )

    platform_base_url: str = Field(
        default="""",
        description=""Must be set so the application knows where it's hosted at. ""
        ""This is necessary to make sure webhooks find their way."",
    )

    frontend_base_url: str = Field(
        default="""",
        description=""Can be used to explicitly set the base URL for the frontend. ""
        ""This value is then used to generate redirect URLs for OAuth flows."",
    )

    media_gcs_bucket_name: str = Field(
        default="""",
        description=""The name of the Google Cloud Storage bucket for media files"",
    )

    @field_validator(""platform_base_url"", ""frontend_base_url"")
    @classmethod
    def validate_platform_base_url(cls, v: str, info: ValidationInfo) -> str:
        if not v:
            return v
        if not v.startswith((""http://"", ""https://"")):
            raise ValueError(
                f""{info.field_name} must be a full URL ""
                ""including a http:// or https:// schema""
            )
        if v.endswith(""/""):
            return v.rstrip(""/"")  # Remove trailing slash
        return v

    app_env: AppEnvironment = Field(
        default=AppEnvironment.LOCAL,
        description=""The name of the app environment: local or dev or prod"",
    )

    behave_as: BehaveAs = Field(
        default=BehaveAs.LOCAL,
        description=""What environment to behave as: local or cloud"",
    )

    execution_event_bus_name: str = Field(
        default=""execution_event"",
        description=""Name of the event bus"",
    )

    trust_endpoints_for_requests: List[str] = Field(
        default_factory=list,
        description=""A whitelist of trusted internal endpoints for the backend to make requests to."",
    )

    backend_cors_allow_origins: List[str] = Field(default_factory=list)

    @field_validator(""backend_cors_allow_origins"")
    @classmethod
    def validate_cors_allow_origins(cls, v: List[str]) -> List[str]:
        out = []
        port = None
        has_localhost = False
        has_127_0_0_1 = False
        for url in v:
            url = url.strip()
            if url.startswith((""http://"", ""https://"")):
                if ""localhost"" in url:
                    port = url.split("":"")[2]
                    has_localhost = True
                if ""127.0.0.1"" in url:
                    port = url.split("":"")[2]
                    has_127_0_0_1 = True
                out.append(url)
            else:
                raise ValueError(f""Invalid URL: {url}"")

        if has_127_0_0_1 and not has_localhost:
            out.append(f""http://localhost:{port}"")
        if has_localhost and not has_127_0_0_1:
            out.append(f""http://127.0.0.1:{port}"")

        return out

    @classmethod
    def settings_customise_sources(
        cls,
        settings_cls: Type[BaseSettings],
        init_settings: PydanticBaseSettingsSource,
        env_settings: PydanticBaseSettingsSource,
        dotenv_settings: PydanticBaseSettingsSource,
        file_secret_settings: PydanticBaseSettingsSource,
    ) -> Tuple[PydanticBaseSettingsSource, ...]:
        return (
            env_settings,
            file_secret_settings,
            dotenv_settings,
            JsonConfigSettingsSource(settings_cls),
            init_settings,
        )


class Secrets(UpdateTrackingModel[""Secrets""], BaseSettings):
    """"""Secrets for the server.""""""

    supabase_url: str = Field(default="""", description=""Supabase URL"")
    supabase_service_role_key: str = Field(
        default="""", description=""Supabase service role key""
    )

    encryption_key: str = Field(default="""", description=""Encryption key"")

    # OAuth server credentials for integrations
    # --8<-- [start:OAuthServerCredentialsExample]
    github_client_id: str = Field(default="""", description=""GitHub OAuth client ID"")
    github_client_secret: str = Field(
        default="""", description=""GitHub OAuth client secret""
    )
    # --8<-- [end:OAuthServerCredentialsExample]
    google_client_id: str = Field(default="""", description=""Google OAuth client ID"")
    google_client_secret: str = Field(
        default="""", description=""Google OAuth client secret""
    )
    notion_client_id: str = Field(default="""", description=""Notion OAuth client ID"")
    notion_client_secret: str = Field(
        default="""", description=""Notion OAuth client secret""
    )

    openai_api_key: str = Field(default="""", description=""OpenAI API key"")
    anthropic_api_key: str = Field(default="""", description=""Anthropic API key"")
    groq_api_key: str = Field(default="""", description=""Groq API key"")
    open_router_api_key: str = Field(default="""", description=""Open Router API Key"")

    reddit_client_id: str = Field(default="""", description=""Reddit client ID"")
    reddit_client_secret: str = Field(default="""", description=""Reddit client secret"")
    reddit_username: str = Field(default="""", description=""Reddit username"")
    reddit_password: str = Field(default="""", description=""Reddit password"")

    openweathermap_api_key: str = Field(
        default="""", description=""OpenWeatherMap API key""
    )

    medium_api_key: str = Field(default="""", description=""Medium API key"")
    medium_author_id: str = Field(default="""", description=""Medium author ID"")
    did_api_key: str = Field(default="""", description=""D-ID API Key"")
    revid_api_key: str = Field(default="""", description=""revid.ai API key"")
    discord_bot_token: str = Field(default="""", description=""Discord bot token"")

    smtp_server: str = Field(default="""", description=""SMTP server IP"")
    smtp_port: str = Field(default="""", description=""SMTP server port"")
    smtp_username: str = Field(default="""", description=""SMTP username"")
    smtp_password: str = Field(default="""", description=""SMTP password"")

    sentry_dsn: str = Field(default="""", description=""Sentry DSN"")

    google_maps_api_key: str = Field(default="""", description=""Google Maps API Key"")

    replicate_api_key: str = Field(default="""", description=""Replicate API Key"")
    unreal_speech_api_key: str = Field(default="""", description=""Unreal Speech API Key"")
    ideogram_api_key: str = Field(default="""", description=""Ideogram API Key"")
    jina_api_key: str = Field(default="""", description=""Jina API Key"")
    unreal_speech_api_key: str = Field(default="""", description=""Unreal Speech API Key"")

    fal_key: str = Field(default="""", description=""FAL API key"")

    # Add more secret fields as needed

    model_config = SettingsConfigDict(
        env_file="".env"",
        env_file_encoding=""utf-8"",
        extra=""allow"",
    )


class Settings(BaseModel):
    config: Config = Config()
    secrets: Secrets = Secrets()

    def save(self) -> None:
        # Save updated config to JSON file
        if self.config.updated_fields:
            config_to_save = self.config.get_updates()
            config_path = os.path.join(get_data_path(), ""config.json"")
            if os.path.exists(config_path):
                with open(config_path, ""r+"") as f:
                    existing_config: Dict[str, Any] = json.load(f)
                    existing_config.update(config_to_save)
                    f.seek(0)
                    json.dump(existing_config, f, indent=2)
                    f.truncate()
            else:
                with open(config_path, ""w"") as f:
                    json.dump(config_to_save, f, indent=2)
            self.config.clear_updates()
","import json
import os
from enum import Enum
from typing import Any, Dict, Generic, List, Set, Tuple, Type, TypeVar

from pydantic import BaseModel, Field, PrivateAttr, ValidationInfo, field_validator
from pydantic_settings import (
    BaseSettings,
    JsonConfigSettingsSource,
    PydanticBaseSettingsSource,
    SettingsConfigDict,
)
from backend.util.data import get_data_path

T = TypeVar(""T"", bound=BaseSettings)


class AppEnvironment(str, Enum):
    LOCAL = ""local""
    DEVELOPMENT = ""dev""
    PRODUCTION = ""prod""


class BehaveAs(str, Enum):
    LOCAL = ""local""
    CLOUD = ""cloud""


class UpdateTrackingModel(BaseModel, Generic[T]):
    _updated_fields: Set[str] = PrivateAttr(default_factory=set)

    def __setattr__(self, name: str, value) -> None:
        if name in self.model_fields:
            self._updated_fields.add(name)
        super().__setattr__(name, value)

    def mark_updated(self, field_name: str) -> None:
        if field_name in self.model_fields:
            self._updated_fields.add(field_name)

    def clear_updates(self) -> None:
        self._updated_fields.clear()

    def get_updates(self) -> Dict[str, Any]:
        return {field: getattr(self, field) for field in self._updated_fields}

    @property
    def updated_fields(self):
        return self._updated_fields


class Config(UpdateTrackingModel[""Config""], BaseSettings):
    """"""Config for the server.""""""

    num_graph_workers: int = Field(
        default=10,
        ge=1,
        le=1000,
        description=""Maximum number of workers to use for graph execution."",
    )
    num_node_workers: int = Field(
        default=5,
        ge=1,
        le=1000,
        description=""Maximum number of workers to use for node execution within a single graph."",
    )
    pyro_host: str = Field(
        default=""localhost"",
        description=""The default hostname of the Pyro server."",
    )
    pyro_client_comm_timeout: float = Field(
        default=15,
        description=""The default timeout in seconds, for Pyro client connections."",
    )
    pyro_client_comm_retry: int = Field(
        default=3,
        description=""The default number of retries for Pyro client connections."",
    )
    enable_auth: bool = Field(
        default=True,
        description=""If authentication is enabled or not"",
    )
    enable_credit: str = Field(
        default=""false"",
        description=""If user credit system is enabled or not"",
    )
    num_user_credits_refill: int = Field(
        default=1500,
        description=""Number of credits to refill for each user"",
    )
    # Add more configuration fields as needed

    model_config = SettingsConfigDict(
        env_file="".env"",
        extra=""allow"",
    )
    websocket_server_host: str = Field(
        default=""0.0.0.0"",
        description=""The host for the websocket server to run on"",
    )
    websocket_server_port: int = Field(
        default=8001,
        description=""The port for the websocket server to run on"",
    )
    execution_manager_port: int = Field(
        default=8002,
        description=""The port for execution manager daemon to run on"",
    )
    execution_scheduler_port: int = Field(
        default=8003,
        description=""The port for execution scheduler daemon to run on"",
    )
    agent_server_port: int = Field(
        default=8004,
        description=""The port for agent server daemon to run on"",
    )
    database_api_port: int = Field(
        default=8005,
        description=""The port for database server API to run on"",
    )
    agent_api_host: str = Field(
        default=""0.0.0.0"",
        description=""The host for agent server API to run on"",
    )
    agent_api_port: int = Field(
        default=8006,
        description=""The port for agent server API to run on"",
    )
    platform_base_url: str = Field(
        default="""",
        description=""Must be set so the application knows where it's hosted at. ""
                    ""This is necessary to make sure webhooks find their way."",
    )
    frontend_base_url: str = Field(
        default="""",
        description=""Can be used to explicitly set the base URL for the frontend. ""
                    ""This value is then used to generate redirect URLs for OAuth flows."",
    )
    media_gcs_bucket_name: str = Field(
        default="""",
        description=""The name of the Google Cloud Storage bucket for media files"",
    )

    @field_validator(""platform_base_url"", ""frontend_base_url"")
    @classmethod
    def validate_platform_base_url(cls, v: str, info: ValidationInfo) -> str:
        if not v:
            return v
        if not v.startswith((""http://"", ""https://"")):
            raise ValueError(
                f""{info.field_name} must be a full URL ""
                ""including a http:// or https:// schema""
            )
        if v.endswith(""/""):
            return v.rstrip(""/"")  # Remove trailing slash
        return v

    app_env: AppEnvironment = Field(
        default=AppEnvironment.LOCAL,
        description=""The name of the app environment: local or dev or prod"",
    )

    behave_as: BehaveAs = Field(
        default=BehaveAs.LOCAL,
        description=""What environment to behave as: local or cloud"",
    )

    execution_event_bus_name: str = Field(
        default=""execution_event"",
        description=""Name of the event bus"",
    )
    trust_endpoints_for_requests: List[str] = Field(
        default_factory=list,
        description=""A whitelist of trusted internal endpoints for the backend to make requests to."",
    )
    backend_cors_allow_origins: List[str] = Field(default_factory=list)

    @field_validator(""backend_cors_allow_origins"")
    @classmethod
    def validate_cors_allow_origins(cls, v: List[str]) -> List[str]:
        out = []
        port = None
        has_localhost = False
        has_127_0_0_1 = False
        for url in v:
            url = url.strip()
            if url.startswith((""http://"", ""https://"")):
                if ""localhost"" in url:
                    port = url.split("":"")[2]
                    has_localhost = True
                if ""127.0.0.1"" in url:
                    port = url.split("":"")[2]
                    has_127_0_0_1 = True
                out.append(url)
            else:
                raise ValueError(f""Invalid URL: {url}"")

        if has_127_0_0_1 and not has_localhost:
            out.append(f""http://localhost:{port}"")
        if has_localhost and not has_127_0_0_1:
            out.append(f""http://127.0.0.1:{port}"")

        return out

    @classmethod
    def settings_customise_sources(
        cls,
        settings_cls: Type[BaseSettings],
        init_settings: PydanticBaseSettingsSource,
        env_settings: PydanticBaseSettingsSource,
        dotenv_settings: PydanticBaseSettingsSource,
        file_secret_settings: PydanticBaseSettingsSource,
    ) -> Tuple[PydanticBaseSettingsSource, ...]:
        return (
            env_settings,
            file_secret_settings,
            dotenv_settings,
            JsonConfigSettingsSource(settings_cls),
            init_settings,
        )


class Secrets(UpdateTrackingModel[""Secrets""], BaseSettings):
    """"""Secrets for the server.""""""

    supabase_url: str = Field(default="""", description=""Supabase URL"")
    supabase_service_role_key: str = Field(
        default="""", description=""Supabase service role key""
    )
    encryption_key: str = Field(default="""", description=""Encryption key"")
    # OAuth server credentials for integrations
    # --8<-- [start:OAuthServerCredentialsExample]
    github_client_id: str = Field(default="""", description=""GitHub OAuth client ID"")
    github_client_secret: str = Field(
        default="""", description=""GitHub OAuth client secret""
    )
    # --8<-- [end:OAuthServerCredentialsExample]
    google_client_id: str = Field(default="""", description=""Google OAuth client ID"")
    google_client_secret: str = Field(
        default="""", description=""Google OAuth client secret""
    )
    notion_client_id: str = Field(default="""", description=""Notion OAuth client ID"")
    notion_client_secret: str = Field(
        default="""", description=""Notion OAuth client secret""
    )

    openai_api_key: str = Field(default="""", description=""OpenAI API key"")
    anthropic_api_key: str = Field(default="""", description=""Anthropic API key"")
    groq_api_key: str = Field(default="""", description=""Groq API key"")
    open_router_api_key: str = Field(default="""", description=""Open Router API Key"")

    reddit_client_id: str = Field(default="""", description=""Reddit client ID"")
    reddit_client_secret: str = Field(default="""", description=""Reddit client secret"")
    reddit_username: str = Field(default="""", description=""Reddit username"")
    reddit_password: str = Field(default="""", description=""Reddit password"")

    openweathermap_api_key: str = Field(
        default="""", description=""OpenWeatherMap API key""
    )

    medium_api_key: str = Field(default="""", description=""Medium API key"")
    medium_author_id: str = Field(default="""", description=""Medium author ID"")
    did_api_key: str = Field(default="""", description=""D-ID API Key"")
    revid_api_key: str = Field(default="""", description=""revid.ai API key"")
    discord_bot_token: str = Field(default="""", description=""Discord bot token"")

    smtp_server: str = Field(default="""", description=""SMTP server IP"")
    smtp_port: str = Field(default="""", description=""SMTP server port"")
    smtp_username: str = Field(default="""", description=""SMTP username"")
    smtp_password: str = Field(default="""", description=""SMTP password"")

    sentry_dsn: str = Field(default="""", description=""Sentry DSN"")

    google_maps_api_key: str = Field(default="""", description=""Google Maps API Key"")

    replicate_api_key: str = Field(default="""", description=""Replicate API Key"")
    unreal_speech_api_key: str = Field(default="""", description=""Unreal Speech API Key"")
    ideogram_api_key: str = Field(default="""", description=""Ideogram API Key"")
    jina_api_key: str = Field(default="""", description=""Jina API Key"")
    unreal_speech_api_key: str = Field(default="""", description=""Unreal Speech API Key"")

    fal_key: str = Field(default="""", description=""FAL API key"")

    # Add more secret fields as needed

    model_config = SettingsConfigDict(
        env_file="".env"",
        env_file_encoding=""utf-8"",
        extra=""allow"",
    )


class Settings(BaseModel):
    config: Config = Config()
    secrets: Secrets = Secrets()

    def save(self) -> None:
        # Save updated config to JSON file
        if self.config.updated_fields:
            config_to_save = self.config.get_updates()
            config_path = os.path.join(get_data_path(), ""config.json"")
            if os.path.exists(config_path):
                with open(config_path, ""r+"") as f:
                    existing_config: Dict[str, Any] = json.load(f)
                    existing_config.update(config_to_save)
                    f.seek(0)
                    json.dump(existing_config, f, indent=2)
                    f.truncate()
            else:
                with open(config_path, ""w"") as f:
                    json.dump(config_to_save, f, indent=2)
            self.config.clear_updates()","{'final_score': 99.82, 'length_similarity': 99.78, 'token_similarity': 100.0, 'structure_similarity': 99.26, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
61,"import json
from typing import Any, Type, TypeVar, overload

import jsonschema
from fastapi.encoders import jsonable_encoder

from .type import type_match


def to_dict(data) -> dict:
    return jsonable_encoder(data)


def dumps(data) -> str:
    return json.dumps(jsonable_encoder(data))


T = TypeVar(""T"")


@overload
def loads(data: str, *args, target_type: Type[T], **kwargs) -> T: ...


@overload
def loads(data: str, *args, **kwargs) -> Any: ...


def loads(data: str, *args, target_type: Type[T] | None = None, **kwargs) -> Any:
    parsed = json.loads(data, *args, **kwargs)
    if target_type:
        return type_match(parsed, target_type)
    return parsed


def validate_with_jsonschema(
    schema: dict[str, Any], data: dict[str, Any]
) -> str | None:
    """"""
    Validate the data against the schema.
    Returns the validation error message if the data does not match the schema.
    """"""
    try:
        jsonschema.validate(data, schema)
        return None
    except jsonschema.ValidationError as e:
        return str(e)
","import json
from typing import Any, Type, TypeVar, overload
import jsonschema
from fastapi.encoders import jsonable_encoder
from .type import type_match

def to_dict(data) -> dict:
    return jsonable_encoder(data)

def dumps(data) -> str:
    return json.dumps(jsonable_encoder(data))

T = TypeVar(""T"")

@overload
def loads(data: str, *args, target_type: Type[T], **kwargs) -> T: ...

@overload
def loads(data: str, *args, **kwargs) -> Any: ...

def loads(data: str, *args, target_type: Type[T] | None = None, **kwargs) -> Any:
    parsed = json.loads(data, *args, **kwargs)
    if target_type:
        return type_match(parsed, target_type)
    return parsed

def validate_with_jsonschema(
    schema: dict[str, Any], data: dict[str, Any]
) -> str | None:
    """"""
    Validate the data against the schema.
    Returns the validation error message if the data does not match the schema.
    """"""
    try:
        jsonschema.validate(data, schema)
        return None
    except jsonschema.ValidationError as e:
        return str(e)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
62,"from urllib.parse import parse_qs, urlparse

from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class TranscribeYoutubeVideoBlock(Block):
    class Input(BlockSchema):
        youtube_url: str = SchemaField(
            title=""YouTube URL"",
            description=""The URL of the YouTube video to transcribe"",
            placeholder=""https://www.youtube.com/watch?v=dQw4w9WgXcQ"",
        )

    class Output(BlockSchema):
        video_id: str = SchemaField(description=""The extracted YouTube video ID"")
        transcript: str = SchemaField(description=""The transcribed text of the video"")
        error: str = SchemaField(
            description=""Any error message if the transcription fails""
        )

    def __init__(self):
        super().__init__(
            id=""f3a8f7e1-4b1d-4e5f-9f2a-7c3d5a2e6b4c"",
            input_schema=TranscribeYoutubeVideoBlock.Input,
            output_schema=TranscribeYoutubeVideoBlock.Output,
            description=""Transcribes a YouTube video."",
            categories={BlockCategory.SOCIAL},
            test_input={""youtube_url"": ""https://www.youtube.com/watch?v=dQw4w9WgXcQ""},
            test_output=[
                (""video_id"", ""dQw4w9WgXcQ""),
                (
                    ""transcript"",
                    ""Never gonna give you up\nNever gonna let you down"",
                ),
            ],
            test_mock={
                ""get_transcript"": lambda video_id: [
                    {""text"": ""Never gonna give you up""},
                    {""text"": ""Never gonna let you down""},
                ],
            },
        )

    @staticmethod
    def extract_video_id(url: str) -> str:
        parsed_url = urlparse(url)
        if parsed_url.netloc == ""youtu.be"":
            return parsed_url.path[1:]
        if parsed_url.netloc in (""www.youtube.com"", ""youtube.com""):
            if parsed_url.path == ""/watch"":
                p = parse_qs(parsed_url.query)
                return p[""v""][0]
            if parsed_url.path[:7] == ""/embed/"":
                return parsed_url.path.split(""/"")[2]
            if parsed_url.path[:3] == ""/v/"":
                return parsed_url.path.split(""/"")[2]
        raise ValueError(f""Invalid YouTube URL: {url}"")

    @staticmethod
    def get_transcript(video_id: str):
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

            if not transcript_list:
                raise ValueError(f""No transcripts found for the video: {video_id}"")

            for transcript in transcript_list:
                first_transcript = transcript_list.find_transcript(
                    [transcript.language_code]
                )
                return YouTubeTranscriptApi.get_transcript(
                    video_id, languages=[first_transcript.language_code]
                )

        except Exception:
            raise ValueError(f""No transcripts found for the video: {video_id}"")

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        video_id = self.extract_video_id(input_data.youtube_url)
        yield ""video_id"", video_id

        transcript = self.get_transcript(video_id)
        formatter = TextFormatter()
        transcript_text = formatter.format_transcript(transcript)

        yield ""transcript"", transcript_text
","from urllib.parse import parse_qs, urlparse
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

class TranscribeYoutubeVideoBlock(Block):
    class Input(BlockSchema):
        youtube_url: str = SchemaField(
            title=""YouTube URL"",
            description=""The URL of the YouTube video to transcribe"",
            placeholder=""https://www.youtube.com/watch?v=dQw4w9WgXcQ"",
        )
        
    class Output(BlockSchema):
        video_id: str = SchemaField(description=""The extracted YouTube video ID"")
        transcript: str = SchemaField(description=""The transcribed text of the video"")
        error: str = SchemaField(
            description=""Any error message if the transcription fails""
        )
        
    def __init__(self):
        super().__init__(
            id=""f3a8f7e1-4b1d-4e5f-9f2a-7c3d5a2e6b4c"",
            input_schema=TranscribeYoutubeVideoBlock.Input,
            output_schema=TranscribeYoutubeVideoBlock.Output,
            description=""Transcribes a YouTube video."",
            categories={BlockCategory.SOCIAL},
            test_input={""youtube_url"": ""https://www.youtube.com/watch?v=dQw4w9WgXcQ""},
            test_output=[
                (""video_id"", ""dQw4w9WgXcQ""),
                (
                    ""transcript"",
                    ""Never gonna give you up\nNever gonna let you down"",
                ),
            ],
            test_mock={
                ""get_transcript"": lambda video_id: [
                    {""text"": ""Never gonna give you up""},
                    {""text"": ""Never gonna let you down""},
                ],
            },
        )

    @staticmethod
    def extract_video_id(url: str) -> str:
        parsed_url = urlparse(url)
        if parsed_url.netloc == ""youtu.be"":
            return parsed_url.path[1:]
        if parsed_url.netloc in (""www.youtube.com"", ""youtube.com""):
            if parsed_url.path == ""/watch"":
                p = parse_qs(parsed_url.query)
                return p[""v""][0]
            if parsed_url.path[:7] == ""/embed/"":
                return parsed_url.path.split(""/"")[2]
            if parsed_url.path[:3] == ""/v/"":
                return parsed_url.path.split(""/"")[2]
        raise ValueError(f""Invalid YouTube URL: {url}"")

    @staticmethod
    def get_transcript(video_id: str):
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

            if not transcript_list:
                raise ValueError(f""No transcripts found for the video: {video_id}"")

            for transcript in transcript_list:
                first_transcript = transcript_list.find_transcript(
                    [transcript.language_code]
                )
                return YouTubeTranscriptApi.get_transcript(
                    video_id, languages=[first_transcript.language_code]
                )

        except Exception:
            raise ValueError(f""No transcripts found for the video: {video_id}"")

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        video_id = self.extract_video_id(input_data.youtube_url)
        yield ""video_id"", video_id
        
        transcript = self.get_transcript(video_id)
        formatter = TextFormatter()
        transcript_text = formatter.format_transcript(transcript)

        yield ""transcript"", transcript_text","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100.0}",
63,"from pathlib import Path

from prisma.models import User

from backend.blocks.basic import StoreValueBlock
from backend.blocks.block import BlockInstallationBlock
from backend.blocks.http import SendWebRequestBlock
from backend.blocks.llm import AITextGeneratorBlock
from backend.blocks.text import ExtractTextInformationBlock, FillTextTemplateBlock
from backend.data.graph import Graph, Link, Node, create_graph
from backend.data.user import get_or_create_user
from backend.util.test import SpinTestServer, wait_execution

sample_block_modules = {
    ""llm"": ""Block that calls the AI model to generate text."",
    ""basic"": ""Block that does basic operations."",
    ""text"": ""Blocks that do text operations."",
    ""reddit"": ""Blocks that interacts with Reddit."",
}
sample_block_codes = {}
for module, description in sample_block_modules.items():
    current_dir = Path(__file__).parent
    file_path = current_dir.parent / ""blocks"" / f""{module}.py""
    with open(file_path, ""r"") as f:
        code = ""\n"".join([""```python"", f.read(), ""```""])
        sample_block_codes[module] = f""[Example: {description}]\n{code}""


async def create_test_user() -> User:
    test_user_data = {
        ""sub"": ""ef3b97d7-1161-4eb4-92b2-10c24fb154c1"",
        ""email"": ""testuser@example.com"",
        ""name"": ""Test User"",
    }
    user = await get_or_create_user(test_user_data)
    return user


def create_test_graph() -> Graph:
    """"""
            StoreValueBlock (input)
                 ||
                 v
        FillTextTemplateBlock (input query)
                 ||
                 v
         SendWebRequestBlock (browse)
                 ||
                 v
     ------> StoreValueBlock===============
    |           |  |                    ||
    |            --                     ||
    |                                   ||
    |                                   ||
    |                                    v
    |        AITextGeneratorBlock  <===== FillTextTemplateBlock (query)
    |            ||                      ^
    |            v                      ||
    |       ExtractTextInformationBlock             ||
    |            ||                     ||
    |            v                      ||
    ------ BlockInstallationBlock  ======
    """"""
    # ======= Nodes ========= #
    input_data = Node(block_id=StoreValueBlock().id)
    input_query_constant = Node(
        block_id=StoreValueBlock().id,
        input_default={""data"": None},
    )
    input_text_formatter = Node(
        block_id=FillTextTemplateBlock().id,
        input_default={
            ""format"": ""Show me how to make a python code for this query: `{query}`"",
        },
    )
    search_http_request = Node(
        block_id=SendWebRequestBlock().id,
        input_default={
            ""url"": ""https://osit-v2.bentlybro.com/search"",
        },
    )
    search_result_constant = Node(
        block_id=StoreValueBlock().id,
        input_default={
            ""data"": None,
        },
    )
    prompt_text_formatter = Node(
        block_id=FillTextTemplateBlock().id,
        input_default={
            ""format"": """"""
Write me a full Block implementation for this query: `{query}`

Here is the information I get to write a Python code for that:
{search_result}

Here is your previous attempt:
{previous_attempt}
"""""",
            ""values_#_previous_attempt"": ""No previous attempt found."",
        },
    )
    code_gen_llm_call = Node(
        block_id=AITextGeneratorBlock().id,
        input_default={
            ""sys_prompt"": f""""""
You are a software engineer and you are asked to write the full class implementation.
The class that you are implementing is extending a class called `Block`.
This class will be used as a node in a graph of other blocks to build a complex system.
This class has a method called `run` that takes an input and returns an output.
It also has an `id` attribute that is a UUID, input_schema, and output_schema.
For UUID, you have to hardcode it, like `d2e2ecd2-9ae6-422d-8dfe-ceca500ce6a6`,
don't use any automatic UUID generation, because it needs to be consistent.
To validate the correctness of your implementation, you can also define a test.
There is `test_input` and `test_output` you can use to validate your implementation.
There is also `test_mock` to mock a helper function on your block class for testing.

Feel free to start your answer by explaining your plan what's required how to test, etc.
But make sure to produce the fully working implementation at the end,
and it should be enclosed within this block format:
```python
<Your implementation here>
```

Here are a couple of sample of the Block class implementation:

{""--------------"".join([sample_block_codes[v] for v in sample_block_modules])}
"""""",
        },
    )
    code_text_parser = Node(
        block_id=ExtractTextInformationBlock().id,
        input_default={
            ""pattern"": ""```python\n(.+?)\n```"",
            ""group"": 1,
        },
    )
    block_installation = Node(
        block_id=BlockInstallationBlock().id,
    )
    nodes = [
        input_data,
        input_query_constant,
        input_text_formatter,
        search_http_request,
        search_result_constant,
        prompt_text_formatter,
        code_gen_llm_call,
        code_text_parser,
        block_installation,
    ]

    # ======= Links ========= #
    links = [
        Link(
            source_id=input_data.id,
            sink_id=input_query_constant.id,
            source_name=""output"",
            sink_name=""input"",
        ),
        Link(
            source_id=input_data.id,
            sink_id=input_text_formatter.id,
            source_name=""output"",
            sink_name=""values_#_query"",
        ),
        Link(
            source_id=input_query_constant.id,
            sink_id=input_query_constant.id,
            source_name=""output"",
            sink_name=""data"",
        ),
        Link(
            source_id=input_text_formatter.id,
            sink_id=search_http_request.id,
            source_name=""output"",
            sink_name=""body_#_query"",
        ),
        Link(
            source_id=search_http_request.id,
            sink_id=search_result_constant.id,
            source_name=""response_#_reply"",
            sink_name=""input"",
        ),
        Link(  # Loopback for constant block
            source_id=search_result_constant.id,
            sink_id=search_result_constant.id,
            source_name=""output"",
            sink_name=""data"",
        ),
        Link(
            source_id=search_result_constant.id,
            sink_id=prompt_text_formatter.id,
            source_name=""output"",
            sink_name=""values_#_search_result"",
        ),
        Link(
            source_id=input_query_constant.id,
            sink_id=prompt_text_formatter.id,
            source_name=""output"",
            sink_name=""values_#_query"",
        ),
        Link(
            source_id=prompt_text_formatter.id,
            sink_id=code_gen_llm_call.id,
            source_name=""output"",
            sink_name=""prompt"",
        ),
        Link(
            source_id=code_gen_llm_call.id,
            sink_id=code_text_parser.id,
            source_name=""response"",
            sink_name=""text"",
        ),
        Link(
            source_id=code_text_parser.id,
            sink_id=block_installation.id,
            source_name=""positive"",
            sink_name=""code"",
        ),
        Link(
            source_id=block_installation.id,
            sink_id=prompt_text_formatter.id,
            source_name=""error"",
            sink_name=""values_#_previous_attempt"",
        ),
        Link(  # Re-trigger search result.
            source_id=block_installation.id,
            sink_id=search_result_constant.id,
            source_name=""error"",
            sink_name=""input"",
        ),
        Link(  # Re-trigger search result.
            source_id=block_installation.id,
            sink_id=input_query_constant.id,
            source_name=""error"",
            sink_name=""input"",
        ),
    ]

    # ======= Graph ========= #
    return Graph(
        name=""BlockAutoGen"",
        description=""Block auto generation agent"",
        nodes=nodes,
        links=links,
    )


async def block_autogen_agent():
    async with SpinTestServer() as server:
        test_user = await create_test_user()
        test_graph = await create_graph(create_test_graph(), user_id=test_user.id)
        input_data = {""input"": ""Write me a block that writes a string into a file.""}
        response = await server.agent_server.test_execute_graph(
            test_graph.id, input_data, test_user.id
        )
        print(response)
        result = await wait_execution(
            graph_id=test_graph.id,
            graph_exec_id=response[""id""],
            timeout=1200,
            user_id=test_user.id,
        )
        print(result)


if __name__ == ""__main__"":
    import asyncio

    asyncio.run(block_autogen_agent())
","from pathlib import Path

from prisma.models import User

from backend.blocks.basic import StoreValueBlock
from backend.blocks.block import BlockInstallationBlock
from backend.blocks.http import SendWebRequestBlock
from backend.blocks.llm import AITextGeneratorBlock
from backend.blocks.text import ExtractTextInformationBlock, FillTextTemplateBlock
from backend.data.graph import Graph, Link, Node, create_graph
from backend.data.user import get_or_create_user
from backend.util.test import SpinTestServer, wait_execution


sample_block_modules = {
    ""llm"": ""Block that calls the AI model to generate text."",
    ""basic"": ""Block that does basic operations."",
    ""text"": ""Blocks that do text operations."",
    ""reddit"": ""Blocks that interacts with Reddit."",
}

sample_block_codes = {}

for module, description in sample_block_modules.items():
    current_dir = Path(__file__).parent
    file_path = current_dir.parent / ""blocks"" / f""{module}.py""
    with open(file_path, ""r"") as f:
        code = ""\n"".join([""```python"", f.read(), ""```""])
        sample_block_codes[module] = f""[Example: {description}]\n{code}""



async def create_test_user() -> User:
    test_user_data = {
        ""sub"": ""ef3b97d7-1161-4eb4-92b2-10c24fb154c1"",
        ""email"": ""testuser@example.com"",
        ""name"": ""Test User"",
    }
    user = await get_or_create_user(test_user_data)
    return user


def create_test_graph() -> Graph:
    """"""
            StoreValueBlock (input)
                 ||
                 v
        FillTextTemplateBlock (input query)
                 ||
                 v
         SendWebRequestBlock (browse)
                 ||
                 v
     ------> StoreValueBlock===============
    |           |  |                    ||
    |            --                     ||
    |                                   ||
    |                                   ||
    |                                    v
    |        AITextGeneratorBlock  <===== FillTextTemplateBlock (query)
    |            ||                      ^
    |            v                      ||
    |       ExtractTextInformationBlock             ||
    |            ||                     ||
    |            v                      ||
    ------ BlockInstallationBlock  ======
    """"""
    # ======= Nodes ========= #
    input_data = Node(block_id=StoreValueBlock().id)
    input_query_constant = Node(
        block_id=StoreValueBlock().id,
        input_default={""data"": None},
    )
    input_text_formatter = Node(
        block_id=FillTextTemplateBlock().id,
        input_default={
            ""format"": ""Show me how to make a python code for this query: `{query}`"",
        },
    )
    search_http_request = Node(
        block_id=SendWebRequestBlock().id,
        input_default={
            ""url"": ""https://osit-v2.bentlybro.com/search"",
        },
    )
    search_result_constant = Node(
        block_id=StoreValueBlock().id,
        input_default={
            ""data"": None,
        },
    )
    prompt_text_formatter = Node(
        block_id=FillTextTemplateBlock().id,
        input_default={
            ""format"": """"""
Write me a full Block implementation for this query: `{query}`

Here is the information I get to write a Python code for that:
{search_result}

Here is your previous attempt:
{previous_attempt}
"""""",
            ""values_#_previous_attempt"": ""No previous attempt found."",
        },
    )
    code_gen_llm_call = Node(
        block_id=AITextGeneratorBlock().id,
        input_default={
            ""sys_prompt"": f""""""
You are a software engineer and you are asked to write the full class implementation.
The class that you are implementing is extending a class called `Block`.
This class will be used as a node in a graph of other blocks to build a complex system.
This class has a method called `run` that takes an input and returns an output.
It also has an `id` attribute that is a UUID, input_schema, and output_schema.
For UUID, you have to hardcode it, like `d2e2ecd2-9ae6-422d-8dfe-ceca500ce6a6`,
don't use any automatic UUID generation, because it needs to be consistent.
To validate the correctness of your implementation, you can also define a test.
There is `test_input` and `test_output` you can use to validate your implementation.
There is also `test_mock` to mock a helper function on your block class for testing.

Feel free to start your answer by explaining your plan what's required how to test, etc.
But make sure to produce the fully working implementation at the end,
and it should be enclosed within this block format:
```python
<Your implementation here>
```

Here are a couple of sample of the Block class implementation:

{""--------------"".join([sample_block_codes[v] for v in sample_block_modules])}
"""""",
        },
    )
    code_text_parser = Node(
        block_id=ExtractTextInformationBlock().id,
        input_default={
            ""pattern"": ""```python\n(.+?)\n```"",
            ""group"": 1,
        },
    )
    block_installation = Node(
        block_id=BlockInstallationBlock().id,
    )
    nodes = [
        input_data,
        input_query_constant,
        input_text_formatter,
        search_http_request,
        search_result_constant,
        prompt_text_formatter,
        code_gen_llm_call,
        code_text_parser,
        block_installation,
    ]

    # ======= Links ========= #
    links = [
        Link(
            source_id=input_data.id,
            sink_id=input_query_constant.id,
            source_name=""output"",
            sink_name=""input"",
        ),
        Link(
            source_id=input_data.id,
            sink_id=input_text_formatter.id,
            source_name=""output"",
            sink_name=""values_#_query"",
        ),
        Link(
            source_id=input_query_constant.id,
            sink_id=input_query_constant.id,
            source_name=""output"",
            sink_name=""data"",
        ),
        Link(
            source_id=input_text_formatter.id,
            sink_id=search_http_request.id,
            source_name=""output"",
            sink_name=""body_#_query"",
        ),
        Link(
            source_id=search_http_request.id,
            sink_id=search_result_constant.id,
            source_name=""response_#_reply"",
            sink_name=""input"",
        ),
        Link(  # Loopback for constant block
            source_id=search_result_constant.id,
            sink_id=search_result_constant.id,
            source_name=""output"",
            sink_name=""data"",
        ),
        Link(
            source_id=search_result_constant.id,
            sink_id=prompt_text_formatter.id,
            source_name=""output"",
            sink_name=""values_#_search_result"",
        ),
        Link(
            source_id=input_query_constant.id,
            sink_id=prompt_text_formatter.id,
            source_name=""output"",
            sink_name=""values_#_query"",
        ),
        Link(
            source_id=prompt_text_formatter.id,
            sink_id=code_gen_llm_call.id,
            source_name=""output"",
            sink_name=""prompt"",
        ),
        Link(
            source_id=code_gen_llm_call.id,
            sink_id=code_text_parser.id,
            source_name=""response"",
            sink_name=""text"",
        ),
        Link(
            source_id=code_text_parser.id,
            sink_id=block_installation.id,
            source_name=""positive"",
            sink_name=""code"",
        ),
        Link(
            source_id=block_installation.id,
            sink_id=prompt_text_formatter.id,
            source_name=""error"",
            sink_name=""values_#_previous_attempt"",
        ),
        Link(  # Re-trigger search result.
            source_id=block_installation.id,
            sink_id=search_result_constant.id,
            source_name=""error"",
            sink_name=""input"",
        ),
        Link(  # Re-trigger search result.
            source_id=block_installation.id,
            sink_id=input_query_constant.id,
            source_name=""error"",
            sink_name=""input"",
        ),
    ]

    # ======= Graph ========= #
    return Graph(
        name=""BlockAutoGen"",
        description=""Block auto generation agent"",
        nodes=nodes,
        links=links,
    )


async def block_autogen_agent():
    async with SpinTestServer() as server:
        test_user = await create_test_user()
        test_graph = await create_graph(create_test_graph(), user_id=test_user.id)
        input_data = {""input"": ""Write me a block that writes a string into a file.""}
        response = await server.agent_server.test_execute_graph(
            test_graph.id, input_data, test_user.id
        )
        print(response)
        result = await wait_execution(
            graph_id=test_graph.id,
            graph_exec_id=response[""id""],
            timeout=1200,
            user_id=test_user.id,
        )
        print(result)


if __name__ == ""__main__"":
    import asyncio

    asyncio.run(block_autogen_agent())","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
64,"""""""
In this problem, we want to determine all possible subsequences
of the given sequence. We use backtracking to solve this problem.

Time complexity: O(2^n),
where n denotes the length of the given sequence.
""""""

from __future__ import annotations

from typing import Any


def generate_all_subsequences(sequence: list[Any]) -> None:
    create_state_space_tree(sequence, [], 0)


def create_state_space_tree(
    sequence: list[Any], current_subsequence: list[Any], index: int
) -> None:
    """"""
    Creates a state space tree to iterate through each branch using DFS.
    We know that each state has exactly two children.
    It terminates when it reaches the end of the given sequence.

    :param sequence: The input sequence for which subsequences are generated.
    :param current_subsequence: The current subsequence being built.
    :param index: The current index in the sequence.

    Example:
    >>> sequence = [3, 2, 1]
    >>> current_subsequence = []
    >>> create_state_space_tree(sequence, current_subsequence, 0)
    []
    [1]
    [2]
    [2, 1]
    [3]
    [3, 1]
    [3, 2]
    [3, 2, 1]

    >>> sequence = [""A"", ""B""]
    >>> current_subsequence = []
    >>> create_state_space_tree(sequence, current_subsequence, 0)
    []
    ['B']
    ['A']
    ['A', 'B']

    >>> sequence = []
    >>> current_subsequence = []
    >>> create_state_space_tree(sequence, current_subsequence, 0)
    []

    >>> sequence = [1, 2, 3, 4]
    >>> current_subsequence = []
    >>> create_state_space_tree(sequence, current_subsequence, 0)
    []
    [4]
    [3]
    [3, 4]
    [2]
    [2, 4]
    [2, 3]
    [2, 3, 4]
    [1]
    [1, 4]
    [1, 3]
    [1, 3, 4]
    [1, 2]
    [1, 2, 4]
    [1, 2, 3]
    [1, 2, 3, 4]
    """"""

    if index == len(sequence):
        print(current_subsequence)
        return

    create_state_space_tree(sequence, current_subsequence, index + 1)
    current_subsequence.append(sequence[index])
    create_state_space_tree(sequence, current_subsequence, index + 1)
    current_subsequence.pop()


if __name__ == ""__main__"":
    seq: list[Any] = [1, 2, 3]
    generate_all_subsequences(seq)

    seq.clear()
    seq.extend([""A"", ""B"", ""C""])
    generate_all_subsequences(seq)
","""""""
In this problem, we want to determine all possible subsequences
of the given sequence. We use backtracking to solve this problem.

Time complexity: O(2^n),
where n denotes the length of the given sequence.
""""""

from __future__ import annotations
from typing import Any


def generate_all_subsequences(sequence: list[Any]) -> None:
    create_state_space_tree(sequence, [], 0)


def create_state_space_tree(
    sequence: list[Any], current_subsequence: list[Any], index: int
) -> None:
    """"""
    Creates a state space tree to iterate through each branch using DFS.
    We know that each state has exactly two children.
    It terminates when it reaches the end of the given sequence.

    :param sequence: The input sequence for which subsequences are generated.
    :param current_subsequence: The current subsequence being built.
    :param index: The current index in the sequence.

    Example:
    >>> sequence = [3, 2, 1]
    >>> current_subsequence = []
    >>> create_state_space_tree(sequence, current_subsequence, 0)
    []
    [1]
    [2]
    [2, 1]
    [3]
    [3, 1]
    [3, 2]
    [3, 2, 1]

    >>> sequence = [""A"", ""B""]
    >>> current_subsequence = []
    >>> create_state_space_tree(sequence, current_subsequence, 0)
    []
    ['B']
    ['A']
    ['A', 'B']

    >>> sequence = []
    >>> current_subsequence = []
    >>> create_state_space_tree(sequence, current_subsequence, 0)
    []

    >>> sequence = [1, 2, 3, 4]
    >>> current_subsequence = []
    >>> create_state_space_tree(sequence, current_subsequence, 0)
    []
    [4]
    [3]
    [3, 4]
    [2]
    [2, 4]
    [2, 3]
    [2, 3, 4]
    [1]
    [1, 4]
    [1, 3]
    [1, 3, 4]
    [1, 2]
    [1, 2, 4]
    [1, 2, 3]
    [1, 2, 3, 4]
    """"""
    if index == len(sequence):
        print(current_subsequence)
        return
    create_state_space_tree(sequence, current_subsequence, index + 1)
    current_subsequence.append(sequence[index])
    create_state_space_tree(sequence, current_subsequence, index + 1)
    current_subsequence.pop()


if __name__ == ""__main__"":
    seq: list[Any] = [1, 2, 3]
    generate_all_subsequences(seq)
    
    seq.clear()
    seq.extend([""A"", ""B"", ""C""])
    generate_all_subsequences(seq)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
65,"""""""
Problem source: https://www.hackerrank.com/challenges/the-power-sum/problem
Find the number of ways that a given integer X, can be expressed as the sum
of the Nth powers of unique, natural numbers. For example, if X=13 and N=2.
We have to find all combinations of unique squares adding up to 13.
The only solution is 2^2+3^2. Constraints: 1<=X<=1000, 2<=N<=10.
""""""


def backtrack(
    needed_sum: int,
    power: int,
    current_number: int,
    current_sum: int,
    solutions_count: int,
) -> tuple[int, int]:
    """"""
    >>> backtrack(13, 2, 1, 0, 0)
    (0, 1)
    >>> backtrack(10, 2, 1, 0, 0)
    (0, 1)
    >>> backtrack(10, 3, 1, 0, 0)
    (0, 0)
    >>> backtrack(20, 2, 1, 0, 0)
    (0, 1)
    >>> backtrack(15, 10, 1, 0, 0)
    (0, 0)
    >>> backtrack(16, 2, 1, 0, 0)
    (0, 1)
    >>> backtrack(20, 1, 1, 0, 0)
    (0, 64)
    """"""
    if current_sum == needed_sum:
        # If the sum of the powers is equal to needed_sum, then we have a solution.
        solutions_count += 1
        return current_sum, solutions_count

    i_to_n = current_number**power
    if current_sum + i_to_n <= needed_sum:
        # If the sum of the powers is less than needed_sum, then continue adding powers.
        current_sum += i_to_n
        current_sum, solutions_count = backtrack(
            needed_sum, power, current_number + 1, current_sum, solutions_count
        )
        current_sum -= i_to_n
    if i_to_n < needed_sum:
        # If the power of i is less than needed_sum, then try with the next power.
        current_sum, solutions_count = backtrack(
            needed_sum, power, current_number + 1, current_sum, solutions_count
        )
    return current_sum, solutions_count


def solve(needed_sum: int, power: int) -> int:
    """"""
    >>> solve(13, 2)
    1
    >>> solve(10, 2)
    1
    >>> solve(10, 3)
    0
    >>> solve(20, 2)
    1
    >>> solve(15, 10)
    0
    >>> solve(16, 2)
    1
    >>> solve(20, 1)
    Traceback (most recent call last):
        ...
    ValueError: Invalid input
    needed_sum must be between 1 and 1000, power between 2 and 10.
    >>> solve(-10, 5)
    Traceback (most recent call last):
        ...
    ValueError: Invalid input
    needed_sum must be between 1 and 1000, power between 2 and 10.
    """"""
    if not (1 <= needed_sum <= 1000 and 2 <= power <= 10):
        raise ValueError(
            ""Invalid input\n""
            ""needed_sum must be between 1 and 1000, power between 2 and 10.""
        )

    return backtrack(needed_sum, power, 1, 0, 0)[1]  # Return the solutions_count


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","""""""
Problem source: https://www.hackerrank.com/challenges/the-power-sum/problem
Find the number of ways that a given integer X, can be expressed as the sum
of the Nth powers of unique, natural numbers. For example, if X=13 and N=2.
We have to find all combinations of unique squares adding up to 13.
The only solution is 2^2+3^2. Constraints: 1<=X<=1000, 2<=N<=10.
""""""

def backtrack(
    needed_sum: int,
    power: int,
    current_number: int,
    current_sum: int,
    solutions_count: int,
) -> tuple[int, int]:
    """"""
    >>> backtrack(13, 2, 1, 0, 0)
    (0, 1)
    >>> backtrack(10, 2, 1, 0, 0)
    (0, 1)
    >>> backtrack(10, 3, 1, 0, 0)
    (0, 0)
    >>> backtrack(20, 2, 1, 0, 0)
    (0, 1)
    >>> backtrack(15, 10, 1, 0, 0)
    (0, 0)
    >>> backtrack(16, 2, 1, 0, 0)
    (0, 1)
    >>> backtrack(20, 1, 1, 0, 0)
    (0, 64)
    """"""

    if current_sum == needed_sum:
        # If the sum of the powers is equal to needed_sum, then we have a solution.
        solutions_count += 1
        return current_sum, solutions_count

    i_to_n = current_number**power

    if current_sum + i_to_n <= needed_sum:
        # If the sum of the powers is less than needed_sum, then continue adding powers.
        current_sum += i_to_n
        current_sum, solutions_count = backtrack(
            needed_sum, power, current_number + 1, current_sum, solutions_count
        )
        current_sum -= i_to_n

    if i_to_n < needed_sum:
        # If the power of i is less than needed_sum, then try with the next power.
        current_sum, solutions_count = backtrack(
            needed_sum, power, current_number + 1, current_sum, solutions_count
        )

    return current_sum, solutions_count

def solve(needed_sum: int, power: int) -> int:
    """"""
    >>> solve(13, 2)
    1
    >>> solve(10, 2)
    1
    >>> solve(10, 3)
    0
    >>> solve(20, 2)
    1
    >>> solve(15, 10)
    0
    >>> solve(16, 2)
    1
    >>> solve(20, 1)
    Traceback (most recent call last):
        ...
    ValueError: Invalid input
    needed_sum must be between 1 and 1000, power between 2 and 10.
    >>> solve(-10, 5)
    Traceback (most recent call last):
        ...
    ValueError: Invalid input
    needed_sum must be between 1 and 1000, power between 2 and 10.
    """"""
    if not (1 <= needed_sum <= 1000 and 2 <= power <= 10):
        raise ValueError(
            ""Invalid input\n""
            ""needed_sum must be between 1 and 1000, power between 2 and 10.""
        )

    return backtrack(needed_sum, power, 1, 0, 0)[1]  # Return the solutions_count

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
66,"import logging
import os
import signal
import sys
from abc import ABC, abstractmethod
from multiprocessing import Process, set_start_method
from typing import Optional

from backend.util.logging import configure_logging
from backend.util.metrics import sentry_init

logger = logging.getLogger(__name__)
_SERVICE_NAME = ""MainProcess""


def get_service_name():
    return _SERVICE_NAME


def set_service_name(name: str):
    global _SERVICE_NAME
    _SERVICE_NAME = name


class AppProcess(ABC):
    """"""
    A class to represent an object that can be executed in a background process.
    """"""

    process: Optional[Process] = None

    set_start_method(""spawn"", force=True)
    configure_logging()
    sentry_init()

    # Methods that are executed INSIDE the process #

    @abstractmethod
    def run(self):
        """"""
        The method that will be executed in the process.
        """"""
        pass

    @classmethod
    @property
    def service_name(cls) -> str:
        return cls.__name__

    def cleanup(self):
        """"""
        Implement this method on a subclass to do post-execution cleanup,
        e.g. disconnecting from a database or terminating child processes.
        """"""
        pass

    def health_check(self):
        """"""
        A method to check the health of the process.
        """"""
        pass

    def execute_run_command(self, silent):
        signal.signal(signal.SIGTERM, self._self_terminate)

        try:
            if silent:
                sys.stdout = open(os.devnull, ""w"")
                sys.stderr = open(os.devnull, ""w"")

            set_service_name(self.service_name)
            logger.info(f""[{self.service_name}] Starting..."")
            self.run()
        except (KeyboardInterrupt, SystemExit) as e:
            logger.warning(f""[{self.service_name}] Terminated: {e}; quitting..."")

    def _self_terminate(self, signum: int, frame):
        self.cleanup()
        sys.exit(0)

    # Methods that are executed OUTSIDE the process #

    def __enter__(self):
        self.start(background=True)
        return self

    def __exit__(self, *args, **kwargs):
        self.stop()

    def start(self, background: bool = False, silent: bool = False, **proc_args) -> int:
        """"""
        Start the background process.
        Args:
            background: Whether to run the process in the background.
            silent: Whether to disable stdout and stderr.
            proc_args: Additional arguments to pass to the process.
        Returns:
            the process id or 0 if the process is not running in the background.
        """"""
        if not background:
            self.execute_run_command(silent)
            return 0

        self.process = Process(
            name=self.__class__.__name__,
            target=self.execute_run_command,
            args=(silent,),
            **proc_args,
        )
        self.process.start()
        self.health_check()
        return self.process.pid or 0

    def stop(self):
        """"""
        Stop the background process.
        """"""
        if not self.process:
            return

        self.process.terminate()
        self.process.join()
        self.process = None
","import logging
import os
import signal
import sys
from abc import ABC, abstractmethod
from multiprocessing import Process, set_start_method
from typing import Optional

from backend.util.logging import configure_logging
from backend.util.metrics import sentry_init

logger = logging.getLogger(__name__)
_SERVICE_NAME = ""MainProcess""

def get_service_name():
    return _SERVICE_NAME

def set_service_name(name: str):
    global _SERVICE_NAME
    _SERVICE_NAME = name

class AppProcess(ABC):
    """"""
    A class to represent an object that can be executed in a background process.
    """"""
    process: Optional[Process] = None

    set_start_method(""spawn"", force=True)
    configure_logging()
    sentry_init()

    # Methods that are executed INSIDE the process #

    @abstractmethod
    def run(self):
        """"""
        The method that will be executed in the process.
        """"""
        pass

    @classmethod
    @property
    def service_name(cls) -> str:
        return cls.__name__

    def cleanup(self):
        """"""
        Implement this method on a subclass to do post-execution cleanup,
        e.g. disconnecting from a database or terminating child processes.
        """"""
        pass

    def health_check(self):
        """"""
        A method to check the health of the process.
        """"""
        pass

    def execute_run_command(self, silent):
        signal.signal(signal.SIGTERM, self._self_terminate)
        try:
            if silent:
                sys.stdout = open(os.devnull, ""w"")
                sys.stderr = open(os.devnull, ""w"")

            set_service_name(self.service_name)
            logger.info(f""[{self.service_name}] Starting..."")
            self.run()

        except (KeyboardInterrupt, SystemExit) as e:
            logger.warning(f""[{self.service_name}] Terminated: {e}; quitting..."")

    def _self_terminate(self, signum: int, frame):
        self.cleanup()
        sys.exit(0)

    # Methods that are executed OUTSIDE the process #

    def __enter__(self):
        self.start(background=True)
        return self

    def __exit__(self, *args, **kwargs):
        self.stop()

    def start(self, background: bool = False, silent: bool = False, **proc_args) -> int:
        """"""
        Start the background process.
        Args:
            background: Whether to run the process in the background.
            silent: Whether to disable stdout and stderr.
            proc_args: Additional arguments to pass to the process.
        Returns:
            the process id or 0 if the process is not running in the background.
        """"""
        if not background:
            self.execute_run_command(silent)
            return 0

        self.process = Process(
            name=self.__class__.__name__,
            target=self.execute_run_command,
            args=(silent,),
            **proc_args,
        )
        self.process.start()
        self.health_check()
        return self.process.pid or 0

    def stop(self):
        """"""
        Stop the background process.
        """"""
        if not self.process:
            return

        self.process.terminate()
        self.process.join()
        self.process = None","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
67,"import logging
from typing import TYPE_CHECKING, Callable, Optional, cast

from backend.data.block import BlockWebhookConfig, get_block
from backend.data.graph import set_node_webhook
from backend.data.model import CREDENTIALS_FIELD_NAME
from backend.integrations.webhooks import WEBHOOK_MANAGERS_BY_NAME

if TYPE_CHECKING:
    from backend.data.graph import GraphModel, NodeModel
    from backend.data.model import Credentials

    from ._base import BaseWebhooksManager

logger = logging.getLogger(__name__)


async def on_graph_activate(
    graph: ""GraphModel"", get_credentials: Callable[[str], ""Credentials | None""]
):
    """"""
    Hook to be called when a graph is activated/created.

    ⚠️ Assuming node entities are not re-used between graph versions, ⚠️
    this hook calls `on_node_activate` on all nodes in this graph.

    Params:
        get_credentials: `credentials_id` -> Credentials
    """"""
    # Compare nodes in new_graph_version with previous_graph_version
    updated_nodes = []
    for new_node in graph.nodes:
        node_credentials = None
        if creds_meta := new_node.input_default.get(CREDENTIALS_FIELD_NAME):
            node_credentials = get_credentials(creds_meta[""id""])
            if not node_credentials:
                raise ValueError(
                    f""Node #{new_node.id} updated with non-existent ""
                    f""credentials #{node_credentials}""
                )

        updated_node = await on_node_activate(
            graph.user_id, new_node, credentials=node_credentials
        )
        updated_nodes.append(updated_node)

    graph.nodes = updated_nodes
    return graph


async def on_graph_deactivate(
    graph: ""GraphModel"", get_credentials: Callable[[str], ""Credentials | None""]
):
    """"""
    Hook to be called when a graph is deactivated/deleted.

    ⚠️ Assuming node entities are not re-used between graph versions, ⚠️
    this hook calls `on_node_deactivate` on all nodes in `graph`.

    Params:
        get_credentials: `credentials_id` -> Credentials
    """"""
    updated_nodes = []
    for node in graph.nodes:
        node_credentials = None
        if creds_meta := node.input_default.get(CREDENTIALS_FIELD_NAME):
            node_credentials = get_credentials(creds_meta[""id""])
            if not node_credentials:
                logger.error(
                    f""Node #{node.id} referenced non-existent ""
                    f""credentials #{creds_meta['id']}""
                )

        updated_node = await on_node_deactivate(node, credentials=node_credentials)
        updated_nodes.append(updated_node)

    graph.nodes = updated_nodes
    return graph


async def on_node_activate(
    user_id: str,
    node: ""NodeModel"",
    *,
    credentials: Optional[""Credentials""] = None,
) -> ""NodeModel"":
    """"""Hook to be called when the node is activated/created""""""

    block = get_block(node.block_id)
    if not block:
        raise ValueError(
            f""Node #{node.id} is instance of unknown block #{node.block_id}""
        )

    if not block.webhook_config:
        return node

    provider = block.webhook_config.provider
    if provider not in WEBHOOK_MANAGERS_BY_NAME:
        raise ValueError(
            f""Block #{block.id} has webhook_config for provider {provider} ""
            ""which does not support webhooks""
        )

    logger.debug(
        f""Activating webhook node #{node.id} with config {block.webhook_config}""
    )

    webhooks_manager = WEBHOOK_MANAGERS_BY_NAME[provider]()

    if auto_setup_webhook := isinstance(block.webhook_config, BlockWebhookConfig):
        try:
            resource = block.webhook_config.resource_format.format(**node.input_default)
        except KeyError:
            resource = None
        logger.debug(
            f""Constructed resource string {resource} from input {node.input_default}""
        )
    else:
        resource = """"  # not relevant for manual webhooks

    needs_credentials = CREDENTIALS_FIELD_NAME in block.input_schema.model_fields
    credentials_meta = (
        node.input_default.get(CREDENTIALS_FIELD_NAME) if needs_credentials else None
    )
    event_filter_input_name = block.webhook_config.event_filter_input
    has_everything_for_webhook = (
        resource is not None
        and (credentials_meta or not needs_credentials)
        and (
            not event_filter_input_name
            or (
                event_filter_input_name in node.input_default
                and any(
                    is_on
                    for is_on in node.input_default[event_filter_input_name].values()
                )
            )
        )
    )

    if has_everything_for_webhook and resource is not None:
        logger.debug(f""Node #{node} has everything for a webhook!"")
        if credentials_meta and not credentials:
            raise ValueError(
                f""Cannot set up webhook for node #{node.id}: ""
                f""credentials #{credentials_meta['id']} not available""
            )

        if event_filter_input_name:
            # Shape of the event filter is enforced in Block.__init__
            event_filter = cast(dict, node.input_default[event_filter_input_name])
            events = [
                block.webhook_config.event_format.format(event=event)
                for event, enabled in event_filter.items()
                if enabled is True
            ]
            logger.debug(f""Webhook events to subscribe to: {', '.join(events)}"")
        else:
            events = []

        # Find/make and attach a suitable webhook to the node
        if auto_setup_webhook:
            assert credentials is not None
            new_webhook = await webhooks_manager.get_suitable_auto_webhook(
                user_id,
                credentials,
                block.webhook_config.webhook_type,
                resource,
                events,
            )
        else:
            # Manual webhook -> no credentials -> don't register but do create
            new_webhook = await webhooks_manager.get_manual_webhook(
                user_id,
                node.graph_id,
                block.webhook_config.webhook_type,
                events,
            )
        logger.debug(f""Acquired webhook: {new_webhook}"")
        return await set_node_webhook(node.id, new_webhook.id)
    else:
        logger.debug(f""Node #{node.id} does not have everything for a webhook"")

    return node


async def on_node_deactivate(
    node: ""NodeModel"",
    *,
    credentials: Optional[""Credentials""] = None,
    webhooks_manager: Optional[""BaseWebhooksManager""] = None,
) -> ""NodeModel"":
    """"""Hook to be called when node is deactivated/deleted""""""

    logger.debug(f""Deactivating node #{node.id}"")
    block = get_block(node.block_id)
    if not block:
        raise ValueError(
            f""Node #{node.id} is instance of unknown block #{node.block_id}""
        )

    if not block.webhook_config:
        return node

    provider = block.webhook_config.provider
    if provider not in WEBHOOK_MANAGERS_BY_NAME:
        raise ValueError(
            f""Block #{block.id} has webhook_config for provider {provider} ""
            ""which does not support webhooks""
        )

    webhooks_manager = WEBHOOK_MANAGERS_BY_NAME[provider]()

    if node.webhook_id:
        logger.debug(f""Node #{node.id} has webhook_id {node.webhook_id}"")
        if not node.webhook:
            logger.error(f""Node #{node.id} has webhook_id but no webhook object"")
            raise ValueError(""node.webhook not included"")

        # Detach webhook from node
        logger.debug(f""Detaching webhook from node #{node.id}"")
        updated_node = await set_node_webhook(node.id, None)

        # Prune and deregister the webhook if it is no longer used anywhere
        webhook = node.webhook
        logger.debug(
            f""Pruning{' and deregistering' if credentials else ''} ""
            f""webhook #{webhook.id}""
        )
        await webhooks_manager.prune_webhook_if_dangling(webhook.id, credentials)
        if (
            CREDENTIALS_FIELD_NAME in block.input_schema.model_fields
            and not credentials
        ):
            logger.warning(
                f""Cannot deregister webhook #{webhook.id}: credentials ""
                f""#{webhook.credentials_id} not available ""
                f""({webhook.provider.value} webhook ID: {webhook.provider_webhook_id})""
            )
        return updated_node

    logger.debug(f""Node #{node.id} has no webhook_id, returning"")
    return node
","import logging
from typing import TYPE_CHECKING, Callable, Optional, cast

from backend.data.block import BlockWebhookConfig, get_block
from backend.data.graph import set_node_webhook
from backend.data.model import CREDENTIALS_FIELD_NAME
from backend.integrations.webhooks import WEBHOOK_MANAGERS_BY_NAME

if TYPE_CHECKING:
    from backend.data.graph import GraphModel, NodeModel
    from backend.data.model import Credentials

    from ._base import BaseWebhooksManager

logger = logging.getLogger(__name__)

async def on_graph_activate(
    graph: ""GraphModel"", get_credentials: Callable[[str], ""Credentials | None""]
):
    """"""
    Hook to be called when a graph is activated/created.

    ⚠️ Assuming node entities are not re-used between graph versions, ⚠️
    this hook calls `on_node_activate` on all nodes in this graph.

    Params:
        get_credentials: `credentials_id` -> Credentials
    """"""
    # Compare nodes in new_graph_version with previous_graph_version
    updated_nodes = []
    
    for new_node in graph.nodes:
        node_credentials = None
        
        if creds_meta := new_node.input_default.get(CREDENTIALS_FIELD_NAME):
            node_credentials = get_credentials(creds_meta[""id""])
            
            if not node_credentials:
                raise ValueError(
                    f""Node #{new_node.id} updated with non-existent ""
                    f""credentials #{node_credentials}""
                )
        
        updated_node = await on_node_activate(
            graph.user_id, new_node, credentials=node_credentials
        )
        updated_nodes.append(updated_node)

    graph.nodes = updated_nodes
    return graph

async def on_graph_deactivate(
    graph: ""GraphModel"", get_credentials: Callable[[str], ""Credentials | None""]
):
    """"""
    Hook to be called when a graph is deactivated/deleted.

    ⚠️ Assuming node entities are not re-used between graph versions, ⚠️
    this hook calls `on_node_deactivate` on all nodes in `graph`.

    Params:
        get_credentials: `credentials_id` -> Credentials
    """"""
    updated_nodes = []
    
    for node in graph.nodes:
        node_credentials = None
        
        if creds_meta := node.input_default.get(CREDENTIALS_FIELD_NAME):
            node_credentials = get_credentials(creds_meta[""id""])
            
            if not node_credentials:
                logger.error(
                    f""Node #{node.id} referenced non-existent ""
                    f""credentials #{creds_meta['id']}""
                )
        
        updated_node = await on_node_deactivate(node, credentials=node_credentials)
        updated_nodes.append(updated_node)
    
    graph.nodes = updated_nodes
    return graph

async def on_node_activate(
    user_id: str,
    node: ""NodeModel"",
    *,
    credentials: Optional[""Credentials""] = None,
) -> ""NodeModel"":
    """"""Hook to be called when the node is activated/created""""""
    block = get_block(node.block_id)
    
    if not block:
        raise ValueError(
            f""Node #{node.id} is instance of unknown block #{node.block_id}""
        )
    if not block.webhook_config:
        return node
    
    provider = block.webhook_config.provider
    
    if provider not in WEBHOOK_MANAGERS_BY_NAME:
        raise ValueError(
            f""Block #{block.id} has webhook_config for provider {provider} ""
            ""which does not support webhooks""
        )
    
    logger.debug(
        f""Activating webhook node #{node.id} with config {block.webhook_config}""
    )
    
    webhooks_manager = WEBHOOK_MANAGERS_BY_NAME[provider]()
    
    if auto_setup_webhook := isinstance(block.webhook_config, BlockWebhookConfig):
        try:
            resource = block.webhook_config.resource_format.format(**node.input_default)
        except KeyError:
            resource = None
        logger.debug(
            f""Constructed resource string {resource} from input {node.input_default}""
        )
    else:
        resource = """"  # not relevant for manual webhooks
    
    needs_credentials = CREDENTIALS_FIELD_NAME in block.input_schema.model_fields
    credentials_meta = (
        node.input_default.get(CREDENTIALS_FIELD_NAME) if needs_credentials else None
    )
    
    event_filter_input_name = block.webhook_config.event_filter_input
    
    has_everything_for_webhook = (
        resource is not None
        and (credentials_meta or not needs_credentials)
        and (
            not event_filter_input_name
            or (
                event_filter_input_name in node.input_default
                and any(
                    is_on
                    for is_on in node.input_default[event_filter_input_name].values()
                )
            )
        )
    )
    
    if has_everything_for_webhook and resource is not None:
        logger.debug(f""Node #{node} has everything for a webhook!"")
        
        if credentials_meta and not credentials:
            raise ValueError(
                f""Cannot set up webhook for node #{node.id}: ""
                f""credentials #{credentials_meta['id']} not available""
            )
        
        if event_filter_input_name:
            # Shape of the event filter is enforced in Block.__init__
            event_filter = cast(dict, node.input_default[event_filter_input_name])
            events = [
                block.webhook_config.event_format.format(event=event)
                for event, enabled in event_filter.items()
                if enabled is True
            ]
            logger.debug(f""Webhook events to subscribe to: {', '.join(events)}"")
        else:
            events = []
        
        # Find/make and attach a suitable webhook to the node
        if auto_setup_webhook:
            assert credentials is not None
            new_webhook = await webhooks_manager.get_suitable_auto_webhook(
                user_id,
                credentials,
                block.webhook_config.webhook_type,
                resource,
                events,
            )
        else:
            # Manual webhook -> no credentials -> don't register but do create
            new_webhook = await webhooks_manager.get_manual_webhook(
                user_id,
                node.graph_id,
                block.webhook_config.webhook_type,
                events,
            )
        
        logger.debug(f""Acquired webhook: {new_webhook}"")
        return await set_node_webhook(node.id, new_webhook.id)
    
    else:
        logger.debug(f""Node #{node.id} does not have everything for a webhook"")
    
    return node

async def on_node_deactivate(
    node: ""NodeModel"",
    *,
    credentials: Optional[""Credentials""] = None,
    webhooks_manager: Optional[""BaseWebhooksManager""] = None,
) -> ""NodeModel"":
    """"""Hook to be called when node is deactivated/deleted""""""
    logger.debug(f""Deactivating node #{node.id}"")
    block = get_block(node.block_id)
    
    if not block:
        raise ValueError(
            f""Node #{node.id} is instance of unknown block #{node.block_id}""
        )
    if not block.webhook_config:
        return node
    
    provider = block.webhook_config.provider
    if provider not in WEBHOOK_MANAGERS_BY_NAME:
        raise ValueError(
            f""Block #{block.id} has webhook_config for provider {provider} ""
            ""which does not support webhooks""
        )
    
    webhooks_manager = WEBHOOK_MANAGERS_BY_NAME[provider]()
    
    if node.webhook_id:
        logger.debug(f""Node #{node.id} has webhook_id {node.webhook_id}"")
        if not node.webhook:
            logger.error(f""Node #{node.id} has webhook_id but no webhook object"")
            raise ValueError(""node.webhook not included"")
        
        # Detach webhook from node
        logger.debug(f""Detaching webhook from node #{node.id}"")
        updated_node = await set_node_webhook(node.id, None)
        
        # Prune and deregister the webhook if it is no longer used anywhere
        webhook = node.webhook
        logger.debug(
            f""Pruning{' and deregistering' if credentials else ''} ""
            f""webhook #{webhook.id}""
        )
        await webhooks_manager.prune_webhook_if_dangling(webhook.id, credentials)
        
        if (
            CREDENTIALS_FIELD_NAME in block.input_schema.model_fields
            and not credentials
        ):
            logger.warning(
                f""Cannot deregister webhook #{webhook.id}: credentials ""
                f""#{webhook.credentials_id} not available ""
                f""({webhook.provider.value} webhook ID: {webhook.provider_webhook_id})""
            )
        return updated_node
    
    logger.debug(f""Node #{node.id} has no webhook_id, returning"")
    return node","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
68,"import asyncio
import contextlib
import logging
from functools import wraps
from typing import Any, Awaitable, Callable, Dict, Optional, TypeVar, Union, cast

import ldclient
from fastapi import HTTPException
from ldclient import Context, LDClient
from ldclient.config import Config
from typing_extensions import ParamSpec

from .config import SETTINGS

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG)

P = ParamSpec(""P"")
T = TypeVar(""T"")


def get_client() -> LDClient:
    """"""Get the LaunchDarkly client singleton.""""""
    return ldclient.get()


def initialize_launchdarkly() -> None:
    sdk_key = SETTINGS.launch_darkly_sdk_key
    logger.debug(
        f""Initializing LaunchDarkly with SDK key: {'present' if sdk_key else 'missing'}""
    )

    if not sdk_key:
        logger.warning(""LaunchDarkly SDK key not configured"")
        return

    config = Config(sdk_key)
    ldclient.set_config(config)

    if ldclient.get().is_initialized():
        logger.info(""LaunchDarkly client initialized successfully"")
    else:
        logger.error(""LaunchDarkly client failed to initialize"")


def shutdown_launchdarkly() -> None:
    """"""Shutdown the LaunchDarkly client.""""""
    if ldclient.get().is_initialized():
        ldclient.get().close()
        logger.info(""LaunchDarkly client closed successfully"")


def create_context(
    user_id: str, additional_attributes: Optional[Dict[str, Any]] = None
) -> Context:
    """"""Create LaunchDarkly context with optional additional attributes.""""""
    builder = Context.builder(str(user_id)).kind(""user"")
    if additional_attributes:
        for key, value in additional_attributes.items():
            builder.set(key, value)
    return builder.build()


def feature_flag(
    flag_key: str,
    default: bool = False,
) -> Callable[
    [Callable[P, Union[T, Awaitable[T]]]], Callable[P, Union[T, Awaitable[T]]]
]:
    """"""
    Decorator for feature flag protected endpoints.
    """"""

    def decorator(
        func: Callable[P, Union[T, Awaitable[T]]],
    ) -> Callable[P, Union[T, Awaitable[T]]]:
        @wraps(func)
        async def async_wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
            try:
                user_id = kwargs.get(""user_id"")
                if not user_id:
                    raise ValueError(""user_id is required"")

                if not get_client().is_initialized():
                    logger.warning(
                        f""LaunchDarkly not initialized, using default={default}""
                    )
                    is_enabled = default
                else:
                    context = create_context(str(user_id))
                    is_enabled = get_client().variation(flag_key, context, default)

                if not is_enabled:
                    raise HTTPException(status_code=404, detail=""Feature not available"")

                result = func(*args, **kwargs)
                if asyncio.iscoroutine(result):
                    return await result
                return cast(T, result)
            except Exception as e:
                logger.error(f""Error evaluating feature flag {flag_key}: {e}"")
                raise

        @wraps(func)
        def sync_wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
            try:
                user_id = kwargs.get(""user_id"")
                if not user_id:
                    raise ValueError(""user_id is required"")

                if not get_client().is_initialized():
                    logger.warning(
                        f""LaunchDarkly not initialized, using default={default}""
                    )
                    is_enabled = default
                else:
                    context = create_context(str(user_id))
                    is_enabled = get_client().variation(flag_key, context, default)

                if not is_enabled:
                    raise HTTPException(status_code=404, detail=""Feature not available"")

                return cast(T, func(*args, **kwargs))
            except Exception as e:
                logger.error(f""Error evaluating feature flag {flag_key}: {e}"")
                raise

        return cast(
            Callable[P, Union[T, Awaitable[T]]],
            async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper,
        )

    return decorator


def percentage_rollout(
    flag_key: str,
    default: bool = False,
) -> Callable[
    [Callable[P, Union[T, Awaitable[T]]]], Callable[P, Union[T, Awaitable[T]]]
]:
    """"""Decorator for percentage-based rollouts.""""""
    return feature_flag(flag_key, default)


def beta_feature(
    flag_key: Optional[str] = None,
    unauthorized_response: Any = {""message"": ""Not available in beta""},
) -> Callable[
    [Callable[P, Union[T, Awaitable[T]]]], Callable[P, Union[T, Awaitable[T]]]
]:
    """"""Decorator for beta features.""""""
    actual_key = f""beta-{flag_key}"" if flag_key else ""beta""
    return feature_flag(actual_key, False)


@contextlib.contextmanager
def mock_flag_variation(flag_key: str, return_value: Any):
    """"""Context manager for testing feature flags.""""""
    original_variation = get_client().variation
    get_client().variation = lambda key, context, default: (
        return_value if key == flag_key else original_variation(key, context, default)
    )
    try:
        yield
    finally:
        get_client().variation = original_variation
","import asyncio
import contextlib
import logging
from functools import wraps
from typing import Any, Awaitable, Callable, Dict, Optional, TypeVar, Union, cast

import ldclient
from fastapi import HTTPException
from ldclient import Context, LDClient
from ldclient.config import Config
from typing_extensions import ParamSpec

from .config import SETTINGS

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG)

P = ParamSpec(""P"")
T = TypeVar(""T"")

def get_client() -> LDClient:
    """"""Get the LaunchDarkly client singleton.""""""
    return ldclient.get()

def initialize_launchdarkly() -> None:
    sdk_key = SETTINGS.launch_darkly_sdk_key
    logger.debug(
        f""Initializing LaunchDarkly with SDK key: {'present' if sdk_key else 'missing'}""
    )
    if not sdk_key:
        logger.warning(""LaunchDarkly SDK key not configured"")
        return
    
    config = Config(sdk_key)
    ldclient.set_config(config)
    
    if ldclient.get().is_initialized():
        logger.info(""LaunchDarkly client initialized successfully"")
    else:
        logger.error(""LaunchDarkly client failed to initialize"")

def shutdown_launchdarkly() -> None:
    """"""Shutdown the LaunchDarkly client.""""""
    if ldclient.get().is_initialized():
        ldclient.get().close()
        logger.info(""LaunchDarkly client closed successfully"")

def create_context(
    user_id: str, additional_attributes: Optional[Dict[str, Any]] = None
) -> Context:
    """"""Create LaunchDarkly context with optional additional attributes.""""""
    builder = Context.builder(str(user_id)).kind(""user"")
    if additional_attributes:
        for key, value in additional_attributes.items():
            builder.set(key, value)
    return builder.build()

def feature_flag(
    flag_key: str,
    default: bool = False,
) -> Callable[
    [Callable[P, Union[T, Awaitable[T]]]], Callable[P, Union[T, Awaitable[T]]]
]:
    """"""
    Decorator for feature flag protected endpoints.
    """"""
    def decorator(
        func: Callable[P, Union[T, Awaitable[T]]],
    ) -> Callable[P, Union[T, Awaitable[T]]]:
        @wraps(func)
        async def async_wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
            try:
                user_id = kwargs.get(""user_id"")
                if not user_id:
                    raise ValueError(""user_id is required"")
                
                if not get_client().is_initialized():
                    logger.warning(
                        f""LaunchDarkly not initialized, using default={default}""
                    )
                    is_enabled = default
                else:
                    context = create_context(str(user_id))
                    is_enabled = get_client().variation(flag_key, context, default)
                
                if not is_enabled:
                    raise HTTPException(status_code=404, detail=""Feature not available"")
                
                result = func(*args, **kwargs)
                if asyncio.iscoroutine(result):
                    return await result
                return cast(T, result)
            except Exception as e:
                logger.error(f""Error evaluating feature flag {flag_key}: {e}"")
                raise
        
        @wraps(func)
        def sync_wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
            try:
                user_id = kwargs.get(""user_id"")
                if not user_id:
                    raise ValueError(""user_id is required"")
                
                if not get_client().is_initialized():
                    logger.warning(
                        f""LaunchDarkly not initialized, using default={default}""
                    )
                    is_enabled = default
                else:
                    context = create_context(str(user_id))
                    is_enabled = get_client().variation(flag_key, context, default)
                
                if not is_enabled:
                    raise HTTPException(status_code=404, detail=""Feature not available"")
                
                return cast(T, func(*args, **kwargs))
            except Exception as e:
                logger.error(f""Error evaluating feature flag {flag_key}: {e}"")
                raise
        
        return cast(
            Callable[P, Union[T, Awaitable[T]]],
            async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper,
        )
    return decorator

def percentage_rollout(
    flag_key: str,
    default: bool = False,
) -> Callable[
    [Callable[P, Union[T, Awaitable[T]]]], Callable[P, Union[T, Awaitable[T]]]
]:
    """"""Decorator for percentage-based rollouts.""""""
    return feature_flag(flag_key, default)

def beta_feature(
    flag_key: Optional[str] = None,
    unauthorized_response: Any = {""message"": ""Not available in beta""},
) -> Callable[
    [Callable[P, Union[T, Awaitable[T]]]], Callable[P, Union[T, Awaitable[T]]]
]:
    """"""Decorator for beta features.""""""
    actual_key = f""beta-{flag_key}"" if flag_key else ""beta""
    return feature_flag(actual_key, False)

@contextlib.contextmanager
def mock_flag_variation(flag_key: str, return_value: Any):
    """"""Context manager for testing feature flags.""""""
    original_variation = get_client().variation
    get_client().variation = lambda key, context, default: (
        return_value if key == flag_key else original_variation(key, context, default)
    )
    try:
        yield
    finally:
        get_client().variation = original_variation","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
69,"import logging
import time
from enum import Enum
from typing import Literal

from pydantic import SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName
from backend.util.request import requests

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""revid"",
    api_key=SecretStr(""mock-revid-api-key""),
    title=""Mock Revid API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class AudioTrack(str, Enum):
    OBSERVER = (""Observer"",)
    FUTURISTIC_BEAT = (""Futuristic Beat"",)
    SCIENCE_DOCUMENTARY = (""Science Documentary"",)
    HOTLINE = (""Hotline"",)
    BLADERUNNER_2049 = (""Bladerunner 2049"",)
    A_FUTURE = (""A Future"",)
    ELYSIAN_EMBERS = (""Elysian Embers"",)
    INSPIRING_CINEMATIC = (""Inspiring Cinematic"",)
    BLADERUNNER_REMIX = (""Bladerunner Remix"",)
    IZZAMUZZIC = (""Izzamuzzic"",)
    NAS = (""Nas"",)
    PARIS_ELSE = (""Paris - Else"",)
    SNOWFALL = (""Snowfall"",)
    BURLESQUE = (""Burlesque"",)
    CORNY_CANDY = (""Corny Candy"",)
    HIGHWAY_NOCTURNE = (""Highway Nocturne"",)
    I_DONT_THINK_SO = (""I Don't Think So"",)
    LOSING_YOUR_MARBLES = (""Losing Your Marbles"",)
    REFRESHER = (""Refresher"",)
    TOURIST = (""Tourist"",)
    TWIN_TYCHES = (""Twin Tyches"",)

    @property
    def audio_url(self):
        audio_urls = {
            AudioTrack.OBSERVER: ""https://cdn.tfrv.xyz/audio/observer.mp3"",
            AudioTrack.FUTURISTIC_BEAT: ""https://cdn.tfrv.xyz/audio/_futuristic-beat.mp3"",
            AudioTrack.SCIENCE_DOCUMENTARY: ""https://cdn.tfrv.xyz/audio/_science-documentary.mp3"",
            AudioTrack.HOTLINE: ""https://cdn.tfrv.xyz/audio/_hotline.mp3"",
            AudioTrack.BLADERUNNER_2049: ""https://cdn.tfrv.xyz/audio/_bladerunner-2049.mp3"",
            AudioTrack.A_FUTURE: ""https://cdn.tfrv.xyz/audio/a-future.mp3"",
            AudioTrack.ELYSIAN_EMBERS: ""https://cdn.tfrv.xyz/audio/elysian-embers.mp3"",
            AudioTrack.INSPIRING_CINEMATIC: ""https://cdn.tfrv.xyz/audio/inspiring-cinematic-ambient.mp3"",
            AudioTrack.BLADERUNNER_REMIX: ""https://cdn.tfrv.xyz/audio/bladerunner-remix.mp3"",
            AudioTrack.IZZAMUZZIC: ""https://cdn.tfrv.xyz/audio/_izzamuzzic.mp3"",
            AudioTrack.NAS: ""https://cdn.tfrv.xyz/audio/_nas.mp3"",
            AudioTrack.PARIS_ELSE: ""https://cdn.tfrv.xyz/audio/_paris-else.mp3"",
            AudioTrack.SNOWFALL: ""https://cdn.tfrv.xyz/audio/_snowfall.mp3"",
            AudioTrack.BURLESQUE: ""https://cdn.tfrv.xyz/audio/burlesque.mp3"",
            AudioTrack.CORNY_CANDY: ""https://cdn.tfrv.xyz/audio/corny-candy.mp3"",
            AudioTrack.HIGHWAY_NOCTURNE: ""https://cdn.tfrv.xyz/audio/highway-nocturne.mp3"",
            AudioTrack.I_DONT_THINK_SO: ""https://cdn.tfrv.xyz/audio/i-dont-think-so.mp3"",
            AudioTrack.LOSING_YOUR_MARBLES: ""https://cdn.tfrv.xyz/audio/losing-your-marbles.mp3"",
            AudioTrack.REFRESHER: ""https://cdn.tfrv.xyz/audio/refresher.mp3"",
            AudioTrack.TOURIST: ""https://cdn.tfrv.xyz/audio/tourist.mp3"",
            AudioTrack.TWIN_TYCHES: ""https://cdn.tfrv.xyz/audio/twin-tynches.mp3"",
        }
        return audio_urls[self]


class GenerationPreset(str, Enum):
    LEONARDO = (""Default"",)
    ANIME = (""Anime"",)
    REALISM = (""Realist"",)
    ILLUSTRATION = (""Illustration"",)
    SKETCH_COLOR = (""Sketch Color"",)
    SKETCH_BW = (""Sketch B&W"",)
    PIXAR = (""Pixar"",)
    INK = (""Japanese Ink"",)
    RENDER_3D = (""3D Render"",)
    LEGO = (""Lego"",)
    SCIFI = (""Sci-Fi"",)
    RECRO_CARTOON = (""Retro Cartoon"",)
    PIXEL_ART = (""Pixel Art"",)
    CREATIVE = (""Creative"",)
    PHOTOGRAPHY = (""Photography"",)
    RAYTRACED = (""Raytraced"",)
    ENVIRONMENT = (""Environment"",)
    FANTASY = (""Fantasy"",)
    ANIME_SR = (""Anime Realism"",)
    MOVIE = (""Movie"",)
    STYLIZED_ILLUSTRATION = (""Stylized Illustration"",)
    MANGA = (""Manga"",)


class Voice(str, Enum):
    LILY = ""Lily""
    DANIEL = ""Daniel""
    BRIAN = ""Brian""
    JESSICA = ""Jessica""
    CHARLOTTE = ""Charlotte""
    CALLUM = ""Callum""

    @property
    def voice_id(self):
        voice_id_map = {
            Voice.LILY: ""pFZP5JQG7iQjIQuC4Bku"",
            Voice.DANIEL: ""onwK4e9ZLuTAKqWW03F9"",
            Voice.BRIAN: ""nPczCjzI2devNBz1zQrb"",
            Voice.JESSICA: ""cgSgspJ2msm6clMCkdW9"",
            Voice.CHARLOTTE: ""XB0fDUnXU5powFXDhCwa"",
            Voice.CALLUM: ""N2lVS1w4EtoT3dr4eOWO"",
        }
        return voice_id_map[self]

    def __str__(self):
        return self.value


class VisualMediaType(str, Enum):
    STOCK_VIDEOS = (""stockVideo"",)
    MOVING_AI_IMAGES = (""movingImage"",)
    AI_VIDEO = (""aiVideo"",)


logger = logging.getLogger(__name__)


class AIShortformVideoCreatorBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.REVID], Literal[""api_key""]
        ] = CredentialsField(
            description=""The revid.ai integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        script: str = SchemaField(
            description=""""""1. Use short and punctuated sentences\n\n2. Use linebreaks to create a new clip\n\n3. Text outside of brackets is spoken by the AI, and [text between brackets] will be used to guide the visual generation. For example, [close-up of a cat] will show a close-up of a cat."""""",
            placeholder=""[close-up of a cat] Meow!"",
        )
        ratio: str = SchemaField(
            description=""Aspect ratio of the video"", default=""9 / 16""
        )
        resolution: str = SchemaField(
            description=""Resolution of the video"", default=""720p""
        )
        frame_rate: int = SchemaField(description=""Frame rate of the video"", default=60)
        generation_preset: GenerationPreset = SchemaField(
            description=""Generation preset for visual style - only effects AI generated visuals"",
            default=GenerationPreset.LEONARDO,
            placeholder=GenerationPreset.LEONARDO,
        )
        background_music: AudioTrack = SchemaField(
            description=""Background music track"",
            default=AudioTrack.HIGHWAY_NOCTURNE,
            placeholder=AudioTrack.HIGHWAY_NOCTURNE,
        )
        voice: Voice = SchemaField(
            description=""AI voice to use for narration"",
            default=Voice.LILY,
            placeholder=Voice.LILY,
        )
        video_style: VisualMediaType = SchemaField(
            description=""Type of visual media to use for the video"",
            default=VisualMediaType.STOCK_VIDEOS,
            placeholder=VisualMediaType.STOCK_VIDEOS,
        )

    class Output(BlockSchema):
        video_url: str = SchemaField(description=""The URL of the created video"")
        error: str = SchemaField(description=""Error message if the request failed"")

    def __init__(self):
        super().__init__(
            id=""361697fb-0c4f-4feb-aed3-8320c88c771b"",
            description=""Creates a shortform video using revid.ai"",
            categories={BlockCategory.SOCIAL, BlockCategory.AI},
            input_schema=AIShortformVideoCreatorBlock.Input,
            output_schema=AIShortformVideoCreatorBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""script"": ""[close-up of a cat] Meow!"",
                ""ratio"": ""9 / 16"",
                ""resolution"": ""720p"",
                ""frame_rate"": 60,
                ""generation_preset"": GenerationPreset.LEONARDO,
                ""background_music"": AudioTrack.HIGHWAY_NOCTURNE,
                ""voice"": Voice.LILY,
                ""video_style"": VisualMediaType.STOCK_VIDEOS,
            },
            test_output=(
                ""video_url"",
                ""https://example.com/video.mp4"",
            ),
            test_mock={
                ""create_webhook"": lambda: (
                    ""test_uuid"",
                    ""https://webhook.site/test_uuid"",
                ),
                ""create_video"": lambda api_key, payload: {""pid"": ""test_pid""},
                ""wait_for_video"": lambda api_key, pid, webhook_token, max_wait_time=1000: ""https://example.com/video.mp4"",
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def create_webhook(self):
        url = ""https://webhook.site/token""
        headers = {""Accept"": ""application/json"", ""Content-Type"": ""application/json""}
        response = requests.post(url, headers=headers)
        webhook_data = response.json()
        return webhook_data[""uuid""], f""https://webhook.site/{webhook_data['uuid']}""

    def create_video(self, api_key: SecretStr, payload: dict) -> dict:
        url = ""https://www.revid.ai/api/public/v2/render""
        headers = {""key"": api_key.get_secret_value()}
        response = requests.post(url, json=payload, headers=headers)
        logger.debug(
            f""API Response Status Code: {response.status_code}, Content: {response.text}""
        )
        return response.json()

    def check_video_status(self, api_key: SecretStr, pid: str) -> dict:
        url = f""https://www.revid.ai/api/public/v2/status?pid={pid}""
        headers = {""key"": api_key.get_secret_value()}
        response = requests.get(url, headers=headers)
        return response.json()

    def wait_for_video(
        self,
        api_key: SecretStr,
        pid: str,
        webhook_token: str,
        max_wait_time: int = 1000,
    ) -> str:
        start_time = time.time()
        while time.time() - start_time < max_wait_time:
            status = self.check_video_status(api_key, pid)
            logger.debug(f""Video status: {status}"")

            if status.get(""status"") == ""ready"" and ""videoUrl"" in status:
                return status[""videoUrl""]
            elif status.get(""status"") == ""error"":
                error_message = status.get(""error"", ""Unknown error occurred"")
                logger.error(f""Video creation failed: {error_message}"")
                raise ValueError(f""Video creation failed: {error_message}"")
            elif status.get(""status"") in [""FAILED"", ""CANCELED""]:
                logger.error(f""Video creation failed: {status.get('message')}"")
                raise ValueError(f""Video creation failed: {status.get('message')}"")

            time.sleep(10)

        logger.error(""Video creation timed out"")
        raise TimeoutError(""Video creation timed out"")

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # Create a new Webhook.site URL
        webhook_token, webhook_url = self.create_webhook()
        logger.debug(f""Webhook URL: {webhook_url}"")

        audio_url = input_data.background_music.audio_url

        payload = {
            ""frameRate"": input_data.frame_rate,
            ""resolution"": input_data.resolution,
            ""frameDurationMultiplier"": 18,
            ""webhook"": webhook_url,
            ""creationParams"": {
                ""mediaType"": input_data.video_style,
                ""captionPresetName"": ""Wrap 1"",
                ""selectedVoice"": input_data.voice.voice_id,
                ""hasEnhancedGeneration"": True,
                ""generationPreset"": input_data.generation_preset.name,
                ""selectedAudio"": input_data.background_music,
                ""origin"": ""/create"",
                ""inputText"": input_data.script,
                ""flowType"": ""text-to-video"",
                ""slug"": ""create-tiktok-video"",
                ""hasToGenerateVoice"": True,
                ""hasToTranscript"": False,
                ""hasToSearchMedia"": True,
                ""hasAvatar"": False,
                ""hasWebsiteRecorder"": False,
                ""hasTextSmallAtBottom"": False,
                ""ratio"": input_data.ratio,
                ""sourceType"": ""contentScraping"",
                ""selectedStoryStyle"": {""value"": ""custom"", ""label"": ""Custom""},
                ""hasToGenerateVideos"": input_data.video_style
                != VisualMediaType.STOCK_VIDEOS,
                ""audioUrl"": audio_url,
            },
        }

        logger.debug(""Creating video..."")
        response = self.create_video(credentials.api_key, payload)
        pid = response.get(""pid"")

        if not pid:
            logger.error(
                f""Failed to create video: No project ID returned. API Response: {response}""
            )
            raise RuntimeError(""Failed to create video: No project ID returned"")
        else:
            logger.debug(
                f""Video created with project ID: {pid}. Waiting for completion...""
            )
            video_url = self.wait_for_video(credentials.api_key, pid, webhook_token)
            logger.debug(f""Video ready: {video_url}"")
            yield ""video_url"", video_url
","import time
from enum import Enum
from typing import Literal

from pydantic import SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)

from backend.integrations.providers import ProviderName
from backend.util.request import requests

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""revid"",
    api_key=SecretStr(""mock-revid-api-key""),
    title=""Mock Revid API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}

class AudioTrack(str, Enum):
    OBSERVER = (""Observer"",)
    FUTURISTIC_BEAT = (""Futuristic Beat"",)
    SCIENCE_DOCUMENTARY = (""Science Documentary"",)
    HOTLINE = (""Hotline"",)
    BLADERUNNER_2049 = (""Bladerunner 2049"",)
    A_FUTURE = (""A Future"",)
    ELYSIAN_EMBERS = (""Elysian Embers"",)
    INSPIRING_CINEMATIC = (""Inspiring Cinematic"",)
    BLADERUNNER_REMIX = (""Bladerunner Remix"",)
    IZZAMUZZIC = (""Izzamuzzic"",)
    NAS = (""Nas"",)
    PARIS_ELSE = (""Paris - Else"",)
    SNOWFALL = (""Snowfall"",)
    BURLESQUE = (""Burlesque"",)
    CORNY_CANDY = (""Corny Candy"",)
    HIGHWAY_NOCTURNE = (""Highway Nocturne"",)
    I_DONT_THINK_SO = (""I Don't Think So"",)
    LOSING_YOUR_MARBLES = (""Losing Your Marbles"",)
    REFRESHER = (""Refresher"",)
    TOURIST = (""Tourist"",)
    TWIN_TYCHES = (""Twin Tyches"",)

    @property
    def audio_url(self):
        audio_urls = {
            AudioTrack.OBSERVER: ""https://cdn.tfrv.xyz/audio/observer.mp3"",
            AudioTrack.FUTURISTIC_BEAT: ""https://cdn.tfrv.xyz/audio/_futuristic-beat.mp3"",
            AudioTrack.SCIENCE_DOCUMENTARY: ""https://cdn.tfrv.xyz/audio/_science-documentary.mp3"",
            AudioTrack.HOTLINE: ""https://cdn.tfrv.xyz/audio/_hotline.mp3"",
            AudioTrack.BLADERUNNER_2049: ""https://cdn.tfrv.xyz/audio/_bladerunner-2049.mp3"",
            AudioTrack.A_FUTURE: ""https://cdn.tfrv.xyz/audio/a-future.mp3"",
            AudioTrack.ELYSIAN_EMBERS: ""https://cdn.tfrv.xyz/audio/elysian-embers.mp3"",
            AudioTrack.INSPIRING_CINEMATIC: ""https://cdn.tfrv.xyz/audio/inspiring-cinematic-ambient.mp3"",
            AudioTrack.BLADERUNNER_REMIX: ""https://cdn.tfrv.xyz/audio/bladerunner-remix.mp3"",
            AudioTrack.IZZAMUZZIC: ""https://cdn.tfrv.xyz/audio/_izzamuzzic.mp3"",
            AudioTrack.NAS: ""https://cdn.tfrv.xyz/audio/_nas.mp3"",
            AudioTrack.PARIS_ELSE: ""https://cdn.tfrv.xyz/audio/_paris-else.mp3"",
            AudioTrack.SNOWFALL: ""https://cdn.tfrv.xyz/audio/_snowfall.mp3"",
            AudioTrack.BURLESQUE: ""https://cdn.tfrv.xyz/audio/burlesque.mp3"",
            AudioTrack.CORNY_CANDY: ""https://cdn.tfrv.xyz/audio/corny-candy.mp3"",
            AudioTrack.HIGHWAY_NOCTURNE: ""https://cdn.tfrv.xyz/audio/highway-nocturne.mp3"",
            AudioTrack.I_DONT_THINK_SO: ""https://cdn.tfrv.xyz/audio/i-dont-think-so.mp3"",
            AudioTrack.LOSING_YOUR_MARBLES: ""https://cdn.tfrv.xyz/audio/losing-your-marbles.mp3"",
            AudioTrack.REFRESHER: ""https://cdn.tfrv.xyz/audio/refresher.mp3"",
            AudioTrack.TOURIST: ""https://cdn.tfrv.xyz/audio/tourist.mp3"",
            AudioTrack.TWIN_TYCHES: ""https://cdn.tfrv.xyz/audio/twin-tynches.mp3"",
        }
        return audio_urls[self]

class GenerationPreset(str, Enum):
    LEONARDO = (""Default"",)
    ANIME = (""Anime"",)
    REALISM = (""Realist"",)
    ILLUSTRATION = (""Illustration"",)
    SKETCH_COLOR = (""Sketch Color"",)
    SKETCH_BW = (""Sketch B&W"",)
    PIXAR = (""Pixar"",)
    INK = (""Japanese Ink"",)
    RENDER_3D = (""3D Render"",)
    LEGO = (""Lego"",)
    SCIFI = (""Sci-Fi"",)
    RECRO_CARTOON = (""Retro Cartoon"",)
    PIXEL_ART = (""Pixel Art"",)
    CREATIVE = (""Creative"",)
    PHOTOGRAPHY = (""Photography"",)
    RAYTRACED = (""Raytraced"",)
    ENVIRONMENT = (""Environment"",)
    FANTASY = (""Fantasy"",)
    ANIME_SR = (""Anime Realism"",)
    MOVIE = (""Movie"",)
    STYLIZED_ILLUSTRATION = (""Stylized Illustration"",)
    MANGA = (""Manga"",)

class Voice(str, Enum):
    LILY = ""Lily""
    DANIEL = ""Daniel""
    BRIAN = ""Brian""
    JESSICA = ""Jessica""
    CHARLOTTE = ""Charlotte""
    CALLUM = ""Callum""

    @property
    def voice_id(self):
        voice_id_map = {
            Voice.LILY: ""pFZP5JQG7iQjIQuC4Bku"",
            Voice.DANIEL: ""onwK4e9ZLuTAKqWW03F9"",
            Voice.BRIAN: ""nPczCjzI2devNBz1zQrb"",
            Voice.JESSICA: ""cgSgspJ2msm6clMCkdW9"",
            Voice.CHARLOTTE: ""XB0fDUnXU5powFXDhCwa"",
            Voice.CALLUM: ""N2lVS1w4EtoT3dr4eOWO"",
        }
        return voice_id_map[self]

    def __str__(self):
        return self.value

class VisualMediaType(str, Enum):
    STOCK_VIDEOS = (""stockVideo"",)
    MOVING_AI_IMAGES = (""movingImage"",)
    AI_VIDEO = (""aiVideo"",)

logger = logging.getLogger(__name__)

class AIShortformVideoCreatorBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.REVID], Literal[""api_key""]
        ] = CredentialsField(
            description=""The revid.ai integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        script: str = SchemaField(
            description=""""""1. Use short and punctuated sentences\n\n2. Use linebreaks to create a new clip\n\n3. Text outside of brackets is spoken by the AI, and [text between brackets] will be used to guide the visual generation. For example, [close-up of a cat] will show a close-up of a cat."""""",
            placeholder=""[close-up of a cat] Meow!"",
        )
        ratio: str = SchemaField(
            description=""Aspect ratio of the video"", default=""9 / 16""
        )
        resolution: str = SchemaField(
            description=""Resolution of the video"", default=""720p""
        )
        frame_rate: int = SchemaField(description=""Frame rate of the video"", default=60)
        generation_preset: GenerationPreset = SchemaField(
            description=""Generation preset for visual style - only effects AI generated visuals"",
            default=GenerationPreset.LEONARDO,
            placeholder=GenerationPreset.LEONARDO,
        )
        background_music: AudioTrack = SchemaField(
            description=""Background music track"",
            default=AudioTrack.HIGHWAY_NOCTURNE,
            placeholder=AudioTrack.HIGHWAY_NOCTURNE,
        )
        voice: Voice = SchemaField(
            description=""AI voice to use for narration"",
            default=Voice.LILY,
            placeholder=Voice.LILY,
        )
        video_style: VisualMediaType = SchemaField(
            description=""Type of visual media to use for the video"",
            default=VisualMediaType.STOCK_VIDEOS,
            placeholder=VisualMediaType.STOCK_VIDEOS,
        )

    class Output(BlockSchema):
        video_url: str = SchemaField(description=""The URL of the created video"")
        error: str = SchemaField(description=""Error message if the request failed"")

    def __init__(self):
        super().__init__(
            id=""361697fb-0c4f-4feb-aed3-8320c88c771b"",
            description=""Creates a shortform video using revid.ai"",
            categories={BlockCategory.SOCIAL, BlockCategory.AI},
            input_schema=AIShortformVideoCreatorBlock.Input,
            output_schema=AIShortformVideoCreatorBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""script"": ""[close-up of a cat] Meow!"",
                ""ratio"": ""9 / 16"",
                ""resolution"": ""720p"",
                ""frame_rate"": 60,
                ""generation_preset"": GenerationPreset.LEONARDO,
                ""background_music"": AudioTrack.HIGHWAY_NOCTURNE,
                ""voice"": Voice.LILY,
                ""video_style"": VisualMediaType.STOCK_VIDEOS,
            },
            test_output=(
                ""video_url"",
                ""https://example.com/video.mp4"",
            ),
            test_mock={
                ""create_webhook"": lambda: (
                    ""test_uuid"",
                    ""https://webhook.site/test_uuid"",
                ),
                ""create_video"": lambda api_key, payload: {""pid"": ""test_pid""},
                ""wait_for_video"": lambda api_key, pid, webhook_token, max_wait_time=1000: ""https://example.com/video.mp4"",
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def create_webhook(self):
        url = ""https://webhook.site/token""
        headers = {""Accept"": ""application/json"", ""Content-Type"": ""application/json""}
        response = requests.post(url, headers=headers)
        webhook_data = response.json()
        return webhook_data[""uuid""], f""https://webhook.site/{webhook_data['uuid']}""

    def create_video(self, api_key: SecretStr, payload: dict) -> dict:
        url = ""https://www.revid.ai/api/public/v2/render""
        headers = {""key"": api_key.get_secret_value()}
        response = requests.post(url, json=payload, headers=headers)
        logger.debug(
            f""API Response Status Code: {response.status_code}, Content: {response.text}""
        )
        return response.json()

    def check_video_status(self, api_key: SecretStr, pid: str) -> dict:
        url = f""https://www.revid.ai/api/public/v2/status?pid={pid}""
        headers = {""key"": api_key.get_secret_value()}
        response = requests.get(url, headers=headers)
        return response.json()

    def wait_for_video(
        self,
        api_key: SecretStr,
        pid: str,
        webhook_token: str,
        max_wait_time: int = 1000,
    ) -> str:
        start_time = time.time()
        while time.time() - start_time < max_wait_time:
            status = self.check_video_status(api_key, pid)
            logger.debug(f""Video status: {status}"")

            if status.get(""status"") == ""ready"" and ""videoUrl"" in status:
                return status[""videoUrl""]
            elif status.get(""status"") == ""error"":
                error_message = status.get(""error"", ""Unknown error occurred"")
                logger.error(f""Video creation failed: {error_message}"")
                raise ValueError(f""Video creation failed: {error_message}"")
            elif status.get(""status"") in [""FAILED"", ""CANCELED""]:
                logger.error(f""Video creation failed: {status.get('message')}"")
                raise ValueError(f""Video creation failed: {status.get('message')}"")

            time.sleep(10)

        logger.error(""Video creation timed out"")
        raise TimeoutError(""Video creation timed out"")

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # Create a new Webhook.site URL
        webhook_token, webhook_url = self.create_webhook()
        logger.debug(f""Webhook URL: {webhook_url}"")

        audio_url = input_data.background_music.audio_url

        payload = {
            ""frameRate"": input_data.frame_rate,
            ""resolution"": input_data.resolution,
            ""frameDurationMultiplier"": 18,
            ""webhook"": webhook_url,
            ""creationParams"": {
                ""mediaType"": input_data.video_style,
                ""captionPresetName"": ""Wrap 1"",
                ""selectedVoice"": input_data.voice.voice_id,
                ""hasEnhancedGeneration"": True,
                ""generationPreset"": input_data.generation_preset.name,
                ""selectedAudio"": input_data.background_music,
                ""origin"": ""/create"",
                ""inputText"": input_data.script,
                ""flowType"": ""text-to-video"",
                ""slug"": ""create-tiktok-video"",
                ""hasToGenerateVoice"": True,
                ""hasToTranscript"": False,
                ""hasToSearchMedia"": True,
                ""hasAvatar"": False,
                ""hasWebsiteRecorder"": False,
                ""hasTextSmallAtBottom"": False,
                ""ratio"": input_data.ratio,
                ""sourceType"": ""contentScraping"",
                ""selectedStoryStyle"": {""value"": ""custom"", ""label"": ""Custom""},
                ""hasToGenerateVideos"": input_data.video_style
                != VisualMediaType.STOCK_VIDEOS,
                ""audioUrl"": audio_url,
            },
        }

        logger.debug(""Creating video..."")
        response = self.create_video(credentials.api_key, payload)
        pid = response.get(""pid"")

        if not pid:
            logger.error(
                f""Failed to create video: No project ID returned. API Response: {response}""
            )
            raise RuntimeError(""Failed to create video: No project ID returned"")
        else:
            logger.debug(
                f""Video created with project ID: {pid}. Waiting for completion...""
            )
            video_url = self.wait_for_video(credentials.api_key, pid, webhook_token)
            logger.debug(f""Video ready: {video_url}"")
            yield ""video_url"", video_url","{'final_score': 99.86, 'length_similarity': 99.87, 'token_similarity': 99.83, 'structure_similarity': 99.66, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
70,"import logging
from typing import TYPE_CHECKING, Annotated, Literal

from fastapi import APIRouter, Body, Depends, HTTPException, Path, Query, Request
from pydantic import BaseModel, Field, SecretStr

from backend.data.graph import set_node_webhook
from backend.data.integrations import (
    WebhookEvent,
    get_all_webhooks_by_creds,
    get_webhook,
    publish_webhook_event,
    wait_for_webhook_event,
)
from backend.data.model import (
    APIKeyCredentials,
    Credentials,
    CredentialsType,
    OAuth2Credentials,
)
from backend.executor.manager import ExecutionManager
from backend.integrations.creds_manager import IntegrationCredentialsManager
from backend.integrations.oauth import HANDLERS_BY_NAME
from backend.integrations.providers import ProviderName
from backend.integrations.webhooks import WEBHOOK_MANAGERS_BY_NAME
from backend.util.exceptions import NeedConfirmation
from backend.util.service import get_service_client
from backend.util.settings import Settings

if TYPE_CHECKING:
    from backend.integrations.oauth import BaseOAuthHandler

from ..utils import get_user_id

logger = logging.getLogger(__name__)
settings = Settings()
router = APIRouter()

creds_manager = IntegrationCredentialsManager()


class LoginResponse(BaseModel):
    login_url: str
    state_token: str


@router.get(""/{provider}/login"")
def login(
    provider: Annotated[
        ProviderName, Path(title=""The provider to initiate an OAuth flow for"")
    ],
    user_id: Annotated[str, Depends(get_user_id)],
    request: Request,
    scopes: Annotated[
        str, Query(title=""Comma-separated list of authorization scopes"")
    ] = """",
) -> LoginResponse:
    handler = _get_provider_oauth_handler(request, provider)

    requested_scopes = scopes.split("","") if scopes else []

    # Generate and store a secure random state token along with the scopes
    state_token = creds_manager.store.store_state_token(
        user_id, provider, requested_scopes
    )

    login_url = handler.get_login_url(requested_scopes, state_token)

    return LoginResponse(login_url=login_url, state_token=state_token)


class CredentialsMetaResponse(BaseModel):
    id: str
    provider: str
    type: CredentialsType
    title: str | None
    scopes: list[str] | None
    username: str | None


@router.post(""/{provider}/callback"")
def callback(
    provider: Annotated[
        ProviderName, Path(title=""The target provider for this OAuth exchange"")
    ],
    code: Annotated[str, Body(title=""Authorization code acquired by user login"")],
    state_token: Annotated[str, Body(title=""Anti-CSRF nonce"")],
    user_id: Annotated[str, Depends(get_user_id)],
    request: Request,
) -> CredentialsMetaResponse:
    logger.debug(f""Received OAuth callback for provider: {provider}"")
    handler = _get_provider_oauth_handler(request, provider)

    # Verify the state token
    if not creds_manager.store.verify_state_token(user_id, state_token, provider):
        logger.warning(f""Invalid or expired state token for user {user_id}"")
        raise HTTPException(status_code=400, detail=""Invalid or expired state token"")

    try:
        scopes = creds_manager.store.get_any_valid_scopes_from_state_token(
            user_id, state_token, provider
        )
        logger.debug(f""Retrieved scopes from state token: {scopes}"")

        scopes = handler.handle_default_scopes(scopes)

        credentials = handler.exchange_code_for_tokens(code, scopes)
        logger.debug(f""Received credentials with final scopes: {credentials.scopes}"")

        # Check if the granted scopes are sufficient for the requested scopes
        if not set(scopes).issubset(set(credentials.scopes)):
            # For now, we'll just log the warning and continue
            logger.warning(
                f""Granted scopes {credentials.scopes} for provider {provider.value} ""
                f""do not include all requested scopes {scopes}""
            )

    except Exception as e:
        logger.error(f""Code->Token exchange failed for provider {provider.value}: {e}"")
        raise HTTPException(
            status_code=400, detail=f""Failed to exchange code for tokens: {str(e)}""
        )

    # TODO: Allow specifying `title` to set on `credentials`
    creds_manager.create(user_id, credentials)

    logger.debug(
        f""Successfully processed OAuth callback for user {user_id} ""
        f""and provider {provider.value}""
    )
    return CredentialsMetaResponse(
        id=credentials.id,
        provider=credentials.provider,
        type=credentials.type,
        title=credentials.title,
        scopes=credentials.scopes,
        username=credentials.username,
    )


@router.get(""/credentials"")
def list_credentials(
    user_id: Annotated[str, Depends(get_user_id)],
) -> list[CredentialsMetaResponse]:
    credentials = creds_manager.store.get_all_creds(user_id)
    return [
        CredentialsMetaResponse(
            id=cred.id,
            provider=cred.provider,
            type=cred.type,
            title=cred.title,
            scopes=cred.scopes if isinstance(cred, OAuth2Credentials) else None,
            username=cred.username if isinstance(cred, OAuth2Credentials) else None,
        )
        for cred in credentials
    ]


@router.get(""/{provider}/credentials"")
def list_credentials_by_provider(
    provider: Annotated[
        ProviderName, Path(title=""The provider to list credentials for"")
    ],
    user_id: Annotated[str, Depends(get_user_id)],
) -> list[CredentialsMetaResponse]:
    credentials = creds_manager.store.get_creds_by_provider(user_id, provider)
    return [
        CredentialsMetaResponse(
            id=cred.id,
            provider=cred.provider,
            type=cred.type,
            title=cred.title,
            scopes=cred.scopes if isinstance(cred, OAuth2Credentials) else None,
            username=cred.username if isinstance(cred, OAuth2Credentials) else None,
        )
        for cred in credentials
    ]


@router.get(""/{provider}/credentials/{cred_id}"")
def get_credential(
    provider: Annotated[
        ProviderName, Path(title=""The provider to retrieve credentials for"")
    ],
    cred_id: Annotated[str, Path(title=""The ID of the credentials to retrieve"")],
    user_id: Annotated[str, Depends(get_user_id)],
) -> Credentials:
    credential = creds_manager.get(user_id, cred_id)
    if not credential:
        raise HTTPException(status_code=404, detail=""Credentials not found"")
    if credential.provider != provider:
        raise HTTPException(
            status_code=404, detail=""Credentials do not match the specified provider""
        )
    return credential


@router.post(""/{provider}/credentials"", status_code=201)
def create_api_key_credentials(
    user_id: Annotated[str, Depends(get_user_id)],
    provider: Annotated[
        ProviderName, Path(title=""The provider to create credentials for"")
    ],
    api_key: Annotated[str, Body(title=""The API key to store"")],
    title: Annotated[str, Body(title=""Optional title for the credentials"")],
    expires_at: Annotated[
        int | None, Body(title=""Unix timestamp when the key expires"")
    ] = None,
) -> APIKeyCredentials:
    new_credentials = APIKeyCredentials(
        provider=provider,
        api_key=SecretStr(api_key),
        title=title,
        expires_at=expires_at,
    )

    try:
        creds_manager.create(user_id, new_credentials)
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f""Failed to store credentials: {str(e)}""
        )
    return new_credentials


class CredentialsDeletionResponse(BaseModel):
    deleted: Literal[True] = True
    revoked: bool | None = Field(
        description=""Indicates whether the credentials were also revoked by their ""
        ""provider. `None`/`null` if not applicable, e.g. when deleting ""
        ""non-revocable credentials such as API keys.""
    )


class CredentialsDeletionNeedsConfirmationResponse(BaseModel):
    deleted: Literal[False] = False
    need_confirmation: Literal[True] = True
    message: str


@router.delete(""/{provider}/credentials/{cred_id}"")
async def delete_credentials(
    request: Request,
    provider: Annotated[
        ProviderName, Path(title=""The provider to delete credentials for"")
    ],
    cred_id: Annotated[str, Path(title=""The ID of the credentials to delete"")],
    user_id: Annotated[str, Depends(get_user_id)],
    force: Annotated[
        bool, Query(title=""Whether to proceed if any linked webhooks are still in use"")
    ] = False,
) -> CredentialsDeletionResponse | CredentialsDeletionNeedsConfirmationResponse:
    creds = creds_manager.store.get_creds_by_id(user_id, cred_id)
    if not creds:
        raise HTTPException(status_code=404, detail=""Credentials not found"")
    if creds.provider != provider:
        raise HTTPException(
            status_code=404, detail=""Credentials do not match the specified provider""
        )

    try:
        await remove_all_webhooks_for_credentials(creds, force)
    except NeedConfirmation as e:
        return CredentialsDeletionNeedsConfirmationResponse(message=str(e))

    creds_manager.delete(user_id, cred_id)

    tokens_revoked = None
    if isinstance(creds, OAuth2Credentials):
        handler = _get_provider_oauth_handler(request, provider)
        tokens_revoked = handler.revoke_tokens(creds)

    return CredentialsDeletionResponse(revoked=tokens_revoked)


# ------------------------- WEBHOOK STUFF -------------------------- #


# ⚠️ Note
# No user auth check because this endpoint is for webhook ingress and relies on
# validation by the provider-specific `WebhooksManager`.
@router.post(""/{provider}/webhooks/{webhook_id}/ingress"")
async def webhook_ingress_generic(
    request: Request,
    provider: Annotated[
        ProviderName, Path(title=""Provider where the webhook was registered"")
    ],
    webhook_id: Annotated[str, Path(title=""Our ID for the webhook"")],
):
    logger.debug(f""Received {provider.value} webhook ingress for ID {webhook_id}"")
    webhook_manager = WEBHOOK_MANAGERS_BY_NAME[provider]()
    webhook = await get_webhook(webhook_id)
    logger.debug(f""Webhook #{webhook_id}: {webhook}"")
    payload, event_type = await webhook_manager.validate_payload(webhook, request)
    logger.debug(
        f""Validated {provider.value} {webhook.webhook_type} {event_type} event ""
        f""with payload {payload}""
    )

    webhook_event = WebhookEvent(
        provider=provider,
        webhook_id=webhook_id,
        event_type=event_type,
        payload=payload,
    )
    await publish_webhook_event(webhook_event)
    logger.debug(f""Webhook event published: {webhook_event}"")

    if not webhook.attached_nodes:
        return

    executor = get_service_client(ExecutionManager)
    for node in webhook.attached_nodes:
        logger.debug(f""Webhook-attached node: {node}"")
        if not node.is_triggered_by_event_type(event_type):
            logger.debug(f""Node #{node.id} doesn't trigger on event {event_type}"")
            continue
        logger.debug(f""Executing graph #{node.graph_id} node #{node.id}"")
        executor.add_execution(
            node.graph_id,
            data={f""webhook_{webhook_id}_payload"": payload},
            user_id=webhook.user_id,
        )


@router.post(""/webhooks/{webhook_id}/ping"")
async def webhook_ping(
    webhook_id: Annotated[str, Path(title=""Our ID for the webhook"")],
    user_id: Annotated[str, Depends(get_user_id)],  # require auth
):
    webhook = await get_webhook(webhook_id)
    webhook_manager = WEBHOOK_MANAGERS_BY_NAME[webhook.provider]()

    credentials = (
        creds_manager.get(user_id, webhook.credentials_id)
        if webhook.credentials_id
        else None
    )
    try:
        await webhook_manager.trigger_ping(webhook, credentials)
    except NotImplementedError:
        return False

    if not await wait_for_webhook_event(webhook_id, event_type=""ping"", timeout=10):
        raise HTTPException(status_code=504, detail=""Webhook ping timed out"")

    return True


# --------------------------- UTILITIES ---------------------------- #


async def remove_all_webhooks_for_credentials(
    credentials: Credentials, force: bool = False
) -> None:
    """"""
    Remove and deregister all webhooks that were registered using the given credentials.

    Params:
        credentials: The credentials for which to remove the associated webhooks.
        force: Whether to proceed if any of the webhooks are still in use.

    Raises:
        NeedConfirmation: If any of the webhooks are still in use and `force` is `False`
    """"""
    webhooks = await get_all_webhooks_by_creds(credentials.id)
    if credentials.provider not in WEBHOOK_MANAGERS_BY_NAME:
        if webhooks:
            logger.error(
                f""Credentials #{credentials.id} for provider {credentials.provider} ""
                f""are attached to {len(webhooks)} webhooks, ""
                f""but there is no available WebhooksHandler for {credentials.provider}""
            )
        return
    if any(w.attached_nodes for w in webhooks) and not force:
        raise NeedConfirmation(
            ""Some webhooks linked to these credentials are still in use by an agent""
        )
    for webhook in webhooks:
        # Unlink all nodes
        for node in webhook.attached_nodes or []:
            await set_node_webhook(node.id, None)

        # Prune the webhook
        webhook_manager = WEBHOOK_MANAGERS_BY_NAME[credentials.provider]()
        success = await webhook_manager.prune_webhook_if_dangling(
            webhook.id, credentials
        )
        if not success:
            logger.warning(f""Webhook #{webhook.id} failed to prune"")


def _get_provider_oauth_handler(
    req: Request, provider_name: ProviderName
) -> ""BaseOAuthHandler"":
    if provider_name not in HANDLERS_BY_NAME:
        raise HTTPException(
            status_code=404,
            detail=f""Provider '{provider_name.value}' does not support OAuth"",
        )

    client_id = getattr(settings.secrets, f""{provider_name.value}_client_id"")
    client_secret = getattr(settings.secrets, f""{provider_name.value}_client_secret"")
    if not (client_id and client_secret):
        raise HTTPException(
            status_code=501,
            detail=(
                f""Integration with provider '{provider_name.value}' is not configured""
            ),
        )

    handler_class = HANDLERS_BY_NAME[provider_name]
    frontend_base_url = (
        settings.config.frontend_base_url
        or settings.config.platform_base_url
        or str(req.base_url)
    )
    return handler_class(
        client_id=client_id,
        client_secret=client_secret,
        redirect_uri=f""{frontend_base_url}/auth/integrations/oauth_callback"",
    )
","import logging
from typing import TYPE_CHECKING, Annotated, Literal

from fastapi import APIRouter, Body, Depends, HTTPException, Path, Query, Request
from pydantic import BaseModel, Field, SecretStr

from backend.data.graph import set_node_webhook
from backend.data.integrations import (
    WebhookEvent,
    get_all_webhooks_by_creds,
    get_webhook,
    publish_webhook_event,
    wait_for_webhook_event,
)
from backend.data.model import (
    APIKeyCredentials,
    Credentials,
    CredentialsType,
    OAuth2Credentials,
)
from backend.executor.manager import ExecutionManager
from backend.integrations.creds_manager import IntegrationCredentialsManager
from backend.integrations.oauth import HANDLERS_BY_NAME
from backend.integrations.providers import ProviderName
from backend.integrations.webhooks import WEBHOOK_MANAGERS_BY_NAME
from backend.util.exceptions import NeedConfirmation
from backend.util.service import get_service_client
from backend.util.settings import Settings

if TYPE_CHECKING:
    from backend.integrations.oauth import BaseOAuthHandler

from ..utils import get_user_id

logger = logging.getLogger(__name__)
settings = Settings()
router = APIRouter()

creds_manager = IntegrationCredentialsManager()

class LoginResponse(BaseModel):
    login_url: str
    state_token: str

@router.get(""/{provider}/login"")
def login(
    provider: Annotated[
        ProviderName, Path(title=""The provider to initiate an OAuth flow for"")
    ],
    user_id: Annotated[str, Depends(get_user_id)],
    request: Request,
    scopes: Annotated[
        str, Query(title=""Comma-separated list of authorization scopes"")
    ] = """",
) -> LoginResponse:
    handler = _get_provider_oauth_handler(request, provider)
    requested_scopes = scopes.split("","") if scopes else []
    state_token = creds_manager.store.store_state_token(
        user_id, provider, requested_scopes
    )
    login_url = handler.get_login_url(requested_scopes, state_token)
    return LoginResponse(login_url=login_url, state_token=state_token)

class CredentialsMetaResponse(BaseModel):
    id: str
    provider: str
    type: CredentialsType
    title: str | None
    scopes: list[str] | None
    username: str | None

@router.post(""/{provider}/callback"")
def callback(
    provider: Annotated[
        ProviderName, Path(title=""The target provider for this OAuth exchange"")
    ],
    code: Annotated[str, Body(title=""Authorization code acquired by user login"")],
    state_token: Annotated[str, Body(title=""Anti-CSRF nonce"")],
    user_id: Annotated[str, Depends(get_user_id)],
    request: Request,
) -> CredentialsMetaResponse:
    logger.debug(f""Received OAuth callback for provider: {provider}"")
    handler = _get_provider_oauth_handler(request, provider)

    # Verify the state token
    if not creds_manager.store.verify_state_token(user_id, state_token, provider):
        logger.warning(f""Invalid or expired state token for user {user_id}"")
        raise HTTPException(status_code=400, detail=""Invalid or expired state token"")

    try:
        scopes = creds_manager.store.get_any_valid_scopes_from_state_token(
            user_id, state_token, provider
        )
        logger.debug(f""Retrieved scopes from state token: {scopes}"")
        scopes = handler.handle_default_scopes(scopes)
        credentials = handler.exchange_code_for_tokens(code, scopes)
        logger.debug(f""Received credentials with final scopes: {credentials.scopes}"")

        # Check if the granted scopes are sufficient for the requested scopes
        if not set(scopes).issubset(set(credentials.scopes)):
            # For now, we'll just log the warning and continue
            logger.warning(
                f""Granted scopes {credentials.scopes} for provider {provider.value} ""
                f""do not include all requested scopes {scopes}""
            )

        # TODO: Allow specifying `title` to set on `credentials`
        creds_manager.create(user_id, credentials)
        logger.debug(
            f""Successfully processed OAuth callback for user {user_id} ""
            f""and provider {provider.value}""
        )
        return CredentialsMetaResponse(
            id=credentials.id,
            provider=credentials.provider,
            type=credentials.type,
            title=credentials.title,
            scopes=credentials.scopes,
            username=credentials.username,
        )
    except Exception as e:
        logger.error(f""Code->Token exchange failed for provider {provider.value}: {e}"")
        raise HTTPException(
            status_code=400, detail=f""Failed to exchange code for tokens: {str(e)}""
        )

@router.get(""/credentials"")
def list_credentials(
    user_id: Annotated[str, Depends(get_user_id)],
) -> list[CredentialsMetaResponse]:
    credentials = creds_manager.store.get_all_creds(user_id)
    return [
        CredentialsMetaResponse(
            id=cred.id,
            provider=cred.provider,
            type=cred.type,
            title=cred.title,
            scopes=cred.scopes if isinstance(cred, OAuth2Credentials) else None,
            username=cred.username if isinstance(cred, OAuth2Credentials) else None,
        )
        for cred in credentials
    ]

@router.get(""/{provider}/credentials"")
def list_credentials_by_provider(
    provider: Annotated[
        ProviderName, Path(title=""The provider to list credentials for"")
    ],
    user_id: Annotated[str, Depends(get_user_id)],
) -> list[CredentialsMetaResponse]:
    credentials = creds_manager.store.get_creds_by_provider(user_id, provider)
    return [
        CredentialsMetaResponse(
            id=cred.id,
            provider=cred.provider,
            type=cred.type,
            title=cred.title,
            scopes=cred.scopes if isinstance(cred, OAuth2Credentials) else None,
            username=cred.username if isinstance(cred, OAuth2Credentials) else None,
        )
        for cred in credentials
    ]

@router.get(""/{provider}/credentials/{cred_id}"")
def get_credential(
    provider: Annotated[
        ProviderName, Path(title=""The provider to retrieve credentials for"")
    ],
    cred_id: Annotated[str, Path(title=""The ID of the credentials to retrieve"")],
    user_id: Annotated[str, Depends(get_user_id)],
) -> Credentials:
    credential = creds_manager.get(user_id, cred_id)
    if not credential:
        raise HTTPException(status_code=404, detail=""Credentials not found"")
    if credential.provider != provider:
        raise HTTPException(
            status_code=404, detail=""Credentials do not match the specified provider""
        )
    return credential

@router.post(""/{provider}/credentials"", status_code=201)
def create_api_key_credentials(
    user_id: Annotated[str, Depends(get_user_id)],
    provider: Annotated[
        ProviderName, Path(title=""The provider to create credentials for"")
    ],
    api_key: Annotated[str, Body(title=""The API key to store"")],
    title: Annotated[str, Body(title=""Optional title for the credentials"")],
    expires_at: Annotated[
        int | None, Body(title=""Unix timestamp when the key expires"")
    ] = None,
) -> APIKeyCredentials:
    new_credentials = APIKeyCredentials(
        provider=provider,
        api_key=SecretStr(api_key),
        title=title,
        expires_at=expires_at,
    )
    try:
        creds_manager.create(user_id, new_credentials)
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f""Failed to store credentials: {str(e)}""
        )
    return new_credentials

class CredentialsDeletionResponse(BaseModel):
    deleted: Literal[True] = True
    revoked: bool | None = Field(
        description=""Indicates whether the credentials were also revoked by their ""
        ""provider. `None`/`null` if not applicable, e.g. when deleting ""
        ""non-revocable credentials such as API keys.""
    )

class CredentialsDeletionNeedsConfirmationResponse(BaseModel):
    deleted: Literal[False] = False
    need_confirmation: Literal[True] = True
    message: str

@router.delete(""/{provider}/credentials/{cred_id}"")
async def delete_credentials(
    request: Request,
    provider: Annotated[
        ProviderName, Path(title=""The provider to delete credentials for"")
    ],
    cred_id: Annotated[str, Path(title=""The ID of the credentials to delete"")],
    user_id: Annotated[str, Depends(get_user_id)],
    force: Annotated[
        bool, Query(title=""Whether to proceed if any linked webhooks are still in use"")
    ] = False,
) -> CredentialsDeletionResponse | CredentialsDeletionNeedsConfirmationResponse:
    creds = creds_manager.store.get_creds_by_id(user_id, cred_id)
    if not creds:
        raise HTTPException(status_code=404, detail=""Credentials not found"")
    if creds.provider != provider:
        raise HTTPException(
            status_code=404, detail=""Credentials do not match the specified provider""
        )
    try:
        await remove_all_webhooks_for_credentials(creds, force)
    except NeedConfirmation as e:
        return CredentialsDeletionNeedsConfirmationResponse(message=str(e))

    creds_manager.delete(user_id, cred_id)
    tokens_revoked = None

    if isinstance(creds, OAuth2Credentials):
        handler = _get_provider_oauth_handler(request, provider)
        tokens_revoked = handler.revoke_tokens(creds)

    return CredentialsDeletionResponse(revoked=tokens_revoked)

@router.post(""/{provider}/webhooks/{webhook_id}/ingress"")
async def webhook_ingress_generic(
    request: Request,
    provider: Annotated[
        ProviderName, Path(title=""Provider where the webhook was registered"")
    ],
    webhook_id: Annotated[str, Path(title=""Our ID for the webhook"")],
):
    logger.debug(f""Received {provider.value} webhook ingress for ID {webhook_id}"")
    webhook_manager = WEBHOOK_MANAGERS_BY_NAME[provider]()
    webhook = await get_webhook(webhook_id)
    logger.debug(f""Webhook #{webhook_id}: {webhook}"")
    payload, event_type = await webhook_manager.validate_payload(webhook, request)
    logger.debug(
        f""Validated {provider.value} {webhook.webhook_type} {event_type} event ""
        f""with payload {payload}""
    )
    webhook_event = WebhookEvent(
        provider=provider,
        webhook_id=webhook_id,
        event_type=event_type,
        payload=payload,
    )
    await publish_webhook_event(webhook_event)
    logger.debug(f""Webhook event published: {webhook_event}"")

    if not webhook.attached_nodes:
        return

    executor = get_service_client(ExecutionManager)

    for node in webhook.attached_nodes:
        logger.debug(f""Webhook-attached node: {node}"")
        if not node.is_triggered_by_event_type(event_type):
            logger.debug(f""Node #{node.id} doesn't trigger on event {event_type}"")
            continue
        logger.debug(f""Executing graph #{node.graph_id} node #{node.id}"")
        executor.add_execution(
            node.graph_id,
            data={f""webhook_{webhook_id}_payload"": payload},
            user_id=webhook.user_id,
        )

@router.post(""/webhooks/{webhook_id}/ping"")
async def webhook_ping(
    webhook_id: Annotated[str, Path(title=""Our ID for the webhook"")],
    user_id: Annotated[str, Depends(get_user_id)],  # require auth
):
    webhook = await get_webhook(webhook_id)
    webhook_manager = WEBHOOK_MANAGERS_BY_NAME[webhook.provider]()
    credentials = (
        creds_manager.get(user_id, webhook.credentials_id)
        if webhook.credentials_id
        else None
    )

    try:
        await webhook_manager.trigger_ping(webhook, credentials)
    except NotImplementedError:
        return False

    if not await wait_for_webhook_event(webhook_id, event_type=""ping"", timeout=10):
        raise HTTPException(status_code=504, detail=""Webhook ping timed out"")

    return True

async def remove_all_webhooks_for_credentials(
    credentials: Credentials, force: bool = False
) -> None:
    """"""
    Remove and deregister all webhooks that were registered using the given credentials.

    Params:
        credentials: The credentials for which to remove the associated webhooks.
        force: Whether to proceed if any of the webhooks are still in use.

    Raises:
        NeedConfirmation: If any of the webhooks are still in use and `force` is `False`
    """"""
    webhooks = await get_all_webhooks_by_creds(credentials.id)
    if credentials.provider not in WEBHOOK_MANAGERS_BY_NAME:
        if webhooks:
            logger.error(
                f""Credentials #{credentials.id} for provider {credentials.provider} ""
                f""are attached to {len(webhooks)} webhooks, ""
                f""but there is no available WebhooksHandler for {credentials.provider}""
            )
        return

    if any(w.attached_nodes for w in webhooks) and not force:
        raise NeedConfirmation(
            ""Some webhooks linked to these credentials are still in use by an agent""
        )

    for webhook in webhooks:
        # Unlink all nodes
        for node in webhook.attached_nodes or []:
            await set_node_webhook(node.id, None)
        # Prune the webhook
        webhook_manager = WEBHOOK_MANAGERS_BY_NAME[credentials.provider]()
        success = await webhook_manager.prune_webhook_if_dangling(
            webhook.id, credentials
        )
        if not success:
            logger.warning(f""Webhook #{webhook.id} failed to prune"")

def _get_provider_oauth_handler(
    req: Request, provider_name: ProviderName
) -> ""BaseOAuthHandler"":
    if provider_name not in HANDLERS_BY_NAME:
        raise HTTPException(
            status_code=404,
            detail=f""Provider '{provider_name.value}' does not support OAuth"",
        )

    client_id = getattr(settings.secrets, f""{provider_name.value}_client_id"")
    client_secret = getattr(settings.secrets, f""{provider_name.value}_client_secret"")
    if not (client_id and client_secret):
        raise HTTPException(
            status_code=501,
            detail=(
                f""Integration with provider '{provider_name.value}' is not configured""
            ),
        )

    handler_class = HANDLERS_BY_NAME[provider_name]
    frontend_base_url = (
        settings.config.frontend_base_url
        or settings.config.platform_base_url
        or str(req.base_url)
    )
    return handler_class(
        client_id=client_id,
        client_secret=client_secret,
        redirect_uri=f""{frontend_base_url}/auth/integrations/oauth_callback"",
    )","{'final_score': 97.27, 'length_similarity': 97.9, 'token_similarity': 95.91, 'structure_similarity': 94.92, 'function_similarity': 100, 'complexity_similarity': 98.25, 'variable_similarity': 100}",
71,"""""""
Given a partially filled 9x9 2D array, the objective is to fill a 9x9
square grid with digits numbered 1 to 9, so that every row, column, and
and each of the nine 3x3 sub-grids contains all of the digits.

This can be solved using Backtracking and is similar to n-queens.
We check to see if a cell is safe or not and recursively call the
function on the next column to see if it returns True. if yes, we
have solved the puzzle. else, we backtrack and place another number
in that cell and repeat this process.
""""""

from __future__ import annotations

Matrix = list[list[int]]

# assigning initial values to the grid
initial_grid: Matrix = [
    [3, 0, 6, 5, 0, 8, 4, 0, 0],
    [5, 2, 0, 0, 0, 0, 0, 0, 0],
    [0, 8, 7, 0, 0, 0, 0, 3, 1],
    [0, 0, 3, 0, 1, 0, 0, 8, 0],
    [9, 0, 0, 8, 6, 3, 0, 0, 5],
    [0, 5, 0, 0, 9, 0, 6, 0, 0],
    [1, 3, 0, 0, 0, 0, 2, 5, 0],
    [0, 0, 0, 0, 0, 0, 0, 7, 4],
    [0, 0, 5, 2, 0, 6, 3, 0, 0],
]

# a grid with no solution
no_solution: Matrix = [
    [5, 0, 6, 5, 0, 8, 4, 0, 3],
    [5, 2, 0, 0, 0, 0, 0, 0, 2],
    [1, 8, 7, 0, 0, 0, 0, 3, 1],
    [0, 0, 3, 0, 1, 0, 0, 8, 0],
    [9, 0, 0, 8, 6, 3, 0, 0, 5],
    [0, 5, 0, 0, 9, 0, 6, 0, 0],
    [1, 3, 0, 0, 0, 0, 2, 5, 0],
    [0, 0, 0, 0, 0, 0, 0, 7, 4],
    [0, 0, 5, 2, 0, 6, 3, 0, 0],
]


def is_safe(grid: Matrix, row: int, column: int, n: int) -> bool:
    """"""
    This function checks the grid to see if each row,
    column, and the 3x3 subgrids contain the digit 'n'.
    It returns False if it is not 'safe' (a duplicate digit
    is found) else returns True if it is 'safe'
    """"""
    for i in range(9):
        if n in {grid[row][i], grid[i][column]}:
            return False

    for i in range(3):
        for j in range(3):
            if grid[(row - row % 3) + i][(column - column % 3) + j] == n:
                return False

    return True


def find_empty_location(grid: Matrix) -> tuple[int, int] | None:
    """"""
    This function finds an empty location so that we can assign a number
    for that particular row and column.
    """"""
    for i in range(9):
        for j in range(9):
            if grid[i][j] == 0:
                return i, j
    return None


def sudoku(grid: Matrix) -> Matrix | None:
    """"""
    Takes a partially filled-in grid and attempts to assign values to
    all unassigned locations in such a way to meet the requirements
    for Sudoku solution (non-duplication across rows, columns, and boxes)

    >>> sudoku(initial_grid)  # doctest: +NORMALIZE_WHITESPACE
    [[3, 1, 6, 5, 7, 8, 4, 9, 2],
     [5, 2, 9, 1, 3, 4, 7, 6, 8],
     [4, 8, 7, 6, 2, 9, 5, 3, 1],
     [2, 6, 3, 4, 1, 5, 9, 8, 7],
     [9, 7, 4, 8, 6, 3, 1, 2, 5],
     [8, 5, 1, 7, 9, 2, 6, 4, 3],
     [1, 3, 8, 9, 4, 7, 2, 5, 6],
     [6, 9, 2, 3, 5, 1, 8, 7, 4],
     [7, 4, 5, 2, 8, 6, 3, 1, 9]]
     >>> sudoku(no_solution) is None
     True
    """"""
    if location := find_empty_location(grid):
        row, column = location
    else:
        # If the location is ``None``, then the grid is solved.
        return grid

    for digit in range(1, 10):
        if is_safe(grid, row, column, digit):
            grid[row][column] = digit

            if sudoku(grid) is not None:
                return grid

            grid[row][column] = 0

    return None


def print_solution(grid: Matrix) -> None:
    """"""
    A function to print the solution in the form
    of a 9x9 grid
    """"""
    for row in grid:
        for cell in row:
            print(cell, end="" "")
        print()


if __name__ == ""__main__"":
    # make a copy of grid so that you can compare with the unmodified grid
    for example_grid in (initial_grid, no_solution):
        print(""\nExample grid:\n"" + ""="" * 20)
        print_solution(example_grid)
        print(""\nExample grid solution:"")
        solution = sudoku(example_grid)
        if solution is not None:
            print_solution(solution)
        else:
            print(""Cannot find a solution."")
","""""""
Given a partially filled 9x9 2D array, the objective is to fill a 9x9
square grid with digits numbered 1 to 9, so that every row, column, and
and each of the nine 3x3 sub-grids contains all of the digits.

This can be solved using Backtracking and is similar to n-queens.
We check to see if a cell is safe or not and recursively call the
function on the next column to see if it returns True. if yes, we
have solved the puzzle. else, we backtrack and place another number
in that cell and repeat this process.
""""""

from __future__ import annotations

Matrix = list[list[int]]

# assigning initial values to the grid
initial_grid: Matrix = [
    [3, 0, 6, 5, 0, 8, 4, 0, 0],
    [5, 2, 0, 0, 0, 0, 0, 0, 0],
    [0, 8, 7, 0, 0, 0, 0, 3, 1],
    [0, 0, 3, 0, 1, 0, 0, 8, 0],
    [9, 0, 0, 8, 6, 3, 0, 0, 5],
    [0, 5, 0, 0, 9, 0, 6, 0, 0],
    [1, 3, 0, 0, 0, 0, 2, 5, 0],
    [0, 0, 0, 0, 0, 0, 0, 7, 4],
    [0, 0, 5, 2, 0, 6, 3, 0, 0],
]

# a grid with no solution
no_solution: Matrix = [
    [5, 0, 6, 5, 0, 8, 4, 0, 3],
    [5, 2, 0, 0, 0, 0, 0, 0, 2],
    [1, 8, 7, 0, 0, 0, 0, 3, 1],
    [0, 0, 3, 0, 1, 0, 0, 8, 0],
    [9, 0, 0, 8, 6, 3, 0, 0, 5],
    [0, 5, 0, 0, 9, 0, 6, 0, 0],
    [1, 3, 0, 0, 0, 0, 2, 5, 0],
    [0, 0, 0, 0, 0, 0, 0, 7, 4],
    [0, 0, 5, 2, 0, 6, 3, 0, 0],
]

def is_safe(grid: Matrix, row: int, column: int, n: int) -> bool:
    """"""
    This function checks the grid to see if each row,
    column, and the 3x3 subgrids contain the digit 'n'.
    It returns False if it is not 'safe' (a duplicate digit
    is found) else returns True if it is 'safe'
    """"""
    for i in range(9):
        if n in {grid[row][i], grid[i][column]}:
            return False
    for i in range(3):
        for j in range(3):
            if grid[(row - row % 3) + i][(column - column % 3) + j] == n:
                return False
    return True

def find_empty_location(grid: Matrix) -> tuple[int, int] | None:
    """"""
    This function finds an empty location so that we can assign a number
    for that particular row and column.
    """"""
    for i in range(9):
        for j in range(9):
            if grid[i][j] == 0:
                return i, j
    return None

def sudoku(grid: Matrix) -> Matrix | None:
    """"""
    Takes a partially filled-in grid and attempts to assign values to
    all unassigned locations in such a way to meet the requirements
    for Sudoku solution (non-duplication across rows, columns, and boxes)
    """"""
    if location := find_empty_location(grid):
        row, column = location
    else:
        # If the location is ``None``, then the grid is solved.
        return grid

    for digit in range(1, 10):
        if is_safe(grid, row, column, digit):
            grid[row][column] = digit
            if sudoku(grid) is not None:
                return grid
            grid[row][column] = 0
    return None

def print_solution(grid: Matrix) -> None:
    """"""
    A function to print the solution in the form
    of a 9x9 grid
    """"""
    for row in grid:
        for cell in row:
            print(cell, end="" "")
        print()

if __name__ == ""__main__"":
    # make a copy of grid so that you can compare with the unmodified grid
    for example_grid in (initial_grid, no_solution):
        print(""\nExample grid:\n"" + ""="" * 20)
        print_solution(example_grid)
        print(""\nExample grid solution:"")
        solution = sudoku(example_grid)
        if solution is not None:
            print_solution(solution)
        else:
            print(""Cannot find a solution."")","{'final_score': 94.48, 'length_similarity': 89.43, 'token_similarity': 94.09, 'structure_similarity': 89.19, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
72,"import json
from enum import Enum
from typing import Any

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class HttpMethod(Enum):
    GET = ""GET""
    POST = ""POST""
    PUT = ""PUT""
    DELETE = ""DELETE""
    PATCH = ""PATCH""
    OPTIONS = ""OPTIONS""
    HEAD = ""HEAD""


class SendWebRequestBlock(Block):
    class Input(BlockSchema):
        url: str = SchemaField(
            description=""The URL to send the request to"",
            placeholder=""https://api.example.com"",
        )
        method: HttpMethod = SchemaField(
            description=""The HTTP method to use for the request"",
            default=HttpMethod.POST,
        )
        headers: dict[str, str] = SchemaField(
            description=""The headers to include in the request"",
            default={},
        )
        json_format: bool = SchemaField(
            title=""JSON format"",
            description=""Whether to send and receive body as JSON"",
            default=True,
        )
        body: Any = SchemaField(
            description=""The body of the request"",
            default=None,
        )

    class Output(BlockSchema):
        response: object = SchemaField(description=""The response from the server"")
        client_error: object = SchemaField(description=""The error on 4xx status codes"")
        server_error: object = SchemaField(description=""The error on 5xx status codes"")

    def __init__(self):
        super().__init__(
            id=""6595ae1f-b924-42cb-9a41-551a0611c4b4"",
            description=""This block makes an HTTP request to the given URL."",
            categories={BlockCategory.OUTPUT},
            input_schema=SendWebRequestBlock.Input,
            output_schema=SendWebRequestBlock.Output,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        if isinstance(input_data.body, str):
            input_data.body = json.loads(input_data.body)

        response = requests.request(
            input_data.method.value,
            input_data.url,
            headers=input_data.headers,
            json=input_data.body if input_data.json_format else None,
            data=input_data.body if not input_data.json_format else None,
        )
        result = response.json() if input_data.json_format else response.text

        if response.status_code // 100 == 2:
            yield ""response"", result
        elif response.status_code // 100 == 4:
            yield ""client_error"", result
        elif response.status_code // 100 == 5:
            yield ""server_error"", result
        else:
            raise ValueError(f""Unexpected status code: {response.status_code}"")
","import json
from enum import Enum
from typing import Any

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests

class HttpMethod(Enum):
    GET = ""GET""
    POST = ""POST""
    PUT = ""PUT""
    DELETE = ""DELETE""
    PATCH = ""PATCH""
    OPTIONS = ""OPTIONS""
    HEAD = ""HEAD""

class SendWebRequestBlock(Block):
    class Input(BlockSchema):
        url: str = SchemaField(
            description=""The URL to send the request to"",
            placeholder=""https://api.example.com"",
        )
        method: HttpMethod = SchemaField(
            description=""The HTTP method to use for the request"",
            default=HttpMethod.POST,
        )
        headers: dict[str, str] = SchemaField(
            description=""The headers to include in the request"",
            default={},
        )
        json_format: bool = SchemaField(
            title=""JSON format"",
            description=""Whether to send and receive body as JSON"",
            default=True,
        )
        body: Any = SchemaField(
            description=""The body of the request"",
            default=None,
        )

    class Output(BlockSchema):
        response: object = SchemaField(description=""The response from the server"")
        client_error: object = SchemaField(description=""The error on 4xx status codes"")
        server_error: object = SchemaField(description=""The error on 5xx status codes"")

    def __init__(self):
        super().__init__(
            id=""6595ae1f-b924-42cb-9a41-551a0611c4b4"",
            description=""This block makes an HTTP request to the given URL."",
            categories={BlockCategory.OUTPUT},
            input_schema=SendWebRequestBlock.Input,
            output_schema=SendWebRequestBlock.Output,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        if isinstance(input_data.body, str):
            input_data.body = json.loads(input_data.body)
        
        response = requests.request(
            input_data.method.value,
            input_data.url,
            headers=input_data.headers,
            json=input_data.body if input_data.json_format else None,
            data=input_data.body if not input_data.json_format else None,
        )
        
        result = response.json() if input_data.json_format else response.text
        
        if response.status_code // 100 == 2:
            yield ""response"", result
        elif response.status_code // 100 == 4:
            yield ""client_error"", result
        elif response.status_code // 100 == 5:
            yield ""server_error"", result
        else:
            raise ValueError(f""Unexpected status code: {response.status_code}"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
73,"import io
import logging
from enum import Enum

import replicate
import replicate.exceptions
import requests
from replicate.helpers import FileOutput

from backend.data.graph import Graph
from backend.util.settings import Settings

logger = logging.getLogger(__name__)


class ImageSize(str, Enum):
    LANDSCAPE = ""1024x768""


class ImageStyle(str, Enum):
    DIGITAL_ART = ""digital art""


async def generate_agent_image(agent: Graph) -> io.BytesIO:
    """"""
    Generate an image for an agent using Flux model via Replicate API.

    Args:
        agent (Graph): The agent to generate an image for

    Returns:
        io.BytesIO: The generated image as bytes
    """"""
    try:
        settings = Settings()

        if not settings.secrets.replicate_api_key:
            raise ValueError(""Missing Replicate API key in settings"")

        # Construct prompt from agent details
        prompt = f""Create a visually engaging app store thumbnail for the AI agent that highlights what it does in a clear and captivating way:\n- **Name**: {agent.name}\n- **Description**: {agent.description}\nFocus on showcasing its core functionality with an appealing design.""

        # Set up Replicate client
        client = replicate.Client(api_token=settings.secrets.replicate_api_key)

        # Model parameters
        input_data = {
            ""prompt"": prompt,
            ""width"": 1024,
            ""height"": 768,
            ""aspect_ratio"": ""4:3"",
            ""output_format"": ""jpg"",
            ""output_quality"": 90,
            ""num_inference_steps"": 30,
            ""guidance"": 3.5,
            ""negative_prompt"": ""blurry, low quality, distorted, deformed"",
            ""disable_safety_checker"": True,
        }

        try:
            # Run model
            output = client.run(""black-forest-labs/flux-1.1-pro"", input=input_data)

            # Depending on the model output, extract the image URL or bytes
            # If the output is a list of FileOutput or URLs
            if isinstance(output, list) and output:
                if isinstance(output[0], FileOutput):
                    image_bytes = output[0].read()
                else:
                    # If it's a URL string, fetch the image bytes
                    result_url = output[0]
                    response = requests.get(result_url)
                    response.raise_for_status()
                    image_bytes = response.content
            elif isinstance(output, FileOutput):
                image_bytes = output.read()
            elif isinstance(output, str):
                # Output is a URL
                response = requests.get(output)
                response.raise_for_status()
                image_bytes = response.content
            else:
                raise RuntimeError(""Unexpected output format from the model."")

            return io.BytesIO(image_bytes)

        except replicate.exceptions.ReplicateError as e:
            if e.status == 401:
                raise RuntimeError(""Invalid Replicate API token"") from e
            raise RuntimeError(f""Replicate API error: {str(e)}"") from e

    except Exception as e:
        logger.exception(""Failed to generate agent image"")
        raise RuntimeError(f""Image generation failed: {str(e)}"")
","import io
import logging
from enum import Enum

import replicate
import replicate.exceptions
import requests
from replicate.helpers import FileOutput

from backend.data.graph import Graph
from backend.util.settings import Settings

logger = logging.getLogger(__name__)


class ImageSize(str, Enum):
    LANDSCAPE = ""1024x768""


class ImageStyle(str, Enum):
    DIGITAL_ART = ""digital art""


async def generate_agent_image(agent: Graph) -> io.BytesIO:
    """"""
    Generate an image for an agent using Flux model via Replicate API.

    Args:
        agent (Graph): The agent to generate an image for

    Returns:
        io.BytesIO: The generated image as bytes
    """"""
    try:
        settings = Settings()

        if not settings.secrets.replicate_api_key:
            raise ValueError(""Missing Replicate API key in settings"")

        # Construct prompt from agent details
        prompt = f""Create a visually engaging app store thumbnail for the AI agent that highlights what it does in a clear and captivating way:\n- **Name**: {agent.name}\n- **Description**: {agent.description}\nFocus on showcasing its core functionality with an appealing design.""

        # Set up Replicate client
        client = replicate.Client(api_token=settings.secrets.replicate_api_key)

        # Model parameters
        input_data = {
            ""prompt"": prompt,
            ""width"": 1024,
            ""height"": 768,
            ""aspect_ratio"": ""4:3"",
            ""output_format"": ""jpg"",
            ""output_quality"": 90,
            ""num_inference_steps"": 30,
            ""guidance"": 3.5,
            ""negative_prompt"": ""blurry, low quality, distorted, deformed"",
            ""disable_safety_checker"": True,
        }

        try:
            # Run model
            output = client.run(""black-forest-labs/flux-1.1-pro"", input=input_data)

            # Depending on the model output, extract the image URL or bytes
            # If the output is a list of FileOutput or URLs
            if isinstance(output, list) and output:
                if isinstance(output[0], FileOutput):
                    image_bytes = output[0].read()
                else:
                    # If it's a URL string, fetch the image bytes
                    result_url = output[0]
                    response = requests.get(result_url)
                    response.raise_for_status()
                    image_bytes = response.content
            elif isinstance(output, FileOutput):
                image_bytes = output.read()
            elif isinstance(output, str):
                # Output is a URL
                response = requests.get(output)
                response.raise_for_status()
                image_bytes = response.content
            else:
                raise RuntimeError(""Unexpected output format from the model."")

            return io.BytesIO(image_bytes)

        except replicate.exceptions.ReplicateError as e:
            if e.status == 401:
                raise RuntimeError(""Invalid Replicate API token"") from e
            raise RuntimeError(f""Replicate API error: {str(e)}"") from e

    except Exception as e:
        logger.exception(""Failed to generate agent image"")
        raise RuntimeError(f""Image generation failed: {str(e)}"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
74,"import logging

from google.auth.external_account_authorized_user import (
    Credentials as ExternalAccountCredentials,
)
from google.auth.transport.requests import AuthorizedSession, Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow
from pydantic import SecretStr

from backend.data.model import OAuth2Credentials
from backend.integrations.providers import ProviderName

from .base import BaseOAuthHandler

logger = logging.getLogger(__name__)


# --8<-- [start:GoogleOAuthHandlerExample]
class GoogleOAuthHandler(BaseOAuthHandler):
    """"""
    Based on the documentation at https://developers.google.com/identity/protocols/oauth2/web-server
    """"""  # noqa

    PROVIDER_NAME = ProviderName.GOOGLE
    EMAIL_ENDPOINT = ""https://www.googleapis.com/oauth2/v2/userinfo""
    DEFAULT_SCOPES = [
        ""https://www.googleapis.com/auth/userinfo.email"",
        ""https://www.googleapis.com/auth/userinfo.profile"",
        ""openid"",
    ]
    # --8<-- [end:GoogleOAuthHandlerExample]

    def __init__(self, client_id: str, client_secret: str, redirect_uri: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.token_uri = ""https://oauth2.googleapis.com/token""
        self.revoke_uri = ""https://oauth2.googleapis.com/revoke""

    def get_login_url(self, scopes: list[str], state: str) -> str:
        all_scopes = list(set(scopes + self.DEFAULT_SCOPES))
        logger.debug(f""Setting up OAuth flow with scopes: {all_scopes}"")
        flow = self._setup_oauth_flow(all_scopes)
        flow.redirect_uri = self.redirect_uri
        authorization_url, _ = flow.authorization_url(
            access_type=""offline"",
            include_granted_scopes=""true"",
            state=state,
            prompt=""consent"",
        )
        return authorization_url

    def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        logger.debug(f""Exchanging code for tokens with scopes: {scopes}"")

        # Use the scopes from the initial request
        flow = self._setup_oauth_flow(scopes)
        flow.redirect_uri = self.redirect_uri

        logger.debug(""Fetching token from Google"")

        # Disable scope check in fetch_token
        flow.oauth2session.scope = None
        token = flow.fetch_token(code=code)
        logger.debug(""Token fetched successfully"")

        # Get the actual scopes granted by Google
        granted_scopes: list[str] = token.get(""scope"", [])

        logger.debug(f""Scopes granted by Google: {granted_scopes}"")

        google_creds = flow.credentials
        logger.debug(f""Received credentials: {google_creds}"")

        logger.debug(""Requesting user email"")
        username = self._request_email(google_creds)
        logger.debug(f""User email retrieved: {username}"")

        assert google_creds.token
        assert google_creds.refresh_token
        assert google_creds.expiry
        assert granted_scopes

        # Create OAuth2Credentials with the granted scopes
        credentials = OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            title=None,
            username=username,
            access_token=SecretStr(google_creds.token),
            refresh_token=(SecretStr(google_creds.refresh_token)),
            access_token_expires_at=(
                int(google_creds.expiry.timestamp()) if google_creds.expiry else None
            ),
            refresh_token_expires_at=None,
            scopes=granted_scopes,
        )
        logger.debug(
            f""OAuth2Credentials object created successfully with scopes: {credentials.scopes}""
        )

        return credentials

    def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        session = AuthorizedSession(credentials)
        session.post(
            self.revoke_uri,
            params={""token"": credentials.access_token.get_secret_value()},
            headers={""content-type"": ""application/x-www-form-urlencoded""},
        )
        return True

    def _request_email(
        self, creds: Credentials | ExternalAccountCredentials
    ) -> str | None:
        session = AuthorizedSession(creds)
        response = session.get(self.EMAIL_ENDPOINT)
        if not response.ok:
            logger.error(
                f""Failed to get user email. Status code: {response.status_code}""
            )
            return None
        return response.json()[""email""]

    def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # Google credentials should ALWAYS have a refresh token
        assert credentials.refresh_token

        google_creds = Credentials(
            token=credentials.access_token.get_secret_value(),
            refresh_token=credentials.refresh_token.get_secret_value(),
            token_uri=self.token_uri,
            client_id=self.client_id,
            client_secret=self.client_secret,
            scopes=credentials.scopes,
        )
        # Google's OAuth library is poorly typed so we need some of these:
        assert google_creds.refresh_token
        assert google_creds.scopes

        google_creds.refresh(Request())
        assert google_creds.expiry

        return OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            id=credentials.id,
            title=credentials.title,
            username=credentials.username,
            access_token=SecretStr(google_creds.token),
            refresh_token=SecretStr(google_creds.refresh_token),
            access_token_expires_at=int(google_creds.expiry.timestamp()),
            refresh_token_expires_at=None,
            scopes=google_creds.scopes,
        )

    def _setup_oauth_flow(self, scopes: list[str]) -> Flow:
        return Flow.from_client_config(
            {
                ""web"": {
                    ""client_id"": self.client_id,
                    ""client_secret"": self.client_secret,
                    ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",
                    ""token_uri"": self.token_uri,
                }
            },
            scopes=scopes,
        )
","import logging
from google.auth.external_account_authorized_user import (
    Credentials as ExternalAccountCredentials,
)
from google.auth.transport.requests import AuthorizedSession, Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow
from pydantic import SecretStr
from backend.data.model import OAuth2Credentials
from backend.integrations.providers import ProviderName
from .base import BaseOAuthHandler

logger = logging.getLogger(__name__)

class GoogleOAuthHandler(BaseOAuthHandler):
    """"""
    Based on the documentation at https://developers.google.com/identity/protocols/oauth2/web-server
    """"""  # noqa

    PROVIDER_NAME = ProviderName.GOOGLE
    EMAIL_ENDPOINT = ""https://www.googleapis.com/oauth2/v2/userinfo""
    DEFAULT_SCOPES = [
        ""https://www.googleapis.com/auth/userinfo.email"",
        ""https://www.googleapis.com/auth/userinfo.profile"",
        ""openid"",
    ]

    def __init__(self, client_id: str, client_secret: str, redirect_uri: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.token_uri = ""https://oauth2.googleapis.com/token""
        self.revoke_uri = ""https://oauth2.googleapis.com/revoke""

    def get_login_url(self, scopes: list[str], state: str) -> str:
        all_scopes = list(set(scopes + self.DEFAULT_SCOPES))
        logger.debug(f""Setting up OAuth flow with scopes: {all_scopes}"")
        flow = self._setup_oauth_flow(all_scopes)
        flow.redirect_uri = self.redirect_uri
        authorization_url, _ = flow.authorization_url(
            access_type=""offline"",
            include_granted_scopes=""true"",
            state=state,
            prompt=""consent"",
        )
        return authorization_url

    def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        logger.debug(f""Exchanging code for tokens with scopes: {scopes}"")
        # Use the scopes from the initial request
        flow = self._setup_oauth_flow(scopes)
        flow.redirect_uri = self.redirect_uri
        logger.debug(""Fetching token from Google"")
        # Disable scope check in fetch_token
        flow.oauth2session.scope = None
        token = flow.fetch_token(code=code)
        logger.debug(""Token fetched successfully"")
        # Get the actual scopes granted by Google
        granted_scopes: list[str] = token.get(""scope"", [])
        logger.debug(f""Scopes granted by Google: {granted_scopes}"")
        google_creds = flow.credentials
        logger.debug(f""Received credentials: {google_creds}"")
        logger.debug(""Requesting user email"")
        username = self._request_email(google_creds)
        logger.debug(f""User email retrieved: {username}"")
        assert google_creds.token
        assert google_creds.refresh_token
        assert google_creds.expiry
        assert granted_scopes
        # Create OAuth2Credentials with the granted scopes
        credentials = OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            title=None,
            username=username,
            access_token=SecretStr(google_creds.token),
            refresh_token=(SecretStr(google_creds.refresh_token)),
            access_token_expires_at=(
                int(google_creds.expiry.timestamp()) if google_creds.expiry else None
            ),
            refresh_token_expires_at=None,
            scopes=granted_scopes,
        )
        logger.debug(
            f""OAuth2Credentials object created successfully with scopes: {credentials.scopes}""
        )
        return credentials

    def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        session = AuthorizedSession(credentials)
        session.post(
            self.revoke_uri,
            params={""token"": credentials.access_token.get_secret_value()},
            headers={""content-type"": ""application/x-www-form-urlencoded""},
        )
        return True

    def _request_email(
        self, creds: Credentials | ExternalAccountCredentials
    ) -> str | None:
        session = AuthorizedSession(creds)
        response = session.get(self.EMAIL_ENDPOINT)
        if not response.ok:
            logger.error(
                f""Failed to get user email. Status code: {response.status_code}""
            )
            return None
        return response.json()[""email""]

    def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # Google credentials should ALWAYS have a refresh token
        assert credentials.refresh_token
        google_creds = Credentials(
            token=credentials.access_token.get_secret_value(),
            refresh_token=credentials.refresh_token.get_secret_value(),
            token_uri=self.token_uri,
            client_id=self.client_id,
            client_secret=self.client_secret,
            scopes=credentials.scopes,
        )
        # Google's OAuth library is poorly typed so we need some of these:
        assert google_creds.refresh_token
        assert google_creds.scopes
        google_creds.refresh(Request())
        assert google_creds.expiry
        return OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            id=credentials.id,
            title=credentials.title,
            username=credentials.username,
            access_token=SecretStr(google_creds.token),
            refresh_token=SecretStr(google_creds.refresh_token),
            access_token_expires_at=int(google_creds.expiry.timestamp()),
            refresh_token_expires_at=None,
            scopes=google_creds.scopes,
        )

    def _setup_oauth_flow(self, scopes: list[str]) -> Flow:
        return Flow.from_client_config(
            {
                ""web"": {
                    ""client_id"": self.client_id,
                    ""client_secret"": self.client_secret,
                    ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",
                    ""token_uri"": self.token_uri,
                }
            },
            scopes=scopes,
        )","{'final_score': 99.13, 'length_similarity': 98.5, 'token_similarity': 98.8, 'structure_similarity': 98.59, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
75,"import pytest

from .utils import remove_color_codes


@pytest.mark.parametrize(
    ""raw_text, clean_text"",
    [
        (
            ""COMMAND = \x1b[36mbrowse_website\x1b[0m  ""
            ""ARGUMENTS = \x1b[36m{'url': 'https://www.google.com',""
            "" 'question': 'What is the capital of France?'}\x1b[0m"",
            ""COMMAND = browse_website  ""
            ""ARGUMENTS = {'url': 'https://www.google.com',""
            "" 'question': 'What is the capital of France?'}"",
        ),
        (
            ""{'Schaue dir meine Projekte auf github () an, als auch meine Webseiten': ""
            ""'https://github.com/Significant-Gravitas/AutoGPT,""
            "" https://discord.gg/autogpt und https://twitter.com/Auto_GPT'}"",
            ""{'Schaue dir meine Projekte auf github () an, als auch meine Webseiten': ""
            ""'https://github.com/Significant-Gravitas/AutoGPT,""
            "" https://discord.gg/autogpt und https://twitter.com/Auto_GPT'}"",
        ),
        ("""", """"),
        (""hello"", ""hello""),
        (""hello\x1b[31m world"", ""hello world""),
        (""\x1b[36mHello,\x1b[32m World!"", ""Hello, World!""),
        (
            ""\x1b[1m\x1b[31mError:\x1b[0m\x1b[31m file not found"",
            ""Error: file not found"",
        ),
    ],
)
def test_remove_color_codes(raw_text, clean_text):
    assert remove_color_codes(raw_text) == clean_text
","import pytest
from .utils import remove_color_codes

@pytest.mark.parametrize(
    ""raw_text, clean_text"",
    [
        (
            ""COMMAND = \x1b[36mbrowse_website\x1b[0m  ""
            ""ARGUMENTS = \x1b[36m{'url': 'https://www.google.com',""
            "" 'question': 'What is the capital of France?'}\x1b[0m"",
            ""COMMAND = browse_website  ""
            ""ARGUMENTS = {'url': 'https://www.google.com',""
            "" 'question': 'What is the capital of France?'}"",
        ),
        (
            ""{'Schaue dir meine Projekte auf github () an, als auch meine Webseiten': ""
            ""'https://github.com/Significant-Gravitas/AutoGPT,""
            "" https://discord.gg/autogpt und https://twitter.com/Auto_GPT'}"",
            ""{'Schaue dir meine Projekte auf github () an, als auch meine Webseiten': ""
            ""'https://github.com/Significant-Gravitas/AutoGPT,""
            "" https://discord.gg/autogpt und https://twitter.com/Auto_GPT'}"",
        ),
        ("""", """"),
        (""hello"", ""hello""),
        (""hello\x1b[31m world"", ""hello world""),
        (""\x1b[36mHello,\x1b[32m World!"", ""Hello, World!""),
        (
            ""\x1b[1m\x1b[31mError:\x1b[0m\x1b[31m file not found"",
            ""Error: file not found"",
        ),
    ],
)
def test_remove_color_codes(raw_text, clean_text):
    assert remove_color_codes(raw_text) == clean_text","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
76,"from enum import Enum


# --8<-- [start:ProviderName]
class ProviderName(str, Enum):
    ANTHROPIC = ""anthropic""
    COMPASS = ""compass""
    DISCORD = ""discord""
    D_ID = ""d_id""
    E2B = ""e2b""
    EXA = ""exa""
    FAL = ""fal""
    GITHUB = ""github""
    GOOGLE = ""google""
    GOOGLE_MAPS = ""google_maps""
    GROQ = ""groq""
    HUBSPOT = ""hubspot""
    IDEOGRAM = ""ideogram""
    JINA = ""jina""
    MEDIUM = ""medium""
    NOTION = ""notion""
    OLLAMA = ""ollama""
    OPENAI = ""openai""
    OPENWEATHERMAP = ""openweathermap""
    OPEN_ROUTER = ""open_router""
    PINECONE = ""pinecone""
    REPLICATE = ""replicate""
    REVID = ""revid""
    SLANT3D = ""slant3d""
    UNREAL_SPEECH = ""unreal_speech""
    # --8<-- [end:ProviderName]
","from enum import Enum
# --8<-- [start:ProviderName]
class ProviderName(str, Enum):
    ANTHROPIC = ""anthropic""
    COMPASS = ""compass""
    DISCORD = ""discord""
    D_ID = ""d_id""
    E2B = ""e2b""
    EXA = ""exa""
    FAL = ""fal""
    GITHUB = ""github""
    GOOGLE = ""google""
    GOOGLE_MAPS = ""google_maps""
    GROQ = ""groq""
    HUBSPOT = ""hubspot""
    IDEOGRAM = ""ideogram""
    JINA = ""jina""
    MEDIUM = ""medium""
    NOTION = ""notion""
    OLLAMA = ""ollama""
    OPENAI = ""openai""
    OPENWEATHERMAP = ""openweathermap""
    OPEN_ROUTER = ""open_router""
    PINECONE = ""pinecone""
    REPLICATE = ""replicate""
    REVID = ""revid""
    SLANT3D = ""slant3d""
    UNREAL_SPEECH = ""unreal_speech""
    # --8<-- [end:ProviderName]

","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
77,"from math import cos, sin, sqrt, tau

from audio_filters.iir_filter import IIRFilter

""""""
Create 2nd-order IIR filters with Butterworth design.

Code based on https://webaudio.github.io/Audio-EQ-Cookbook/audio-eq-cookbook.html
Alternatively you can use scipy.signal.butter, which should yield the same results.
""""""


def make_lowpass(
    frequency: int,
    samplerate: int,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a low-pass filter

    >>> filter = make_lowpass(1000, 48000)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [1.0922959556412573, -1.9828897227476208, 0.9077040443587427, 0.004277569313094809,
     0.008555138626189618, 0.004277569313094809]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)

    b0 = (1 - _cos) / 2
    b1 = 1 - _cos

    a0 = 1 + alpha
    a1 = -2 * _cos
    a2 = 1 - alpha

    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b0])
    return filt


def make_highpass(
    frequency: int,
    samplerate: int,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a high-pass filter

    >>> filter = make_highpass(1000, 48000)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [1.0922959556412573, -1.9828897227476208, 0.9077040443587427, 0.9957224306869052,
     -1.9914448613738105, 0.9957224306869052]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)

    b0 = (1 + _cos) / 2
    b1 = -1 - _cos

    a0 = 1 + alpha
    a1 = -2 * _cos
    a2 = 1 - alpha

    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b0])
    return filt


def make_bandpass(
    frequency: int,
    samplerate: int,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a band-pass filter

    >>> filter = make_bandpass(1000, 48000)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [1.0922959556412573, -1.9828897227476208, 0.9077040443587427, 0.06526309611002579,
     0, -0.06526309611002579]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)

    b0 = _sin / 2
    b1 = 0
    b2 = -b0

    a0 = 1 + alpha
    a1 = -2 * _cos
    a2 = 1 - alpha

    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])
    return filt


def make_allpass(
    frequency: int,
    samplerate: int,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates an all-pass filter

    >>> filter = make_allpass(1000, 48000)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [1.0922959556412573, -1.9828897227476208, 0.9077040443587427, 0.9077040443587427,
     -1.9828897227476208, 1.0922959556412573]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)

    b0 = 1 - alpha
    b1 = -2 * _cos
    b2 = 1 + alpha

    filt = IIRFilter(2)
    filt.set_coefficients([b2, b1, b0], [b0, b1, b2])
    return filt


def make_peak(
    frequency: int,
    samplerate: int,
    gain_db: float,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a peak filter

    >>> filter = make_peak(1000, 48000, 6)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [1.0653405327119334, -1.9828897227476208, 0.9346594672880666, 1.1303715025601122,
     -1.9828897227476208, 0.8696284974398878]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)
    big_a = 10 ** (gain_db / 40)

    b0 = 1 + alpha * big_a
    b1 = -2 * _cos
    b2 = 1 - alpha * big_a
    a0 = 1 + alpha / big_a
    a1 = -2 * _cos
    a2 = 1 - alpha / big_a

    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])
    return filt


def make_lowshelf(
    frequency: int,
    samplerate: int,
    gain_db: float,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a low-shelf filter

    >>> filter = make_lowshelf(1000, 48000, 6)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [3.0409336710888786, -5.608870992220748, 2.602157875636628, 3.139954022810743,
     -5.591841778072785, 2.5201667380627257]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)
    big_a = 10 ** (gain_db / 40)
    pmc = (big_a + 1) - (big_a - 1) * _cos
    ppmc = (big_a + 1) + (big_a - 1) * _cos
    mpc = (big_a - 1) - (big_a + 1) * _cos
    pmpc = (big_a - 1) + (big_a + 1) * _cos
    aa2 = 2 * sqrt(big_a) * alpha

    b0 = big_a * (pmc + aa2)
    b1 = 2 * big_a * mpc
    b2 = big_a * (pmc - aa2)
    a0 = ppmc + aa2
    a1 = -2 * pmpc
    a2 = ppmc - aa2

    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])
    return filt


def make_highshelf(
    frequency: int,
    samplerate: int,
    gain_db: float,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a high-shelf filter

    >>> filter = make_highshelf(1000, 48000, 6)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [2.2229172136088806, -3.9587208137297303, 1.7841414181566304, 4.295432981120543,
     -7.922740859457287, 3.6756456963725253]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)
    big_a = 10 ** (gain_db / 40)
    pmc = (big_a + 1) - (big_a - 1) * _cos
    ppmc = (big_a + 1) + (big_a - 1) * _cos
    mpc = (big_a - 1) - (big_a + 1) * _cos
    pmpc = (big_a - 1) + (big_a + 1) * _cos
    aa2 = 2 * sqrt(big_a) * alpha

    b0 = big_a * (ppmc + aa2)
    b1 = -2 * big_a * pmpc
    b2 = big_a * (ppmc - aa2)
    a0 = pmc + aa2
    a1 = 2 * mpc
    a2 = pmc - aa2

    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])
    return filt
","from math import cos, sin, sqrt, tau
from audio_filters.iir_filter import IIRFilter

""""""
Create 2nd-order IIR filters with Butterworth design.

Code based on https://webaudio.github.io/Audio-EQ-Cookbook/audio-eq-cookbook.html
Alternatively you can use scipy.signal.butter, which should yield the same results.
""""""


def make_lowpass(
    frequency: int,
    samplerate: int,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a low-pass filter

    >>> filter = make_lowpass(1000, 48000)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [1.0922959556412573, -1.9828897227476208, 0.9077040443587427, 0.004277569313094809,
     0.008555138626189618, 0.004277569313094809]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)
    b0 = (1 - _cos) / 2
    b1 = 1 - _cos
    a0 = 1 + alpha
    a1 = -2 * _cos
    a2 = 1 - alpha
    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b0])
    return filt


def make_highpass(
    frequency: int,
    samplerate: int,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a high-pass filter

    >>> filter = make_highpass(1000, 48000)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [1.0922959556412573, -1.9828897227476208, 0.9077040443587427, 0.9957224306869052,
     -1.9914448613738105, 0.9957224306869052]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)
    b0 = (1 + _cos) / 2
    b1 = -1 - _cos
    a0 = 1 + alpha
    a1 = -2 * _cos
    a2 = 1 - alpha
    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b0])
    return filt


def make_bandpass(
    frequency: int,
    samplerate: int,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a band-pass filter

    >>> filter = make_bandpass(1000, 48000)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [1.0922959556412573, -1.9828897227476208, 0.9077040443587427, 0.06526309611002579,
     0, -0.06526309611002579]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)
    b0 = _sin / 2
    b1 = 0
    b2 = -b0
    a0 = 1 + alpha
    a1 = -2 * _cos
    a2 = 1 - alpha
    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])
    return filt


def make_allpass(
    frequency: int,
    samplerate: int,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates an all-pass filter

    >>> filter = make_allpass(1000, 48000)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [1.0922959556412573, -1.9828897227476208, 0.9077040443587427, 0.9077040443587427,
     -1.9828897227476208, 1.0922959556412573]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)
    b0 = 1 - alpha
    b1 = -2 * _cos
    b2 = 1 + alpha
    filt = IIRFilter(2)
    filt.set_coefficients([b2, b1, b0], [b0, b1, b2])
    return filt


def make_peak(
    frequency: int,
    samplerate: int,
    gain_db: float,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a peak filter

    >>> filter = make_peak(1000, 48000, 6)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [1.0653405327119334, -1.9828897227476208, 0.9346594672880666, 1.1303715025601122,
     -1.9828897227476208, 0.8696284974398878]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)
    big_a = 10 ** (gain_db / 40)
    b0 = 1 + alpha * big_a
    b1 = -2 * _cos
    b2 = 1 - alpha * big_a
    a0 = 1 + alpha / big_a
    a1 = -2 * _cos
    a2 = 1 - alpha / big_a
    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])
    return filt


def make_lowshelf(
    frequency: int,
    samplerate: int,
    gain_db: float,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a low-shelf filter

    >>> filter = make_lowshelf(1000, 48000, 6)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [3.0409336710888786, -5.608870992220748, 2.602157875636628, 3.139954022810743,
     -5.591841778072785, 2.5201667380627257]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)
    big_a = 10 ** (gain_db / 40)
    pmc = (big_a + 1) - (big_a - 1) * _cos
    ppmc = (big_a + 1) + (big_a - 1) * _cos
    mpc = (big_a - 1) - (big_a + 1) * _cos
    pmpc = (big_a - 1) + (big_a + 1) * _cos
    aa2 = 2 * sqrt(big_a) * alpha

    b0 = big_a * (pmc + aa2)
    b1 = 2 * big_a * mpc
    b2 = big_a * (pmc - aa2)
    a0 = ppmc + aa2
    a1 = -2 * pmpc
    a2 = ppmc - aa2
    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])
    return filt


def make_highshelf(
    frequency: int,
    samplerate: int,
    gain_db: float,
    q_factor: float = 1 / sqrt(2),
) -> IIRFilter:
    """"""
    Creates a high-shelf filter

    >>> filter = make_highshelf(1000, 48000, 6)
    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE
    [2.2229172136088806, -3.9587208137297303, 1.7841414181566304, 4.295432981120543,
     -7.922740859457287, 3.6756456963725253]
    """"""
    w0 = tau * frequency / samplerate
    _sin = sin(w0)
    _cos = cos(w0)
    alpha = _sin / (2 * q_factor)
    big_a = 10 ** (gain_db / 40)
    pmc = (big_a + 1) - (big_a - 1) * _cos
    ppmc = (big_a + 1) + (big_a - 1) * _cos
    mpc = (big_a - 1) - (big_a + 1) * _cos
    pmpc = (big_a - 1) + (big_a + 1) * _cos
    aa2 = 2 * sqrt(big_a) * alpha

    b0 = big_a * (ppmc + aa2)
    b1 = -2 * big_a * pmpc
    b2 = big_a * (ppmc - aa2)
    a0 = pmc + aa2
    a1 = 2 * mpc
    a2 = pmc - aa2
    filt = IIRFilter(2)
    filt.set_coefficients([a0, a1, a2], [b0, b1, b2])
    return filt","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
78,"import os
import pathlib
import sys


def get_frontend_path() -> pathlib.Path:
    if getattr(sys, ""frozen"", False):
        # The application is frozen
        datadir = pathlib.Path(os.path.dirname(sys.executable)) / ""example_files""
    else:
        # The application is not frozen
        # Change this bit to match where you store your data files:
        filedir = os.path.dirname(__file__)
        datadir = pathlib.Path(filedir).parent.parent.parent / ""example_files""
    return pathlib.Path(datadir)


def get_data_path() -> pathlib.Path:
    if getattr(sys, ""frozen"", False):
        # The application is frozen
        datadir = os.path.dirname(sys.executable)
    else:
        # The application is not frozen
        # Change this bit to match where you store your data files:
        filedir = os.path.dirname(__file__)
        datadir = pathlib.Path(filedir).parent.parent
    return pathlib.Path(datadir)
","import os
import pathlib
import sys


def get_frontend_path() -> pathlib.Path:
    if getattr(sys, ""frozen"", False):
        # The application is frozen
        datadir = pathlib.Path(os.path.dirname(sys.executable)) / ""example_files""
    else:
        # The application is not frozen
        # Change this bit to match where you store your data files:
        filedir = os.path.dirname(__file__)
        datadir = pathlib.Path(filedir).parent.parent.parent / ""example_files""
    return pathlib.Path(datadir)


def get_data_path() -> pathlib.Path:
    if getattr(sys, ""frozen"", False):
        # The application is frozen
        datadir = os.path.dirname(sys.executable)
    else:
        # The application is not frozen
        # Change this bit to match where you store your data files:
        filedir = os.path.dirname(__file__)
        datadir = pathlib.Path(filedir).parent.parent
    return pathlib.Path(datadir)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
79,"# https://www.tutorialspoint.com/python3/bitwise_operators_example.htm


def binary_and(a: int, b: int) -> str:
    """"""
    Take in 2 integers, convert them to binary,
    return a binary number that is the
    result of a binary and operation on the integers provided.

    >>> binary_and(25, 32)
    '0b000000'
    >>> binary_and(37, 50)
    '0b100000'
    >>> binary_and(21, 30)
    '0b10100'
    >>> binary_and(58, 73)
    '0b0001000'
    >>> binary_and(0, 255)
    '0b00000000'
    >>> binary_and(256, 256)
    '0b100000000'
    >>> binary_and(0, -1)
    Traceback (most recent call last):
        ...
    ValueError: the value of both inputs must be positive
    >>> binary_and(0, 1.1)
    Traceback (most recent call last):
        ...
    ValueError: Unknown format code 'b' for object of type 'float'
    >>> binary_and(""0"", ""1"")
    Traceback (most recent call last):
        ...
    TypeError: '<' not supported between instances of 'str' and 'int'
    """"""
    if a < 0 or b < 0:
        raise ValueError(""the value of both inputs must be positive"")

    a_binary = format(a, ""b"")
    b_binary = format(b, ""b"")

    max_len = max(len(a_binary), len(b_binary))

    return ""0b"" + """".join(
        str(int(char_a == ""1"" and char_b == ""1""))
        for char_a, char_b in zip(a_binary.zfill(max_len), b_binary.zfill(max_len))
    )


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","def binary_and(a: int, b: int) -> str:
    """"""
    Take in 2 integers, convert them to binary,
    return a binary number that is the
    result of a binary and operation on the integers provided.

    >>> binary_and(25, 32)
    '0b000000'
    >>> binary_and(37, 50)
    '0b100000'
    >>> binary_and(21, 30)
    '0b10100'
    >>> binary_and(58, 73)
    '0b0001000'
    >>> binary_and(0, 255)
    '0b00000000'
    >>> binary_and(256, 256)
    '0b100000000'
    >>> binary_and(0, -1)
    Traceback (most recent call last):
        ...
    ValueError: the value of both inputs must be positive
    >>> binary_and(0, 1.1)
    Traceback (most recent call last):
        ...
    ValueError: Unknown format code 'b' for object of type 'float'
    >>> binary_and(""0"", ""1"")
    Traceback (most recent call last):
        ...
    TypeError: '<' not supported between instances of 'str' and 'int'
    """"""
    if a < 0 or b < 0:
        raise ValueError(""the value of both inputs must be positive"")
    
    a_binary = format(a, ""b"")
    b_binary = format(b, ""b"")
    
    max_len = max(len(a_binary), len(b_binary))
    
    return ""0b"" + """".join(
        str(int(char_a == ""1"" and char_b == ""1""))
        for char_a, char_b in zip(a_binary.zfill(max_len), b_binary.zfill(max_len))
    )

# https://www.tutorialspoint.com/python3/bitwise_operators_example.htm



if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 99.07, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 95.35, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
80,"import logging


class BelowLevelFilter(logging.Filter):
    """"""Filter for logging levels below a certain threshold.""""""

    def __init__(self, below_level: int):
        super().__init__()
        self.below_level = below_level

    def filter(self, record: logging.LogRecord):
        return record.levelno < self.below_level
","import logging

class BelowLevelFilter(logging.Filter):
    """"""Filter for logging levels below a certain threshold.""""""

    def __init__(self, below_level: int):
        super().__init__()
        self.below_level = below_level

    def filter(self, record: logging.LogRecord):
        return record.levelno < self.below_level","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
81,"import pytest

from backend.util.service import AppService, expose, get_service_client

TEST_SERVICE_PORT = 8765


class ServiceTest(AppService):
    def __init__(self):
        super().__init__()

    @classmethod
    def get_port(cls) -> int:
        return TEST_SERVICE_PORT

    @expose
    def add(self, a: int, b: int) -> int:
        return a + b

    @expose
    def subtract(self, a: int, b: int) -> int:
        return a - b

    @expose
    def fun_with_async(self, a: int, b: int) -> int:
        async def add_async(a: int, b: int) -> int:
            return a + b

        return self.run_and_wait(add_async(a, b))


@pytest.mark.asyncio(scope=""session"")
async def test_service_creation(server):
    with ServiceTest():
        client = get_service_client(ServiceTest)
        assert client.add(5, 3) == 8
        assert client.subtract(10, 4) == 6
        assert client.fun_with_async(5, 3) == 8
","import pytest
from backend.util.service import AppService, expose, get_service_client

TEST_SERVICE_PORT = 8765

class ServiceTest(AppService):
    def __init__(self):
        super().__init__()

    @classmethod
    def get_port(cls) -> int:
        return TEST_SERVICE_PORT

    @expose
    def add(self, a: int, b: int) -> int:
        return a + b

    @expose
    def subtract(self, a: int, b: int) -> int:
        return a - b

    @expose
    def fun_with_async(self, a: int, b: int) -> int:
        async def add_async(a: int, b: int) -> int:
            return a + b

        return self.run_and_wait(add_async(a, b))


@pytest.mark.asyncio(scope=""session"")
async def test_service_creation(server):
    with ServiceTest():
        client = get_service_client(ServiceTest)
        assert client.add(5, 3) == 8
        assert client.subtract(10, 4) == 6
        assert client.fun_with_async(5, 3) == 8","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
82,"def gray_code(bit_count: int) -> list:
    """"""
    Takes in an integer n and returns a n-bit
    gray code sequence
    An n-bit gray code sequence is a sequence of 2^n
    integers where:

    a) Every integer is between [0,2^n -1] inclusive
    b) The sequence begins with 0
    c) An integer appears at most one times in the sequence
    d)The binary representation of every pair of integers differ
       by exactly one bit
    e) The binary representation of first and last bit also
       differ by exactly one bit

    >>> gray_code(2)
    [0, 1, 3, 2]

    >>> gray_code(1)
    [0, 1]

    >>> gray_code(3)
    [0, 1, 3, 2, 6, 7, 5, 4]

    >>> gray_code(-1)
    Traceback (most recent call last):
        ...
    ValueError: The given input must be positive

    >>> gray_code(10.6)
    Traceback (most recent call last):
        ...
    TypeError: unsupported operand type(s) for <<: 'int' and 'float'
    """"""

    # bit count represents no. of bits in the gray code
    if bit_count < 0:
        raise ValueError(""The given input must be positive"")

    # get the generated string sequence
    sequence = gray_code_sequence_string(bit_count)
    #
    # convert them to integers
    for i in range(len(sequence)):
        sequence[i] = int(sequence[i], 2)

    return sequence


def gray_code_sequence_string(bit_count: int) -> list:
    """"""
    Will output the n-bit grey sequence as a
    string of bits

    >>> gray_code_sequence_string(2)
    ['00', '01', '11', '10']

    >>> gray_code_sequence_string(1)
    ['0', '1']
    """"""

    # The approach is a recursive one
    # Base case achieved when either n = 0 or n=1
    if bit_count == 0:
        return [""0""]

    if bit_count == 1:
        return [""0"", ""1""]

    seq_len = 1 << bit_count  # defines the length of the sequence
    # 1<< n is equivalent to 2^n

    # recursive answer will generate answer for n-1 bits
    smaller_sequence = gray_code_sequence_string(bit_count - 1)

    sequence = []

    # append 0 to first half of the smaller sequence generated
    for i in range(seq_len // 2):
        generated_no = ""0"" + smaller_sequence[i]
        sequence.append(generated_no)

    # append 1 to second half ... start from the end of the list
    for i in reversed(range(seq_len // 2)):
        generated_no = ""1"" + smaller_sequence[i]
        sequence.append(generated_no)

    return sequence


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","def gray_code(bit_count: int) -> list:
    """"""
    Takes in an integer n and returns a n-bit
    gray code sequence
    An n-bit gray code sequence is a sequence of 2^n
    integers where:

    a) Every integer is between [0,2^n -1] inclusive
    b) The sequence begins with 0
    c) An integer appears at most one times in the sequence
    d)The binary representation of every pair of integers differ
       by exactly one bit
    e) The binary representation of first and last bit also
       differ by exactly one bit

    >>> gray_code(2)
    [0, 1, 3, 2]

    >>> gray_code(1)
    [0, 1]

    >>> gray_code(3)
    [0, 1, 3, 2, 6, 7, 5, 4]

    >>> gray_code(-1)
    Traceback (most recent call last):
        ...
    ValueError: The given input must be positive

    >>> gray_code(10.6)
    Traceback (most recent call last):
        ...
    TypeError: unsupported operand type(s) for <<: 'int' and 'float'
    """"""

    # bit count represents no. of bits in the gray code
    if bit_count < 0:
        raise ValueError(""The given input must be positive"")

    # get the generated string sequence
    sequence = gray_code_sequence_string(bit_count)

    #
    # convert them to integers
    for i in range(len(sequence)):
        sequence[i] = int(sequence[i], 2)

    return sequence

def gray_code_sequence_string(bit_count: int) -> list:
    """"""
    Will output the n-bit grey sequence as a
    string of bits

    >>> gray_code_sequence_string(2)
    ['00', '01', '11', '10']

    >>> gray_code_sequence_string(1)
    ['0', '1']
    """"""
    # The approach is a recursive one
    # Base case achieved when either n = 0 or n=1
    if bit_count == 0:
        return [""0""]
    if bit_count == 1:
        return [""0"", ""1""]

    seq_len = 1 << bit_count  # defines the length of the sequence
    # 1<< n is equivalent to 2^n

    # recursive answer will generate answer for n-1 bits
    smaller_sequence = gray_code_sequence_string(bit_count - 1)
    sequence = []

    # append 0 to first half of the smaller sequence generated
    for i in range(seq_len // 2):
        generated_no = ""0"" + smaller_sequence[i]
        sequence.append(generated_no)

    # append 1 to second half ... start from the end of the list
    for i in reversed(range(seq_len // 2)):
        generated_no = ""1"" + smaller_sequence[i]
        sequence.append(generated_no)

    return sequence

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
83,"from dataclasses import dataclass

DEFAULT_USER_ID = ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
DEFAULT_EMAIL = ""default@example.com""


# Using dataclass here to avoid adding dependency on pydantic
@dataclass(frozen=True)
class User:
    user_id: str
    email: str
    phone_number: str
    role: str

    @classmethod
    def from_payload(cls, payload):
        return cls(
            user_id=payload[""sub""],
            email=payload.get(""email"", """"),
            phone_number=payload.get(""phone"", """"),
            role=payload[""role""],
        )
","from dataclasses import dataclass

DEFAULT_USER_ID = ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
DEFAULT_EMAIL = ""default@example.com""

# Using dataclass here to avoid adding dependency on pydantic
@dataclass(frozen=True)
class User:
    user_id: str
    email: str
    phone_number: str
    role: str

    @classmethod
    def from_payload(cls, payload):
        return cls(
            user_id=payload[""sub""],
            email=payload.get(""email"", """"),
            phone_number=payload.get(""phone"", """"),
            role=payload[""role""],
        )","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
84,"from datetime import datetime, timezone
from unittest.mock import AsyncMock

import pytest
from fastapi import WebSocket

from backend.data.execution import ExecutionResult, ExecutionStatus
from backend.server.conn_manager import ConnectionManager
from backend.server.model import Methods, WsMessage


@pytest.fixture
def connection_manager() -> ConnectionManager:
    return ConnectionManager()


@pytest.fixture
def mock_websocket() -> AsyncMock:
    websocket: AsyncMock = AsyncMock(spec=WebSocket)
    websocket.send_text = AsyncMock()
    return websocket


@pytest.mark.asyncio
async def test_connect(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    await connection_manager.connect(mock_websocket)
    assert mock_websocket in connection_manager.active_connections
    mock_websocket.accept.assert_called_once()


def test_disconnect(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    connection_manager.active_connections.add(mock_websocket)
    connection_manager.subscriptions[""test_graph""] = {mock_websocket}

    connection_manager.disconnect(mock_websocket)

    assert mock_websocket not in connection_manager.active_connections
    assert mock_websocket not in connection_manager.subscriptions[""test_graph""]


@pytest.mark.asyncio
async def test_subscribe(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    await connection_manager.subscribe(""test_graph"", mock_websocket)
    assert mock_websocket in connection_manager.subscriptions[""test_graph""]


@pytest.mark.asyncio
async def test_unsubscribe(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    connection_manager.subscriptions[""test_graph""] = {mock_websocket}

    await connection_manager.unsubscribe(""test_graph"", mock_websocket)

    assert ""test_graph"" not in connection_manager.subscriptions


@pytest.mark.asyncio
async def test_send_execution_result(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    connection_manager.subscriptions[""test_graph""] = {mock_websocket}
    result: ExecutionResult = ExecutionResult(
        graph_id=""test_graph"",
        graph_version=1,
        graph_exec_id=""test_exec_id"",
        node_exec_id=""test_node_exec_id"",
        node_id=""test_node_id"",
        block_id=""test_block_id"",
        status=ExecutionStatus.COMPLETED,
        input_data={""input1"": ""value1""},
        output_data={""output1"": [""result1""]},
        add_time=datetime.now(tz=timezone.utc),
        queue_time=None,
        start_time=datetime.now(tz=timezone.utc),
        end_time=datetime.now(tz=timezone.utc),
    )

    await connection_manager.send_execution_result(result)

    mock_websocket.send_text.assert_called_once_with(
        WsMessage(
            method=Methods.EXECUTION_EVENT,
            channel=""test_graph"",
            data=result.model_dump(),
        ).model_dump_json()
    )


@pytest.mark.asyncio
async def test_send_execution_result_no_subscribers(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    result: ExecutionResult = ExecutionResult(
        graph_id=""test_graph"",
        graph_version=1,
        graph_exec_id=""test_exec_id"",
        node_exec_id=""test_node_exec_id"",
        node_id=""test_node_id"",
        block_id=""test_block_id"",
        status=ExecutionStatus.COMPLETED,
        input_data={""input1"": ""value1""},
        output_data={""output1"": [""result1""]},
        add_time=datetime.now(),
        queue_time=None,
        start_time=datetime.now(),
        end_time=datetime.now(),
    )

    await connection_manager.send_execution_result(result)

    mock_websocket.send_text.assert_not_called()
","from datetime import datetime, timezone
from unittest.mock import AsyncMock
import pytest
from fastapi import WebSocket
from backend.data.execution import ExecutionResult, ExecutionStatus
from backend.server.conn_manager import ConnectionManager
from backend.server.model import Methods, WsMessage

@pytest.fixture
def connection_manager() -> ConnectionManager:
    return ConnectionManager()

@pytest.fixture
def mock_websocket() -> AsyncMock:
    websocket: AsyncMock = AsyncMock(spec=WebSocket)
    websocket.send_text = AsyncMock()
    return websocket

@pytest.mark.asyncio
async def test_connect(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    await connection_manager.connect(mock_websocket)
    assert mock_websocket in connection_manager.active_connections
    mock_websocket.accept.assert_called_once()

def test_disconnect(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    connection_manager.active_connections.add(mock_websocket)
    connection_manager.subscriptions[""test_graph""] = {mock_websocket}
    connection_manager.disconnect(mock_websocket)
    assert mock_websocket not in connection_manager.active_connections
    assert mock_websocket not in connection_manager.subscriptions[""test_graph""]

@pytest.mark.asyncio
async def test_subscribe(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    await connection_manager.subscribe(""test_graph"", mock_websocket)
    assert mock_websocket in connection_manager.subscriptions[""test_graph""]

@pytest.mark.asyncio
async def test_unsubscribe(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    connection_manager.subscriptions[""test_graph""] = {mock_websocket}
    await connection_manager.unsubscribe(""test_graph"", mock_websocket)
    assert ""test_graph"" not in connection_manager.subscriptions

@pytest.mark.asyncio
async def test_send_execution_result(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    connection_manager.subscriptions[""test_graph""] = {mock_websocket}
    result: ExecutionResult = ExecutionResult(
        graph_id=""test_graph"",
        graph_version=1,
        graph_exec_id=""test_exec_id"",
        node_exec_id=""test_node_exec_id"",
        node_id=""test_node_id"",
        block_id=""test_block_id"",
        status=ExecutionStatus.COMPLETED,
        input_data={""input1"": ""value1""},
        output_data={""output1"": [""result1""]},
        add_time=datetime.now(tz=timezone.utc),
        queue_time=None,
        start_time=datetime.now(tz=timezone.utc),
        end_time=datetime.now(tz=timezone.utc),
    )
    await connection_manager.send_execution_result(result)
    mock_websocket.send_text.assert_called_once_with(
        WsMessage(
            method=Methods.EXECUTION_EVENT,
            channel=""test_graph"",
            data=result.model_dump(),
        ).model_dump_json()
    )

@pytest.mark.asyncio
async def test_send_execution_result_no_subscribers(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    result: ExecutionResult = ExecutionResult(
        graph_id=""test_graph"",
        graph_version=1,
        graph_exec_id=""test_exec_id"",
        node_exec_id=""test_node_exec_id"",
        node_id=""test_node_id"",
        block_id=""test_block_id"",
        status=ExecutionStatus.COMPLETED,
        input_data={""input1"": ""value1""},
        output_data={""output1"": [""result1""]},
        add_time=datetime.now(),
        queue_time=None,
        start_time=datetime.now(),
        end_time=datetime.now(),
    )
    await connection_manager.send_execution_result(result)
    mock_websocket.send_text.assert_not_called()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
85,"from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

from ._auth import (
    GOOGLE_OAUTH_IS_CONFIGURED,
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GoogleCredentials,
    GoogleCredentialsField,
    GoogleCredentialsInput,
)


class GoogleSheetsReadBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/spreadsheets.readonly""]
        )
        spreadsheet_id: str = SchemaField(
            description=""The ID of the spreadsheet to read from"",
        )
        range: str = SchemaField(
            description=""The A1 notation of the range to read"",
        )

    class Output(BlockSchema):
        result: list[list[str]] = SchemaField(
            description=""The data read from the spreadsheet"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""5724e902-3635-47e9-a108-aaa0263a4988"",
            description=""This block reads data from a Google Sheets spreadsheet."",
            categories={BlockCategory.DATA},
            input_schema=GoogleSheetsReadBlock.Input,
            output_schema=GoogleSheetsReadBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""spreadsheet_id"": ""1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"",
                ""range"": ""Sheet1!A1:B2"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    [
                        [""Name"", ""Score""],
                        [""Alice"", ""85""],
                    ],
                ),
            ],
            test_mock={
                ""_read_sheet"": lambda *args, **kwargs: [
                    [""Name"", ""Score""],
                    [""Alice"", ""85""],
                ],
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = self._build_service(credentials, **kwargs)
        data = self._read_sheet(service, input_data.spreadsheet_id, input_data.range)
        yield ""result"", data

    @staticmethod
    def _build_service(credentials: GoogleCredentials, **kwargs):
        creds = Credentials(
            token=(
                credentials.access_token.get_secret_value()
                if credentials.access_token
                else None
            ),
            refresh_token=(
                credentials.refresh_token.get_secret_value()
                if credentials.refresh_token
                else None
            ),
            token_uri=""https://oauth2.googleapis.com/token"",
            client_id=kwargs.get(""client_id""),
            client_secret=kwargs.get(""client_secret""),
            scopes=credentials.scopes,
        )
        return build(""sheets"", ""v4"", credentials=creds)

    def _read_sheet(self, service, spreadsheet_id: str, range: str) -> list[list[str]]:
        sheet = service.spreadsheets()
        result = sheet.values().get(spreadsheetId=spreadsheet_id, range=range).execute()
        return result.get(""values"", [])


class GoogleSheetsWriteBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/spreadsheets""]
        )
        spreadsheet_id: str = SchemaField(
            description=""The ID of the spreadsheet to write to"",
        )
        range: str = SchemaField(
            description=""The A1 notation of the range to write"",
        )
        values: list[list[str]] = SchemaField(
            description=""The data to write to the spreadsheet"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""The result of the write operation"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""d9291e87-301d-47a8-91fe-907fb55460e5"",
            description=""This block writes data to a Google Sheets spreadsheet."",
            categories={BlockCategory.DATA},
            input_schema=GoogleSheetsWriteBlock.Input,
            output_schema=GoogleSheetsWriteBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""spreadsheet_id"": ""1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"",
                ""range"": ""Sheet1!A1:B2"",
                ""values"": [
                    [""Name"", ""Score""],
                    [""Bob"", ""90""],
                ],
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""updatedCells"": 4, ""updatedColumns"": 2, ""updatedRows"": 2},
                ),
            ],
            test_mock={
                ""_write_sheet"": lambda *args, **kwargs: {
                    ""updatedCells"": 4,
                    ""updatedColumns"": 2,
                    ""updatedRows"": 2,
                },
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GoogleSheetsReadBlock._build_service(credentials, **kwargs)
        result = self._write_sheet(
            service,
            input_data.spreadsheet_id,
            input_data.range,
            input_data.values,
        )
        yield ""result"", result

    def _write_sheet(
        self, service, spreadsheet_id: str, range: str, values: list[list[str]]
    ) -> dict:
        body = {""values"": values}
        result = (
            service.spreadsheets()
            .values()
            .update(
                spreadsheetId=spreadsheet_id,
                range=range,
                valueInputOption=""USER_ENTERED"",
                body=body,
            )
            .execute()
        )
        return result
","from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from ._auth import (
    GOOGLE_OAUTH_IS_CONFIGURED,
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GoogleCredentials,
    GoogleCredentialsField,
    GoogleCredentialsInput,
)

class GoogleSheetsReadBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/spreadsheets.readonly""]
        )
        spreadsheet_id: str = SchemaField(
            description=""The ID of the spreadsheet to read from"",
        )
        range: str = SchemaField(
            description=""The A1 notation of the range to read"",
        )

    class Output(BlockSchema):
        result: list[list[str]] = SchemaField(
            description=""The data read from the spreadsheet"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""5724e902-3635-47e9-a108-aaa0263a4988"",
            description=""This block reads data from a Google Sheets spreadsheet."",
            categories={BlockCategory.DATA},
            input_schema=GoogleSheetsReadBlock.Input,
            output_schema=GoogleSheetsReadBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""spreadsheet_id"": ""1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"",
                ""range"": ""Sheet1!A1:B2"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    [
                        [""Name"", ""Score""],
                        [""Alice"", ""85""],
                    ],
                ),
            ],
            test_mock={
                ""_read_sheet"": lambda *args, **kwargs: [
                    [""Name"", ""Score""],
                    [""Alice"", ""85""],
                ],
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = self._build_service(credentials, **kwargs)
        data = self._read_sheet(service, input_data.spreadsheet_id, input_data.range)
        yield ""result"", data

    @staticmethod
    def _build_service(credentials: GoogleCredentials, **kwargs):
        creds = Credentials(
            token=(
                credentials.access_token.get_secret_value()
                if credentials.access_token
                else None
            ),
            refresh_token=(
                credentials.refresh_token.get_secret_value()
                if credentials.refresh_token
                else None
            ),
            token_uri=""https://oauth2.googleapis.com/token"",
            client_id=kwargs.get(""client_id""),
            client_secret=kwargs.get(""client_secret""),
            scopes=credentials.scopes,
        )
        return build(""sheets"", ""v4"", credentials=creds)

    def _read_sheet(self, service, spreadsheet_id: str, range: str) -> list[list[str]]:
        sheet = service.spreadsheets()
        result = sheet.values().get(spreadsheetId=spreadsheet_id, range=range).execute()
        return result.get(""values"", [])

class GoogleSheetsWriteBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/spreadsheets""]
        )
        spreadsheet_id: str = SchemaField(
            description=""The ID of the spreadsheet to write to"",
        )
        range: str = SchemaField(
            description=""The A1 notation of the range to write"",
        )
        values: list[list[str]] = SchemaField(
            description=""The data to write to the spreadsheet"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""The result of the write operation"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""d9291e87-301d-47a8-91fe-907fb55460e5"",
            description=""This block writes data to a Google Sheets spreadsheet."",
            categories={BlockCategory.DATA},
            input_schema=GoogleSheetsWriteBlock.Input,
            output_schema=GoogleSheetsWriteBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""spreadsheet_id"": ""1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"",
                ""range"": ""Sheet1!A1:B2"",
                ""values"": [
                    [""Name"", ""Score""],
                    [""Bob"", ""90""],
                ],
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""updatedCells"": 4, ""updatedColumns"": 2, ""updatedRows"": 2},
                ),
            ],
            test_mock={
                ""_write_sheet"": lambda *args, **kwargs: {
                    ""updatedCells"": 4,
                    ""updatedColumns"": 2,
                    ""updatedRows"": 2,
                },
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GoogleSheetsReadBlock._build_service(credentials, **kwargs)
        result = self._write_sheet(
            service,
            input_data.spreadsheet_id,
            input_data.range,
            input_data.values,
        )
        yield ""result"", result

    def _write_sheet(
        self, service, spreadsheet_id: str, range: str, values: list[list[str]]
    ) -> dict:
        body = {""values"": values}
        result = (
            service.spreadsheets()
            .values()
            .update(
                spreadsheetId=spreadsheet_id,
                range=range,
                valueInputOption=""USER_ENTERED"",
                body=body,
            )
            .execute()
        )
        return result","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
86,"from datetime import datetime

import pytest
from prisma.models import CreditTransaction

from backend.blocks.llm import AITextGeneratorBlock
from backend.data.credit import UserCredit
from backend.data.user import DEFAULT_USER_ID
from backend.integrations.credentials_store import openai_credentials
from backend.util.test import SpinTestServer

REFILL_VALUE = 1000
user_credit = UserCredit(REFILL_VALUE)


@pytest.mark.asyncio(scope=""session"")
async def test_block_credit_usage(server: SpinTestServer):
    current_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)

    spending_amount_1 = await user_credit.spend_credits(
        DEFAULT_USER_ID,
        current_credit,
        AITextGeneratorBlock().id,
        {
            ""model"": ""gpt-4-turbo"",
            ""credentials"": {
                ""id"": openai_credentials.id,
                ""provider"": openai_credentials.provider,
                ""type"": openai_credentials.type,
            },
        },
        0.0,
        0.0,
        validate_balance=False,
    )
    assert spending_amount_1 > 0

    spending_amount_2 = await user_credit.spend_credits(
        DEFAULT_USER_ID,
        current_credit,
        AITextGeneratorBlock().id,
        {""model"": ""gpt-4-turbo"", ""api_key"": ""owned_api_key""},
        0.0,
        0.0,
        validate_balance=False,
    )
    assert spending_amount_2 == 0

    new_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    assert new_credit == current_credit - spending_amount_1 - spending_amount_2


@pytest.mark.asyncio(scope=""session"")
async def test_block_credit_top_up(server: SpinTestServer):
    current_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)

    await user_credit.top_up_credits(DEFAULT_USER_ID, 100)

    new_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    assert new_credit == current_credit + 100


@pytest.mark.asyncio(scope=""session"")
async def test_block_credit_reset(server: SpinTestServer):
    month1 = datetime(2022, 1, 15)
    month2 = datetime(2022, 2, 15)

    user_credit.time_now = lambda: month2
    month2credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)

    # Month 1 result should only affect month 1
    user_credit.time_now = lambda: month1
    month1credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    await user_credit.top_up_credits(DEFAULT_USER_ID, 100)
    assert await user_credit.get_or_refill_credit(DEFAULT_USER_ID) == month1credit + 100

    # Month 2 balance is unaffected
    user_credit.time_now = lambda: month2
    assert await user_credit.get_or_refill_credit(DEFAULT_USER_ID) == month2credit


@pytest.mark.asyncio(scope=""session"")
async def test_credit_refill(server: SpinTestServer):
    # Clear all transactions within the month
    await CreditTransaction.prisma().update_many(
        where={
            ""userId"": DEFAULT_USER_ID,
            ""createdAt"": {
                ""gte"": datetime(2022, 2, 1),
                ""lt"": datetime(2022, 3, 1),
            },
        },
        data={""isActive"": False},
    )
    user_credit.time_now = lambda: datetime(2022, 2, 15)

    balance = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    assert balance == REFILL_VALUE
","from datetime import datetime
import pytest
from prisma.models import CreditTransaction
from backend.blocks.llm import AITextGeneratorBlock
from backend.data.credit import UserCredit
from backend.data.user import DEFAULT_USER_ID
from backend.integrations.credentials_store import openai_credentials
from backend.util.test import SpinTestServer

REFILL_VALUE = 1000
user_credit = UserCredit(REFILL_VALUE)

@pytest.mark.asyncio(scope=""session"")
async def test_block_credit_usage(server: SpinTestServer):
    current_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    spending_amount_1 = await user_credit.spend_credits(
        DEFAULT_USER_ID,
        current_credit,
        AITextGeneratorBlock().id,
        {
            ""model"": ""gpt-4-turbo"",
            ""credentials"": {
                ""id"": openai_credentials.id,
                ""provider"": openai_credentials.provider,
                ""type"": openai_credentials.type,
            },
        },
        0.0,
        0.0,
        validate_balance=False,
    )
    assert spending_amount_1 > 0
    spending_amount_2 = await user_credit.spend_credits(
        DEFAULT_USER_ID,
        current_credit,
        AITextGeneratorBlock().id,
        {""model"": ""gpt-4-turbo"", ""api_key"": ""owned_api_key""},
        0.0,
        0.0,
        validate_balance=False,
    )
    assert spending_amount_2 == 0
    new_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    assert new_credit == current_credit - spending_amount_1 - spending_amount_2

@pytest.mark.asyncio(scope=""session"")
async def test_block_credit_top_up(server: SpinTestServer):
    current_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    await user_credit.top_up_credits(DEFAULT_USER_ID, 100)
    new_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    assert new_credit == current_credit + 100

@pytest.mark.asyncio(scope=""session"")
async def test_block_credit_reset(server: SpinTestServer):
    month1 = datetime(2022, 1, 15)
    month2 = datetime(2022, 2, 15)
    user_credit.time_now = lambda: month2
    month2credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    user_credit.time_now = lambda: month1
    month1credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    await user_credit.top_up_credits(DEFAULT_USER_ID, 100)
    assert await user_credit.get_or_refill_credit(DEFAULT_USER_ID) == month1credit + 100
    user_credit.time_now = lambda: month2
    assert await user_credit.get_or_refill_credit(DEFAULT_USER_ID) == month2credit

@pytest.mark.asyncio(scope=""session"")
async def test_credit_refill(server: SpinTestServer):
    # Clear all transactions within the month
    await CreditTransaction.prisma().update_many(
        where={
            ""userId"": DEFAULT_USER_ID,
            ""createdAt"": {
                ""gte"": datetime(2022, 2, 1),
                ""lt"": datetime(2022, 3, 1),
            },
        },
        data={""isActive"": False},
    )
    user_credit.time_now = lambda: datetime(2022, 2, 15)
    balance = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    assert balance == REFILL_VALUE","{'final_score': 96.79, 'length_similarity': 97.39, 'token_similarity': 92.31, 'structure_similarity': 97.44, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
87,"from prisma.models import User

from backend.blocks.llm import AIStructuredResponseGeneratorBlock
from backend.blocks.reddit import GetRedditPostsBlock, PostRedditCommentBlock
from backend.blocks.text import FillTextTemplateBlock, MatchTextPatternBlock
from backend.data.graph import Graph, Link, Node, create_graph
from backend.data.user import get_or_create_user
from backend.util.test import SpinTestServer, wait_execution


def create_test_graph() -> Graph:
    """"""
                    subreddit
                       ||
                        v
        GetRedditPostsBlock (post_id, post_title, post_body)
                  //     ||     \\
              post_id  post_title  post_body
                 ||       ||        ||
                 v        v         v
              FillTextTemplateBlock (format)
                      ||
                      v
            AIStructuredResponseBlock / TextRelevancy
                 ||       ||       ||
            post_id  is_relevant  marketing_text
               ||       ||        ||
               v        v         v
                 MatchTextPatternBlock
                 ||       ||
              positive  negative
                ||
                v
        PostRedditCommentBlock
    """"""
    # Hardcoded inputs
    reddit_get_post_input = {
        ""post_limit"": 3,
    }
    text_formatter_input = {
        ""format"": """"""
Based on the following post, write your marketing comment:
* Post ID: {id}
* Post Subreddit: {subreddit}
* Post Title: {title}
* Post Body: {body}"""""".strip(),
    }
    llm_call_input = {
        ""sys_prompt"": """"""
You are an expert at marketing.
You have been tasked with picking Reddit posts that are relevant to your product.
The product you are marketing is: Auto-GPT an autonomous AI agent utilizing GPT model.
You reply the post that you find it relevant to be replied with marketing text.
Make sure to only comment on a relevant post.
"""""",
        ""expected_format"": {
            ""post_id"": ""str, the reddit post id"",
            ""is_relevant"": ""bool, whether the post is relevant for marketing"",
            ""marketing_text"": ""str, marketing text, this is empty on irrelevant posts"",
        },
    }
    text_matcher_input = {""match"": ""true"", ""case_sensitive"": False}
    reddit_comment_input = {}

    # Nodes
    reddit_get_post_node = Node(
        block_id=GetRedditPostsBlock().id,
        input_default=reddit_get_post_input,
    )
    text_formatter_node = Node(
        block_id=FillTextTemplateBlock().id,
        input_default=text_formatter_input,
    )
    llm_call_node = Node(
        block_id=AIStructuredResponseGeneratorBlock().id, input_default=llm_call_input
    )
    text_matcher_node = Node(
        block_id=MatchTextPatternBlock().id,
        input_default=text_matcher_input,
    )
    reddit_comment_node = Node(
        block_id=PostRedditCommentBlock().id,
        input_default=reddit_comment_input,
    )

    nodes = [
        reddit_get_post_node,
        text_formatter_node,
        llm_call_node,
        text_matcher_node,
        reddit_comment_node,
    ]

    # Links
    links = [
        Link(
            source_id=reddit_get_post_node.id,
            sink_id=text_formatter_node.id,
            source_name=""post"",
            sink_name=""values"",
        ),
        Link(
            source_id=text_formatter_node.id,
            sink_id=llm_call_node.id,
            source_name=""output"",
            sink_name=""prompt"",
        ),
        Link(
            source_id=llm_call_node.id,
            sink_id=text_matcher_node.id,
            source_name=""response"",
            sink_name=""data"",
        ),
        Link(
            source_id=llm_call_node.id,
            sink_id=text_matcher_node.id,
            source_name=""response_#_is_relevant"",
            sink_name=""text"",
        ),
        Link(
            source_id=text_matcher_node.id,
            sink_id=reddit_comment_node.id,
            source_name=""positive_#_post_id"",
            sink_name=""data_#_post_id"",
        ),
        Link(
            source_id=text_matcher_node.id,
            sink_id=reddit_comment_node.id,
            source_name=""positive_#_marketing_text"",
            sink_name=""data_#_comment"",
        ),
    ]

    # Create graph
    test_graph = Graph(
        name=""RedditMarketingAgent"",
        description=""Reddit marketing agent"",
        nodes=nodes,
        links=links,
    )
    return test_graph


async def create_test_user() -> User:
    test_user_data = {
        ""sub"": ""ef3b97d7-1161-4eb4-92b2-10c24fb154c1"",
        ""email"": ""testuser@example.com"",
        ""name"": ""Test User"",
    }
    user = await get_or_create_user(test_user_data)
    return user


async def reddit_marketing_agent():
    async with SpinTestServer() as server:
        test_user = await create_test_user()
        test_graph = await create_graph(create_test_graph(), user_id=test_user.id)
        input_data = {""subreddit"": ""AutoGPT""}
        response = await server.agent_server.test_execute_graph(
            test_graph.id, input_data, test_user.id
        )
        print(response)
        result = await wait_execution(test_user.id, test_graph.id, response[""id""], 120)
        print(result)


if __name__ == ""__main__"":
    import asyncio

    asyncio.run(reddit_marketing_agent())
","from prisma.models import User
from backend.blocks.llm import AIStructuredResponseGeneratorBlock
from backend.blocks.reddit import GetRedditPostsBlock, PostRedditCommentBlock
from backend.blocks.text import FillTextTemplateBlock, MatchTextPatternBlock
from backend.data.graph import Graph, Link, Node, create_graph
from backend.data.user import get_or_create_user
from backend.util.test import SpinTestServer, wait_execution


def create_test_graph() -> Graph:
    """"""
                    subreddit
                       ||
                        v
        GetRedditPostsBlock (post_id, post_title, post_body)
                  //     ||     \\
              post_id  post_title  post_body
                 ||       ||        ||
                 v        v         v
              FillTextTemplateBlock (format)
                      ||
                      v
            AIStructuredResponseBlock / TextRelevancy
                 ||       ||       ||
            post_id  is_relevant  marketing_text
               ||       ||        ||
               v        v         v
                 MatchTextPatternBlock
                 ||       ||
              positive  negative
                ||
                v
        PostRedditCommentBlock
    """"""
    # Hardcoded inputs
    reddit_get_post_input = {
        ""post_limit"": 3,
    }
    text_formatter_input = {
        ""format"": """"""
Based on the following post, write your marketing comment:
* Post ID: {id}
* Post Subreddit: {subreddit}
* Post Title: {title}
* Post Body: {body}"""""".strip(),
    }
    llm_call_input = {
        ""sys_prompt"": """"""
You are an expert at marketing.
You have been tasked with picking Reddit posts that are relevant to your product.
The product you are marketing is: Auto-GPT an autonomous AI agent utilizing GPT model.
You reply the post that you find it relevant to be replied with marketing text.
Make sure to only comment on a relevant post.
"""""",
        ""expected_format"": {
            ""post_id"": ""str, the reddit post id"",
            ""is_relevant"": ""bool, whether the post is relevant for marketing"",
            ""marketing_text"": ""str, marketing text, this is empty on irrelevant posts"",
        },
    }
    text_matcher_input = {""match"": ""true"", ""case_sensitive"": False}
    reddit_comment_input = {}

    # Nodes
    reddit_get_post_node = Node(
        block_id=GetRedditPostsBlock().id,
        input_default=reddit_get_post_input,
    )
    text_formatter_node = Node(
        block_id=FillTextTemplateBlock().id,
        input_default=text_formatter_input,
    )
    llm_call_node = Node(
        block_id=AIStructuredResponseGeneratorBlock().id, input_default=llm_call_input
    )
    text_matcher_node = Node(
        block_id=MatchTextPatternBlock().id,
        input_default=text_matcher_input,
    )
    reddit_comment_node = Node(
        block_id=PostRedditCommentBlock().id,
        input_default=reddit_comment_input,
    )

    nodes = [
        reddit_get_post_node,
        text_formatter_node,
        llm_call_node,
        text_matcher_node,
        reddit_comment_node,
    ]

    # Links
    links = [
        Link(
            source_id=reddit_get_post_node.id,
            sink_id=text_formatter_node.id,
            source_name=""post"",
            sink_name=""values"",
        ),
        Link(
            source_id=text_formatter_node.id,
            sink_id=llm_call_node.id,
            source_name=""output"",
            sink_name=""prompt"",
        ),
        Link(
            source_id=llm_call_node.id,
            sink_id=text_matcher_node.id,
            source_name=""response"",
            sink_name=""data"",
        ),
        Link(
            source_id=llm_call_node.id,
            sink_id=text_matcher_node.id,
            source_name=""response_#_is_relevant"",
            sink_name=""text"",
        ),
        Link(
            source_id=text_matcher_node.id,
            sink_id=reddit_comment_node.id,
            source_name=""positive_#_post_id"",
            sink_name=""data_#_post_id"",
        ),
        Link(
            source_id=text_matcher_node.id,
            sink_id=reddit_comment_node.id,
            source_name=""positive_#_marketing_text"",
            sink_name=""data_#_comment"",
        ),
    ]

    # Create graph
    test_graph = Graph(
        name=""RedditMarketingAgent"",
        description=""Reddit marketing agent"",
        nodes=nodes,
        links=links,
    )
    return test_graph


async def create_test_user() -> User:
    test_user_data = {
        ""sub"": ""ef3b97d7-1161-4eb4-92b2-10c24fb154c1"",
        ""email"": ""testuser@example.com"",
        ""name"": ""Test User"",
    }
    user = await get_or_create_user(test_user_data)
    return user


async def reddit_marketing_agent():
    async with SpinTestServer() as server:
        test_user = await create_test_user()
        test_graph = await create_graph(create_test_graph(), user_id=test_user.id)
        input_data = {""subreddit"": ""AutoGPT""}

        response = await server.agent_server.test_execute_graph(
            test_graph.id, input_data, test_user.id
        )
        print(response)

        result = await wait_execution(test_user.id, test_graph.id, response[""id""], 120)
        print(result)


if __name__ == ""__main__"":
    import asyncio

    asyncio.run(reddit_marketing_agent())","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
88,"def binary_count_setbits(a: int) -> int:
    """"""
    Take in 1 integer, return a number that is
    the number of 1's in binary representation of that number.

    >>> binary_count_setbits(25)
    3
    >>> binary_count_setbits(36)
    2
    >>> binary_count_setbits(16)
    1
    >>> binary_count_setbits(58)
    4
    >>> binary_count_setbits(4294967295)
    32
    >>> binary_count_setbits(0)
    0
    >>> binary_count_setbits(-10)
    Traceback (most recent call last):
        ...
    ValueError: Input value must be a positive integer
    >>> binary_count_setbits(0.8)
    Traceback (most recent call last):
        ...
    TypeError: Input value must be a 'int' type
    >>> binary_count_setbits(""0"")
    Traceback (most recent call last):
        ...
    TypeError: '<' not supported between instances of 'str' and 'int'
    """"""
    if a < 0:
        raise ValueError(""Input value must be a positive integer"")
    elif isinstance(a, float):
        raise TypeError(""Input value must be a 'int' type"")
    return bin(a).count(""1"")


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","def binary_count_setbits(a: int) -> int:
    """"""
    Take in 1 integer, return a number that is
    the number of 1's in binary representation of that number.

    >>> binary_count_setbits(25)
    3
    >>> binary_count_setbits(36)
    2
    >>> binary_count_setbits(16)
    1
    >>> binary_count_setbits(58)
    4
    >>> binary_count_setbits(4294967295)
    32
    >>> binary_count_setbits(0)
    0
    >>> binary_count_setbits(-10)
    Traceback (most recent call last):
        ...
    ValueError: Input value must be a positive integer
    >>> binary_count_setbits(0.8)
    Traceback (most recent call last):
        ...
    TypeError: Input value must be a 'int' type
    >>> binary_count_setbits(""0"")
    Traceback (most recent call last):
        ...
    TypeError: '<' not supported between instances of 'str' and 'int'
    """"""
    if a < 0:
        raise ValueError(""Input value must be a positive integer"")
    elif isinstance(a, float):
        raise TypeError(""Input value must be a 'int' type"")
    return bin(a).count(""1"")

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
89,"# Information on 2's complement: https://en.wikipedia.org/wiki/Two%27s_complement


def twos_complement(number: int) -> str:
    """"""
    Take in a negative integer 'number'.
    Return the two's complement representation of 'number'.

    >>> twos_complement(0)
    '0b0'
    >>> twos_complement(-1)
    '0b11'
    >>> twos_complement(-5)
    '0b1011'
    >>> twos_complement(-17)
    '0b101111'
    >>> twos_complement(-207)
    '0b100110001'
    >>> twos_complement(1)
    Traceback (most recent call last):
        ...
    ValueError: input must be a negative integer
    """"""
    if number > 0:
        raise ValueError(""input must be a negative integer"")
    binary_number_length = len(bin(number)[3:])
    twos_complement_number = bin(abs(number) - (1 << binary_number_length))[3:]
    twos_complement_number = (
        (
            ""1""
            + ""0"" * (binary_number_length - len(twos_complement_number))
            + twos_complement_number
        )
        if number < 0
        else ""0""
    )
    return ""0b"" + twos_complement_number


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","# Information on 2's complement: https://en.wikipedia.org/wiki/Two%27s_complement



def twos_complement(number: int) -> str:
    """"""
    Take in a negative integer 'number'.
    Return the two's complement representation of 'number'.

    >>> twos_complement(0)
    '0b0'
    >>> twos_complement(-1)
    '0b11'
    >>> twos_complement(-5)
    '0b1011'
    >>> twos_complement(-17)
    '0b101111'
    >>> twos_complement(-207)
    '0b100110001'
    >>> twos_complement(1)
    Traceback (most recent call last):
        ...
    ValueError: input must be a negative integer
    """"""
    if number > 0:
        raise ValueError(""input must be a negative integer"")
    binary_number_length = len(bin(number)[3:])
    twos_complement_number = bin(abs(number) - (1 << binary_number_length))[3:]
    twos_complement_number = (
        (
            ""1""
            + ""0"" * (binary_number_length - len(twos_complement_number))
            + twos_complement_number
        )
        if number < 0
        else ""0""
    )
    return ""0b"" + twos_complement_number

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
90,"import time
from datetime import datetime, timedelta
from typing import Any, Union

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class GetCurrentTimeBlock(Block):
    class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current time""
        )
        format: str = SchemaField(
            description=""Format of the time to output"", default=""%H:%M:%S""
        )

    class Output(BlockSchema):
        time: str = SchemaField(
            description=""Current time in the specified format (default: %H:%M:%S)""
        )

    def __init__(self):
        super().__init__(
            id=""a892b8d9-3e4e-4e9c-9c1e-75f8efcf1bfa"",
            description=""This block outputs the current time."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentTimeBlock.Input,
            output_schema=GetCurrentTimeBlock.Output,
            test_input=[
                {""trigger"": ""Hello""},
                {""trigger"": ""Hello"", ""format"": ""%H:%M""},
            ],
            test_output=[
                (""time"", lambda _: time.strftime(""%H:%M:%S"")),
                (""time"", lambda _: time.strftime(""%H:%M"")),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_time = time.strftime(input_data.format)
        yield ""time"", current_time


class GetCurrentDateBlock(Block):
    class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current date""
        )
        offset: Union[int, str] = SchemaField(
            title=""Days Offset"",
            description=""Offset in days from the current date"",
            default=0,
        )
        format: str = SchemaField(
            description=""Format of the date to output"", default=""%Y-%m-%d""
        )

    class Output(BlockSchema):
        date: str = SchemaField(
            description=""Current date in the specified format (default: YYYY-MM-DD)""
        )

    def __init__(self):
        super().__init__(
            id=""b29c1b50-5d0e-4d9f-8f9d-1b0e6fcbf0b1"",
            description=""This block outputs the current date with an optional offset."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentDateBlock.Input,
            output_schema=GetCurrentDateBlock.Output,
            test_input=[
                {""trigger"": ""Hello"", ""offset"": ""7""},
                {""trigger"": ""Hello"", ""offset"": ""7"", ""format"": ""%m/%d/%Y""},
            ],
            test_output=[
                (
                    ""date"",
                    lambda t: abs(datetime.now() - datetime.strptime(t, ""%Y-%m-%d""))
                    < timedelta(days=8),  # 7 days difference + 1 day error margin.
                ),
                (
                    ""date"",
                    lambda t: abs(datetime.now() - datetime.strptime(t, ""%m/%d/%Y""))
                    < timedelta(days=8),
                    # 7 days difference + 1 day error margin.
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        try:
            offset = int(input_data.offset)
        except ValueError:
            offset = 0
        current_date = datetime.now() - timedelta(days=offset)
        yield ""date"", current_date.strftime(input_data.format)


class GetCurrentDateAndTimeBlock(Block):
    class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current date and time""
        )
        format: str = SchemaField(
            description=""Format of the date and time to output"",
            default=""%Y-%m-%d %H:%M:%S"",
        )

    class Output(BlockSchema):
        date_time: str = SchemaField(
            description=""Current date and time in the specified format (default: YYYY-MM-DD HH:MM:SS)""
        )

    def __init__(self):
        super().__init__(
            id=""716a67b3-6760-42e7-86dc-18645c6e00fc"",
            description=""This block outputs the current date and time."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentDateAndTimeBlock.Input,
            output_schema=GetCurrentDateAndTimeBlock.Output,
            test_input=[
                {""trigger"": ""Hello""},
            ],
            test_output=[
                (
                    ""date_time"",
                    lambda t: abs(
                        datetime.now() - datetime.strptime(t, ""%Y-%m-%d %H:%M:%S"")
                    )
                    < timedelta(seconds=10),  # 10 seconds error margin.
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_date_time = time.strftime(input_data.format)
        yield ""date_time"", current_date_time


class CountdownTimerBlock(Block):
    class Input(BlockSchema):
        input_message: Any = SchemaField(
            advanced=False,
            description=""Message to output after the timer finishes"",
            default=""timer finished"",
        )
        seconds: Union[int, str] = SchemaField(
            advanced=False, description=""Duration in seconds"", default=0
        )
        minutes: Union[int, str] = SchemaField(
            advanced=False, description=""Duration in minutes"", default=0
        )
        hours: Union[int, str] = SchemaField(
            advanced=False, description=""Duration in hours"", default=0
        )
        days: Union[int, str] = SchemaField(
            advanced=False, description=""Duration in days"", default=0
        )

    class Output(BlockSchema):
        output_message: Any = SchemaField(
            description=""Message after the timer finishes""
        )

    def __init__(self):
        super().__init__(
            id=""d67a9c52-5e4e-11e2-bcfd-0800200c9a71"",
            description=""This block triggers after a specified duration."",
            categories={BlockCategory.TEXT},
            input_schema=CountdownTimerBlock.Input,
            output_schema=CountdownTimerBlock.Output,
            test_input=[
                {""seconds"": 1},
                {""input_message"": ""Custom message""},
            ],
            test_output=[
                (""output_message"", ""timer finished""),
                (""output_message"", ""Custom message""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        seconds = int(input_data.seconds)
        minutes = int(input_data.minutes)
        hours = int(input_data.hours)
        days = int(input_data.days)

        total_seconds = seconds + minutes * 60 + hours * 3600 + days * 86400

        time.sleep(total_seconds)
        yield ""output_message"", input_data.input_message
","import time
from datetime import datetime, timedelta
from typing import Any, Union
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

class GetCurrentTimeBlock(Block):
    class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current time""
        )
        format: str = SchemaField(
            description=""Format of the time to output"", default=""%H:%M:%S""
        )

    class Output(BlockSchema):
        time: str = SchemaField(
            description=""Current time in the specified format (default: %H:%M:%S)""
        )

    def __init__(self):
        super().__init__(
            id=""a892b8d9-3e4e-4e9c-9c1e-75f8efcf1bfa"",
            description=""This block outputs the current time."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentTimeBlock.Input,
            output_schema=GetCurrentTimeBlock.Output,
            test_input=[
                {""trigger"": ""Hello""},
                {""trigger"": ""Hello"", ""format"": ""%H:%M""},
            ],
            test_output=[
                (""time"", lambda _: time.strftime(""%H:%M:%S"")),
                (""time"", lambda _: time.strftime(""%H:%M"")),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_time = time.strftime(input_data.format)
        yield ""time"", current_time


class GetCurrentDateBlock(Block):
    class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current date""
        )
        offset: Union[int, str] = SchemaField(
            title=""Days Offset"",
            description=""Offset in days from the current date"",
            default=0,
        )
        format: str = SchemaField(
            description=""Format of the date to output"", default=""%Y-%m-%d""
        )

    class Output(BlockSchema):
        date: str = SchemaField(
            description=""Current date in the specified format (default: YYYY-MM-DD)""
        )

    def __init__(self):
        super().__init__(
            id=""b29c1b50-5d0e-4d9f-8f9d-1b0e6fcbf0b1"",
            description=""This block outputs the current date with an optional offset."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentDateBlock.Input,
            output_schema=GetCurrentDateBlock.Output,
            test_input=[
                {""trigger"": ""Hello"", ""offset"": ""7""},
                {""trigger"": ""Hello"", ""offset"": ""7"", ""format"": ""%m/%d/%Y""},
            ],
            test_output=[
                (
                    ""date"",
                    lambda t: abs(datetime.now() - datetime.strptime(t, ""%Y-%m-%d""))
                    < timedelta(days=8),  # 7 days difference + 1 day error margin.
                ),
                (
                    ""date"",
                    lambda t: abs(datetime.now() - datetime.strptime(t, ""%m/%d/%Y""))
                    < timedelta(days=8),
                    # 7 days difference + 1 day error margin.
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        try:
            offset = int(input_data.offset)
        except ValueError:
            offset = 0
        current_date = datetime.now() - timedelta(days=offset)
        yield ""date"", current_date.strftime(input_data.format)


class GetCurrentDateAndTimeBlock(Block):
    class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current date and time""
        )
        format: str = SchemaField(
            description=""Format of the date and time to output"",
            default=""%Y-%m-%d %H:%M:%S"",
        )

    class Output(BlockSchema):
        date_time: str = SchemaField(
            description=""Current date and time in the specified format (default: YYYY-MM-DD HH:MM:SS)""
        )

    def __init__(self):
        super().__init__(
            id=""716a67b3-6760-42e7-86dc-18645c6e00fc"",
            description=""This block outputs the current date and time."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentDateAndTimeBlock.Input,
            output_schema=GetCurrentDateAndTimeBlock.Output,
            test_input=[
                {""trigger"": ""Hello""},
            ],
            test_output=[
                (
                    ""date_time"",
                    lambda t: abs(
                        datetime.now() - datetime.strptime(t, ""%Y-%m-%d %H:%M:%S"")
                    )
                    < timedelta(seconds=10),  # 10 seconds error margin.
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_date_time = time.strftime(input_data.format)
        yield ""date_time"", current_date_time


class CountdownTimerBlock(Block):
    class Input(BlockSchema):
        input_message: Any = SchemaField(
            advanced=False,
            description=""Message to output after the timer finishes"",
            default=""timer finished"",
        )
        seconds: Union[int, str] = SchemaField(
            advanced=False, description=""Duration in seconds"", default=0
        )
        minutes: Union[int, str] = SchemaField(
            advanced=False, description=""Duration in minutes"", default=0
        )
        hours: Union[int, str] = SchemaField(
            advanced=False, description=""Duration in hours"", default=0
        )
        days: Union[int, str] = SchemaField(
            advanced=False, description=""Duration in days"", default=0
        )

    class Output(BlockSchema):
        output_message: Any = SchemaField(
            description=""Message after the timer finishes""
        )

    def __init__(self):
        super().__init__(
            id=""d67a9c52-5e4e-11e2-bcfd-0800200c9a71"",
            description=""This block triggers after a specified duration."",
            categories={BlockCategory.TEXT},
            input_schema=CountdownTimerBlock.Input,
            output_schema=CountdownTimerBlock.Output,
            test_input=[
                {""seconds"": 1},
                {""input_message"": ""Custom message""},
            ],
            test_output=[
                (""output_message"", ""timer finished""),
                (""output_message"", ""Custom message""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        seconds = int(input_data.seconds)
        minutes = int(input_data.minutes)
        hours = int(input_data.hours)
        days = int(input_data.days)

        total_seconds = seconds + minutes * 60 + hours * 3600 + days * 86400

        time.sleep(total_seconds)
        yield ""output_message"", input_data.input_message","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
91,"import os
import subprocess

directory = os.path.dirname(os.path.realpath(__file__))

BACKEND_DIR = "".""
LIBS_DIR = ""../autogpt_libs""
TARGET_DIRS = [BACKEND_DIR, LIBS_DIR]


def run(*command: str) -> None:
    print(f"">>>>> Running poetry run {' '.join(command)}"")
    subprocess.run([""poetry"", ""run""] + list(command), cwd=directory, check=True)


def lint():
    try:
        run(""ruff"", ""check"", *TARGET_DIRS, ""--exit-zero"")
        run(""ruff"", ""format"", ""--diff"", ""--check"", LIBS_DIR)
        run(""isort"", ""--diff"", ""--check"", ""--profile"", ""black"", BACKEND_DIR)
        run(""black"", ""--diff"", ""--check"", BACKEND_DIR)
        run(""pyright"", *TARGET_DIRS)
    except subprocess.CalledProcessError as e:
        print(""Lint failed, try running `poetry run format` to fix the issues: "", e)
        raise e


def format():
    run(""ruff"", ""check"", ""--fix"", *TARGET_DIRS)
    run(""ruff"", ""format"", LIBS_DIR)
    run(""isort"", ""--profile"", ""black"", BACKEND_DIR)
    run(""black"", BACKEND_DIR)
    run(""pyright"", *TARGET_DIRS)
","import os
import subprocess

directory = os.path.dirname(os.path.realpath(__file__))

BACKEND_DIR = "".""
LIBS_DIR = ""../autogpt_libs""
TARGET_DIRS = [BACKEND_DIR, LIBS_DIR]

def run(*command: str) -> None:
    print(f"">>>>> Running poetry run {' '.join(command)}"")
    subprocess.run([""poetry"", ""run""] + list(command), cwd=directory, check=True)

def lint():
    try:
        run(""ruff"", ""check"", *TARGET_DIRS, ""--exit-zero"")
        run(""ruff"", ""format"", ""--diff"", ""--check"", LIBS_DIR)
        run(""isort"", ""--diff"", ""--check"", ""--profile"", ""black"", BACKEND_DIR)
        run(""black"", ""--diff"", ""--check"", BACKEND_DIR)
        run(""pyright"", *TARGET_DIRS)
    except subprocess.CalledProcessError as e:
        print(""Lint failed, try running `poetry run format` to fix the issues: "", e)
        raise e

def format():
    run(""ruff"", ""check"", ""--fix"", *TARGET_DIRS)
    run(""ruff"", ""format"", LIBS_DIR)
    run(""isort"", ""--profile"", ""black"", BACKEND_DIR)
    run(""black"", BACKEND_DIR)
    run(""pyright"", *TARGET_DIRS)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
92,"# https://www.tutorialspoint.com/python3/bitwise_operators_example.htm


def binary_xor(a: int, b: int) -> str:
    """"""
    Take in 2 integers, convert them to binary,
    return a binary number that is the
    result of a binary xor operation on the integers provided.

    >>> binary_xor(25, 32)
    '0b111001'
    >>> binary_xor(37, 50)
    '0b010111'
    >>> binary_xor(21, 30)
    '0b01011'
    >>> binary_xor(58, 73)
    '0b1110011'
    >>> binary_xor(0, 255)
    '0b11111111'
    >>> binary_xor(256, 256)
    '0b000000000'
    >>> binary_xor(0, -1)
    Traceback (most recent call last):
        ...
    ValueError: the value of both inputs must be positive
    >>> binary_xor(0, 1.1)
    Traceback (most recent call last):
        ...
    TypeError: 'float' object cannot be interpreted as an integer
    >>> binary_xor(""0"", ""1"")
    Traceback (most recent call last):
        ...
    TypeError: '<' not supported between instances of 'str' and 'int'
    """"""
    if a < 0 or b < 0:
        raise ValueError(""the value of both inputs must be positive"")

    a_binary = str(bin(a))[2:]  # remove the leading ""0b""
    b_binary = str(bin(b))[2:]  # remove the leading ""0b""

    max_len = max(len(a_binary), len(b_binary))

    return ""0b"" + """".join(
        str(int(char_a != char_b))
        for char_a, char_b in zip(a_binary.zfill(max_len), b_binary.zfill(max_len))
    )


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","# https://www.tutorialspoint.com/python3/bitwise_operators_example.htm


def binary_xor(a: int, b: int) -> str:
    """"""
    Take in 2 integers, convert them to binary,
    return a binary number that is the
    result of a binary xor operation on the integers provided.

    >>> binary_xor(25, 32)
    '0b111001'
    >>> binary_xor(37, 50)
    '0b010111'
    >>> binary_xor(21, 30)
    '0b01011'
    >>> binary_xor(58, 73)
    '0b1110011'
    >>> binary_xor(0, 255)
    '0b11111111'
    >>> binary_xor(256, 256)
    '0b000000000'
    >>> binary_xor(0, -1)
    Traceback (most recent call last):
        ...
    ValueError: the value of both inputs must be positive
    >>> binary_xor(0, 1.1)
    Traceback (most recent call last):
        ...
    TypeError: 'float' object cannot be interpreted as an integer
    >>> binary_xor(""0"", ""1"")
    Traceback (most recent call last):
        ...
    TypeError: '<' not supported between instances of 'str' and 'int'
    """"""
    if a < 0 or b < 0:
        raise ValueError(""the value of both inputs must be positive"")
    a_binary = str(bin(a))[2:]  # remove the leading ""0b""
    b_binary = str(bin(b))[2:]  # remove the leading ""0b""
    max_len = max(len(a_binary), len(b_binary))
    return ""0b"" + """".join(
        str(int(char_a != char_b))
        for char_a, char_b in zip(a_binary.zfill(max_len), b_binary.zfill(max_len))
    )


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
93,"# https://www.geeksforgeeks.org/solve-crossword-puzzle/


def is_valid(
    puzzle: list[list[str]], word: str, row: int, col: int, vertical: bool
) -> bool:
    """"""
    Check if a word can be placed at the given position.

    >>> puzzle = [
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', '']
    ... ]
    >>> is_valid(puzzle, 'word', 0, 0, True)
    True
    >>> puzzle = [
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', '']
    ... ]
    >>> is_valid(puzzle, 'word', 0, 0, False)
    True
    """"""
    for i in range(len(word)):
        if vertical:
            if row + i >= len(puzzle) or puzzle[row + i][col] != """":
                return False
        elif col + i >= len(puzzle[0]) or puzzle[row][col + i] != """":
            return False
    return True


def place_word(
    puzzle: list[list[str]], word: str, row: int, col: int, vertical: bool
) -> None:
    """"""
    Place a word at the given position.

    >>> puzzle = [
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', '']
    ... ]
    >>> place_word(puzzle, 'word', 0, 0, True)
    >>> puzzle
    [['w', '', '', ''], ['o', '', '', ''], ['r', '', '', ''], ['d', '', '', '']]
    """"""
    for i, char in enumerate(word):
        if vertical:
            puzzle[row + i][col] = char
        else:
            puzzle[row][col + i] = char


def remove_word(
    puzzle: list[list[str]], word: str, row: int, col: int, vertical: bool
) -> None:
    """"""
    Remove a word from the given position.

    >>> puzzle = [
    ...     ['w', '', '', ''],
    ...     ['o', '', '', ''],
    ...     ['r', '', '', ''],
    ...     ['d', '', '', '']
    ... ]
    >>> remove_word(puzzle, 'word', 0, 0, True)
    >>> puzzle
    [['', '', '', ''], ['', '', '', ''], ['', '', '', ''], ['', '', '', '']]
    """"""
    for i in range(len(word)):
        if vertical:
            puzzle[row + i][col] = """"
        else:
            puzzle[row][col + i] = """"


def solve_crossword(puzzle: list[list[str]], words: list[str]) -> bool:
    """"""
    Solve the crossword puzzle using backtracking.

    >>> puzzle = [
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', '']
    ... ]

    >>> words = ['word', 'four', 'more', 'last']
    >>> solve_crossword(puzzle, words)
    True
    >>> puzzle = [
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', '']
    ... ]
    >>> words = ['word', 'four', 'more', 'paragraphs']
    >>> solve_crossword(puzzle, words)
    False
    """"""
    for row in range(len(puzzle)):
        for col in range(len(puzzle[0])):
            if puzzle[row][col] == """":
                for word in words:
                    for vertical in [True, False]:
                        if is_valid(puzzle, word, row, col, vertical):
                            place_word(puzzle, word, row, col, vertical)
                            words.remove(word)
                            if solve_crossword(puzzle, words):
                                return True
                            words.append(word)
                            remove_word(puzzle, word, row, col, vertical)
                return False
    return True


if __name__ == ""__main__"":
    PUZZLE = [[""""] * 3 for _ in range(3)]
    WORDS = [""cat"", ""dog"", ""car""]

    if solve_crossword(PUZZLE, WORDS):
        print(""Solution found:"")
        for row in PUZZLE:
            print("" "".join(row))
    else:
        print(""No solution found:"")
","# https://www.geeksforgeeks.org/solve-crossword-puzzle/



def is_valid(
    puzzle: list[list[str]], word: str, row: int, col: int, vertical: bool
) -> bool:
    """"""
    Check if a word can be placed at the given position.

    >>> puzzle = [
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', '']
    ... ]
    >>> is_valid(puzzle, 'word', 0, 0, True)
    True
    >>> puzzle = [
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', '']
    ... ]
    >>> is_valid(puzzle, 'word', 0, 0, False)
    True
    """"""
    for i in range(len(word)):
        if vertical:
            if row + i >= len(puzzle) or puzzle[row + i][col] != """":
                return False
        elif col + i >= len(puzzle[0]) or puzzle[row][col + i] != """":
            return False
    return True


def place_word(
    puzzle: list[list[str]], word: str, row: int, col: int, vertical: bool
) -> None:
    """"""
    Place a word at the given position.

    >>> puzzle = [
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', '']
    ... ]
    >>> place_word(puzzle, 'word', 0, 0, True)
    >>> puzzle
    [['w', '', '', ''], ['o', '', '', ''], ['r', '', '', ''], ['d', '', '', '']]
    """"""
    for i, char in enumerate(word):
        if vertical:
            puzzle[row + i][col] = char
        else:
            puzzle[row][col + i] = char


def remove_word(
    puzzle: list[list[str]], word: str, row: int, col: int, vertical: bool
) -> None:
    """"""
    Remove a word from the given position.

    >>> puzzle = [
    ...     ['w', '', '', ''],
    ...     ['o', '', '', ''],
    ...     ['r', '', '', ''],
    ...     ['d', '', '', '']
    ... ]
    >>> remove_word(puzzle, 'word', 0, 0, True)
    >>> puzzle
    [['', '', '', ''], ['', '', '', ''], ['', '', '', ''], ['', '', '', '']]
    """"""
    for i in range(len(word)):
        if vertical:
            puzzle[row + i][col] = """"
        else:
            puzzle[row][col + i] = """"


def solve_crossword(puzzle: list[list[str]], words: list[str]) -> bool:
    """"""
    Solve the crossword puzzle using backtracking.

    >>> puzzle = [
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', '']
    ... ]

    >>> words = ['word', 'four', 'more', 'last']
    >>> solve_crossword(puzzle, words)
    True
    >>> puzzle = [
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', ''],
    ...     ['', '', '', '']
    ... ]
    >>> words = ['word', 'four', 'more', 'paragraphs']
    >>> solve_crossword(puzzle, words)
    False
    """"""
    for row in range(len(puzzle)):
        for col in range(len(puzzle[0])):
            if puzzle[row][col] == """":
                for word in words:
                    for vertical in [True, False]:
                        if is_valid(puzzle, word, row, col, vertical):
                            place_word(puzzle, word, row, col, vertical)
                            words.remove(word)
                            if solve_crossword(puzzle, words):
                                return True
                            words.append(word)
                            remove_word(puzzle, word, row, col, vertical)
                return False
    return True

if __name__ == ""__main__"":
    PUZZLE = [[""""] * 3 for _ in range(3)]
    WORDS = [""cat"", ""dog"", ""car""]

    if solve_crossword(PUZZLE, WORDS):
        print(""Solution found:"")
        for row in PUZZLE:
            print("" "".join(row))
    else:
        print(""No solution found:"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
94,"""""""
Minimax helps to achieve maximum score in a game by checking all possible moves
depth is current depth in game tree.

nodeIndex is index of current node in scores[].
if move is of maximizer return true else false
leaves of game tree is stored in scores[]
height is maximum height of Game tree
""""""

from __future__ import annotations

import math


def minimax(
    depth: int, node_index: int, is_max: bool, scores: list[int], height: float
) -> int:
    """"""
    This function implements the minimax algorithm, which helps achieve the optimal
    score for a player in a two-player game by checking all possible moves.
    If the player is the maximizer, then the score is maximized.
    If the player is the minimizer, then the score is minimized.

    Parameters:
    - depth: Current depth in the game tree.
    - node_index: Index of the current node in the scores list.
    - is_max: A boolean indicating whether the current move
              is for the maximizer (True) or minimizer (False).
    - scores: A list containing the scores of the leaves of the game tree.
    - height: The maximum height of the game tree.

    Returns:
    - An integer representing the optimal score for the current player.

    >>> import math
    >>> scores = [90, 23, 6, 33, 21, 65, 123, 34423]
    >>> height = math.log(len(scores), 2)
    >>> minimax(0, 0, True, scores, height)
    65
    >>> minimax(-1, 0, True, scores, height)
    Traceback (most recent call last):
        ...
    ValueError: Depth cannot be less than 0
    >>> minimax(0, 0, True, [], 2)
    Traceback (most recent call last):
        ...
    ValueError: Scores cannot be empty
    >>> scores = [3, 5, 2, 9, 12, 5, 23, 23]
    >>> height = math.log(len(scores), 2)
    >>> minimax(0, 0, True, scores, height)
    12
    """"""

    if depth < 0:
        raise ValueError(""Depth cannot be less than 0"")
    if len(scores) == 0:
        raise ValueError(""Scores cannot be empty"")

    # Base case: If the current depth equals the height of the tree,
    # return the score of the current node.
    if depth == height:
        return scores[node_index]

    # If it's the maximizer's turn, choose the maximum score
    # between the two possible moves.
    if is_max:
        return max(
            minimax(depth + 1, node_index * 2, False, scores, height),
            minimax(depth + 1, node_index * 2 + 1, False, scores, height),
        )

    # If it's the minimizer's turn, choose the minimum score
    # between the two possible moves.
    return min(
        minimax(depth + 1, node_index * 2, True, scores, height),
        minimax(depth + 1, node_index * 2 + 1, True, scores, height),
    )


def main() -> None:
    # Sample scores and height calculation
    scores = [90, 23, 6, 33, 21, 65, 123, 34423]
    height = math.log(len(scores), 2)

    # Calculate and print the optimal value using the minimax algorithm
    print(""Optimal value : "", end="""")
    print(minimax(0, 0, True, scores, height))


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
    main()
","""""""
Minimax helps to achieve maximum score in a game by checking all possible moves
depth is current depth in game tree.

nodeIndex is index of current node in scores[].
if move is of maximizer return true else false
leaves of game tree is stored in scores[]
height is maximum height of Game tree
""""""

from __future__ import annotations

import math

def minimax(
    depth: int, node_index: int, is_max: bool, scores: list[int], height: float
) -> int:
    """"""
    This function implements the minimax algorithm, which helps achieve the optimal
    score for a player in a two-player game by checking all possible moves.
    If the player is the maximizer, then the score is maximized.
    If the player is the minimizer, then the score is minimized.

    Parameters:
    - depth: Current depth in the game tree.
    - node_index: Index of the current node in the scores list.
    - is_max: A boolean indicating whether the current move
              is for the maximizer (True) or minimizer (False).
    - scores: A list containing the scores of the leaves of the game tree.
    - height: The maximum height of the game tree.

    Returns:
    - An integer representing the optimal score for the current player.

    >>> import math
    >>> scores = [90, 23, 6, 33, 21, 65, 123, 34423]
    >>> height = math.log(len(scores), 2)
    >>> minimax(0, 0, True, scores, height)
    65
    >>> minimax(-1, 0, True, scores, height)
    Traceback (most recent call last):
        ...
    ValueError: Depth cannot be less than 0
    >>> minimax(0, 0, True, [], 2)
    Traceback (most recent call last):
        ...
    ValueError: Scores cannot be empty
    >>> scores = [3, 5, 2, 9, 12, 5, 23, 23]
    >>> height = math.log(len(scores), 2)
    >>> minimax(0, 0, True, scores, height)
    12
    """"""
    if depth < 0:
        raise ValueError(""Depth cannot be less than 0"")

    if len(scores) == 0:
        raise ValueError(""Scores cannot be empty"")

    # Base case: If the current depth equals the height of the tree,
    # return the score of the current node.
    if depth == height:
        return scores[node_index]

    # If it's the maximizer's turn, choose the maximum score
    # between the two possible moves.
    if is_max:
        return max(
            minimax(depth + 1, node_index * 2, False, scores, height),
            minimax(depth + 1, node_index * 2 + 1, False, scores, height),
        )

    # If it's the minimizer's turn, choose the minimum score
    # between the two possible moves.
    return min(
        minimax(depth + 1, node_index * 2, True, scores, height),
        minimax(depth + 1, node_index * 2 + 1, True, scores, height),
    )

def main() -> None:
    # Sample scores and height calculation
    scores = [90, 23, 6, 33, 21, 65, 123, 34423]
    height = math.log(len(scores), 2)

    # Calculate and print the optimal value using the minimax algorithm
    print(""Optimal value : "", end="""")
    print(minimax(0, 0, True, scores, height))

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
    main()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
95,"from typing import Any
from uuid import UUID

import pytest

from backend.blocks.basic import AgentInputBlock, AgentOutputBlock, StoreValueBlock
from backend.data.block import BlockSchema
from backend.data.graph import Graph, Link, Node
from backend.data.model import SchemaField
from backend.data.user import DEFAULT_USER_ID
from backend.server.model import CreateGraph
from backend.util.test import SpinTestServer


@pytest.mark.asyncio(scope=""session"")
async def test_graph_creation(server: SpinTestServer):
    """"""
    Test the creation of a graph with nodes and links.

    This test ensures that:
    1. A graph can be successfully created with valid connections.
    2. The created graph has the correct structure and properties.

    Args:
        server (SpinTestServer): The test server instance.
    """"""
    value_block = StoreValueBlock().id
    input_block = AgentInputBlock().id

    graph = Graph(
        id=""test_graph"",
        name=""TestGraph"",
        description=""Test graph"",
        nodes=[
            Node(id=""node_1"", block_id=value_block),
            Node(id=""node_2"", block_id=input_block, input_default={""name"": ""input""}),
            Node(id=""node_3"", block_id=value_block),
        ],
        links=[
            Link(
                source_id=""node_1"",
                sink_id=""node_2"",
                source_name=""output"",
                sink_name=""name"",
            ),
        ],
    )
    create_graph = CreateGraph(graph=graph)
    created_graph = await server.agent_server.test_create_graph(
        create_graph, DEFAULT_USER_ID
    )

    assert UUID(created_graph.id)
    assert created_graph.name == ""TestGraph""

    assert len(created_graph.nodes) == 3
    assert UUID(created_graph.nodes[0].id)
    assert UUID(created_graph.nodes[1].id)
    assert UUID(created_graph.nodes[2].id)

    nodes = created_graph.nodes
    links = created_graph.links
    assert len(links) == 1
    assert links[0].source_id != links[0].sink_id
    assert links[0].source_id in {nodes[0].id, nodes[1].id, nodes[2].id}
    assert links[0].sink_id in {nodes[0].id, nodes[1].id, nodes[2].id}


@pytest.mark.asyncio(scope=""session"")
async def test_get_input_schema(server: SpinTestServer):
    """"""
    Test the get_input_schema method of a created graph.

    This test ensures that:
    1. A graph can be created with a single node.
    2. The input schema of the created graph is correctly generated.
    3. The input schema contains the expected input name and node id.

    Args:
        server (SpinTestServer): The test server instance.
    """"""
    value_block = StoreValueBlock().id
    input_block = AgentInputBlock().id
    output_block = AgentOutputBlock().id

    graph = Graph(
        name=""TestInputSchema"",
        description=""Test input schema"",
        nodes=[
            Node(
                id=""node_0_a"",
                block_id=input_block,
                input_default={""name"": ""in_key_a"", ""title"": ""Key A"", ""value"": ""A""},
                metadata={""id"": ""node_0_a""},
            ),
            Node(
                id=""node_0_b"",
                block_id=input_block,
                input_default={""name"": ""in_key_b"", ""advanced"": True},
                metadata={""id"": ""node_0_b""},
            ),
            Node(id=""node_1"", block_id=value_block, metadata={""id"": ""node_1""}),
            Node(
                id=""node_2"",
                block_id=output_block,
                input_default={
                    ""name"": ""out_key"",
                    ""description"": ""This is an output key"",
                },
                metadata={""id"": ""node_2""},
            ),
        ],
        links=[
            Link(
                source_id=""node_0_a"",
                sink_id=""node_1"",
                source_name=""result"",
                sink_name=""input"",
            ),
            Link(
                source_id=""node_0_b"",
                sink_id=""node_1"",
                source_name=""result"",
                sink_name=""input"",
            ),
            Link(
                source_id=""node_1"",
                sink_id=""node_2"",
                source_name=""output"",
                sink_name=""value"",
            ),
        ],
    )

    create_graph = CreateGraph(graph=graph)
    created_graph = await server.agent_server.test_create_graph(
        create_graph, DEFAULT_USER_ID
    )

    class ExpectedInputSchema(BlockSchema):
        in_key_a: Any = SchemaField(title=""Key A"", default=""A"", advanced=False)
        in_key_b: Any = SchemaField(title=""in_key_b"", advanced=True)

    class ExpectedOutputSchema(BlockSchema):
        out_key: Any = SchemaField(
            description=""This is an output key"",
            title=""out_key"",
            advanced=False,
        )

    input_schema = created_graph.input_schema
    input_schema[""title""] = ""ExpectedInputSchema""
    assert input_schema == ExpectedInputSchema.jsonschema()

    output_schema = created_graph.output_schema
    output_schema[""title""] = ""ExpectedOutputSchema""
    assert output_schema == ExpectedOutputSchema.jsonschema()
","from typing import Any
from uuid import UUID

import pytest

from backend.blocks.basic import AgentInputBlock, AgentOutputBlock, StoreValueBlock
from backend.data.block import BlockSchema
from backend.data.graph import Graph, Link, Node
from backend.data.model import SchemaField
from backend.data.user import DEFAULT_USER_ID
from backend.server.model import CreateGraph
from backend.util.test import SpinTestServer


@pytest.mark.asyncio(scope=""session"")
async def test_graph_creation(server: SpinTestServer):
    """"""
    Test the creation of a graph with nodes and links.

    This test ensures that:
    1. A graph can be successfully created with valid connections.
    2. The created graph has the correct structure and properties.

    Args:
        server (SpinTestServer): The test server instance.
    """"""
    value_block = StoreValueBlock().id
    input_block = AgentInputBlock().id

    graph = Graph(
        id=""test_graph"",
        name=""TestGraph"",
        description=""Test graph"",
        nodes=[
            Node(id=""node_1"", block_id=value_block),
            Node(id=""node_2"", block_id=input_block, input_default={""name"": ""input""}),
            Node(id=""node_3"", block_id=value_block),
        ],
        links=[
            Link(
                source_id=""node_1"",
                sink_id=""node_2"",
                source_name=""output"",
                sink_name=""name"",
            ),
        ],
    )

    create_graph = CreateGraph(graph=graph)
    created_graph = await server.agent_server.test_create_graph(
        create_graph, DEFAULT_USER_ID
    )

    assert UUID(created_graph.id)
    assert created_graph.name == ""TestGraph""

    assert len(created_graph.nodes) == 3
    assert UUID(created_graph.nodes[0].id)
    assert UUID(created_graph.nodes[1].id)
    assert UUID(created_graph.nodes[2].id)

    nodes = created_graph.nodes
    links = created_graph.links

    assert len(links) == 1
    assert links[0].source_id != links[0].sink_id
    assert links[0].source_id in {nodes[0].id, nodes[1].id, nodes[2].id}
    assert links[0].sink_id in {nodes[0].id, nodes[1].id, nodes[2].id}


@pytest.mark.asyncio(scope=""session"")
async def test_get_input_schema(server: SpinTestServer):
    """"""
    Test the get_input_schema method of a created graph.

    This test ensures that:
    1. A graph can be created with a single node.
    2. The input schema of the created graph is correctly generated.
    3. The input schema contains the expected input name and node id.

    Args:
        server (SpinTestServer): The test server instance.
    """"""
    value_block = StoreValueBlock().id
    input_block = AgentInputBlock().id
    output_block = AgentOutputBlock().id

    graph = Graph(
        name=""TestInputSchema"",
        description=""Test input schema"",
        nodes=[
            Node(
                id=""node_0_a"",
                block_id=input_block,
                input_default={""name"": ""in_key_a"", ""title"": ""Key A"", ""value"": ""A""},
                metadata={""id"": ""node_0_a""},
            ),
            Node(
                id=""node_0_b"",
                block_id=input_block,
                input_default={""name"": ""in_key_b"", ""advanced"": True},
                metadata={""id"": ""node_0_b""},
            ),
            Node(id=""node_1"", block_id=value_block, metadata={""id"": ""node_1""}),
            Node(
                id=""node_2"",
                block_id=output_block,
                input_default={
                    ""name"": ""out_key"",
                    ""description"": ""This is an output key"",
                },
                metadata={""id"": ""node_2""},
            ),
        ],
        links=[
            Link(
                source_id=""node_0_a"",
                sink_id=""node_1"",
                source_name=""result"",
                sink_name=""input"",
            ),
            Link(
                source_id=""node_0_b"",
                sink_id=""node_1"",
                source_name=""result"",
                sink_name=""input"",
            ),
            Link(
                source_id=""node_1"",
                sink_id=""node_2"",
                source_name=""output"",
                sink_name=""value"",
            ),
        ],
    )

    create_graph = CreateGraph(graph=graph)
    created_graph = await server.agent_server.test_create_graph(
        create_graph, DEFAULT_USER_ID
    )

    class ExpectedInputSchema(BlockSchema):
        in_key_a: Any = SchemaField(title=""Key A"", default=""A"", advanced=False)
        in_key_b: Any = SchemaField(title=""in_key_b"", advanced=True)

    class ExpectedOutputSchema(BlockSchema):
        out_key: Any = SchemaField(
            description=""This is an output key"",
            title=""out_key"",
            advanced=False,
        )

    input_schema = created_graph.input_schema
    input_schema[""title""] = ""ExpectedInputSchema""
    assert input_schema == ExpectedInputSchema.jsonschema()

    output_schema = created_graph.output_schema
    output_schema[""title""] = ""ExpectedOutputSchema""
    assert output_schema == ExpectedOutputSchema.jsonschema()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
96,"from typing import Optional

from pydantic import BaseModel

from backend.data.model import SchemaField


class TextSettings(BaseModel):
    max_characters: int = SchemaField(
        default=1000,
        description=""Maximum number of characters to return"",
        placeholder=""1000"",
    )
    include_html_tags: bool = SchemaField(
        default=False,
        description=""Whether to include HTML tags in the text"",
        placeholder=""False"",
    )


class HighlightSettings(BaseModel):
    num_sentences: int = SchemaField(
        default=3,
        description=""Number of sentences per highlight"",
        placeholder=""3"",
    )
    highlights_per_url: int = SchemaField(
        default=3,
        description=""Number of highlights per URL"",
        placeholder=""3"",
    )


class SummarySettings(BaseModel):
    query: Optional[str] = SchemaField(
        default="""",
        description=""Query string for summarization"",
        placeholder=""Enter query"",
    )


class ContentSettings(BaseModel):
    text: TextSettings = SchemaField(
        default=TextSettings(),
        description=""Text content settings"",
    )
    highlights: HighlightSettings = SchemaField(
        default=HighlightSettings(),
        description=""Highlight settings"",
    )
    summary: SummarySettings = SchemaField(
        default=SummarySettings(),
        description=""Summary settings"",
    )
","from typing import Optional
from pydantic import BaseModel
from backend.data.model import SchemaField

class TextSettings(BaseModel):
    max_characters: int = SchemaField(
        default=1000,
        description=""Maximum number of characters to return"",
        placeholder=""1000"",
    )
    include_html_tags: bool = SchemaField(
        default=False,
        description=""Whether to include HTML tags in the text"",
        placeholder=""False"",
    )

class HighlightSettings(BaseModel):
    num_sentences: int = SchemaField(
        default=3,
        description=""Number of sentences per highlight"",
        placeholder=""3"",
    )
    highlights_per_url: int = SchemaField(
        default=3,
        description=""Number of highlights per URL"",
        placeholder=""3"",
    )

class SummarySettings(BaseModel):
    query: Optional[str] = SchemaField(
        default="""",
        description=""Query string for summarization"",
        placeholder=""Enter query"",
    )

class ContentSettings(BaseModel):
    text: TextSettings = SchemaField(
        default=TextSettings(),
        description=""Text content settings"",
    )
    highlights: HighlightSettings = SchemaField(
        default=HighlightSettings(),
        description=""Highlight settings"",
    )
    summary: SummarySettings = SchemaField(
        default=SummarySettings(),
        description=""Summary settings"",
    )","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
98,"# -*- coding: utf-8 -*-

import unittest

from validate.format import error_message
from validate.format import get_categories_content
from validate.format import check_alphabetical_order
from validate.format import check_title
from validate.format import check_description, max_description_length
from validate.format import check_auth, auth_keys
from validate.format import check_https, https_keys
from validate.format import check_cors, cors_keys
from validate.format import check_entry
from validate.format import check_file_format, min_entries_per_category, num_segments


class TestValidadeFormat(unittest.TestCase):
    
    def test_error_message_return_and_return_type(self):
        line_num_unity = 1
        line_num_ten = 10
        line_num_hundred = 100
        line_num_thousand = 1000

        msg = 'This is a unit test'

        err_msg_unity = error_message(line_num_unity, msg)
        err_msg_ten = error_message(line_num_ten, msg)
        err_msg_hundred = error_message(line_num_hundred, msg)
        err_msg_thousand = error_message(line_num_thousand, msg)

        self.assertIsInstance(err_msg_unity, str)
        self.assertIsInstance(err_msg_ten, str)
        self.assertIsInstance(err_msg_hundred, str)
        self.assertIsInstance(err_msg_thousand, str)

        self.assertEqual(err_msg_unity, '(L002) This is a unit test')
        self.assertEqual(err_msg_ten, '(L011) This is a unit test')
        self.assertEqual(err_msg_hundred, '(L101) This is a unit test')
        self.assertEqual(err_msg_thousand, '(L1001) This is a unit test')

    def test_if_get_categories_content_return_correct_data_of_categories(self):
        fake_contents = [
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '',
            '### B',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [BA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |'
        ]

        result = get_categories_content(fake_contents)
        self.assertIsInstance(result, tuple)

        categories, category_line_num = result
        self.assertIsInstance(categories, dict)
        self.assertIsInstance(category_line_num, dict)

        expected_result = ({'A': ['AA', 'AB'], 'B': ['BA', 'BB']}, {'A': 0, 'B': 6})

        for res, ex_res in zip(result, expected_result):

            with self.subTest():
                self.assertEqual(res, ex_res)

    def test_if_check_alphabetical_order_return_correct_msg_error(self):
        correct_lines = [
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '',
            '### B',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [BA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |'
        ]

        incorrect_lines = [
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '',
            '### B',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [BB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |'
        ]


        err_msgs_1 = check_alphabetical_order(correct_lines)
        err_msgs_2 = check_alphabetical_order(incorrect_lines)

        self.assertIsInstance(err_msgs_1, list)
        self.assertIsInstance(err_msgs_2, list)

        self.assertEqual(len(err_msgs_1), 0)
        self.assertEqual(len(err_msgs_2), 2)

        expected_err_msgs = [
            '(L001) A category is not alphabetical order',
            '(L007) B category is not alphabetical order'
        ]

        for err_msg, ex_err_msg in zip(err_msgs_2, expected_err_msgs):

            with self.subTest():
                self.assertEqual(err_msg, ex_err_msg)
    
    def test_check_title_with_correct_title(self):
        raw_title = '[A](https://www.ex.com)'

        err_msgs = check_title(0, raw_title)

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 0)
        self.assertEqual(err_msgs, [])

    def test_check_title_with_markdown_syntax_incorrect(self):
        raw_title = '[A(https://www.ex.com)'

        err_msgs = check_title(0, raw_title)

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        
        err_msg = err_msgs[0]
        expected_err_msg = '(L001) Title syntax should be ""[TITLE](LINK)""'

        self.assertEqual(err_msg, expected_err_msg)

    def test_check_title_with_api_at_the_end_of_the_title(self):
        raw_title = '[A API](https://www.ex.com)'

        err_msgs = check_title(0, raw_title)
        
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        
        err_msg = err_msgs[0]
        expected_err_msg = '(L001) Title should not end with ""... API"". Every entry is an API here!'

        self.assertEqual(err_msg, expected_err_msg)

    def test_check_description_with_correct_description(self):
        desc = 'This is a fake description'

        err_msgs = check_description(0, desc)

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 0)
        self.assertEqual(err_msgs, [])
    
    def test_check_description_with_first_char_is_not_capitalized(self):
        desc = 'this is a fake description'

        err_msgs = check_description(0, desc)

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        
        err_msg = err_msgs[0]
        expected_err_msg = '(L001) first character of description is not capitalized'

        self.assertIsInstance(err_msg, str)
        self.assertEqual(err_msg, expected_err_msg)
    
    def test_check_description_with_punctuation_in_the_end(self):
        base_desc = 'This is a fake description'
        punctuation = r""""""!""#$%&'*+,-./:;<=>?@[\]^_`{|}~""""""
        desc_with_punc = [base_desc + punc for punc in punctuation]
        
        for desc in desc_with_punc:

            with self.subTest():
                err_msgs = check_description(0, desc)

                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 1)
        
                err_msg = err_msgs[0]
                expected_err_msg = f'(L001) description should not end with {desc[-1]}'

                self.assertIsInstance(err_msg, str)
                self.assertEqual(err_msg, expected_err_msg)

    def test_check_description_that_exceeds_the_character_limit(self):
        long_desc = 'Desc' * max_description_length
        long_desc_length = len(long_desc)

        err_msgs = check_description(0, long_desc)

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)

        err_msg = err_msgs[0]
        expected_err_msg = f'(L001) description should not exceed {max_description_length} characters (currently {long_desc_length})'

        self.assertIsInstance(err_msg, str)
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_auth_with_valid_auth(self):
        auth_valid = [f'`{auth}`' for auth in auth_keys if auth != 'No']
        auth_valid.append('No')

        for auth in auth_valid:
            with self.subTest():
                err_msgs = check_auth(0, auth)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 0)
                self.assertEqual(err_msgs, [])

    def test_check_auth_without_backtick(self):
        auth_without_backtick = [auth for auth in auth_keys if auth != 'No']

        for auth in auth_without_backtick:
            with self.subTest():
                err_msgs = check_auth(0, auth)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 1)

                err_msg = err_msgs[0]
                expected_err_msg = '(L001) auth value is not enclosed with `backticks`'

                self.assertIsInstance(err_msg, str)
                self.assertEqual(err_msg, expected_err_msg)

    def test_check_auth_with_invalid_auth(self):
        auth_invalid_without_backtick = ['Yes', 'yes', 'no', 'random', 'Unknown']
        auth_invalid_with_backtick = ['`Yes`', '`yes`', '`no`', '`random`', '`Unknown`']

        for auth in auth_invalid_without_backtick:
            with self.subTest():
                err_msgs = check_auth(0, auth)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 2)

                err_msg_1 = err_msgs[0]
                err_msg_2 = err_msgs[1]

                expected_err_msg_1 = f'(L001) auth value is not enclosed with `backticks`'
                expected_err_msg_2 = f'(L001) {auth} is not a valid Auth option'

                self.assertIsInstance(err_msg_1, str)
                self.assertIsInstance(err_msg_2, str)
                self.assertEqual(err_msg_1, expected_err_msg_1)
                self.assertEqual(err_msg_2, expected_err_msg_2)

        for auth in auth_invalid_with_backtick:
            with self.subTest():
                err_msgs = check_auth(0, auth)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 1)

                err_msg = err_msgs[0]
                expected_err_msg = f'(L001) {auth} is not a valid Auth option'

                self.assertIsInstance(err_msg, str)
                self.assertEqual(err_msg, expected_err_msg)

    def test_check_https_with_valid_https(self):
        for https in https_keys:
            with self.subTest():
                err_msgs = check_https(0, https)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 0)
                self.assertEqual(err_msgs, [])

    def test_check_https_with_invalid_https(self):
        invalid_https_keys = ['yes', 'no', 'Unknown', 'https', 'http']

        for https in invalid_https_keys:
            with self.subTest():
                err_msgs = check_https(0, https)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 1)

                err_msg = err_msgs[0]
                expected_err_msg = f'(L001) {https} is not a valid HTTPS option'

                self.assertIsInstance(err_msg, str)
                self.assertEqual(err_msg, expected_err_msg)

    def test_check_cors_with_valid_cors(self):
        for cors in cors_keys:
            with self.subTest():
                err_msgs = check_cors(0, cors)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 0)
                self.assertEqual(err_msgs, [])

    def test_check_cors_with_invalid_cors(self):
        invalid_cors_keys = ['yes', 'no', 'unknown', 'cors']

        for cors in invalid_cors_keys:
            with self.subTest():
                err_msgs = check_cors(0, cors)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 1)

                err_msg = err_msgs[0]
                expected_err_msg = f'(L001) {cors} is not a valid CORS option'

                self.assertIsInstance(err_msg, str)
                self.assertEqual(err_msg, expected_err_msg)

    def test_check_entry_with_correct_segments(self):
        correct_segments = ['[A](https://www.ex.com)', 'Desc', '`apiKey`', 'Yes', 'Yes']

        err_msgs = check_entry(0, correct_segments)
        
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 0)
        self.assertEqual(err_msgs, [])

    def test_check_entry_with_incorrect_segments(self):
        incorrect_segments = ['[A API](https://www.ex.com)', 'desc.', 'yes', 'yes', 'yes']

        err_msgs = check_entry(0, incorrect_segments)
        expected_err_msgs = [
            '(L001) Title should not end with ""... API"". Every entry is an API here!',
            '(L001) first character of description is not capitalized',
            '(L001) description should not end with .',
            '(L001) auth value is not enclosed with `backticks`',
            '(L001) yes is not a valid Auth option',
            '(L001) yes is not a valid HTTPS option',
            '(L001) yes is not a valid CORS option'
        ]

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 7)
        for err_msg in err_msgs:
            with self.subTest():
                self.assertIsInstance(err_msg, str)
        self.assertEqual(err_msgs, expected_err_msgs)

    def test_check_file_format_with_correct_format(self):
        correct_format = [
            '## Index',
            '* [A](#a)',
            '* [B](#b)',
            '',
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '',
            '### B',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [BA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |'
        ]

        err_msgs = check_file_format(lines=correct_format)

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 0)
        self.assertEqual(err_msgs, [])

    def test_check_file_format_with_category_header_not_added_to_index(self):
        incorrect_format = [
            '## Index',
            '',
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
        ]

        err_msgs = check_file_format(lines=incorrect_format)
        expected_err_msg = '(L003) category header (A) not added to Index section'

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_file_format_with_category_without_min_entries(self):
        incorrect_format = [
            '## Index',
            '* [A](#a)',
            '* [B](#b)',
            '',
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '',
            '### B',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [BA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |'
        ]

        category_with_err = 'A'
        num_in_category = 1

        err_msgs = check_file_format(lines=incorrect_format)
        expected_err_msg = f'(L005) {category_with_err} category does not have the minimum {min_entries_per_category} entries (only has {num_in_category})'

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_file_format_entry_without_all_necessary_columns(self):
        incorrect_format = [
            '## Index',
            '* [A](#a)',
            '',
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AB](https://www.ex.com) | Desc | `apiKey` |',  # missing https and cors
            '| [AC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
        ]

        current_segments_num = 3

        err_msgs = check_file_format(lines=incorrect_format)
        expected_err_msg = f'(L008) entry does not have all the required columns (have {current_segments_num}, need {num_segments})'

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_file_format_without_1_space_between_the_segments(self):
        incorrect_format = [
            '## Index',
            '* [A](#a)',
            '',
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc |`apiKey`| Yes | Yes |',  # space between segment of auth column missing
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
        ]

        err_msgs = check_file_format(lines=incorrect_format)
        expected_err_msg = f'(L007) each segment must start and end with exactly 1 space'

        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        self.assertEqual(err_msg, expected_err_msg)
","# -*- coding: utf-8 -*-
import unittest
from validate.format import error_message
from validate.format import get_categories_content
from validate.format import check_alphabetical_order
from validate.format import check_title
from validate.format import check_description, max_description_length
from validate.format import check_auth, auth_keys
from validate.format import check_https, https_keys
from validate.format import check_cors, cors_keys
from validate.format import check_entry
from validate.format import check_file_format, min_entries_per_category, num_segments

class TestValidateFormat(unittest.TestCase):

    def test_error_message_return_and_return_type(self):
        line_num_unity = 1
        line_num_ten = 10
        line_num_hundred = 100
        line_num_thousand = 1000

        msg = 'This is a unit test'

        err_msg_unity = error_message(line_num_unity, msg)
        err_msg_ten = error_message(line_num_ten, msg)
        err_msg_hundred = error_message(line_num_hundred, msg)
        err_msg_thousand = error_message(line_num_thousand, msg)

        self.assertIsInstance(err_msg_unity, str)
        self.assertIsInstance(err_msg_ten, str)
        self.assertIsInstance(err_msg_hundred, str)
        self.assertIsInstance(err_msg_thousand, str)

        self.assertEqual(err_msg_unity, '(L002) This is a unit test')
        self.assertEqual(err_msg_ten, '(L011) This is a unit test')
        self.assertEqual(err_msg_hundred, '(L101) This is a unit test')
        self.assertEqual(err_msg_thousand, '(L1001) This is a unit test')

    def test_if_get_categories_content_return_correct_data_of_categories(self):
        fake_contents = [
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '',
            '### B',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [BA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |'
        ]
        result = get_categories_content(fake_contents)
        self.assertIsInstance(result, tuple)

        categories, category_line_num = result
        self.assertIsInstance(categories, dict)
        self.assertIsInstance(category_line_num, dict)

        expected_result = ({'A': ['AA', 'AB'], 'B': ['BA', 'BB']}, {'A': 0, 'B': 6})
        for res, ex_res in zip(result, expected_result):
            with self.subTest():
                self.assertEqual(res, ex_res)

    def test_if_check_alphabetical_order_return_correct_msg_error(self):
        correct_lines = [
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '',
            '### B',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [BA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |'
        ]
        incorrect_lines = [
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '',
            '### B',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [BB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |'
        ]
        err_msgs_1 = check_alphabetical_order(correct_lines)
        err_msgs_2 = check_alphabetical_order(incorrect_lines)

        self.assertIsInstance(err_msgs_1, list)
        self.assertIsInstance(err_msgs_2, list)

        self.assertEqual(len(err_msgs_1), 0)
        self.assertEqual(len(err_msgs_2), 2)

        expected_err_msgs = [
            '(L001) A category is not alphabetical order',
            '(L007) B category is not alphabetical order'
        ]
        for err_msg, ex_err_msg in zip(err_msgs_2, expected_err_msgs):
            with self.subTest():
                self.assertEqual(err_msg, ex_err_msg)

    def test_check_title_with_correct_title(self):
        raw_title = '[A](https://www.ex.com)'
        err_msgs = check_title(0, raw_title)
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 0)
        self.assertEqual(err_msgs, [])

    def test_check_title_with_markdown_syntax_incorrect(self):
        raw_title = '[A(https://www.ex.com)'
        err_msgs = check_title(0, raw_title)
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        expected_err_msg = '(L001) Title syntax should be ""[TITLE](LINK)""'
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_title_with_api_at_the_end_of_the_title(self):
        raw_title = '[A API](https://www.ex.com)'
        err_msgs = check_title(0, raw_title)
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        expected_err_msg = '(L001) Title should not end with ""... API"". Every entry is an API here!'
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_description_with_correct_description(self):
        desc = 'This is a fake description'
        err_msgs = check_description(0, desc)
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 0)
        self.assertEqual(err_msgs, [])

    def test_check_description_with_first_char_is_not_capitalized(self):
        desc = 'this is a fake description'
        err_msgs = check_description(0, desc)
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        expected_err_msg = '(L001) first character of description is not capitalized'
        self.assertIsInstance(err_msg, str)
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_description_with_punctuation_in_the_end(self):
        base_desc = 'This is a fake description'
        punctuation = r""""""!\""#$%&'*+,-./:;<=>?@[\\]^_`{|}~""""""
        desc_with_punc = [base_desc + punc for punc in punctuation]
        for desc in desc_with_punc:
            with self.subTest():
                err_msgs = check_description(0, desc)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 1)
                err_msg = err_msgs[0]
                expected_err_msg = f'(L001) description should not end with {desc[-1]}'
                self.assertIsInstance(err_msg, str)
                self.assertEqual(err_msg, expected_err_msg)

    def test_check_description_that_exceeds_the_character_limit(self):
        long_desc = 'Desc' * max_description_length
        long_desc_length = len(long_desc)
        err_msgs = check_description(0, long_desc)
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        expected_err_msg = f'(L001) description should not exceed {max_description_length} characters (currently {long_desc_length})'
        self.assertIsInstance(err_msg, str)
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_auth_with_valid_auth(self):
        auth_valid = [f'`{auth}`' for auth in auth_keys if auth != 'No']
        auth_valid.append('No')
        for auth in auth_valid:
            with self.subTest():
                err_msgs = check_auth(0, auth)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 0)
                self.assertEqual(err_msgs, [])

    def test_check_auth_without_backtick(self):
        auth_without_backtick = [auth for auth in auth_keys if auth != 'No']
        for auth in auth_without_backtick:
            with self.subTest():
                err_msgs = check_auth(0, auth)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 1)
                err_msg = err_msgs[0]
                expected_err_msg = '(L001) auth value is not enclosed with `backticks`'
                self.assertIsInstance(err_msg, str)
                self.assertEqual(err_msg, expected_err_msg)

    def test_check_auth_with_invalid_auth(self):
        auth_invalid_without_backtick = ['Yes', 'yes', 'no', 'random', 'Unknown']
        auth_invalid_with_backtick = ['`Yes`', '`yes`', '`no`', '`random`', '`Unknown`']
        for auth in auth_invalid_without_backtick:
            with self.subTest():
                err_msgs = check_auth(0, auth)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 2)
                err_msg_1 = err_msgs[0]
                err_msg_2 = err_msgs[1]
                expected_err_msg_1 = f'(L001) auth value is not enclosed with `backticks`'
                expected_err_msg_2 = f'(L001) {auth} is not a valid Auth option'
                self.assertIsInstance(err_msg_1, str)
                self.assertIsInstance(err_msg_2, str)
                self.assertEqual(err_msg_1, expected_err_msg_1)
                self.assertEqual(err_msg_2, expected_err_msg_2)

        for auth in auth_invalid_with_backtick:
            with self.subTest():
                err_msgs = check_auth(0, auth)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 1)
                err_msg = err_msgs[0]
                expected_err_msg = f'(L001) {auth} is not a valid Auth option'
                self.assertIsInstance(err_msg, str)
                self.assertEqual(err_msg, expected_err_msg)

    def test_check_https_with_valid_https(self):
        for https in https_keys:
            with self.subTest():
                err_msgs = check_https(0, https)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 0)
                self.assertEqual(err_msgs, [])

    def test_check_https_with_invalid_https(self):
        invalid_https_keys = ['yes', 'no', 'Unknown', 'https', 'http']
        for https in invalid_https_keys:
            with self.subTest():
                err_msgs = check_https(0, https)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 1)
                err_msg = err_msgs[0]
                expected_err_msg = f'(L001) {https} is not a valid HTTPS option'
                self.assertIsInstance(err_msg, str)
                self.assertEqual(err_msg, expected_err_msg)

    def test_check_cors_with_valid_cors(self):
        for cors in cors_keys:
            with self.subTest():
                err_msgs = check_cors(0, cors)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 0)
                self.assertEqual(err_msgs, [])

    def test_check_cors_with_invalid_cors(self):
        invalid_cors_keys = ['yes', 'no', 'unknown', 'cors']
        for cors in invalid_cors_keys:
            with self.subTest():
                err_msgs = check_cors(0, cors)
                self.assertIsInstance(err_msgs, list)
                self.assertEqual(len(err_msgs), 1)
                err_msg = err_msgs[0]
                expected_err_msg = f'(L001) {cors} is not a valid CORS option'
                self.assertIsInstance(err_msg, str)
                self.assertEqual(err_msg, expected_err_msg)

    def test_check_entry_with_correct_segments(self):
        correct_segments = ['[A](https://www.ex.com)', 'Desc', '`apiKey`', 'Yes', 'Yes']
        err_msgs = check_entry(0, correct_segments)
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 0)
        self.assertEqual(err_msgs, [])

    def test_check_entry_with_incorrect_segments(self):
        incorrect_segments = ['[A API](https://www.ex.com)', 'desc.', 'yes', 'yes', 'yes']
        err_msgs = check_entry(0, incorrect_segments)
        expected_err_msgs = [
            '(L001) Title should not end with ""... API"". Every entry is an API here!',
            '(L001) first character of description is not capitalized',
            '(L001) description should not end with .',
            '(L001) auth value is not enclosed with `backticks`',
            '(L001) yes is not a valid Auth option',
            '(L001) yes is not a valid HTTPS option',
            '(L001) yes is not a valid CORS option'
        ]
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 7)
        for err_msg in err_msgs:
            with self.subTest():
                self.assertIsInstance(err_msg, str)
        self.assertEqual(err_msgs, expected_err_msgs)

    def test_check_file_format_with_correct_format(self):
        correct_format = [
            '## Index',
            '* [A](#a)',
            '* [B](#b)',
            '',
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '',
            '### B',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [BA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |'
        ]
        err_msgs = check_file_format(lines=correct_format)
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 0)
        self.assertEqual(err_msgs, [])

    def test_check_file_format_with_category_header_not_added_to_index(self):
        incorrect_format = [
            '## Index',
            '',
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
        ]
        err_msgs = check_file_format(lines=incorrect_format)
        expected_err_msg = '(L003) category header (A) not added to Index section'
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_file_format_with_category_without_min_entries(self):
        incorrect_format = [
            '## Index',
            '* [A](#a)',
            '* [B](#b)',
            '',
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '',
            '### B',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [BA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [BC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |'
        ]
        category_with_err = 'A'
        num_in_category = 1
        err_msgs = check_file_format(lines=incorrect_format)
        expected_err_msg = f'(L005) {category_with_err} category does not have the minimum {min_entries_per_category} entries (only has {num_in_category})'
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_file_format_entry_without_all_necessary_columns(self):
        incorrect_format = [
            '## Index',
            '* [A](#a)',
            '',
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AB](https://www.ex.com) | Desc | `apiKey` |',  # missing https and cors
            '| [AC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
        ]
        current_segments_num = 3
        err_msgs = check_file_format(lines=incorrect_format)
        expected_err_msg = f'(L008) entry does not have all the required columns (have {current_segments_num}, need {num_segments})'
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        self.assertEqual(err_msg, expected_err_msg)

    def test_check_file_format_without_1_space_between_the_segments(self):
        incorrect_format = [
            '## Index',
            '* [A](#a)',
            '',
            '### A',
            'API | Description | Auth | HTTPS | CORS |',
            '|---|---|---|---|---|',
            '| [AA](https://www.ex.com) | Desc |`apiKey`| Yes | Yes |',  # space between segment of auth column missing
            '| [AB](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
            '| [AC](https://www.ex.com) | Desc | `apiKey` | Yes | Yes |',
        ]
        err_msgs = check_file_format(lines=incorrect_format)
        expected_err_msg = f'(L007) each segment must start and end with exactly 1 space'
        self.assertIsInstance(err_msgs, list)
        self.assertEqual(len(err_msgs), 1)
        err_msg = err_msgs[0]
        self.assertEqual(err_msg, expected_err_msg)","{'final_score': 99.46, 'length_similarity': 99.99, 'token_similarity': 98.21, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
99,"from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class WordCharacterCountBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""Input text to count words and characters"",
            placeholder=""Enter your text here"",
            advanced=False,
        )

    class Output(BlockSchema):
        word_count: int = SchemaField(description=""Number of words in the input text"")
        character_count: int = SchemaField(
            description=""Number of characters in the input text""
        )
        error: str = SchemaField(
            description=""Error message if the counting operation failed""
        )

    def __init__(self):
        super().__init__(
            id=""ab2a782d-22cf-4587-8a70-55b59b3f9f90"",
            description=""Counts the number of words and characters in a given text."",
            categories={BlockCategory.TEXT},
            input_schema=WordCharacterCountBlock.Input,
            output_schema=WordCharacterCountBlock.Output,
            test_input={""text"": ""Hello, how are you?""},
            test_output=[(""word_count"", 4), (""character_count"", 19)],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        try:
            text = input_data.text
            word_count = len(text.split())
            character_count = len(text)

            yield ""word_count"", word_count
            yield ""character_count"", character_count

        except Exception as e:
            yield ""error"", str(e)
","from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class WordCharacterCountBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""Input text to count words and characters"",
            placeholder=""Enter your text here"",
            advanced=False,
        )

    class Output(BlockSchema):
        word_count: int = SchemaField(description=""Number of words in the input text"")
        character_count: int = SchemaField(
            description=""Number of characters in the input text""
        )
        error: str = SchemaField(
            description=""Error message if the counting operation failed""
        )

    def __init__(self):
        super().__init__(
            id=""ab2a782d-22cf-4587-8a70-55b59b3f9f90"",
            description=""Counts the number of words and characters in a given text."",
            categories={BlockCategory.TEXT},
            input_schema=WordCharacterCountBlock.Input,
            output_schema=WordCharacterCountBlock.Output,
            test_input={""text"": ""Hello, how are you?""},
            test_output=[(""word_count"", 4), (""character_count"", 19)],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        try:
            text = input_data.text
            word_count = len(text.split())
            character_count = len(text)

            yield ""word_count"", word_count
            yield ""character_count"", character_count

        except Exception as e:
            yield ""error"", str(e)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
100,"from backend.blocks.jina._auth import (
    JinaCredentials,
    JinaCredentialsField,
    JinaCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class JinaChunkingBlock(Block):
    class Input(BlockSchema):
        texts: list = SchemaField(description=""List of texts to chunk"")

        credentials: JinaCredentialsInput = JinaCredentialsField()
        max_chunk_length: int = SchemaField(
            description=""Maximum length of each chunk"", default=1000
        )
        return_tokens: bool = SchemaField(
            description=""Whether to return token information"", default=False
        )

    class Output(BlockSchema):
        chunks: list = SchemaField(description=""List of chunked texts"")
        tokens: list = SchemaField(
            description=""List of token information for each chunk"", optional=True
        )

    def __init__(self):
        super().__init__(
            id=""806fb15e-830f-4796-8692-557d300ff43c"",
            description=""Chunks texts using Jina AI's segmentation service"",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=JinaChunkingBlock.Input,
            output_schema=JinaChunkingBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://segment.jina.ai/""
        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }

        all_chunks = []
        all_tokens = []

        for text in input_data.texts:
            data = {
                ""content"": text,
                ""return_tokens"": str(input_data.return_tokens).lower(),
                ""return_chunks"": ""true"",
                ""max_chunk_length"": str(input_data.max_chunk_length),
            }

            response = requests.post(url, headers=headers, json=data)
            result = response.json()

            all_chunks.extend(result.get(""chunks"", []))
            if input_data.return_tokens:
                all_tokens.extend(result.get(""tokens"", []))

        yield ""chunks"", all_chunks
        if input_data.return_tokens:
            yield ""tokens"", all_tokens
","from backend.blocks.jina._auth import (
    JinaCredentials,
    JinaCredentialsField,
    JinaCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests



class JinaChunkingBlock(Block):
    class Input(BlockSchema):
        texts: list = SchemaField(description=""List of texts to chunk"")
        credentials: JinaCredentialsInput = JinaCredentialsField()
        max_chunk_length: int = SchemaField(
            description=""Maximum length of each chunk"", default=1000
        )
        return_tokens: bool = SchemaField(
            description=""Whether to return token information"", default=False
        )

    class Output(BlockSchema):
        chunks: list = SchemaField(description=""List of chunked texts"")
        tokens: list = SchemaField(
            description=""List of token information for each chunk"", optional=True
        )

    def __init__(self):
        super().__init__(
            id=""806fb15e-830f-4796-8692-557d300ff43c"",
            description=""Chunks texts using Jina AI's segmentation service"",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=JinaChunkingBlock.Input,
            output_schema=JinaChunkingBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://segment.jina.ai/""
        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }

        all_chunks = []
        all_tokens = []

        for text in input_data.texts:
            data = {
                ""content"": text,
                ""return_tokens"": str(input_data.return_tokens).lower(),
                ""return_chunks"": ""true"",
                ""max_chunk_length"": str(input_data.max_chunk_length),
            }
            response = requests.post(url, headers=headers, json=data)
            result = response.json()
            all_chunks.extend(result.get(""chunks"", []))
            if input_data.return_tokens:
                all_tokens.extend(result.get(""tokens"", []))

        yield ""chunks"", all_chunks
        if input_data.return_tokens:
            yield ""tokens"", all_tokens","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
101,"from typing import Literal
from urllib.parse import quote

from pydantic import SecretStr

from backend.blocks.helpers.http import GetRequest
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName


class GetWikipediaSummaryBlock(Block, GetRequest):
    class Input(BlockSchema):
        topic: str = SchemaField(description=""The topic to fetch the summary for"")

    class Output(BlockSchema):
        summary: str = SchemaField(description=""The summary of the given topic"")
        error: str = SchemaField(
            description=""Error message if the summary cannot be retrieved""
        )

    def __init__(self):
        super().__init__(
            id=""f5b0f5d0-1862-4d61-94be-3ad0fa772760"",
            description=""This block fetches the summary of a given topic from Wikipedia."",
            categories={BlockCategory.SEARCH},
            input_schema=GetWikipediaSummaryBlock.Input,
            output_schema=GetWikipediaSummaryBlock.Output,
            test_input={""topic"": ""Artificial Intelligence""},
            test_output=(""summary"", ""summary content""),
            test_mock={""get_request"": lambda url, json: {""extract"": ""summary content""}},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        topic = input_data.topic
        url = f""https://en.wikipedia.org/api/rest_v1/page/summary/{topic}""
        response = self.get_request(url, json=True)
        if ""extract"" not in response:
            raise RuntimeError(f""Unable to parse Wikipedia response: {response}"")
        yield ""summary"", response[""extract""]


TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""openweathermap"",
    api_key=SecretStr(""mock-openweathermap-api-key""),
    title=""Mock OpenWeatherMap API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class GetWeatherInformationBlock(Block, GetRequest):
    class Input(BlockSchema):
        location: str = SchemaField(
            description=""Location to get weather information for""
        )
        credentials: CredentialsMetaInput[
            Literal[ProviderName.OPENWEATHERMAP], Literal[""api_key""]
        ] = CredentialsField(
            description=""The OpenWeatherMap integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        use_celsius: bool = SchemaField(
            default=True,
            description=""Whether to use Celsius or Fahrenheit for temperature"",
        )

    class Output(BlockSchema):
        temperature: str = SchemaField(
            description=""Temperature in the specified location""
        )
        humidity: str = SchemaField(description=""Humidity in the specified location"")
        condition: str = SchemaField(
            description=""Weather condition in the specified location""
        )
        error: str = SchemaField(
            description=""Error message if the weather information cannot be retrieved""
        )

    def __init__(self):
        super().__init__(
            id=""f7a8b2c3-6d4e-5f8b-9e7f-6d4e5f8b9e7f"",
            input_schema=GetWeatherInformationBlock.Input,
            output_schema=GetWeatherInformationBlock.Output,
            description=""Retrieves weather information for a specified location using OpenWeatherMap API."",
            test_input={
                ""location"": ""New York"",
                ""use_celsius"": True,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (""temperature"", ""21.66""),
                (""humidity"", ""32""),
                (""condition"", ""overcast clouds""),
            ],
            test_mock={
                ""get_request"": lambda url, json: {
                    ""main"": {""temp"": 21.66, ""humidity"": 32},
                    ""weather"": [{""description"": ""overcast clouds""}],
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        units = ""metric"" if input_data.use_celsius else ""imperial""
        api_key = credentials.api_key
        location = input_data.location
        url = f""http://api.openweathermap.org/data/2.5/weather?q={quote(location)}&appid={api_key}&units={units}""
        weather_data = self.get_request(url, json=True)

        if ""main"" in weather_data and ""weather"" in weather_data:
            yield ""temperature"", str(weather_data[""main""][""temp""])
            yield ""humidity"", str(weather_data[""main""][""humidity""])
            yield ""condition"", weather_data[""weather""][0][""description""]
        else:
            raise RuntimeError(f""Expected keys not found in response: {weather_data}"")
","from typing import Literal
from urllib.parse import quote

from pydantic import SecretStr

from backend.blocks.helpers.http import GetRequest
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema

from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)

from backend.integrations.providers import ProviderName


class GetWikipediaSummaryBlock(Block, GetRequest):
    class Input(BlockSchema):
        topic: str = SchemaField(description=""The topic to fetch the summary for"")

    class Output(BlockSchema):
        summary: str = SchemaField(description=""The summary of the given topic"")
        error: str = SchemaField(
            description=""Error message if the summary cannot be retrieved""
        )

    def __init__(self):
        super().__init__(
            id=""f5b0f5d0-1862-4d61-94be-3ad0fa772760"",
            description=""This block fetches the summary of a given topic from Wikipedia."",
            categories={BlockCategory.SEARCH},
            input_schema=GetWikipediaSummaryBlock.Input,
            output_schema=GetWikipediaSummaryBlock.Output,
            test_input={""topic"": ""Artificial Intelligence""},
            test_output=(""summary"", ""summary content""),
            test_mock={""get_request"": lambda url, json: {""extract"": ""summary content""}},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        topic = input_data.topic
        url = f""https://en.wikipedia.org/api/rest_v1/page/summary/{topic}""
        response = self.get_request(url, json=True)
        if ""extract"" not in response:
            raise RuntimeError(f""Unable to parse Wikipedia response: {response}"")
        yield ""summary"", response[""extract""]


TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""openweathermap"",
    api_key=SecretStr(""mock-openweathermap-api-key""),
    title=""Mock OpenWeatherMap API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class GetWeatherInformationBlock(Block, GetRequest):
    class Input(BlockSchema):
        location: str = SchemaField(
            description=""Location to get weather information for""
        )
        credentials: CredentialsMetaInput[
            Literal[ProviderName.OPENWEATHERMAP], Literal[""api_key""]
        ] = CredentialsField(
            description=""The OpenWeatherMap integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        use_celsius: bool = SchemaField(
            default=True,
            description=""Whether to use Celsius or Fahrenheit for temperature"",
        )

    class Output(BlockSchema):
        temperature: str = SchemaField(
            description=""Temperature in the specified location""
        )
        humidity: str = SchemaField(description=""Humidity in the specified location"")
        condition: str = SchemaField(
            description=""Weather condition in the specified location""
        )
        error: str = SchemaField(
            description=""Error message if the weather information cannot be retrieved""
        )

    def __init__(self):
        super().__init__(
            id=""f7a8b2c3-6d4e-5f8b-9e7f-6d4e5f8b9e7f"",
            input_schema=GetWeatherInformationBlock.Input,
            output_schema=GetWeatherInformationBlock.Output,
            description=""Retrieves weather information for a specified location using OpenWeatherMap API."",
            test_input={
                ""location"": ""New York"",
                ""use_celsius"": True,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (""temperature"", ""21.66""),
                (""humidity"", ""32""),
                (""condition"", ""overcast clouds""),
            ],
            test_mock={
                ""get_request"": lambda url, json: {
                    ""main"": {""temp"": 21.66, ""humidity"": 32},
                    ""weather"": [{""description"": ""overcast clouds""}],
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        units = ""metric"" if input_data.use_celsius else ""imperial""
        api_key = credentials.api_key
        location = input_data.location
        url = f""http://api.openweathermap.org/data/2.5/weather?q={quote(location)}&appid={api_key}&units={units}""
        weather_data = self.get_request(url, json=True)
        
        if ""main"" in weather_data and ""weather"" in weather_data:
            yield ""temperature"", str(weather_data[""main""][""temp""])
            yield ""humidity"", str(weather_data[""main""][""humidity""])
            yield ""condition"", weather_data[""weather""][0][""description""]
        else:
            raise RuntimeError(f""Expected keys not found in response: {weather_data}"")
","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
102,"# -*- coding: utf-8 -*-

from mrjob.job import MRJob


class RemoveDuplicateUrls(MRJob):

    def mapper(self, _, line):
        yield line, 1

    def reducer(self, key, values):
        total = sum(values)
        if total == 1:
            yield key, total

    def steps(self):
        """"""Run the map and reduce steps.""""""
        return [
            self.mr(mapper=self.mapper,
                    reducer=self.reducer)
        ]


if __name__ == '__main__':
    RemoveDuplicateUrls.run()
","# -*- coding: utf-8 -*-
from mrjob.job import MRJob

class RemoveDuplicateUrls(MRJob):

    def mapper(self, _, line):
        yield line, 1

    def reducer(self, key, values):
        total = sum(values)
        if total == 1:
            yield key, total

    def steps(self):
        """"""Run the map and reduce steps.""""""
        return [
            self.mr(mapper=self.mapper,
                    reducer=self.reducer)
        ]

if __name__ == '__main__':
    RemoveDuplicateUrls.run()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
103,"import secrets
from datetime import datetime, timedelta, timezone
from typing import TYPE_CHECKING

from pydantic import SecretStr

if TYPE_CHECKING:
    from backend.executor.database import DatabaseManager

from autogpt_libs.utils.cache import thread_cached
from autogpt_libs.utils.synchronize import RedisKeyedMutex

from backend.data.model import (
    APIKeyCredentials,
    Credentials,
    OAuth2Credentials,
    OAuthState,
    UserIntegrations,
)
from backend.util.settings import Settings

settings = Settings()

revid_credentials = APIKeyCredentials(
    id=""fdb7f412-f519-48d1-9b5f-d2f73d0e01fe"",
    provider=""revid"",
    api_key=SecretStr(settings.secrets.revid_api_key),
    title=""Use Credits for Revid"",
    expires_at=None,
)
ideogram_credentials = APIKeyCredentials(
    id=""760f84fc-b270-42de-91f6-08efe1b512d0"",
    provider=""ideogram"",
    api_key=SecretStr(settings.secrets.ideogram_api_key),
    title=""Use Credits for Ideogram"",
    expires_at=None,
)
replicate_credentials = APIKeyCredentials(
    id=""6b9fc200-4726-4973-86c9-cd526f5ce5db"",
    provider=""replicate"",
    api_key=SecretStr(settings.secrets.replicate_api_key),
    title=""Use Credits for Replicate"",
    expires_at=None,
)
openai_credentials = APIKeyCredentials(
    id=""53c25cb8-e3ee-465c-a4d1-e75a4c899c2a"",
    provider=""openai"",
    api_key=SecretStr(settings.secrets.openai_api_key),
    title=""Use Credits for OpenAI"",
    expires_at=None,
)
anthropic_credentials = APIKeyCredentials(
    id=""24e5d942-d9e3-4798-8151-90143ee55629"",
    provider=""anthropic"",
    api_key=SecretStr(settings.secrets.anthropic_api_key),
    title=""Use Credits for Anthropic"",
    expires_at=None,
)
groq_credentials = APIKeyCredentials(
    id=""4ec22295-8f97-4dd1-b42b-2c6957a02545"",
    provider=""groq"",
    api_key=SecretStr(settings.secrets.groq_api_key),
    title=""Use Credits for Groq"",
    expires_at=None,
)
did_credentials = APIKeyCredentials(
    id=""7f7b0654-c36b-4565-8fa7-9a52575dfae2"",
    provider=""d_id"",
    api_key=SecretStr(settings.secrets.did_api_key),
    title=""Use Credits for D-ID"",
    expires_at=None,
)
jina_credentials = APIKeyCredentials(
    id=""7f26de70-ba0d-494e-ba76-238e65e7b45f"",
    provider=""jina"",
    api_key=SecretStr(settings.secrets.jina_api_key),
    title=""Use Credits for Jina"",
    expires_at=None,
)
unreal_credentials = APIKeyCredentials(
    id=""66f20754-1b81-48e4-91d0-f4f0dd82145f"",
    provider=""unreal"",
    api_key=SecretStr(settings.secrets.unreal_speech_api_key),
    title=""Use Credits for Unreal"",
    expires_at=None,
)
open_router_credentials = APIKeyCredentials(
    id=""b5a0e27d-0c98-4df3-a4b9-10193e1f3c40"",
    provider=""open_router"",
    api_key=SecretStr(settings.secrets.open_router_api_key),
    title=""Use Credits for Open Router"",
    expires_at=None,
)


DEFAULT_CREDENTIALS = [
    revid_credentials,
    ideogram_credentials,
    replicate_credentials,
    openai_credentials,
    anthropic_credentials,
    groq_credentials,
    did_credentials,
    jina_credentials,
    unreal_credentials,
    open_router_credentials,
]


class IntegrationCredentialsStore:
    def __init__(self):
        from backend.data.redis import get_redis

        self.locks = RedisKeyedMutex(get_redis())

    @property
    @thread_cached
    def db_manager(self) -> ""DatabaseManager"":
        from backend.executor.database import DatabaseManager
        from backend.util.service import get_service_client

        return get_service_client(DatabaseManager)

    def add_creds(self, user_id: str, credentials: Credentials) -> None:
        with self.locked_user_integrations(user_id):
            if self.get_creds_by_id(user_id, credentials.id):
                raise ValueError(
                    f""Can not re-create existing credentials #{credentials.id} ""
                    f""for user #{user_id}""
                )
            self._set_user_integration_creds(
                user_id, [*self.get_all_creds(user_id), credentials]
            )

    def get_all_creds(self, user_id: str) -> list[Credentials]:
        users_credentials = self._get_user_integrations(user_id).credentials
        all_credentials = users_credentials
        if settings.secrets.revid_api_key:
            all_credentials.append(revid_credentials)
        if settings.secrets.ideogram_api_key:
            all_credentials.append(ideogram_credentials)
        if settings.secrets.groq_api_key:
            all_credentials.append(groq_credentials)
        if settings.secrets.replicate_api_key:
            all_credentials.append(replicate_credentials)
        if settings.secrets.openai_api_key:
            all_credentials.append(openai_credentials)
        if settings.secrets.anthropic_api_key:
            all_credentials.append(anthropic_credentials)
        if settings.secrets.did_api_key:
            all_credentials.append(did_credentials)
        if settings.secrets.jina_api_key:
            all_credentials.append(jina_credentials)
        if settings.secrets.unreal_speech_api_key:
            all_credentials.append(unreal_credentials)
        if settings.secrets.open_router_api_key:
            all_credentials.append(open_router_credentials)
        return all_credentials

    def get_creds_by_id(self, user_id: str, credentials_id: str) -> Credentials | None:
        all_credentials = self.get_all_creds(user_id)
        return next((c for c in all_credentials if c.id == credentials_id), None)

    def get_creds_by_provider(self, user_id: str, provider: str) -> list[Credentials]:
        credentials = self.get_all_creds(user_id)
        return [c for c in credentials if c.provider == provider]

    def get_authorized_providers(self, user_id: str) -> list[str]:
        credentials = self.get_all_creds(user_id)
        return list(set(c.provider for c in credentials))

    def update_creds(self, user_id: str, updated: Credentials) -> None:
        with self.locked_user_integrations(user_id):
            current = self.get_creds_by_id(user_id, updated.id)
            if not current:
                raise ValueError(
                    f""Credentials with ID {updated.id} ""
                    f""for user with ID {user_id} not found""
                )
            if type(current) is not type(updated):
                raise TypeError(
                    f""Can not update credentials with ID {updated.id} ""
                    f""from type {type(current)} ""
                    f""to type {type(updated)}""
                )

            # Ensure no scopes are removed when updating credentials
            if (
                isinstance(updated, OAuth2Credentials)
                and isinstance(current, OAuth2Credentials)
                and not set(updated.scopes).issuperset(current.scopes)
            ):
                raise ValueError(
                    f""Can not update credentials with ID {updated.id} ""
                    f""and scopes {current.scopes} ""
                    f""to more restrictive set of scopes {updated.scopes}""
                )

            # Update the credentials
            updated_credentials_list = [
                updated if c.id == updated.id else c
                for c in self.get_all_creds(user_id)
            ]
            self._set_user_integration_creds(user_id, updated_credentials_list)

    def delete_creds_by_id(self, user_id: str, credentials_id: str) -> None:
        with self.locked_user_integrations(user_id):
            filtered_credentials = [
                c for c in self.get_all_creds(user_id) if c.id != credentials_id
            ]
            self._set_user_integration_creds(user_id, filtered_credentials)

    def store_state_token(self, user_id: str, provider: str, scopes: list[str]) -> str:
        token = secrets.token_urlsafe(32)
        expires_at = datetime.now(timezone.utc) + timedelta(minutes=10)

        state = OAuthState(
            token=token,
            provider=provider,
            expires_at=int(expires_at.timestamp()),
            scopes=scopes,
        )

        with self.locked_user_integrations(user_id):
            user_integrations = self._get_user_integrations(user_id)
            oauth_states = user_integrations.oauth_states
            oauth_states.append(state)
            user_integrations.oauth_states = oauth_states

            self.db_manager.update_user_integrations(
                user_id=user_id, data=user_integrations
            )

        return token

    def get_any_valid_scopes_from_state_token(
        self, user_id: str, token: str, provider: str
    ) -> list[str]:
        """"""
        Get the valid scopes from the OAuth state token. This will return any valid scopes
        from any OAuth state token for the given provider. If no valid scopes are found,
        an empty list is returned. DO NOT RELY ON THIS TOKEN TO AUTHENTICATE A USER, AS IT
        IS TO CHECK IF THE USER HAS GIVEN PERMISSIONS TO THE APPLICATION BEFORE EXCHANGING
        THE CODE FOR TOKENS.
        """"""
        user_integrations = self._get_user_integrations(user_id)
        oauth_states = user_integrations.oauth_states

        now = datetime.now(timezone.utc)
        valid_state = next(
            (
                state
                for state in oauth_states
                if state.token == token
                and state.provider == provider
                and state.expires_at > now.timestamp()
            ),
            None,
        )

        if valid_state:
            return valid_state.scopes

        return []

    def verify_state_token(self, user_id: str, token: str, provider: str) -> bool:
        with self.locked_user_integrations(user_id):
            user_integrations = self._get_user_integrations(user_id)
            oauth_states = user_integrations.oauth_states

            now = datetime.now(timezone.utc)
            valid_state = next(
                (
                    state
                    for state in oauth_states
                    if state.token == token
                    and state.provider == provider
                    and state.expires_at > now.timestamp()
                ),
                None,
            )

            if valid_state:
                # Remove the used state
                oauth_states.remove(valid_state)
                user_integrations.oauth_states = oauth_states
                self.db_manager.update_user_integrations(user_id, user_integrations)
                return True

        return False

    def _set_user_integration_creds(
        self, user_id: str, credentials: list[Credentials]
    ) -> None:
        integrations = self._get_user_integrations(user_id)
        # Remove default credentials from the list
        credentials = [c for c in credentials if c not in DEFAULT_CREDENTIALS]
        integrations.credentials = credentials
        self.db_manager.update_user_integrations(user_id, integrations)

    def _get_user_integrations(self, user_id: str) -> UserIntegrations:
        integrations: UserIntegrations = self.db_manager.get_user_integrations(
            user_id=user_id
        )
        return integrations

    def locked_user_integrations(self, user_id: str):
        key = (f""user:{user_id}"", ""integrations"")
        return self.locks.locked(key)
","import secrets
from datetime import datetime, timedelta, timezone
from typing import TYPE_CHECKING

from pydantic import SecretStr

if TYPE_CHECKING:
    from backend.executor.database import DatabaseManager

from autogpt_libs.utils.cache import thread_cached
from autogpt_libs.utils.synchronize import RedisKeyedMutex

from backend.data.model import (
    APIKeyCredentials,
    Credentials,
    OAuth2Credentials,
    OAuthState,
    UserIntegrations,
)

from backend.util.settings import Settings

settings = Settings()

revid_credentials = APIKeyCredentials(
    id=""fdb7f412-f519-48d1-9b5f-d2f73d0e01fe"",
    provider=""revid"",
    api_key=SecretStr(settings.secrets.revid_api_key),
    title=""Use Credits for Revid"",
    expires_at=None,
)

ideogram_credentials = APIKeyCredentials(
    id=""760f84fc-b270-42de-91f6-08efe1b512d0"",
    provider=""ideogram"",
    api_key=SecretStr(settings.secrets.ideogram_api_key),
    title=""Use Credits for Ideogram"",
    expires_at=None,
)

replicate_credentials = APIKeyCredentials(
    id=""6b9fc200-4726-4973-86c9-cd526f5ce5db"",
    provider=""replicate"",
    api_key=SecretStr(settings.secrets.replicate_api_key),
    title=""Use Credits for Replicate"",
    expires_at=None,
)

openai_credentials = APIKeyCredentials(
    id=""53c25cb8-e3ee-465c-a4d1-e75a4c899c2a"",
    provider=""openai"",
    api_key=SecretStr(settings.secrets.openai_api_key),
    title=""Use Credits for OpenAI"",
    expires_at=None,
)

anthropic_credentials = APIKeyCredentials(
    id=""24e5d942-d9e3-4798-8151-90143ee55629"",
    provider=""anthropic"",
    api_key=SecretStr(settings.secrets.anthropic_api_key),
    title=""Use Credits for Anthropic"",
    expires_at=None,
)

groq_credentials = APIKeyCredentials(
    id=""4ec22295-8f97-4dd1-b42b-2c6957a02545"",
    provider=""groq"",
    api_key=SecretStr(settings.secrets.groq_api_key),
    title=""Use Credits for Groq"",
    expires_at=None,
)

did_credentials = APIKeyCredentials(
    id=""7f7b0654-c36b-4565-8fa7-9a52575dfae2"",
    provider=""d_id"",
    api_key=SecretStr(settings.secrets.did_api_key),
    title=""Use Credits for D-ID"",
    expires_at=None,
)

jina_credentials = APIKeyCredentials(
    id=""7f26de70-ba0d-494e-ba76-238e65e7b45f"",
    provider=""jina"",
    api_key=SecretStr(settings.secrets.jina_api_key),
    title=""Use Credits for Jina"",
    expires_at=None,
)

unreal_credentials = APIKeyCredentials(
    id=""66f20754-1b81-48e4-91d0-f4f0dd82145f"",
    provider=""unreal"",
    api_key=SecretStr(settings.secrets.unreal_speech_api_key),
    title=""Use Credits for Unreal"",
    expires_at=None,
)

open_router_credentials = APIKeyCredentials(
    id=""b5a0e27d-0c98-4df3-a4b9-10193e1f3c40"",
    provider=""open_router"",
    api_key=SecretStr(settings.secrets.open_router_api_key),
    title=""Use Credits for Open Router"",
    expires_at=None,
)

DEFAULT_CREDENTIALS = [
    revid_credentials,
    ideogram_credentials,
    replicate_credentials,
    openai_credentials,
    anthropic_credentials,
    groq_credentials,
    did_credentials,
    jina_credentials,
    unreal_credentials,
    open_router_credentials,
]


class IntegrationCredentialsStore:
    def __init__(self):
        from backend.data.redis import get_redis

        self.locks = RedisKeyedMutex(get_redis())

    @property
    @thread_cached
    def db_manager(self) -> ""DatabaseManager"":
        from backend.executor.database import DatabaseManager
        from backend.util.service import get_service_client

        return get_service_client(DatabaseManager)

    def add_creds(self, user_id: str, credentials: Credentials) -> None:
        with self.locked_user_integrations(user_id):
            if self.get_creds_by_id(user_id, credentials.id):
                raise ValueError(
                    f""Can not re-create existing credentials #{credentials.id} ""
                    f""for user #{user_id}""
                )
            self._set_user_integration_creds(
                user_id, [*self.get_all_creds(user_id), credentials]
            )

    def get_all_creds(self, user_id: str) -> list[Credentials]:
        users_credentials = self._get_user_integrations(user_id).credentials
        all_credentials = users_credentials
        if settings.secrets.revid_api_key:
            all_credentials.append(revid_credentials)
        if settings.secrets.ideogram_api_key:
            all_credentials.append(ideogram_credentials)
        if settings.secrets.groq_api_key:
            all_credentials.append(groq_credentials)
        if settings.secrets.replicate_api_key:
            all_credentials.append(replicate_credentials)
        if settings.secrets.openai_api_key:
            all_credentials.append(openai_credentials)
        if settings.secrets.anthropic_api_key:
            all_credentials.append(anthropic_credentials)
        if settings.secrets.did_api_key:
            all_credentials.append(did_credentials)
        if settings.secrets.jina_api_key:
            all_credentials.append(jina_credentials)
        if settings.secrets.unreal_speech_api_key:
            all_credentials.append(unreal_credentials)
        if settings.secrets.open_router_api_key:
            all_credentials.append(open_router_credentials)
        return all_credentials

    def get_creds_by_id(self, user_id: str, credentials_id: str) -> Credentials | None:
        all_credentials = self.get_all_creds(user_id)
        return next((c for c in all_credentials if c.id == credentials_id), None)

    def get_creds_by_provider(self, user_id: str, provider: str) -> list[Credentials]:
        credentials = self.get_all_creds(user_id)
        return [c for c in credentials if c.provider == provider]

    def get_authorized_providers(self, user_id: str) -> list[str]:
        credentials = self.get_all_creds(user_id)
        return list(set(c.provider for c in credentials))

    def update_creds(self, user_id: str, updated: Credentials) -> None:
        with self.locked_user_integrations(user_id):
            current = self.get_creds_by_id(user_id, updated.id)
            if not current:
                raise ValueError(
                    f""Credentials with ID {updated.id} ""
                    f""for user with ID {user_id} not found""
                )
            if type(current) is not type(updated):
                raise TypeError(
                    f""Can not update credentials with ID {updated.id} ""
                    f""from type {type(current)} ""
                    f""to type {type(updated)}""
                )

            # Ensure no scopes are removed when updating credentials
            if (
                isinstance(updated, OAuth2Credentials)
                and isinstance(current, OAuth2Credentials)
                and not set(updated.scopes).issuperset(current.scopes)
            ):
                raise ValueError(
                    f""Can not update credentials with ID {updated.id} ""
                    f""and scopes {current.scopes} ""
                    f""to more restrictive set of scopes {updated.scopes}""
                )

            # Update the credentials
            updated_credentials_list = [
                updated if c.id == updated.id else c
                for c in self.get_all_creds(user_id)
            ]
            self._set_user_integration_creds(user_id, updated_credentials_list)

    def delete_creds_by_id(self, user_id: str, credentials_id: str) -> None:
        with self.locked_user_integrations(user_id):
            filtered_credentials = [
                c for c in self.get_all_creds(user_id) if c.id != credentials_id
            ]
            self._set_user_integration_creds(user_id, filtered_credentials)

    def store_state_token(self, user_id: str, provider: str, scopes: list[str]) -> str:
        token = secrets.token_urlsafe(32)
        expires_at = datetime.now(timezone.utc) + timedelta(minutes=10)

        state = OAuthState(
            token=token,
            provider=provider,
            expires_at=int(expires_at.timestamp()),
            scopes=scopes,
        )

        with self.locked_user_integrations(user_id):
            user_integrations = self._get_user_integrations(user_id)
            oauth_states = user_integrations.oauth_states
            oauth_states.append(state)
            user_integrations.oauth_states = oauth_states

            self.db_manager.update_user_integrations(
                user_id=user_id, data=user_integrations
            )

        return token

    def get_any_valid_scopes_from_state_token(
        self, user_id: str, token: str, provider: str
    ) -> list[str]:
        """"""
        Get the valid scopes from the OAuth state token. This will return any valid scopes
        from any OAuth state token for the given provider. If no valid scopes are found,
        an empty list is returned. DO NOT RELY ON THIS TOKEN TO AUTHENTICATE A USER, AS IT
        IS TO CHECK IF THE USER HAS GIVEN PERMISSIONS TO THE APPLICATION BEFORE EXCHANGING
        THE CODE FOR TOKENS.
        """"""
        user_integrations = self._get_user_integrations(user_id)
        oauth_states = user_integrations.oauth_states

        now = datetime.now(timezone.utc)
        valid_state = next(
            (
                state
                for state in oauth_states
                if state.token == token
                and state.provider == provider
                and state.expires_at > now.timestamp()
            ),
            None,
        )

        if valid_state:
            return valid_state.scopes

        return []

    def verify_state_token(self, user_id: str, token: str, provider: str) -> bool:
        with self.locked_user_integrations(user_id):
            user_integrations = self._get_user_integrations(user_id)
            oauth_states = user_integrations.oauth_states

            now = datetime.now(timezone.utc)
            valid_state = next(
                (
                    state
                    for state in oauth_states
                    if state.token == token
                    and state.provider == provider
                    and state.expires_at > now.timestamp()
                ),
                None,
            )

            if valid_state:
                # Remove the used state
                oauth_states.remove(valid_state)
                user_integrations.oauth_states = oauth_states
                self.db_manager.update_user_integrations(user_id, user_integrations)
                return True

        return False

    def _set_user_integration_creds(
        self, user_id: str, credentials: list[Credentials]
    ) -> None:
        integrations = self._get_user_integrations(user_id)
        # Remove default credentials from the list
        credentials = [c for c in credentials if c not in DEFAULT_CREDENTIALS]
        integrations.credentials = credentials
        self.db_manager.update_user_integrations(user_id, integrations)

    def _get_user_integrations(self, user_id: str) -> UserIntegrations:
        integrations: UserIntegrations = self.db_manager.get_user_integrations(
            user_id=user_id
        )
        return integrations

    def locked_user_integrations(self, user_id: str):
        key = (f""user:{user_id}"", ""integrations"")
        return self.locks.locked(key)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
104,"import datetime
import typing

import fuzzywuzzy.fuzz
import prisma.enums
import prisma.errors
import prisma.models
import prisma.types
import pydantic

import market.model
import market.utils.extension_types


class AgentQueryError(Exception):
    """"""Custom exception for agent query errors""""""

    pass


class TopAgentsDBResponse(pydantic.BaseModel):
    """"""
    Represents a response containing a list of top agents.

    Attributes:
        analytics (list[AgentResponse]): The list of top agents.
        total_count (int): The total count of agents.
        page (int): The current page number.
        page_size (int): The number of agents per page.
        total_pages (int): The total number of pages.
    """"""

    analytics: list[prisma.models.AnalyticsTracker]
    total_count: int
    page: int
    page_size: int
    total_pages: int


class FeaturedAgentResponse(pydantic.BaseModel):
    """"""
    Represents a response containing a list of featured agents.

    Attributes:
        featured_agents (list[FeaturedAgent]): The list of featured agents.
        total_count (int): The total count of featured agents.
        page (int): The current page number.
        page_size (int): The number of agents per page.
        total_pages (int): The total number of pages.
    """"""

    featured_agents: list[prisma.models.FeaturedAgent]
    total_count: int
    page: int
    page_size: int
    total_pages: int


async def delete_agent(agent_id: str) -> prisma.models.Agents | None:
    """"""
    Delete an agent from the database.

    Args:
        agent_id (str): The ID of the agent to delete.

    Returns:
        prisma.models.Agents | None: The deleted agent if found, None otherwise.

    Raises:
        AgentQueryError: If there is an error deleting the agent from the database.
    """"""
    try:
        deleted_agent = await prisma.models.Agents.prisma().delete(
            where={""id"": agent_id}
        )
        return deleted_agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def create_agent_entry(
    name: str,
    description: str,
    author: str,
    keywords: typing.List[str],
    categories: typing.List[str],
    graph: prisma.Json,
    submission_state: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.PENDING,
):
    """"""
    Create a new agent entry in the database.

    Args:
        name (str): The name of the agent.
        description (str): The description of the agent.
        author (str): The author of the agent.
        keywords (List[str]): The keywords associated with the agent.
        categories (List[str]): The categories associated with the agent.
        graph (dict): The graph data of the agent.

    Returns:
        dict: The newly created agent entry.

    Raises:
        AgentQueryError: If there is an error creating the agent entry.
    """"""
    try:
        agent = await prisma.models.Agents.prisma().create(
            data={
                ""name"": name,
                ""description"": description,
                ""author"": author,
                ""keywords"": keywords,
                ""categories"": categories,
                ""graph"": graph,
                ""AnalyticsTracker"": {""create"": {""downloads"": 0, ""views"": 0}},
                ""submissionStatus"": submission_state,
            }
        )

        return agent

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def update_agent_entry(
    agent_id: str,
    version: int,
    submission_state: prisma.enums.SubmissionStatus,
    comments: str | None = None,
) -> prisma.models.Agents | None:
    """"""
    Update an existing agent entry in the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int): The version of the agent.
        submission_state (prisma.enums.SubmissionStatus): The submission state of the agent.
    """"""

    try:
        agent = await prisma.models.Agents.prisma().update(
            where={""id"": agent_id},
            data={
                ""version"": version,
                ""submissionStatus"": submission_state,
                ""submissionReviewDate"": datetime.datetime.now(datetime.timezone.utc),
                ""submissionReviewComments"": comments,
            },
        )

        return agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Agent Update Failed Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def get_agents(
    page: int = 1,
    page_size: int = 10,
    name: str | None = None,
    keyword: str | None = None,
    category: str | None = None,
    description: str | None = None,
    description_threshold: int = 60,
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
    sort_by: str = ""createdAt"",
    sort_order: typing.Literal[""desc""] | typing.Literal[""asc""] = ""desc"",
):
    """"""
    Retrieve a list of agents from the database based on the provided filters and pagination parameters.

    Args:
        page (int, optional): The page number to retrieve. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.
        name (str, optional): Filter agents by name. Defaults to None.
        keyword (str, optional): Filter agents by keyword. Defaults to None.
        category (str, optional): Filter agents by category. Defaults to None.
        description (str, optional): Filter agents by description. Defaults to None.
        description_threshold (int, optional): The minimum fuzzy search threshold for the description. Defaults to 60.
        sort_by (str, optional): The field to sort the agents by. Defaults to ""createdAt"".
        sort_order (str, optional): The sort order (""asc"" or ""desc""). Defaults to ""desc"".

    Returns:
        dict: A dictionary containing the list of agents, total count, current page number, page size, and total number of pages.
    """"""
    try:
        # Define the base query
        query = {}

        # Add optional filters
        if name:
            query[""name""] = {""contains"": name, ""mode"": ""insensitive""}
        if keyword:
            query[""keywords""] = {""has"": keyword}
        if category:
            query[""categories""] = {""has"": category}

        query[""submissionStatus""] = submission_status

        # Define sorting
        order = {sort_by: sort_order}

        # Calculate pagination
        skip = (page - 1) * page_size

        # Execute the query
        try:
            agents = await prisma.models.Agents.prisma().find_many(
                where=query,  # type: ignore
                order=order,  # type: ignore
                skip=skip,
                take=page_size,
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        # Apply fuzzy search on description if provided
        if description:
            try:
                filtered_agents = []
                for agent in agents:
                    if (
                        agent.description
                        and fuzzywuzzy.fuzz.partial_ratio(
                            description.lower(), agent.description.lower()
                        )
                        >= description_threshold
                    ):
                        filtered_agents.append(agent)
                agents = filtered_agents
            except AttributeError as e:
                raise AgentQueryError(f""Error during fuzzy search: {str(e)}"")

        # Get total count for pagination info
        total_count = len(agents)

        return {
            ""agents"": agents,
            ""total_count"": total_count,
            ""page"": page,
            ""page_size"": page_size,
            ""total_pages"": (total_count + page_size - 1) // page_size,
        }

    except AgentQueryError as e:
        # Log the error or handle it as needed
        raise e
    except ValueError as e:
        raise AgentQueryError(f""Invalid input parameter: {str(e)}"")
    except Exception as e:
        # Catch any other unexpected exceptions
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def get_agent_details(agent_id: str, version: int | None = None):
    """"""
    Retrieve agent details from the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int | None, optional): The version of the agent. Defaults to None.

    Returns:
        dict: The agent details.

    Raises:
        AgentQueryError: If the agent is not found or if there is an error querying the database.
    """"""
    try:
        query = {""id"": agent_id}
        if version is not None:
            query[""version""] = version  # type: ignore

        agent = await prisma.models.Agents.prisma().find_first(where=query)  # type: ignore

        if not agent:
            raise AgentQueryError(""Agent not found"")

        return agent

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def search_db(
    query: str,
    page: int = 1,
    page_size: int = 10,
    categories: typing.List[str] | None = None,
    description_threshold: int = 60,
    sort_by: str = ""rank"",
    sort_order: typing.Literal[""desc""] | typing.Literal[""asc""] = ""desc"",
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
) -> market.model.ListResponse[market.utils.extension_types.AgentsWithRank]:
    """"""Perform a search for agents based on the provided query string.

    Args:
        query (str): the search string
        page (int, optional): page for searching. Defaults to 1.
        page_size (int, optional): the number of results to return. Defaults to 10.
        categories (List[str] | None, optional): list of category filters. Defaults to None.
        description_threshold (int, optional): number of characters to return. Defaults to 60.
        sort_by (str, optional): sort by option. Defaults to ""rank"".
        sort_order (""asc"" | ""desc"", optional): the sort order. Defaults to ""desc"".

    Raises:
        AgentQueryError: Raises an error if the query fails.
        AgentQueryError: Raises if an unexpected error occurs.

    Returns:
        List[AgentsWithRank]: List of agents matching the search criteria.
    """"""
    try:
        offset = (page - 1) * page_size

        category_filter = ""1=1""
        if categories:
            category_conditions = [f""'{cat}' = ANY(categories)"" for cat in categories]
            category_filter = ""AND ("" + "" OR "".join(category_conditions) + "")""

        # Construct the ORDER BY clause based on the sort_by parameter
        if sort_by in [""createdAt"", ""updatedAt""]:
            order_by_clause = f'""{sort_by}"" {sort_order.upper()}, rank DESC'
        elif sort_by == ""name"":
            order_by_clause = f""name {sort_order.upper()}, rank DESC""
        else:
            order_by_clause = 'rank DESC, ""createdAt"" DESC'

        submission_status_filter = f""""""""submissionStatus"" = '{submission_status}'""""""

        sql_query = f""""""
        WITH query AS (
            SELECT to_tsquery(string_agg(lexeme || ':*', ' & ' ORDER BY positions)) AS q 
            FROM unnest(to_tsvector('{query}'))
        )
        SELECT 
            id, 
            ""createdAt"", 
            ""updatedAt"", 
            version, 
            name, 
            LEFT(description, {description_threshold}) AS description, 
            author, 
            keywords, 
            categories, 
            graph,
            ""submissionStatus"",
            ""submissionDate"",
            CASE 
                WHEN query.q::text = '' THEN 1.0
                ELSE COALESCE(ts_rank(CAST(search AS tsvector), query.q), 0.0)
            END AS rank
        FROM market.""Agents"", query
        WHERE 
            (query.q::text = '' OR search @@ query.q)
            AND {category_filter} 
            AND {submission_status_filter}
        ORDER BY {order_by_clause}
        LIMIT {page_size}
        OFFSET {offset};
        """"""

        results = await prisma.client.get_client().query_raw(
            query=sql_query,
            model=market.utils.extension_types.AgentsWithRank,
        )

        class CountResponse(pydantic.BaseModel):
            count: int

        count_query = f""""""
        WITH query AS (
            SELECT to_tsquery(string_agg(lexeme || ':*', ' & ' ORDER BY positions)) AS q 
            FROM unnest(to_tsvector('{query}'))
        )
        SELECT COUNT(*)
        FROM market.""Agents"", query
        WHERE (search @@ query.q OR query.q = '') AND {category_filter} AND {submission_status_filter};
        """"""

        total_count = await prisma.client.get_client().query_first(
            query=count_query,
            model=CountResponse,
        )
        total_count = total_count.count if total_count else 0

        return market.model.ListResponse(
            items=results,
            total_count=total_count,
            page=page,
            page_size=page_size,
            total_pages=(total_count + page_size - 1) // page_size,
        )

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def get_top_agents_by_downloads(
    page: int = 1,
    page_size: int = 10,
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
) -> market.model.ListResponse[prisma.models.AnalyticsTracker]:
    """"""Retrieve the top agents by download count.

    Args:
        page (int, optional): The page number. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.

    Returns:
        dict: A dictionary containing the list of agents, total count, current page number, page size, and total number of pages.
    """"""
    try:
        # Calculate pagination
        skip = (page - 1) * page_size

        # Execute the query
        try:
            # Agents with no downloads will not be included in the results... is this the desired behavior?
            analytics = await prisma.models.AnalyticsTracker.prisma().find_many(
                include={""agent"": True},
                order={""downloads"": ""desc""},
                where={""agent"": {""is"": {""submissionStatus"": submission_status}}},
                skip=skip,
                take=page_size,
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        try:
            total_count = await prisma.models.AnalyticsTracker.prisma().count(
                where={""agent"": {""is"": {""submissionStatus"": submission_status}}},
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        return market.model.ListResponse(
            items=analytics,
            total_count=total_count,
            page=page,
            page_size=page_size,
            total_pages=(total_count + page_size - 1) // page_size,
        )

    except AgentQueryError as e:
        # Log the error or handle it as needed
        raise e from e
    except ValueError as e:
        raise AgentQueryError(f""Invalid input parameter: {str(e)}"") from e
    except Exception as e:
        # Catch any other unexpected exceptions
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"") from e


async def set_agent_featured(
    agent_id: str, is_active: bool = True, featured_categories: list[str] = [""featured""]
) -> prisma.models.FeaturedAgent:
    """"""Set an agent as featured in the database.

    Args:
        agent_id (str): The ID of the agent.
        category (str, optional): The category to set the agent as featured. Defaults to ""featured"".

    Raises:
        AgentQueryError: If there is an error setting the agent as featured.
    """"""
    try:
        agent = await prisma.models.Agents.prisma().find_unique(where={""id"": agent_id})
        if not agent:
            raise AgentQueryError(f""Agent with ID {agent_id} not found."")

        featured = await prisma.models.FeaturedAgent.prisma().upsert(
            where={""agentId"": agent_id},
            data={
                ""update"": {
                    ""featuredCategories"": featured_categories,
                    ""isActive"": is_active,
                },
                ""create"": {
                    ""featuredCategories"": featured_categories,
                    ""isActive"": is_active,
                    ""agent"": {""connect"": {""id"": agent_id}},
                },
            },
        )
        return featured

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def get_featured_agents(
    category: str = ""featured"",
    page: int = 1,
    page_size: int = 10,
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
) -> FeaturedAgentResponse:
    """"""Retrieve a list of featured agents from the database based on the provided category.

    Args:
        category (str, optional): The category of featured agents to retrieve. Defaults to ""featured"".
        page (int, optional): The page number to retrieve. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.

    Returns:
        dict: A dictionary containing the list of featured agents, total count, current page number, page size, and total number of pages.
    """"""
    try:
        # Calculate pagination
        skip = (page - 1) * page_size

        # Execute the query
        try:
            featured_agents = await prisma.models.FeaturedAgent.prisma().find_many(
                where={
                    ""featuredCategories"": {""has"": category},
                    ""isActive"": True,
                    ""agent"": {""is"": {""submissionStatus"": submission_status}},
                },
                include={""agent"": {""include"": {""AnalyticsTracker"": True}}},
                skip=skip,
                take=page_size,
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        # Get total count for pagination info
        total_count = len(featured_agents)

        return FeaturedAgentResponse(
            featured_agents=featured_agents,
            total_count=total_count,
            page=page,
            page_size=page_size,
            total_pages=(total_count + page_size - 1) // page_size,
        )

    except AgentQueryError as e:
        # Log the error or handle it as needed
        raise e from e
    except ValueError as e:
        raise AgentQueryError(f""Invalid input parameter: {str(e)}"") from e
    except Exception as e:
        # Catch any other unexpected exceptions
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"") from e


async def remove_featured_category(
    agent_id: str, category: str
) -> prisma.models.FeaturedAgent | None:
    """"""Adds a featured category to an agent.

    Args:
        agent_id (str): The ID of the agent.
        category (str): The category to add to the agent.

    Returns:
        FeaturedAgentResponse: The updated list of featured agents.
    """"""
    try:
        # get the existing categories
        featured_agent = await prisma.models.FeaturedAgent.prisma().find_unique(
            where={""agentId"": agent_id},
            include={""agent"": True},
        )

        if not featured_agent:
            raise AgentQueryError(f""Agent with ID {agent_id} not found."")

        # remove the category from the list
        featured_agent.featuredCategories.remove(category)

        featured_agent = await prisma.models.FeaturedAgent.prisma().update(
            where={""agentId"": agent_id},
            data={""featuredCategories"": featured_agent.featuredCategories},
        )

        return featured_agent

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def add_featured_category(
    agent_id: str, category: str
) -> prisma.models.FeaturedAgent | None:
    """"""Removes a featured category from an agent.

    Args:
        agent_id (str): The ID of the agent.
        category (str): The category to remove from the agent.

    Returns:
        FeaturedAgentResponse: The updated list of featured agents.
    """"""
    try:
        featured_agent = await prisma.models.FeaturedAgent.prisma().update(
            where={""agentId"": agent_id},
            data={""featuredCategories"": {""push"": [category]}},
        )

        return featured_agent

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def get_agent_featured(agent_id: str) -> prisma.models.FeaturedAgent | None:
    """"""Retrieve an agent's featured categories from the database.

    Args:
        agent_id (str): The ID of the agent.

    Returns:
        FeaturedAgentResponse: The list of featured agents.
    """"""
    try:
        featured_agent = await prisma.models.FeaturedAgent.prisma().find_unique(
            where={""agentId"": agent_id},
        )
        return featured_agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def get_not_featured_agents(
    page: int = 1, page_size: int = 10
) -> typing.List[prisma.models.Agents]:
    """"""
    Retrieve a list of not featured agents from the database.
    """"""
    try:
        agents = await prisma.client.get_client().query_raw(
            query=f""""""
            SELECT 
                ""market"".""Agents"".id, 
                ""market"".""Agents"".""createdAt"", 
                ""market"".""Agents"".""updatedAt"", 
                ""market"".""Agents"".version, 
                ""market"".""Agents"".name, 
                LEFT(""market"".""Agents"".description, 500) AS description, 
                ""market"".""Agents"".author, 
                ""market"".""Agents"".keywords, 
                ""market"".""Agents"".categories, 
                ""market"".""Agents"".graph,
                ""market"".""Agents"".""submissionStatus"",
                ""market"".""Agents"".""submissionDate"",
                ""market"".""Agents"".search::text AS search
            FROM ""market"".""Agents""
            LEFT JOIN ""market"".""FeaturedAgent"" ON ""market"".""Agents"".""id"" = ""market"".""FeaturedAgent"".""agentId""
            WHERE (""market"".""FeaturedAgent"".""agentId"" IS NULL OR ""market"".""FeaturedAgent"".""featuredCategories"" = '{{}}')
                AND ""market"".""Agents"".""submissionStatus"" = 'APPROVED'
            ORDER BY ""market"".""Agents"".""createdAt"" DESC
            LIMIT {page_size} OFFSET {page_size * (page - 1)}
            """""",
            model=prisma.models.Agents,
        )
        return agents
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")


async def get_all_categories() -> market.model.CategoriesResponse:
    """"""
    Retrieve all unique categories from the database.

    Returns:
        CategoriesResponse: A list of unique categories.
    """"""
    try:
        agents = await prisma.models.Agents.prisma().find_many(distinct=[""categories""])

        # Aggregate categories on the Python side
        all_categories = set()
        for agent in agents:
            all_categories.update(agent.categories)

        unique_categories = sorted(list(all_categories))

        return market.model.CategoriesResponse(unique_categories=unique_categories)
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception:
        # Return an empty list of categories in case of unexpected errors
        return market.model.CategoriesResponse(unique_categories=[])


async def create_agent_installed_event(
    event_data: market.model.AgentInstalledFromMarketplaceEventData,
):
    try:
        await prisma.models.InstallTracker.prisma().create(
            data={
                ""installedAgentId"": event_data.installed_agent_id,
                ""marketplaceAgentId"": event_data.marketplace_agent_id,
                ""installationLocation"": prisma.enums.InstallationLocation(
                    event_data.installation_location.name
                ),
            }
        )
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")
","import datetime
import typing
import fuzzywuzzy.fuzz
import prisma.enums
import prisma.errors
import prisma.models
import prisma.types
import pydantic
import market.model
import market.utils.extension_types

class AgentQueryError(Exception):
    """"""Custom exception for agent query errors""""""
    pass

class TopAgentsDBResponse(pydantic.BaseModel):
    """"""
    Represents a response containing a list of top agents.

    Attributes:
        analytics (list[AgentResponse]): The list of top agents.
        total_count (int): The total count of agents.
        page (int): The current page number.
        page_size (int): The number of agents per page.
        total_pages (int): The total number of pages.
    """"""
    analytics: list[prisma.models.AnalyticsTracker]
    total_count: int
    page: int
    page_size: int
    total_pages: int

class FeaturedAgentResponse(pydantic.BaseModel):
    """"""
    Represents a response containing a list of featured agents.

    Attributes:
        featured_agents (list[FeaturedAgent]): The list of featured agents.
        total_count (int): The total count of featured agents.
        page (int): The current page number.
        page_size (int): The number of agents per page.
        total_pages (int): The total number of pages.
    """"""
    featured_agents: list[prisma.models.FeaturedAgent]
    total_count: int
    page: int
    page_size: int
    total_pages: int

async def delete_agent(agent_id: str) -> prisma.models.Agents | None:
    """"""
    Delete an agent from the database.

    Args:
        agent_id (str): The ID of the agent to delete.

    Returns:
        prisma.models.Agents | None: The deleted agent if found, None otherwise.

    Raises:
        AgentQueryError: If there is an error deleting the agent from the database.
    """"""
    try:
        deleted_agent = await prisma.models.Agents.prisma().delete(
            where={""id"": agent_id}
        )
        return deleted_agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def create_agent_entry(
    name: str,
    description: str,
    author: str,
    keywords: typing.List[str],
    categories: typing.List[str],
    graph: prisma.Json,
    submission_state: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.PENDING,
):
    """"""
    Create a new agent entry in the database.

    Args:
        name (str): The name of the agent.
        description (str): The description of the agent.
        author (str): The author of the agent.
        keywords (List[str]): The keywords associated with the agent.
        categories (List[str]): The categories associated with the agent.
        graph (dict): The graph data of the agent.

    Returns:
        dict: The newly created agent entry.

    Raises:
        AgentQueryError: If there is an error creating the agent entry.
    """"""
    try:
        agent = await prisma.models.Agents.prisma().create(
            data={
                ""name"": name,
                ""description"": description,
                ""author"": author,
                ""keywords"": keywords,
                ""categories"": categories,
                ""graph"": graph,
                ""AnalyticsTracker"": {""create"": {""downloads"": 0, ""views"": 0}},
                ""submissionStatus"": submission_state,
            }
        )
        return agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def update_agent_entry(
    agent_id: str,
    version: int,
    submission_state: prisma.enums.SubmissionStatus,
    comments: str | None = None,
) -> prisma.models.Agents | None:
    """"""
    Update an existing agent entry in the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int): The version of the agent.
        submission_state (prisma.enums.SubmissionStatus): The submission state of the agent.
    """"""
    try:
        agent = await prisma.models.Agents.prisma().update(
            where={""id"": agent_id},
            data={
                ""version"": version,
                ""submissionStatus"": submission_state,
                ""submissionReviewDate"": datetime.datetime.now(datetime.timezone.utc),
                ""submissionReviewComments"": comments,
            },
        )
        return agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Agent Update Failed Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def get_agents(
    page: int = 1,
    page_size: int = 10,
    name: str | None = None,
    keyword: str | None = None,
    category: str | None = None,
    description: str | None = None,
    description_threshold: int = 60,
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
    sort_by: str = ""createdAt"",
    sort_order: typing.Literal[""desc""] | typing.Literal[""asc""] = ""desc"",
):
    """"""
    Retrieve a list of agents from the database based on the provided filters and pagination parameters.

    Args:
        page (int, optional): The page number to retrieve. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.
        name (str, optional): Filter agents by name. Defaults to None.
        keyword (str, optional): Filter agents by keyword. Defaults to None.
        category (str, optional): Filter agents by category. Defaults to None.
        description (str, optional): Filter agents by description. Defaults to None.
        description_threshold (int, optional): The minimum fuzzy search threshold for the description. Defaults to 60.
        sort_by (str, optional): The field to sort the agents by. Defaults to ""createdAt"".
        sort_order (str, optional): The sort order (""asc"" or ""desc""). Defaults to ""desc"".

    Returns:
        dict: A dictionary containing the list of agents, total count, current page number, page size, and total number of pages.
    """"""
    try:
        # Define the base query
        query = {}

        # Add optional filters
        if name:
            query[""name""] = {""contains"": name, ""mode"": ""insensitive""}
        if keyword:
            query[""keywords""] = {""has"": keyword}
        if category:
            query[""categories""] = {""has"": category}

        query[""submissionStatus""] = submission_status

        # Define sorting
        order = {sort_by: sort_order}

        # Calculate pagination
        skip = (page - 1) * page_size

        # Execute the query
        try:
            agents = await prisma.models.Agents.prisma().find_many(
                where=query,  # type: ignore
                order=order,  # type: ignore
                skip=skip,
                take=page_size,
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        # Apply fuzzy search on description if provided
        if description:
            try:
                filtered_agents = []
                for agent in agents:
                    if (
                        agent.description
                        and fuzzywuzzy.fuzz.partial_ratio(
                            description.lower(), agent.description.lower()
                        )
                        >= description_threshold
                    ):
                        filtered_agents.append(agent)
                agents = filtered_agents
            except AttributeError as e:
                raise AgentQueryError(f""Error during fuzzy search: {str(e)}"")

        # Get total count for pagination info
        total_count = len(agents)

        return {
            ""agents"": agents,
            ""total_count"": total_count,
            ""page"": page,
            ""page_size"": page_size,
            ""total_pages"": (total_count + page_size - 1) // page_size,
        }
    except AgentQueryError as e:
        # Log the error or handle it as needed
        raise e
    except ValueError as e:
        raise AgentQueryError(f""Invalid input parameter: {str(e)}"")
    except Exception as e:
        # Catch any other unexpected exceptions
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def get_agent_details(agent_id: str, version: int | None = None):
    """"""
    Retrieve agent details from the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int | None, optional): The version of the agent. Defaults to None.

    Returns:
        dict: The agent details.

    Raises:
        AgentQueryError: If the agent is not found or if there is an error querying the database.
    """"""
    try:
        query = {""id"": agent_id}

        if version is not None:
            query[""version""] = version  # type: ignore

        agent = await prisma.models.Agents.prisma().find_first(where=query)  # type: ignore

        if not agent:
            raise AgentQueryError(""Agent not found"")

        return agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def search_db(
    query: str,
    page: int = 1,
    page_size: int = 10,
    categories: typing.List[str] | None = None,
    description_threshold: int = 60,
    sort_by: str = ""rank"",
    sort_order: typing.Literal[""desc""] | typing.Literal[""asc""] = ""desc"",
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
) -> market.model.ListResponse[market.utils.extension_types.AgentsWithRank]:
    """"""Perform a search for agents based on the provided query string.

    Args:
        query (str): the search string
        page (int, optional): page for searching. Defaults to 1.
        page_size (int, optional): the number of results to return. Defaults to 10.
        categories (List[str] | None, optional): list of category filters. Defaults to None.
        description_threshold (int, optional): number of characters to return. Defaults to 60.
        sort_by (str, optional): sort by option. Defaults to ""rank"".
        sort_order (""asc"" | ""desc"", optional): the sort order. Defaults to ""desc"".

    Raises:
        AgentQueryError: Raises an error if the query fails.
        AgentQueryError: Raises if an unexpected error occurs.

    Returns:
        List[AgentsWithRank]: List of agents matching the search criteria.
    """"""
    try:
        offset = (page - 1) * page_size

        category_filter = ""1=1""

        if categories:
            category_conditions = [f""'{cat}' = ANY(categories)"" for cat in categories]
            category_filter = ""AND ("" + "" OR "".join(category_conditions) + "")""

        # Construct the ORDER BY clause based on the sort_by parameter
        if sort_by in [""createdAt"", ""updatedAt""]:
            order_by_clause = f'""{sort_by}"" {sort_order.upper()}, rank DESC'
        elif sort_by == ""name"":
            order_by_clause = f""name {sort_order.upper()}, rank DESC""
        else:
            order_by_clause = 'rank DESC, ""createdAt"" DESC'

        submission_status_filter = f'""""submissionStatus"""" = \'{submission_status}\''

        sql_query = f""""""
        WITH query AS (
            SELECT to_tsquery(string_agg(lexeme || ':*', ' & ' ORDER BY positions)) AS q 
            FROM unnest(to_tsvector('{query}'))
        )
        SELECT 
            id, 
            ""createdAt"", 
            ""updatedAt"", 
            version, 
            name, 
            LEFT(description, {description_threshold}) AS description, 
            author, 
            keywords, 
            categories, 
            graph,
            ""submissionStatus"",
            ""submissionDate"",
            CASE 
                WHEN query.q::text = '' THEN 1.0
                ELSE COALESCE(ts_rank(CAST(search AS tsvector), query.q), 0.0)
            END AS rank
        FROM market.""Agents"", query
        WHERE 
            (query.q::text = '' OR search @@ query.q)
            AND {category_filter} 
            AND {submission_status_filter}
        ORDER BY {order_by_clause}
        LIMIT {page_size}
        OFFSET {offset};
        """"""
        results = await prisma.client.get_client().query_raw(
            query=sql_query,
            model=market.utils.extension_types.AgentsWithRank,
        )

        class CountResponse(pydantic.BaseModel):
            count: int

        count_query = f""""""
        WITH query AS (
            SELECT to_tsquery(string_agg(lexeme || ':*', ' & ' ORDER BY positions)) AS q 
            FROM unnest(to_tsvector('{query}'))
        )
        SELECT COUNT(*)
        FROM market.""Agents"", query
        WHERE (search @@ query.q OR query.q = '') AND {category_filter} AND {submission_status_filter};
        """"""
        total_count = await prisma.client.get_client().query_first(
            query=count_query,
            model=CountResponse,
        )
        total_count = total_count.count if total_count else 0

        return market.model.ListResponse(
            items=results,
            total_count=total_count,
            page=page,
            page_size=page_size,
            total_pages=(total_count + page_size - 1) // page_size,
        )
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def get_top_agents_by_downloads(
    page: int = 1,
    page_size: int = 10,
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
) -> market.model.ListResponse[prisma.models.AnalyticsTracker]:
    """"""Retrieve the top agents by download count.

    Args:
        page (int, optional): The page number. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.

    Returns:
        dict: A dictionary containing the list of agents, total count, current page number, page size, and total number of pages.
    """"""
    try:
        # Calculate pagination
        skip = (page - 1) * page_size

        # Execute the query
        try:
            # Agents with no downloads will not be included in the results... is this the desired behavior?
            analytics = await prisma.models.AnalyticsTracker.prisma().find_many(
                include={""agent"": True},
                order={""downloads"": ""desc""},
                where={""agent"": {""is"": {""submissionStatus"": submission_status}}},
                skip=skip,
                take=page_size,
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        try:
            total_count = await prisma.models.AnalyticsTracker.prisma().count(
                where={""agent"": {""is"": {""submissionStatus"": submission_status}}},
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        return market.model.ListResponse(
            items=analytics,
            total_count=total_count,
            page=page,
            page_size=page_size,
            total_pages=(total_count + page_size - 1) // page_size,
        )
    except AgentQueryError as e:
        # Log the error or handle it as needed
        raise e from e
    except ValueError as e:
        raise AgentQueryError(f""Invalid input parameter: {str(e)}"") from e
    except Exception as e:
        # Catch any other unexpected exceptions
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"") from e

async def set_agent_featured(
    agent_id: str, is_active: bool = True, featured_categories: list[str] = [""featured""]
) -> prisma.models.FeaturedAgent:
    """"""Set an agent as featured in the database.

    Args:
        agent_id (str): The ID of the agent.
        category (str, optional): The category to set the agent as featured. Defaults to ""featured"".

    Raises:
        AgentQueryError: If there is an error setting the agent as featured.
    """"""
    try:
        agent = await prisma.models.Agents.prisma().find_unique(where={""id"": agent_id})

        if not agent:
            raise AgentQueryError(f""Agent with ID {agent_id} not found."")

        featured = await prisma.models.FeaturedAgent.prisma().upsert(
            where={""agentId"": agent_id},
            data={
                ""update"": {
                    ""featuredCategories"": featured_categories,
                    ""isActive"": is_active,
                },
                ""create"": {
                    ""featuredCategories"": featured_categories,
                    ""isActive"": is_active,
                    ""agent"": {""connect"": {""id"": agent_id}},
                },
            },
        )
        return featured
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def get_featured_agents(
    category: str = ""featured"",
    page: int = 1,
    page_size: int = 10,
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
) -> FeaturedAgentResponse:
    """"""Retrieve a list of featured agents from the database based on the provided category.

    Args:
        category (str, optional): The category of featured agents to retrieve. Defaults to ""featured"".
        page (int, optional): The page number to retrieve. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.

    Returns:
        dict: A dictionary containing the list of featured agents, total count, current page number, page size, and total number of pages.
    """"""
    try:
        # Calculate pagination
        skip = (page - 1) * page_size

        # Execute the query
        try:
            featured_agents = await prisma.models.FeaturedAgent.prisma().find_many(
                where={
                    ""featuredCategories"": {""has"": category},
                    ""isActive"": True,
                    ""agent"": {""is"": {""submissionStatus"": submission_status}},
                },
                include={""agent"": {""include"": {""AnalyticsTracker"": True}}},
                skip=skip,
                take=page_size,
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        # Get total count for pagination info
        total_count = len(featured_agents)

        return FeaturedAgentResponse(
            featured_agents=featured_agents,
            total_count=total_count,
            page=page,
            page_size=page_size,
            total_pages=(total_count + page_size - 1) // page_size,
        )
    except AgentQueryError as e:
        # Log the error or handle it as needed
        raise e from e
    except ValueError as e:
        raise AgentQueryError(f""Invalid input parameter: {str(e)}"") from e
    except Exception as e:
        # Catch any other unexpected exceptions
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"") from e

async def remove_featured_category(
    agent_id: str, category: str
) -> prisma.models.FeaturedAgent | None:
    """"""Adds a featured category to an agent.

    Args:
        agent_id (str): The ID of the agent.
        category (str): The category to add to the agent.

    Returns:
        FeaturedAgentResponse: The updated list of featured agents.
    """"""
    try:
        # get the existing categories
        featured_agent = await prisma.models.FeaturedAgent.prisma().find_unique(
            where={""agentId"": agent_id},
            include={""agent"": True},
        )

        if not featured_agent:
            raise AgentQueryError(f""Agent with ID {agent_id} not found."")

        # remove the category from the list
        featured_agent.featuredCategories.remove(category)

        featured_agent = await prisma.models.FeaturedAgent.prisma().update(
            where={""agentId"": agent_id},
            data={""featuredCategories"": featured_agent.featuredCategories},
        )
        return featured_agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def add_featured_category(
    agent_id: str, category: str
) -> prisma.models.FeaturedAgent | None:
    """"""Removes a featured category from an agent.

    Args:
        agent_id (str): The ID of the agent.
        category (str): The category to remove from the agent.

    Returns:
        FeaturedAgentResponse: The updated list of featured agents.
    """"""
    try:
        featured_agent = await prisma.models.FeaturedAgent.prisma().update(
            where={""agentId"": agent_id},
            data={""featuredCategories"": {""push"": [category]}},
        )
        return featured_agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def get_agent_featured(agent_id: str) -> prisma.models.FeaturedAgent | None:
    """"""Retrieve an agent's featured categories from the database.

    Args:
        agent_id (str): The ID of the agent.

    Returns:
        FeaturedAgentResponse: The list of featured agents.
    """"""
    try:
        featured_agent = await prisma.models.FeaturedAgent.prisma().find_unique(
            where={""agentId"": agent_id},
        )
        return featured_agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def get_not_featured_agents(
    page: int = 1, page_size: int = 10
) -> typing.List[prisma.models.Agents]:
    """"""
    Retrieve a list of not featured agents from the database.
    """"""
    try:
        agents = await prisma.client.get_client().query_raw(
            query=f""""""
            SELECT 
                ""market"".""Agents"".id, 
                ""market"".""Agents"".""createdAt"", 
                ""market"".""Agents"".""updatedAt"", 
                ""market"".""Agents"".version, 
                ""market"".""Agents"".name, 
                LEFT(""market"".""Agents"".description, 500) AS description, 
                ""market"".""Agents"".author, 
                ""market"".""Agents"".keywords, 
                ""market"".""Agents"".categories, 
                ""market"".""Agents"".graph,
                ""market"".""Agents"".""submissionStatus"",
                ""market"".""Agents"".""submissionDate"",
                ""market"".""Agents"".search::text AS search
            FROM ""market"".""Agents""
            LEFT JOIN ""market"".""FeaturedAgent"" ON ""market"".""Agents"".""id"" = ""market"".""FeaturedAgent"".""agentId""
            WHERE (""market"".""FeaturedAgent"".""agentId"" IS NULL OR ""market"".""FeaturedAgent"".""featuredCategories"" = '{{}}')
                AND ""market"".""Agents"".""submissionStatus"" = 'APPROVED'
            ORDER BY ""market"".""Agents"".""createdAt"" DESC
            LIMIT {page_size} OFFSET {page_size * (page - 1)}
            """""",
            model=prisma.models.Agents,
        )
        return agents
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")

async def get_all_categories() -> market.model.CategoriesResponse:
    """"""
    Retrieve all unique categories from the database.

    Returns:
        CategoriesResponse: A list of unique categories.
    """"""
    try:
        agents = await prisma.models.Agents.prisma().find_many(distinct=[""categories""])

        # Aggregate categories on the Python side
        all_categories = set()

        for agent in agents:
            all_categories.update(agent.categories)

        unique_categories = sorted(list(all_categories))

        return market.model.CategoriesResponse(unique_categories=unique_categories)
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception:
        # Return an empty list of categories in case of unexpected errors
        return market.model.CategoriesResponse(unique_categories=[])

async def create_agent_installed_event(
    event_data: market.model.AgentInstalledFromMarketplaceEventData,
):
    try:
        await prisma.models.InstallTracker.prisma().create(
            data={
                ""installedAgentId"": event_data.installed_agent_id,
                ""marketplaceAgentId"": event_data.marketplace_agent_id,
                ""installationLocation"": prisma.enums.InstallationLocation(
                    event_data.installation_location.name
                ),
            }
        )
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","{'final_score': 99.75, 'length_similarity': 100.0, 'token_similarity': 99.16, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
105,"class Node(object):

    def __init__(self, results):
        self.results = results
        self.next = next


class LinkedList(object):

    def __init__(self):
        self.head = None
        self.tail = None

    def move_to_front(self, node):
        pass

    def append_to_front(self, node):
        pass

    def remove_from_tail(self):
        pass


class Cache(object):

    def __init__(self, MAX_SIZE):
        self.MAX_SIZE = MAX_SIZE
        self.size = 0
        self.lookup = {}  # key: query, value: node
        self.linked_list = LinkedList()

    def get(self, query):
        """"""Get the stored query result from the cache.

        Accessing a node updates its position to the front of the LRU list.
        """"""
        node = self.lookup.get(query)
        if node is None:
            return None
        self.linked_list.move_to_front(node)
        return node.results

    def set(self, results, query):
        """"""Set the result for the given query key in the cache.

        When updating an entry, updates its position to the front of the LRU list.
        If the entry is new and the cache is at capacity, removes the oldest entry
        before the new entry is added.
        """"""
        node = self.lookup.get(query)
        if node is not None:
            # Key exists in cache, update the value
            node.results = results
            self.linked_list.move_to_front(node)
        else:
            # Key does not exist in cache
            if self.size == self.MAX_SIZE:
                # Remove the oldest entry from the linked list and lookup
                self.lookup.pop(self.linked_list.tail.query, None)
                self.linked_list.remove_from_tail()
            else:
                self.size += 1
            # Add the new key and value
            new_node = Node(results)
            self.linked_list.append_to_front(new_node)
            self.lookup[query] = new_node
","class Node(object):

    def __init__(self, results):
        self.results = results
        self.next = None  # Corrected from next to None
        self.prev = None  # Added to support doubly linked list functionality


class LinkedList(object):

    def __init__(self):
        self.head = None
        self.tail = None

    def move_to_front(self, node):
        if node is self.head:
            return

        # Remove node from its position
        if node.prev:
            node.prev.next = node.next
        if node.next:
            node.next.prev = node.prev

        if node is self.tail:
            self.tail = node.prev

        # Place node at the front
        node.next = self.head
        node.prev = None

        if self.head:
            self.head.prev = node

        self.head = node

        if self.tail is None:
            self.tail = node

    def append_to_front(self, node):
        if not self.head:
            self.head = node
            self.tail = node
        else:
            node.next = self.head
            self.head.prev = node
            self.head = node

    def remove_from_tail(self):
        if not self.tail:
            return

        if self.tail.prev:
            self.tail = self.tail.prev
            self.tail.next = None
        else:
            self.head = None
            self.tail = None


class Cache(object):

    def __init__(self, MAX_SIZE):
        self.MAX_SIZE = MAX_SIZE
        self.size = 0
        self.lookup = {}  # key: query, value: node
        self.linked_list = LinkedList()

    def get(self, query):
        """"""Get the stored query result from the cache.

        Accessing a node updates its position to the front of the LRU list.
        """"""
        node = self.lookup.get(query)
        if node is None:
            return None
        self.linked_list.move_to_front(node)
        return node.results

    def set(self, results, query):
        """"""Set the result for the given query key in the cache.

        When updating an entry, updates its position to the front of the LRU list.
        If the entry is new and the cache is at capacity, removes the oldest entry
        before the new entry is added.
        """"""
        node = self.lookup.get(query)
        if node is not None:
            # Key exists in cache, update the value
            node.results = results
            self.linked_list.move_to_front(node)
        else:
            # Key does not exist in cache
            if self.size == self.MAX_SIZE:
                # Remove the oldest entry from the linked list and lookup
                if self.linked_list.tail:
                    tail_query = self.linked_list.tail.results
                    self.lookup.pop(tail_query, None)
                self.linked_list.remove_from_tail()
            else:
                self.size += 1

            # Add the new key and value
            new_node = Node(results)
            self.linked_list.append_to_front(new_node)
            self.lookup[query] = new_node","{'final_score': 74.63, 'length_similarity': 63.99, 'token_similarity': 81.06, 'structure_similarity': 61.9, 'function_similarity': 100, 'complexity_similarity': 33.33, 'variable_similarity': 100}",
106,"from enum import Enum
from typing import Literal

from e2b_code_interpreter import Sandbox
from pydantic import SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""e2b"",
    api_key=SecretStr(""mock-e2b-api-key""),
    title=""Mock E2B API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class ProgrammingLanguage(Enum):
    PYTHON = ""python""
    JAVASCRIPT = ""js""
    BASH = ""bash""
    R = ""r""
    JAVA = ""java""


class CodeExecutionBlock(Block):
    # TODO : Add support to upload and download files
    # Currently, You can customized the CPU and Memory, only by creating a pre customized sandbox template
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.E2B], Literal[""api_key""]
        ] = CredentialsField(
            description=""Enter your api key for the E2B Sandbox. You can get it in here - https://e2b.dev/docs"",
        )

        # Todo : Option to run commond in background
        setup_commands: list[str] = SchemaField(
            description=(
                ""Shell commands to set up the sandbox before running the code. ""
                ""You can use `curl` or `git` to install your desired Debian based ""
                ""package manager. `pip` and `npm` are pre-installed.\n\n""
                ""These commands are executed with `sh`, in the foreground.""
            ),
            placeholder=""pip install cowsay"",
            default=[],
            advanced=False,
        )

        code: str = SchemaField(
            description=""Code to execute in the sandbox"",
            placeholder=""print('Hello, World!')"",
            default="""",
            advanced=False,
        )

        language: ProgrammingLanguage = SchemaField(
            description=""Programming language to execute"",
            default=ProgrammingLanguage.PYTHON,
            advanced=False,
        )

        timeout: int = SchemaField(
            description=""Execution timeout in seconds"", default=300
        )

        template_id: str = SchemaField(
            description=(
                ""You can use an E2B sandbox template by entering its ID here. ""
                ""Check out the E2B docs for more details: ""
                ""[E2B - Sandbox template](https://e2b.dev/docs/sandbox-template)""
            ),
            default="""",
            advanced=True,
        )

    class Output(BlockSchema):
        response: str = SchemaField(description=""Response from code execution"")
        stdout_logs: str = SchemaField(
            description=""Standard output logs from execution""
        )
        stderr_logs: str = SchemaField(description=""Standard error logs from execution"")
        error: str = SchemaField(description=""Error message if execution failed"")

    def __init__(self):
        super().__init__(
            id=""0b02b072-abe7-11ef-8372-fb5d162dd712"",
            description=""Executes code in an isolated sandbox environment with internet access."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=CodeExecutionBlock.Input,
            output_schema=CodeExecutionBlock.Output,
            test_credentials=TEST_CREDENTIALS,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""code"": ""print('Hello World')"",
                ""language"": ProgrammingLanguage.PYTHON.value,
                ""setup_commands"": [],
                ""timeout"": 300,
                ""template_id"": """",
            },
            test_output=[
                (""response"", ""Hello World""),
                (""stdout_logs"", ""Hello World\n""),
            ],
            test_mock={
                ""execute_code"": lambda code, language, setup_commands, timeout, api_key, template_id: (
                    ""Hello World"",
                    ""Hello World\n"",
                    """",
                ),
            },
        )

    def execute_code(
        self,
        code: str,
        language: ProgrammingLanguage,
        setup_commands: list[str],
        timeout: int,
        api_key: str,
        template_id: str,
    ):
        try:
            sandbox = None
            if template_id:
                sandbox = Sandbox(
                    template=template_id, api_key=api_key, timeout=timeout
                )
            else:
                sandbox = Sandbox(api_key=api_key, timeout=timeout)

            if not sandbox:
                raise Exception(""Sandbox not created"")

            # Running setup commands
            for cmd in setup_commands:
                sandbox.commands.run(cmd)

            # Executing the code
            execution = sandbox.run_code(
                code,
                language=language.value,
                on_error=lambda e: sandbox.kill(),  # Kill the sandbox if there is an error
            )

            if execution.error:
                raise Exception(execution.error)

            response = execution.text
            stdout_logs = """".join(execution.logs.stdout)
            stderr_logs = """".join(execution.logs.stderr)

            return response, stdout_logs, stderr_logs

        except Exception as e:
            raise e

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            response, stdout_logs, stderr_logs = self.execute_code(
                input_data.code,
                input_data.language,
                input_data.setup_commands,
                input_data.timeout,
                credentials.api_key.get_secret_value(),
                input_data.template_id,
            )

            if response:
                yield ""response"", response
            if stdout_logs:
                yield ""stdout_logs"", stdout_logs
            if stderr_logs:
                yield ""stderr_logs"", stderr_logs
        except Exception as e:
            yield ""error"", str(e)
","from enum import Enum
from typing import Literal

from e2b_code_interpreter import Sandbox
from pydantic import SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""e2b"",
    api_key=SecretStr(""mock-e2b-api-key""),
    title=""Mock E2B API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}

class ProgrammingLanguage(Enum):
    PYTHON = ""python""
    JAVASCRIPT = ""js""
    BASH = ""bash""
    R = ""r""
    JAVA = ""java""

class CodeExecutionBlock(Block):
    # TODO : Add support to upload and download files
    # Currently, You can customized the CPU and Memory, only by creating a pre customized sandbox template
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.E2B], Literal[""api_key""]
        ] = CredentialsField(
            description=""Enter your api key for the E2B Sandbox. You can get it in here - https://e2b.dev/docs"",
        )

        # Todo : Option to run commond in background
        setup_commands: list[str] = SchemaField(
            description=(
                ""Shell commands to set up the sandbox before running the code. ""
                ""You can use `curl` or `git` to install your desired Debian based ""
                ""package manager. `pip` and `npm` are pre-installed.\n\n""
                ""These commands are executed with `sh`, in the foreground.""
            ),
            placeholder=""pip install cowsay"",
            default=[],
            advanced=False,
        )

        code: str = SchemaField(
            description=""Code to execute in the sandbox"",
            placeholder=""print('Hello, World!')"",
            default="""",
            advanced=False,
        )

        language: ProgrammingLanguage = SchemaField(
            description=""Programming language to execute"",
            default=ProgrammingLanguage.PYTHON,
            advanced=False,
        )

        timeout: int = SchemaField(
            description=""Execution timeout in seconds"", default=300
        )

        template_id: str = SchemaField(
            description=(
                ""You can use an E2B sandbox template by entering its ID here. ""
                ""Check out the E2B docs for more details: ""
                ""[E2B - Sandbox template](https://e2b.dev/docs/sandbox-template)""
            ),
            default="""",
            advanced=True,
        )

    class Output(BlockSchema):
        response: str = SchemaField(description=""Response from code execution"")
        stdout_logs: str = SchemaField(
            description=""Standard output logs from execution""
        )
        stderr_logs: str = SchemaField(description=""Standard error logs from execution"")
        error: str = SchemaField(description=""Error message if execution failed"")

    def __init__(self):
        super().__init__(
            id=""0b02b072-abe7-11ef-8372-fb5d162dd712"",
            description=""Executes code in an isolated sandbox environment with internet access."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=CodeExecutionBlock.Input,
            output_schema=CodeExecutionBlock.Output,
            test_credentials=TEST_CREDENTIALS,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""code"": ""print('Hello World')"",
                ""language"": ProgrammingLanguage.PYTHON.value,
                ""setup_commands"": [],
                ""timeout"": 300,
                ""template_id"": """",
            },
            test_output=[
                (""response"", ""Hello World""),
                (""stdout_logs"", ""Hello World\n""),
            ],
            test_mock={
                ""execute_code"": lambda code, language, setup_commands, timeout, api_key, template_id: (
                    ""Hello World"",
                    ""Hello World\n"",
                    """",
                ),
            },
        )

    def execute_code(
        self,
        code: str,
        language: ProgrammingLanguage,
        setup_commands: list[str],
        timeout: int,
        api_key: str,
        template_id: str,
    ):
        try:
            sandbox = None

            if template_id:
                sandbox = Sandbox(
                    template=template_id, api_key=api_key, timeout=timeout
                )
            else:
                sandbox = Sandbox(api_key=api_key, timeout=timeout)

            if not sandbox:
                raise Exception(""Sandbox not created"")

            # Running setup commands
            for cmd in setup_commands:
                sandbox.commands.run(cmd)

            # Executing the code
            execution = sandbox.run_code(
                code,
                language=language.value,
                on_error=lambda e: sandbox.kill(),  # Kill the sandbox if there is an error
            )

            if execution.error:
                raise Exception(execution.error)

            response = execution.text
            stdout_logs = """".join(execution.logs.stdout)
            stderr_logs = """".join(execution.logs.stderr)

            return response, stdout_logs, stderr_logs

        except Exception as e:
            raise e

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            response, stdout_logs, stderr_logs = self.execute_code(
                input_data.code,
                input_data.language,
                input_data.setup_commands,
                input_data.timeout,
                credentials.api_key.get_secret_value(),
                input_data.template_id,
            )

            if response:
                yield ""response"", response
            if stdout_logs:
                yield ""stdout_logs"", stdout_logs
            if stderr_logs:
                yield ""stderr_logs"", stderr_logs

        except Exception as e:
            yield ""error"", str(e)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
107,"from typing import Any

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.json import json


class StepThroughItemsBlock(Block):
    class Input(BlockSchema):
        items: list = SchemaField(
            advanced=False,
            description=""The list or dictionary of items to iterate over"",
            placeholder=""[1, 2, 3, 4, 5] or {'key1': 'value1', 'key2': 'value2'}"",
            default=[],
        )
        items_object: dict = SchemaField(
            advanced=False,
            description=""The list or dictionary of items to iterate over"",
            placeholder=""[1, 2, 3, 4, 5] or {'key1': 'value1', 'key2': 'value2'}"",
            default={},
        )
        items_str: str = SchemaField(
            advanced=False,
            description=""The list or dictionary of items to iterate over"",
            placeholder=""[1, 2, 3, 4, 5] or {'key1': 'value1', 'key2': 'value2'}"",
            default="""",
        )

    class Output(BlockSchema):
        item: Any = SchemaField(description=""The current item in the iteration"")
        key: Any = SchemaField(
            description=""The key or index of the current item in the iteration"",
        )

    def __init__(self):
        super().__init__(
            id=""f66a3543-28d3-4ab5-8945-9b336371e2ce"",
            input_schema=StepThroughItemsBlock.Input,
            output_schema=StepThroughItemsBlock.Output,
            categories={BlockCategory.LOGIC},
            description=""Iterates over a list or dictionary and outputs each item."",
            test_input={""items"": [1, 2, 3, {""key1"": ""value1"", ""key2"": ""value2""}]},
            test_output=[
                (""item"", 1),
                (""key"", 0),
                (""item"", 2),
                (""key"", 1),
                (""item"", 3),
                (""key"", 2),
                (""item"", {""key1"": ""value1"", ""key2"": ""value2""}),
                (""key"", 3),
            ],
            test_mock={},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        for data in [input_data.items, input_data.items_object, input_data.items_str]:
            if not data:
                continue
            if isinstance(data, str):
                items = json.loads(data)
            else:
                items = data
            if isinstance(items, dict):
                # If items is a dictionary, iterate over its values
                for item in items.values():
                    yield ""item"", item
                    yield ""key"", item
            else:
                # If items is a list, iterate over the list
                for index, item in enumerate(items):
                    yield ""item"", item
                    yield ""key"", index
","from typing import Any
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.json import json

class StepThroughItemsBlock(Block):
    class Input(BlockSchema):
        items: list = SchemaField(
            advanced=False,
            description=""The list or dictionary of items to iterate over"",
            placeholder=""[1, 2, 3, 4, 5] or {'key1': 'value1', 'key2': 'value2'}"",
            default=[],
        )
        items_object: dict = SchemaField(
            advanced=False,
            description=""The list or dictionary of items to iterate over"",
            placeholder=""[1, 2, 3, 4, 5] or {'key1': 'value1', 'key2': 'value2'}"",
            default={},
        )
        items_str: str = SchemaField(
            advanced=False,
            description=""The list or dictionary of items to iterate over"",
            placeholder=""[1, 2, 3, 4, 5] or {'key1': 'value1', 'key2': 'value2'}"",
            default="""",
        )

    class Output(BlockSchema):
        item: Any = SchemaField(description=""The current item in the iteration"")
        key: Any = SchemaField(
            description=""The key or index of the current item in the iteration"",
        )

    def __init__(self):
        super().__init__(
            id=""f66a3543-28d3-4ab5-8945-9b336371e2ce"",
            input_schema=StepThroughItemsBlock.Input,
            output_schema=StepThroughItemsBlock.Output,
            categories={BlockCategory.LOGIC},
            description=""Iterates over a list or dictionary and outputs each item."",
            test_input={""items"": [1, 2, 3, {""key1"": ""value1"", ""key2"": ""value2""}]},
            test_output=[
                (""item"", 1),
                (""key"", 0),
                (""item"", 2),
                (""key"", 1),
                (""item"", 3),
                (""key"", 2),
                (""item"", {""key1"": ""value1"", ""key2"": ""value2""}),
                (""key"", 3),
            ],
            test_mock={},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        for data in [input_data.items, input_data.items_object, input_data.items_str]:
            if not data:
                continue
            if isinstance(data, str):
                items = json.loads(data)
            else:
                items = data
            if isinstance(items, dict):
                # If items is a dictionary, iterate over its values
                for item in items.values():
                    yield ""item"", item
                    yield ""key"", item
            else:
                # If items is a list, iterate over the list
                for index, item in enumerate(items):
                    yield ""item"", item
                    yield ""key"", index","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
108,"from base64 import b64encode
from urllib.parse import urlencode

from backend.data.model import OAuth2Credentials
from backend.integrations.providers import ProviderName
from backend.util.request import requests

from .base import BaseOAuthHandler


class NotionOAuthHandler(BaseOAuthHandler):
    """"""
    Based on the documentation at https://developers.notion.com/docs/authorization

    Notes:
    - Notion uses non-expiring access tokens and therefore doesn't have a refresh flow
    - Notion doesn't use scopes
    """"""

    PROVIDER_NAME = ProviderName.NOTION

    def __init__(self, client_id: str, client_secret: str, redirect_uri: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.auth_base_url = ""https://api.notion.com/v1/oauth/authorize""
        self.token_url = ""https://api.notion.com/v1/oauth/token""

    def get_login_url(self, scopes: list[str], state: str) -> str:
        params = {
            ""client_id"": self.client_id,
            ""redirect_uri"": self.redirect_uri,
            ""response_type"": ""code"",
            ""owner"": ""user"",
            ""state"": state,
        }
        return f""{self.auth_base_url}?{urlencode(params)}""

    def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        request_body = {
            ""grant_type"": ""authorization_code"",
            ""code"": code,
            ""redirect_uri"": self.redirect_uri,
        }
        auth_str = b64encode(f""{self.client_id}:{self.client_secret}"".encode()).decode()
        headers = {
            ""Authorization"": f""Basic {auth_str}"",
            ""Accept"": ""application/json"",
        }
        response = requests.post(self.token_url, json=request_body, headers=headers)
        token_data = response.json()
        # Email is only available for non-bot users
        email = (
            token_data[""owner""][""person""][""email""]
            if ""person"" in token_data[""owner""]
            and ""email"" in token_data[""owner""][""person""]
            else None
        )

        return OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            title=token_data.get(""workspace_name""),
            username=email,
            access_token=token_data[""access_token""],
            refresh_token=None,
            access_token_expires_at=None,  # Notion tokens don't expire
            refresh_token_expires_at=None,
            scopes=[],
            metadata={
                ""owner"": token_data[""owner""],
                ""bot_id"": token_data[""bot_id""],
                ""workspace_id"": token_data[""workspace_id""],
                ""workspace_name"": token_data.get(""workspace_name""),
                ""workspace_icon"": token_data.get(""workspace_icon""),
            },
        )

    def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        # Notion doesn't support token revocation
        return False

    def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # Notion doesn't support token refresh
        return credentials

    def needs_refresh(self, credentials: OAuth2Credentials) -> bool:
        # Notion access tokens don't expire
        return False
","from base64 import b64encode
from urllib.parse import urlencode
from backend.data.model import OAuth2Credentials
from backend.integrations.providers import ProviderName
from backend.util.request import requests
from .base import BaseOAuthHandler

class NotionOAuthHandler(BaseOAuthHandler):
    """"""
    Based on the documentation at https://developers.notion.com/docs/authorization

    Notes:
    - Notion uses non-expiring access tokens and therefore doesn't have a refresh flow
    - Notion doesn't use scopes
    """"""
    PROVIDER_NAME = ProviderName.NOTION

    def __init__(self, client_id: str, client_secret: str, redirect_uri: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.auth_base_url = ""https://api.notion.com/v1/oauth/authorize""
        self.token_url = ""https://api.notion.com/v1/oauth/token""

    def get_login_url(self, scopes: list[str], state: str) -> str:
        params = {
            ""client_id"": self.client_id,
            ""redirect_uri"": self.redirect_uri,
            ""response_type"": ""code"",
            ""owner"": ""user"",
            ""state"": state,
        }
        return f""{self.auth_base_url}?{urlencode(params)}""

    def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        request_body = {
            ""grant_type"": ""authorization_code"",
            ""code"": code,
            ""redirect_uri"": self.redirect_uri,
        }
        auth_str = b64encode(f""{self.client_id}:{self.client_secret}"".encode()).decode()
        headers = {
            ""Authorization"": f""Basic {auth_str}"",
            ""Accept"": ""application/json"",
        }
        response = requests.post(self.token_url, json=request_body, headers=headers)
        token_data = response.json()
        # Email is only available for non-bot users
        email = (
            token_data[""owner""][""person""][""email""]
            if ""person"" in token_data[""owner""]
            and ""email"" in token_data[""owner""][""person""]
            else None
        )

        return OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            title=token_data.get(""workspace_name""),
            username=email,
            access_token=token_data[""access_token""],
            refresh_token=None,
            access_token_expires_at=None,  # Notion tokens don't expire
            refresh_token_expires_at=None,
            scopes=[],
            metadata={
                ""owner"": token_data[""owner""],
                ""bot_id"": token_data[""bot_id""],
                ""workspace_id"": token_data[""workspace_id""],
                ""workspace_name"": token_data.get(""workspace_name""),
                ""workspace_icon"": token_data.get(""workspace_icon""),
            },
        )

    def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        # Notion doesn't support token revocation
        return False

    def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # Notion doesn't support token refresh
        return credentials

    def needs_refresh(self, credentials: OAuth2Credentials) -> bool:
        # Notion access tokens don't expire
        return False","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
109,"# Information on binary shifts:
# https://docs.python.org/3/library/stdtypes.html#bitwise-operations-on-integer-types
# https://www.interviewcake.com/concept/java/bit-shift


def logical_left_shift(number: int, shift_amount: int) -> str:
    """"""
    Take in 2 positive integers.
    'number' is the integer to be logically left shifted 'shift_amount' times.
    i.e. (number << shift_amount)
    Return the shifted binary representation.

    >>> logical_left_shift(0, 1)
    '0b00'
    >>> logical_left_shift(1, 1)
    '0b10'
    >>> logical_left_shift(1, 5)
    '0b100000'
    >>> logical_left_shift(17, 2)
    '0b1000100'
    >>> logical_left_shift(1983, 4)
    '0b111101111110000'
    >>> logical_left_shift(1, -1)
    Traceback (most recent call last):
        ...
    ValueError: both inputs must be positive integers
    """"""
    if number < 0 or shift_amount < 0:
        raise ValueError(""both inputs must be positive integers"")

    binary_number = str(bin(number))
    binary_number += ""0"" * shift_amount
    return binary_number


def logical_right_shift(number: int, shift_amount: int) -> str:
    """"""
    Take in positive 2 integers.
    'number' is the integer to be logically right shifted 'shift_amount' times.
    i.e. (number >>> shift_amount)
    Return the shifted binary representation.

    >>> logical_right_shift(0, 1)
    '0b0'
    >>> logical_right_shift(1, 1)
    '0b0'
    >>> logical_right_shift(1, 5)
    '0b0'
    >>> logical_right_shift(17, 2)
    '0b100'
    >>> logical_right_shift(1983, 4)
    '0b1111011'
    >>> logical_right_shift(1, -1)
    Traceback (most recent call last):
        ...
    ValueError: both inputs must be positive integers
    """"""
    if number < 0 or shift_amount < 0:
        raise ValueError(""both inputs must be positive integers"")

    binary_number = str(bin(number))[2:]
    if shift_amount >= len(binary_number):
        return ""0b0""
    shifted_binary_number = binary_number[: len(binary_number) - shift_amount]
    return ""0b"" + shifted_binary_number


def arithmetic_right_shift(number: int, shift_amount: int) -> str:
    """"""
    Take in 2 integers.
    'number' is the integer to be arithmetically right shifted 'shift_amount' times.
    i.e. (number >> shift_amount)
    Return the shifted binary representation.

    >>> arithmetic_right_shift(0, 1)
    '0b00'
    >>> arithmetic_right_shift(1, 1)
    '0b00'
    >>> arithmetic_right_shift(-1, 1)
    '0b11'
    >>> arithmetic_right_shift(17, 2)
    '0b000100'
    >>> arithmetic_right_shift(-17, 2)
    '0b111011'
    >>> arithmetic_right_shift(-1983, 4)
    '0b111110000100'
    """"""
    if number >= 0:  # Get binary representation of positive number
        binary_number = ""0"" + str(bin(number)).strip(""-"")[2:]
    else:  # Get binary (2's complement) representation of negative number
        binary_number_length = len(bin(number)[3:])  # Find 2's complement of number
        binary_number = bin(abs(number) - (1 << binary_number_length))[3:]
        binary_number = (
            ""1"" + ""0"" * (binary_number_length - len(binary_number)) + binary_number
        )

    if shift_amount >= len(binary_number):
        return ""0b"" + binary_number[0] * len(binary_number)
    return (
        ""0b""
        + binary_number[0] * shift_amount
        + binary_number[: len(binary_number) - shift_amount]
    )


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","# Information on binary shifts:
# https://docs.python.org/3/library/stdtypes.html#bitwise-operations-on-integer-types
# https://www.interviewcake.com/concept/java/bit-shift



def logical_left_shift(number: int, shift_amount: int) -> str:
    """"""
    Take in 2 positive integers.
    'number' is the integer to be logically left shifted 'shift_amount' times.
    i.e. (number << shift_amount)
    Return the shifted binary representation.

    >>> logical_left_shift(0, 1)
    '0b00'
    >>> logical_left_shift(1, 1)
    '0b10'
    >>> logical_left_shift(1, 5)
    '0b100000'
    >>> logical_left_shift(17, 2)
    '0b1000100'
    >>> logical_left_shift(1983, 4)
    '0b111101111110000'
    >>> logical_left_shift(1, -1)
    Traceback (most recent call last):
        ...
    ValueError: both inputs must be positive integers
    """"""
    if number < 0 or shift_amount < 0:
        raise ValueError(""both inputs must be positive integers"")
    binary_number = str(bin(number))
    binary_number += ""0"" * shift_amount
    return binary_number

def logical_right_shift(number: int, shift_amount: int) -> str:
    """"""
    Take in positive 2 integers.
    'number' is the integer to be logically right shifted 'shift_amount' times.
    i.e. (number >>> shift_amount)
    Return the shifted binary representation.

    >>> logical_right_shift(0, 1)
    '0b0'
    >>> logical_right_shift(1, 1)
    '0b0'
    >>> logical_right_shift(1, 5)
    '0b0'
    >>> logical_right_shift(17, 2)
    '0b100'
    >>> logical_right_shift(1983, 4)
    '0b1111011'
    >>> logical_right_shift(1, -1)
    Traceback (most recent call last):
        ...
    ValueError: both inputs must be positive integers
    """"""
    if number < 0 or shift_amount < 0:
        raise ValueError(""both inputs must be positive integers"")
    binary_number = str(bin(number))[2:]
    if shift_amount >= len(binary_number):
        return ""0b0""
    shifted_binary_number = binary_number[: len(binary_number) - shift_amount]
    return ""0b"" + shifted_binary_number

def arithmetic_right_shift(number: int, shift_amount: int) -> str:
    """"""
    Take in 2 integers.
    'number' is the integer to be arithmetically right shifted 'shift_amount' times.
    i.e. (number >> shift_amount)
    Return the shifted binary representation.

    >>> arithmetic_right_shift(0, 1)
    '0b00'
    >>> arithmetic_right_shift(1, 1)
    '0b00'
    >>> arithmetic_right_shift(-1, 1)
    '0b11'
    >>> arithmetic_right_shift(17, 2)
    '0b000100'
    >>> arithmetic_right_shift(-17, 2)
    '0b111011'
    >>> arithmetic_right_shift(-1983, 4)
    '0b111110000100'
    """"""
    if number >= 0:  # Get binary representation of positive number
        binary_number = ""0"" + str(bin(number)).strip(""-"")[2:]
    else:  # Get binary (2's complement) representation of negative number
        binary_number_length = len(bin(number)[3:])  # Find 2's complement of number
        binary_number = bin(abs(number) - (1 << binary_number_length))[3:]
        binary_number = (
            ""1"" + ""0"" * (binary_number_length - len(binary_number)) + binary_number
        )
    if shift_amount >= len(binary_number):
        return ""0b"" + binary_number[0] * len(binary_number)
    return (
        ""0b""
        + binary_number[0] * shift_amount
        + binary_number[: len(binary_number) - shift_amount]
    )

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
110,"import prisma

AGENT_NODE_INCLUDE: prisma.types.AgentNodeInclude = {
    ""Input"": True,
    ""Output"": True,
    ""Webhook"": True,
    ""AgentBlock"": True,
}

AGENT_GRAPH_INCLUDE: prisma.types.AgentGraphInclude = {
    ""AgentNodes"": {""include"": AGENT_NODE_INCLUDE}  # type: ignore
}

EXECUTION_RESULT_INCLUDE: prisma.types.AgentNodeExecutionInclude = {
    ""Input"": True,
    ""Output"": True,
    ""AgentNode"": True,
    ""AgentGraphExecution"": True,
}

GRAPH_EXECUTION_INCLUDE: prisma.types.AgentGraphExecutionInclude = {
    ""AgentNodeExecutions"": {
        ""include"": {
            ""Input"": True,
            ""Output"": True,
            ""AgentNode"": True,
            ""AgentGraphExecution"": True,
        }
    }
}

INTEGRATION_WEBHOOK_INCLUDE: prisma.types.IntegrationWebhookInclude = {
    ""AgentNodes"": {""include"": AGENT_NODE_INCLUDE}  # type: ignore
}
","import prisma

AGENT_NODE_INCLUDE: prisma.types.AgentNodeInclude = {
    ""Input"": True,
    ""Output"": True,
    ""Webhook"": True,
    ""AgentBlock"": True,
}

AGENT_GRAPH_INCLUDE: prisma.types.AgentGraphInclude = {
    ""AgentNodes"": {""include"": AGENT_NODE_INCLUDE}  # type: ignore
}

EXECUTION_RESULT_INCLUDE: prisma.types.AgentNodeExecutionInclude = {
    ""Input"": True,
    ""Output"": True,
    ""AgentNode"": True,
    ""AgentGraphExecution"": True,
}

GRAPH_EXECUTION_INCLUDE: prisma.types.AgentGraphExecutionInclude = {
    ""AgentNodeExecutions"": {
        ""include"": {
            ""Input"": True,
            ""Output"": True,
            ""AgentNode"": True,
            ""AgentGraphExecution"": True,
        }
    }
}

INTEGRATION_WEBHOOK_INCLUDE: prisma.types.IntegrationWebhookInclude = {
    ""AgentNodes"": {""include"": AGENT_NODE_INCLUDE}  # type: ignore
}","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
111,"import sentry_sdk

from backend.util.settings import Settings


def sentry_init():
    sentry_dsn = Settings().secrets.sentry_dsn
    sentry_sdk.init(dsn=sentry_dsn, traces_sample_rate=1.0, profiles_sample_rate=1.0)
","import sentry_sdk
from backend.util.settings import Settings

def sentry_init():
    sentry_dsn = Settings().secrets.sentry_dsn
    sentry_sdk.init(dsn=sentry_dsn, traces_sample_rate=1.0, profiles_sample_rate=1.0)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
112,"class MockObject:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

    def __getattr__(self, name):
        return self.__dict__.get(name)

    def __call__(self, *args, **kwargs):
        return self

    def __setattr__(self, name, value):
        self.__dict__[name] = value
","class MockObject:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

    def __getattr__(self, name):
        return self.__dict__.get(name)

    def __call__(self, *args, **kwargs):
        return self

    def __setattr__(self, name, value):
        self.__dict__[name] = value","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
113,"from typing import Literal

import googlemaps
from pydantic import BaseModel, SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""google_maps"",
    api_key=SecretStr(""mock-google-maps-api-key""),
    title=""Mock Google Maps API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class Place(BaseModel):
    name: str
    address: str
    phone: str
    rating: float
    reviews: int
    website: str


class GoogleMapsSearchBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.GOOGLE_MAPS], Literal[""api_key""]
        ] = CredentialsField(description=""Google Maps API Key"")
        query: str = SchemaField(
            description=""Search query for local businesses"",
            placeholder=""e.g., 'restaurants in New York'"",
        )
        radius: int = SchemaField(
            description=""Search radius in meters (max 50000)"",
            default=5000,
            ge=1,
            le=50000,
        )
        max_results: int = SchemaField(
            description=""Maximum number of results to return (max 60)"",
            default=20,
            ge=1,
            le=60,
        )

    class Output(BlockSchema):
        place: Place = SchemaField(description=""Place found"")
        error: str = SchemaField(description=""Error message if the search failed"")

    def __init__(self):
        super().__init__(
            id=""f47ac10b-58cc-4372-a567-0e02b2c3d479"",
            description=""This block searches for local businesses using Google Maps API."",
            categories={BlockCategory.SEARCH},
            input_schema=GoogleMapsSearchBlock.Input,
            output_schema=GoogleMapsSearchBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""query"": ""restaurants in new york"",
                ""radius"": 5000,
                ""max_results"": 5,
            },
            test_output=[
                (
                    ""place"",
                    {
                        ""name"": ""Test Restaurant"",
                        ""address"": ""123 Test St, New York, NY 10001"",
                        ""phone"": ""+1 (555) 123-4567"",
                        ""rating"": 4.5,
                        ""reviews"": 100,
                        ""website"": ""https://testrestaurant.com"",
                    },
                ),
            ],
            test_mock={
                ""search_places"": lambda *args, **kwargs: [
                    {
                        ""name"": ""Test Restaurant"",
                        ""address"": ""123 Test St, New York, NY 10001"",
                        ""phone"": ""+1 (555) 123-4567"",
                        ""rating"": 4.5,
                        ""reviews"": 100,
                        ""website"": ""https://testrestaurant.com"",
                    }
                ]
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        places = self.search_places(
            credentials.api_key,
            input_data.query,
            input_data.radius,
            input_data.max_results,
        )
        for place in places:
            yield ""place"", place

    def search_places(self, api_key: SecretStr, query, radius, max_results):
        client = googlemaps.Client(key=api_key.get_secret_value())
        return self._search_places(client, query, radius, max_results)

    def _search_places(self, client, query, radius, max_results):
        results = []
        next_page_token = None
        while len(results) < max_results:
            response = client.places(
                query=query,
                radius=radius,
                page_token=next_page_token,
            )
            for place in response[""results""]:
                if len(results) >= max_results:
                    break
                place_details = client.place(place[""place_id""])[""result""]
                results.append(
                    Place(
                        name=place_details.get(""name"", """"),
                        address=place_details.get(""formatted_address"", """"),
                        phone=place_details.get(""formatted_phone_number"", """"),
                        rating=place_details.get(""rating"", 0),
                        reviews=place_details.get(""user_ratings_total"", 0),
                        website=place_details.get(""website"", """"),
                    )
                )
            next_page_token = response.get(""next_page_token"")
            if not next_page_token:
                break
        return results
","from typing import Literal
import googlemaps
from pydantic import BaseModel, SecretStr
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""google_maps"",
    api_key=SecretStr(""mock-google-maps-api-key""),
    title=""Mock Google Maps API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}

class Place(BaseModel):
    name: str
    address: str
    phone: str
    rating: float
    reviews: int
    website: str

class GoogleMapsSearchBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.GOOGLE_MAPS], Literal[""api_key""]
        ] = CredentialsField(description=""Google Maps API Key"")
        query: str = SchemaField(
            description=""Search query for local businesses"",
            placeholder=""e.g., 'restaurants in New York'"",
        )
        radius: int = SchemaField(
            description=""Search radius in meters (max 50000)"",
            default=5000,
            ge=1,
            le=50000,
        )
        max_results: int = SchemaField(
            description=""Maximum number of results to return (max 60)"",
            default=20,
            ge=1,
            le=60,
        )

    class Output(BlockSchema):
        place: Place = SchemaField(description=""Place found"")
        error: str = SchemaField(description=""Error message if the search failed"")

    def __init__(self):
        super().__init__(
            id=""f47ac10b-58cc-4372-a567-0e02b2c3d479"",
            description=""This block searches for local businesses using Google Maps API."",
            categories={BlockCategory.SEARCH},
            input_schema=GoogleMapsSearchBlock.Input,
            output_schema=GoogleMapsSearchBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""query"": ""restaurants in new york"",
                ""radius"": 5000,
                ""max_results"": 5,
            },
            test_output=[
                (
                    ""place"",
                    {
                        ""name"": ""Test Restaurant"",
                        ""address"": ""123 Test St, New York, NY 10001"",
                        ""phone"": ""+1 (555) 123-4567"",
                        ""rating"": 4.5,
                        ""reviews"": 100,
                        ""website"": ""https://testrestaurant.com"",
                    },
                ),
            ],
            test_mock={
                ""search_places"": lambda *args, **kwargs: [
                    {
                        ""name"": ""Test Restaurant"",
                        ""address"": ""123 Test St, New York, NY 10001"",
                        ""phone"": ""+1 (555) 123-4567"",
                        ""rating"": 4.5,
                        ""reviews"": 100,
                        ""website"": ""https://testrestaurant.com"",
                    }
                ]
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        places = self.search_places(
            credentials.api_key,
            input_data.query,
            input_data.radius,
            input_data.max_results,
        )
        for place in places:
            yield ""place"", place

    def search_places(self, api_key: SecretStr, query, radius, max_results):
        client = googlemaps.Client(key=api_key.get_secret_value())
        return self._search_places(client, query, radius, max_results)

    def _search_places(self, client, query, radius, max_results):
        results = []
        next_page_token = None
        while len(results) < max_results:
            response = client.places(
                query=query,
                radius=radius,
                page_token=next_page_token,
            )
            for place in response[""results""]:
                if len(results) >= max_results:
                    break
                place_details = client.place(place[""place_id""])[""result""]
                results.append(
                    Place(
                        name=place_details.get(""name"", """"),
                        address=place_details.get(""formatted_address"", """"),
                        phone=place_details.get(""formatted_phone_number"", """"),
                        rating=place_details.get(""rating"", 0),
                        reviews=place_details.get(""user_ratings_total"", 0),
                        website=place_details.get(""website"", """"),
                    )
                )
            next_page_token = response.get(""next_page_token"")
            if not next_page_token:
                break
        return results","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
114,"from backend.app import run_processes
from backend.executor import DatabaseManager, ExecutionScheduler
from backend.server.rest_api import AgentServer


def main():
    """"""
    Run all the processes required for the AutoGPT-server REST API.
    """"""
    run_processes(
        DatabaseManager(),
        ExecutionScheduler(),
        AgentServer(),
    )


if __name__ == ""__main__"":
    main()
","from backend.app import run_processes
from backend.executor import DatabaseManager, ExecutionScheduler
from backend.server.rest_api import AgentServer

def main():
    """"""
    Run all the processes required for the AutoGPT-server REST API.
    """"""
    run_processes(
        DatabaseManager(),
        ExecutionScheduler(),
        AgentServer(),
    )

if __name__ == ""__main__"":
    main()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
115,"from typing import Literal

from pydantic import SecretStr

from backend.data.model import APIKeyCredentials, CredentialsField, CredentialsMetaInput
from backend.integrations.providers import ProviderName

JinaCredentials = APIKeyCredentials
JinaCredentialsInput = CredentialsMetaInput[
    Literal[ProviderName.JINA],
    Literal[""api_key""],
]


def JinaCredentialsField() -> JinaCredentialsInput:
    """"""
    Creates a Jina credentials input on a block.

    """"""
    return CredentialsField(
        description=""The Jina integration can be used with an API Key."",
    )


TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""jina"",
    api_key=SecretStr(""mock-jina-api-key""),
    title=""Mock Jina API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}
","from typing import Literal
from pydantic import SecretStr
from backend.data.model import APIKeyCredentials, CredentialsField, CredentialsMetaInput
from backend.integrations.providers import ProviderName

JinaCredentials = APIKeyCredentials
JinaCredentialsInput = CredentialsMetaInput[
    Literal[ProviderName.JINA],
    Literal[""api_key""],
]

def JinaCredentialsField() -> JinaCredentialsInput:
    """"""
    Creates a Jina credentials input on a block.

    """"""
    return CredentialsField(
        description=""The Jina integration can be used with an API Key."",
    )

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""jina"",
    api_key=SecretStr(""mock-jina-api-key""),
    title=""Mock Jina API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
116,"import logging

from colorama import Fore, Style
from google.cloud.logging_v2.handlers import CloudLoggingFilter, StructuredLogHandler

from .utils import remove_color_codes


class FancyConsoleFormatter(logging.Formatter):
    """"""
    A custom logging formatter designed for console output.

    This formatter enhances the standard logging output with color coding. The color
    coding is based on the level of the log message, making it easier to distinguish
    between different types of messages in the console output.

    The color for each level is defined in the LEVEL_COLOR_MAP class attribute.
    """"""

    # level -> (level & text color, title color)
    LEVEL_COLOR_MAP = {
        logging.DEBUG: Fore.LIGHTBLACK_EX,
        logging.INFO: Fore.BLUE,
        logging.WARNING: Fore.YELLOW,
        logging.ERROR: Fore.RED,
        logging.CRITICAL: Fore.RED + Style.BRIGHT,
    }

    def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif type(record.msg) is not str:
            record.msg = str(record.msg)

        # Determine default color based on error level
        level_color = """"
        if record.levelno in self.LEVEL_COLOR_MAP:
            level_color = self.LEVEL_COLOR_MAP[record.levelno]
            record.levelname = f""{level_color}{record.levelname}{Style.RESET_ALL}""

        # Determine color for message
        color = getattr(record, ""color"", level_color)
        color_is_specified = hasattr(record, ""color"")

        # Don't color INFO messages unless the color is explicitly specified.
        if color and (record.levelno != logging.INFO or color_is_specified):
            record.msg = f""{color}{record.msg}{Style.RESET_ALL}""

        return super().format(record)


class AGPTFormatter(FancyConsoleFormatter):
    def __init__(self, *args, no_color: bool = False, **kwargs):
        super().__init__(*args, **kwargs)
        self.no_color = no_color

    def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif type(record.msg) is not str:
            record.msg = str(record.msg)

        # Strip color from the message to prevent color spoofing
        if record.msg and not getattr(record, ""preserve_color"", False):
            record.msg = remove_color_codes(record.msg)

        # Determine color for title
        title = getattr(record, ""title"", """")
        title_color = getattr(record, ""title_color"", """") or self.LEVEL_COLOR_MAP.get(
            record.levelno, """"
        )
        if title and title_color:
            title = f""{title_color + Style.BRIGHT}{title}{Style.RESET_ALL}""
        # Make sure record.title is set, and padded with a space if not empty
        record.title = f""{title} "" if title else """"

        if self.no_color:
            return remove_color_codes(super().format(record))
        else:
            return super().format(record)


class StructuredLoggingFormatter(StructuredLogHandler, logging.Formatter):
    def __init__(self):
        # Set up CloudLoggingFilter to add diagnostic info to the log records
        self.cloud_logging_filter = CloudLoggingFilter()

        # Init StructuredLogHandler
        super().__init__()

    def format(self, record: logging.LogRecord) -> str:
        self.cloud_logging_filter.filter(record)
        return super().format(record)
","import logging
from colorama import Fore, Style
from google.cloud.logging_v2.handlers import CloudLoggingFilter, StructuredLogHandler
from .utils import remove_color_codes


class FancyConsoleFormatter(logging.Formatter):
    """"""
    A custom logging formatter designed for console output.

    This formatter enhances the standard logging output with color coding. The color
    coding is based on the level of the log message, making it easier to distinguish
    between different types of messages in the console output.

    The color for each level is defined in the LEVEL_COLOR_MAP class attribute.
    """"""

    # level -> (level & text color, title color)
    LEVEL_COLOR_MAP = {
        logging.DEBUG: Fore.LIGHTBLACK_EX,
        logging.INFO: Fore.BLUE,
        logging.WARNING: Fore.YELLOW,
        logging.ERROR: Fore.RED,
        logging.CRITICAL: Fore.RED + Style.BRIGHT,
    }

    def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif type(record.msg) is not str:
            record.msg = str(record.msg)

        # Determine default color based on error level
        level_color = """"
        if record.levelno in self.LEVEL_COLOR_MAP:
            level_color = self.LEVEL_COLOR_MAP[record.levelno]
            record.levelname = f""{level_color}{record.levelname}{Style.RESET_ALL}""

        # Determine color for message
        color = getattr(record, ""color"", level_color)
        color_is_specified = hasattr(record, ""color"")

        # Don't color INFO messages unless the color is explicitly specified.
        if color and (record.levelno != logging.INFO or color_is_specified):
            record.msg = f""{color}{record.msg}{Style.RESET_ALL}""

        return super().format(record)


class AGPTFormatter(FancyConsoleFormatter):
    def __init__(self, *args, no_color: bool = False, **kwargs):
        super().__init__(*args, **kwargs)
        self.no_color = no_color

    def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif type(record.msg) is not str:
            record.msg = str(record.msg)

        # Strip color from the message to prevent color spoofing
        if record.msg and not getattr(record, ""preserve_color"", False):
            record.msg = remove_color_codes(record.msg)

        # Determine color for title
        title = getattr(record, ""title"", """")
        title_color = getattr(record, ""title_color"", """") or self.LEVEL_COLOR_MAP.get(
            record.levelno, """"
        )

        if title and title_color:
            title = f""{title_color + Style.BRIGHT}{title}{Style.RESET_ALL}""

        # Make sure record.title is set, and padded with a space if not empty
        record.title = f""{title} "" if title else """"

        if self.no_color:
            return remove_color_codes(super().format(record))
        else:
            return super().format(record)


class StructuredLoggingFormatter(StructuredLogHandler, logging.Formatter):
    def __init__(self):
        # Set up CloudLoggingFilter to add diagnostic info to the log records
        self.cloud_logging_filter = CloudLoggingFilter()

        # Init StructuredLogHandler
        super().__init__()

    def format(self, record: logging.LogRecord) -> str:
        self.cloud_logging_filter.filter(record)
        return super().format(record)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
117,"from typing import Type

import pytest

from backend.data.block import Block, get_blocks
from backend.util.test import execute_block_test


@pytest.mark.parametrize(""block"", get_blocks().values(), ids=lambda b: b.name)
def test_available_blocks(block: Type[Block]):
    execute_block_test(block())
","from typing import Type
import pytest
from backend.data.block import Block, get_blocks
from backend.util.test import execute_block_test

@pytest.mark.parametrize(""block"", get_blocks().values(), ids=lambda b: b.name)
def test_available_blocks(block: Type[Block]):
    execute_block_test(block())","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
118,"""""""
author: Aayush Soni
Given n pairs of parentheses, write a function to generate all
combinations of well-formed parentheses.
Input: n = 2
Output: [""(())"",""()()""]
Leetcode link: https://leetcode.com/problems/generate-parentheses/description/
""""""


def backtrack(
    partial: str, open_count: int, close_count: int, n: int, result: list[str]
) -> None:
    """"""
    Generate valid combinations of balanced parentheses using recursion.

    :param partial: A string representing the current combination.
    :param open_count: An integer representing the count of open parentheses.
    :param close_count: An integer representing the count of close parentheses.
    :param n: An integer representing the total number of pairs.
    :param result: A list to store valid combinations.
    :return: None

    This function uses recursion to explore all possible combinations,
    ensuring that at each step, the parentheses remain balanced.

    Example:
    >>> result = []
    >>> backtrack("""", 0, 0, 2, result)
    >>> result
    ['(())', '()()']
    """"""
    if len(partial) == 2 * n:
        # When the combination is complete, add it to the result.
        result.append(partial)
        return

    if open_count < n:
        # If we can add an open parenthesis, do so, and recurse.
        backtrack(partial + ""("", open_count + 1, close_count, n, result)

    if close_count < open_count:
        # If we can add a close parenthesis (it won't make the combination invalid),
        # do so, and recurse.
        backtrack(partial + "")"", open_count, close_count + 1, n, result)


def generate_parenthesis(n: int) -> list[str]:
    """"""
    Generate valid combinations of balanced parentheses for a given n.

    :param n: An integer representing the number of pairs of parentheses.
    :return: A list of strings with valid combinations.

    This function uses a recursive approach to generate the combinations.

    Time Complexity: O(2^(2n)) - In the worst case, we have 2^(2n) combinations.
    Space Complexity: O(n) - where 'n' is the number of pairs.

    Example 1:
    >>> generate_parenthesis(3)
    ['((()))', '(()())', '(())()', '()(())', '()()()']

    Example 2:
    >>> generate_parenthesis(1)
    ['()']
    """"""

    result: list[str] = []
    backtrack("""", 0, 0, n, result)
    return result


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","""""""
author: Aayush Soni
Given n pairs of parentheses, write a function to generate all
combinations of well-formed parentheses.
Input: n = 2
Output: [""(())"",""()()""]
Leetcode link: https://leetcode.com/problems/generate-parentheses/description/
""""""

def backtrack(
    partial: str, open_count: int, close_count: int, n: int, result: list[str]
) -> None:
    """"""
    Generate valid combinations of balanced parentheses using recursion.

    :param partial: A string representing the current combination.
    :param open_count: An integer representing the count of open parentheses.
    :param close_count: An integer representing the count of close parentheses.
    :param n: An integer representing the total number of pairs.
    :param result: A list to store valid combinations.
    :return: None

    This function uses recursion to explore all possible combinations,
    ensuring that at each step, the parentheses remain balanced.

    Example:
    >>> result = []
    >>> backtrack("""", 0, 0, 2, result)
    >>> result
    ['(())', '()()']
    """"""
    if len(partial) == 2 * n:
        # When the combination is complete, add it to the result.
        result.append(partial)
        return
    if open_count < n:
        # If we can add an open parenthesis, do so, and recurse.
        backtrack(partial + ""("", open_count + 1, close_count, n, result)
    if close_count < open_count:
        # If we can add a close parenthesis (it won't make the combination invalid),
        # do so, and recurse.
        backtrack(partial + "")"", open_count, close_count + 1, n, result)

def generate_parenthesis(n: int) -> list[str]:
    """"""
    Generate valid combinations of balanced parentheses for a given n.

    :param n: An integer representing the number of pairs of parentheses.
    :return: A list of strings with valid combinations.

    This function uses a recursive approach to generate the combinations.

    Time Complexity: O(2^(2n)) - In the worst case, we have 2^(2n) combinations.
    Space Complexity: O(n) - where 'n' is the number of pairs.

    Example 1:
    >>> generate_parenthesis(3)
    ['((()))', '(()())', '(())()', '()(())', '()()()']

    Example 2:
    >>> generate_parenthesis(1)
    ['()']
    """"""
    result: list[str] = []
    backtrack("""", 0, 0, n, result)
    return result

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
119,"from timeit import timeit


def get_set_bits_count_using_brian_kernighans_algorithm(number: int) -> int:
    """"""
    Count the number of set bits in a 32 bit integer
    >>> get_set_bits_count_using_brian_kernighans_algorithm(25)
    3
    >>> get_set_bits_count_using_brian_kernighans_algorithm(37)
    3
    >>> get_set_bits_count_using_brian_kernighans_algorithm(21)
    3
    >>> get_set_bits_count_using_brian_kernighans_algorithm(58)
    4
    >>> get_set_bits_count_using_brian_kernighans_algorithm(0)
    0
    >>> get_set_bits_count_using_brian_kernighans_algorithm(256)
    1
    >>> get_set_bits_count_using_brian_kernighans_algorithm(-1)
    Traceback (most recent call last):
        ...
    ValueError: the value of input must not be negative
    """"""
    if number < 0:
        raise ValueError(""the value of input must not be negative"")
    result = 0
    while number:
        number &= number - 1
        result += 1
    return result


def get_set_bits_count_using_modulo_operator(number: int) -> int:
    """"""
    Count the number of set bits in a 32 bit integer
    >>> get_set_bits_count_using_modulo_operator(25)
    3
    >>> get_set_bits_count_using_modulo_operator(37)
    3
    >>> get_set_bits_count_using_modulo_operator(21)
    3
    >>> get_set_bits_count_using_modulo_operator(58)
    4
    >>> get_set_bits_count_using_modulo_operator(0)
    0
    >>> get_set_bits_count_using_modulo_operator(256)
    1
    >>> get_set_bits_count_using_modulo_operator(-1)
    Traceback (most recent call last):
        ...
    ValueError: the value of input must not be negative
    """"""
    if number < 0:
        raise ValueError(""the value of input must not be negative"")
    result = 0
    while number:
        if number % 2 == 1:
            result += 1
        number >>= 1
    return result


def benchmark() -> None:
    """"""
    Benchmark code for comparing 2 functions, with different length int values.
    Brian Kernighan's algorithm is consistently faster than using modulo_operator.
    """"""

    def do_benchmark(number: int) -> None:
        setup = ""import __main__ as z""
        print(f""Benchmark when {number = }:"")
        print(f""{get_set_bits_count_using_modulo_operator(number) = }"")
        timing = timeit(
            f""z.get_set_bits_count_using_modulo_operator({number})"", setup=setup
        )
        print(f""timeit() runs in {timing} seconds"")
        print(f""{get_set_bits_count_using_brian_kernighans_algorithm(number) = }"")
        timing = timeit(
            f""z.get_set_bits_count_using_brian_kernighans_algorithm({number})"",
            setup=setup,
        )
        print(f""timeit() runs in {timing} seconds"")

    for number in (25, 37, 58, 0):
        do_benchmark(number)
        print()


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
    benchmark()
","from timeit import timeit

def get_set_bits_count_using_brian_kernighans_algorithm(number: int) -> int:
    """"""
    Count the number of set bits in a 32 bit integer
    >>> get_set_bits_count_using_brian_kernighans_algorithm(25)
    3
    >>> get_set_bits_count_using_brian_kernighans_algorithm(37)
    3
    >>> get_set_bits_count_using_brian_kernighans_algorithm(21)
    3
    >>> get_set_bits_count_using_brian_kernighans_algorithm(58)
    4
    >>> get_set_bits_count_using_brian_kernighans_algorithm(0)
    0
    >>> get_set_bits_count_using_brian_kernighans_algorithm(256)
    1
    >>> get_set_bits_count_using_brian_kernighans_algorithm(-1)
    Traceback (most recent call last):
        ...
    ValueError: the value of input must not be negative
    """"""
    if number < 0:
        raise ValueError(""the value of input must not be negative"")
    result = 0
    while number:
        number &= number - 1
        result += 1
    return result

def get_set_bits_count_using_modulo_operator(number: int) -> int:
    """"""
    Count the number of set bits in a 32 bit integer
    >>> get_set_bits_count_using_modulo_operator(25)
    3
    >>> get_set_bits_count_using_modulo_operator(37)
    3
    >>> get_set_bits_count_using_modulo_operator(21)
    3
    >>> get_set_bits_count_using_modulo_operator(58)
    4
    >>> get_set_bits_count_using_modulo_operator(0)
    0
    >>> get_set_bits_count_using_modulo_operator(256)
    1
    >>> get_set_bits_count_using_modulo_operator(-1)
    Traceback (most recent call last):
        ...
    ValueError: the value of input must not be negative
    """"""
    if number < 0:
        raise ValueError(""the value of input must not be negative"")
    result = 0
    while number:
        if number % 2 == 1:
            result += 1
        number >>= 1
    return result

def benchmark() -> None:
    """"""
    Benchmark code for comparing 2 functions, with different length int values.
    Brian Kernighan's algorithm is consistently faster than using modulo_operator.
    """"""
    def do_benchmark(number: int) -> None:
        setup = ""import __main__ as z""
        print(f""Benchmark when {number = }:"")
        print(f""{get_set_bits_count_using_modulo_operator(number) = }"")
        timing = timeit(
            f""z.get_set_bits_count_using_modulo_operator({number})"", setup=setup
        )
        print(f""timeit() runs in {timing} seconds"")
        print(f""{get_set_bits_count_using_brian_kernighans_algorithm(number) = }"")
        timing = timeit(
            f""z.get_set_bits_count_using_brian_kernighans_algorithm({number})"",
            setup=setup,
        )
        print(f""timeit() runs in {timing} seconds"")

    for number in (25, 37, 58, 0):
        do_benchmark(number)
        print()

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
    benchmark()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
120,"import inspect
from abc import ABC, abstractmethod
from enum import Enum
from typing import (
    Any,
    ClassVar,
    Generator,
    Generic,
    Optional,
    Type,
    TypeVar,
    cast,
    get_origin,
)

import jsonref
import jsonschema
from prisma.models import AgentBlock
from pydantic import BaseModel

from backend.util import json
from backend.util.settings import Config

from .model import (
    CREDENTIALS_FIELD_NAME,
    ContributorDetails,
    Credentials,
    CredentialsMetaInput,
)

app_config = Config()

BlockData = tuple[str, Any]  # Input & Output data should be a tuple of (name, data).
BlockInput = dict[str, Any]  # Input: 1 input pin consumes 1 data.
BlockOutput = Generator[BlockData, None, None]  # Output: 1 output pin produces n data.
CompletedBlockOutput = dict[str, list[Any]]  # Completed stream, collected as a dict.


class BlockType(Enum):
    STANDARD = ""Standard""
    INPUT = ""Input""
    OUTPUT = ""Output""
    NOTE = ""Note""
    WEBHOOK = ""Webhook""
    WEBHOOK_MANUAL = ""Webhook (manual)""
    AGENT = ""Agent""


class BlockCategory(Enum):
    AI = ""Block that leverages AI to perform a task.""
    SOCIAL = ""Block that interacts with social media platforms.""
    TEXT = ""Block that processes text data.""
    SEARCH = ""Block that searches or extracts information from the internet.""
    BASIC = ""Block that performs basic operations.""
    INPUT = ""Block that interacts with input of the graph.""
    OUTPUT = ""Block that interacts with output of the graph.""
    LOGIC = ""Programming logic to control the flow of your agent""
    COMMUNICATION = ""Block that interacts with communication platforms.""
    DEVELOPER_TOOLS = ""Developer tools such as GitHub blocks.""
    DATA = ""Block that interacts with structured data.""
    HARDWARE = ""Block that interacts with hardware.""
    AGENT = ""Block that interacts with other agents.""
    CRM = ""Block that interacts with CRM services.""

    def dict(self) -> dict[str, str]:
        return {""category"": self.name, ""description"": self.value}


class BlockSchema(BaseModel):
    cached_jsonschema: ClassVar[dict[str, Any]]

    @classmethod
    def jsonschema(cls) -> dict[str, Any]:
        if cls.cached_jsonschema:
            return cls.cached_jsonschema

        model = jsonref.replace_refs(cls.model_json_schema(), merge_props=True)

        def ref_to_dict(obj):
            if isinstance(obj, dict):
                # OpenAPI <3.1 does not support sibling fields that has a $ref key
                # So sometimes, the schema has an ""allOf""/""anyOf""/""oneOf"" with 1 item.
                keys = {""allOf"", ""anyOf"", ""oneOf""}
                one_key = next((k for k in keys if k in obj and len(obj[k]) == 1), None)
                if one_key:
                    obj.update(obj[one_key][0])

                return {
                    key: ref_to_dict(value)
                    for key, value in obj.items()
                    if not key.startswith(""$"") and key != one_key
                }
            elif isinstance(obj, list):
                return [ref_to_dict(item) for item in obj]

            return obj

        cls.cached_jsonschema = cast(dict[str, Any], ref_to_dict(model))

        # Set default properties values
        for field in cls.cached_jsonschema.get(""properties"", {}).values():
            if isinstance(field, dict) and ""advanced"" not in field:
                field[""advanced""] = True

        return cls.cached_jsonschema

    @classmethod
    def validate_data(cls, data: BlockInput) -> str | None:
        return json.validate_with_jsonschema(schema=cls.jsonschema(), data=data)

    @classmethod
    def validate_field(cls, field_name: str, data: BlockInput) -> str | None:
        """"""
        Validate the data against a specific property (one of the input/output name).
        Returns the validation error message if the data does not match the schema.
        """"""
        model_schema = cls.jsonschema().get(""properties"", {})
        if not model_schema:
            return f""Invalid model schema {cls}""

        property_schema = model_schema.get(field_name)
        if not property_schema:
            return f""Invalid property name {field_name}""

        try:
            jsonschema.validate(json.to_dict(data), property_schema)
            return None
        except jsonschema.ValidationError as e:
            return str(e)

    @classmethod
    def get_fields(cls) -> set[str]:
        return set(cls.model_fields.keys())

    @classmethod
    def get_required_fields(cls) -> set[str]:
        return {
            field
            for field, field_info in cls.model_fields.items()
            if field_info.is_required()
        }

    @classmethod
    def __pydantic_init_subclass__(cls, **kwargs):
        """"""Validates the schema definition. Rules:
        - Only one `CredentialsMetaInput` field may be present.
          - This field MUST be called `credentials`.
        - A field that is called `credentials` MUST be a `CredentialsMetaInput`.
        """"""
        super().__pydantic_init_subclass__(**kwargs)

        # Reset cached JSON schema to prevent inheriting it from parent class
        cls.cached_jsonschema = {}

        credentials_fields = [
            field_name
            for field_name, info in cls.model_fields.items()
            if (
                inspect.isclass(info.annotation)
                and issubclass(
                    get_origin(info.annotation) or info.annotation,
                    CredentialsMetaInput,
                )
            )
        ]
        if len(credentials_fields) > 1:
            raise ValueError(
                f""{cls.__qualname__} can only have one CredentialsMetaInput field""
            )
        elif (
            len(credentials_fields) == 1
            and credentials_fields[0] != CREDENTIALS_FIELD_NAME
        ):
            raise ValueError(
                f""CredentialsMetaInput field on {cls.__qualname__} ""
                ""must be named 'credentials'""
            )
        elif (
            len(credentials_fields) == 0
            and CREDENTIALS_FIELD_NAME in cls.model_fields.keys()
        ):
            raise TypeError(
                f""Field 'credentials' on {cls.__qualname__} ""
                f""must be of type {CredentialsMetaInput.__name__}""
            )
        if credentials_field := cls.model_fields.get(CREDENTIALS_FIELD_NAME):
            credentials_input_type = cast(
                CredentialsMetaInput, credentials_field.annotation
            )
            credentials_input_type.validate_credentials_field_schema(cls)


BlockSchemaInputType = TypeVar(""BlockSchemaInputType"", bound=BlockSchema)
BlockSchemaOutputType = TypeVar(""BlockSchemaOutputType"", bound=BlockSchema)


class EmptySchema(BlockSchema):
    pass


# --8<-- [start:BlockWebhookConfig]
class BlockManualWebhookConfig(BaseModel):
    """"""
    Configuration model for webhook-triggered blocks on which
    the user has to manually set up the webhook at the provider.
    """"""

    provider: str
    """"""The service provider that the webhook connects to""""""

    webhook_type: str
    """"""
    Identifier for the webhook type. E.g. GitHub has repo and organization level hooks.

    Only for use in the corresponding `WebhooksManager`.
    """"""

    event_filter_input: str = """"
    """"""
    Name of the block's event filter input.
    Leave empty if the corresponding webhook doesn't have distinct event/payload types.
    """"""

    event_format: str = ""{event}""
    """"""
    Template string for the event(s) that a block instance subscribes to.
    Applied individually to each event selected in the event filter input.

    Example: `""pull_request.{event}""` -> `""pull_request.opened""`
    """"""


class BlockWebhookConfig(BlockManualWebhookConfig):
    """"""
    Configuration model for webhook-triggered blocks for which
    the webhook can be automatically set up through the provider's API.
    """"""

    resource_format: str
    """"""
    Template string for the resource that a block instance subscribes to.
    Fields will be filled from the block's inputs (except `payload`).

    Example: `f""{repo}/pull_requests""` (note: not how it's actually implemented)

    Only for use in the corresponding `WebhooksManager`.
    """"""
    # --8<-- [end:BlockWebhookConfig]


class Block(ABC, Generic[BlockSchemaInputType, BlockSchemaOutputType]):
    def __init__(
        self,
        id: str = """",
        description: str = """",
        contributors: list[ContributorDetails] = [],
        categories: set[BlockCategory] | None = None,
        input_schema: Type[BlockSchemaInputType] = EmptySchema,
        output_schema: Type[BlockSchemaOutputType] = EmptySchema,
        test_input: BlockInput | list[BlockInput] | None = None,
        test_output: BlockData | list[BlockData] | None = None,
        test_mock: dict[str, Any] | None = None,
        test_credentials: Optional[Credentials] = None,
        disabled: bool = False,
        static_output: bool = False,
        block_type: BlockType = BlockType.STANDARD,
        webhook_config: Optional[BlockWebhookConfig | BlockManualWebhookConfig] = None,
    ):
        """"""
        Initialize the block with the given schema.

        Args:
            id: The unique identifier for the block, this value will be persisted in the
                DB. So it should be a unique and constant across the application run.
                Use the UUID format for the ID.
            description: The description of the block, explaining what the block does.
            contributors: The list of contributors who contributed to the block.
            input_schema: The schema, defined as a Pydantic model, for the input data.
            output_schema: The schema, defined as a Pydantic model, for the output data.
            test_input: The list or single sample input data for the block, for testing.
            test_output: The list or single expected output if the test_input is run.
            test_mock: function names on the block implementation to mock on test run.
            disabled: If the block is disabled, it will not be available for execution.
            static_output: Whether the output links of the block are static by default.
        """"""
        self.id = id
        self.input_schema = input_schema
        self.output_schema = output_schema
        self.test_input = test_input
        self.test_output = test_output
        self.test_mock = test_mock
        self.test_credentials = test_credentials
        self.description = description
        self.categories = categories or set()
        self.contributors = contributors or set()
        self.disabled = disabled
        self.static_output = static_output
        self.block_type = block_type
        self.webhook_config = webhook_config
        self.execution_stats = {}

        if self.webhook_config:
            if isinstance(self.webhook_config, BlockWebhookConfig):
                # Enforce presence of credentials field on auto-setup webhook blocks
                if CREDENTIALS_FIELD_NAME not in self.input_schema.model_fields:
                    raise TypeError(
                        ""credentials field is required on auto-setup webhook blocks""
                    )
                self.block_type = BlockType.WEBHOOK
            else:
                self.block_type = BlockType.WEBHOOK_MANUAL

            # Enforce shape of webhook event filter, if present
            if self.webhook_config.event_filter_input:
                event_filter_field = self.input_schema.model_fields[
                    self.webhook_config.event_filter_input
                ]
                if not (
                    isinstance(event_filter_field.annotation, type)
                    and issubclass(event_filter_field.annotation, BaseModel)
                    and all(
                        field.annotation is bool
                        for field in event_filter_field.annotation.model_fields.values()
                    )
                ):
                    raise NotImplementedError(
                        f""{self.name} has an invalid webhook event selector: ""
                        ""field must be a BaseModel and all its fields must be boolean""
                    )

            # Enforce presence of 'payload' input
            if ""payload"" not in self.input_schema.model_fields:
                raise TypeError(
                    f""{self.name} is webhook-triggered but has no 'payload' input""
                )

            # Disable webhook-triggered block if webhook functionality not available
            if not app_config.platform_base_url:
                self.disabled = True

    @classmethod
    def create(cls: Type[""Block""]) -> ""Block"":
        return cls()

    @abstractmethod
    def run(self, input_data: BlockSchemaInputType, **kwargs) -> BlockOutput:
        """"""
        Run the block with the given input data.
        Args:
            input_data: The input data with the structure of input_schema.
        Returns:
            A Generator that yields (output_name, output_data).
            output_name: One of the output name defined in Block's output_schema.
            output_data: The data for the output_name, matching the defined schema.
        """"""
        pass

    def run_once(self, input_data: BlockSchemaInputType, output: str, **kwargs) -> Any:
        for name, data in self.run(input_data, **kwargs):
            if name == output:
                return data
        raise ValueError(f""{self.name} did not produce any output for {output}"")

    def merge_stats(self, stats: dict[str, Any]) -> dict[str, Any]:
        for key, value in stats.items():
            if isinstance(value, dict):
                self.execution_stats.setdefault(key, {}).update(value)
            elif isinstance(value, (int, float)):
                self.execution_stats.setdefault(key, 0)
                self.execution_stats[key] += value
            elif isinstance(value, list):
                self.execution_stats.setdefault(key, [])
                self.execution_stats[key].extend(value)
            else:
                self.execution_stats[key] = value
        return self.execution_stats

    @property
    def name(self):
        return self.__class__.__name__

    def to_dict(self):
        return {
            ""id"": self.id,
            ""name"": self.name,
            ""inputSchema"": self.input_schema.jsonschema(),
            ""outputSchema"": self.output_schema.jsonschema(),
            ""description"": self.description,
            ""categories"": [category.dict() for category in self.categories],
            ""contributors"": [
                contributor.model_dump() for contributor in self.contributors
            ],
            ""staticOutput"": self.static_output,
            ""uiType"": self.block_type.value,
        }

    def execute(self, input_data: BlockInput, **kwargs) -> BlockOutput:
        if error := self.input_schema.validate_data(input_data):
            raise ValueError(
                f""Unable to execute block with invalid input data: {error}""
            )

        for output_name, output_data in self.run(
            self.input_schema(**input_data), **kwargs
        ):
            if output_name == ""error"":
                raise RuntimeError(output_data)
            if self.block_type == BlockType.STANDARD and (
                error := self.output_schema.validate_field(output_name, output_data)
            ):
                raise ValueError(f""Block produced an invalid output data: {error}"")
            yield output_name, output_data


# ======================= Block Helper Functions ======================= #


def get_blocks() -> dict[str, Type[Block]]:
    from backend.blocks import AVAILABLE_BLOCKS  # noqa: E402

    return AVAILABLE_BLOCKS


async def initialize_blocks() -> None:
    for cls in get_blocks().values():
        block = cls()
        existing_block = await AgentBlock.prisma().find_first(
            where={""OR"": [{""id"": block.id}, {""name"": block.name}]}
        )
        if not existing_block:
            await AgentBlock.prisma().create(
                data={
                    ""id"": block.id,
                    ""name"": block.name,
                    ""inputSchema"": json.dumps(block.input_schema.jsonschema()),
                    ""outputSchema"": json.dumps(block.output_schema.jsonschema()),
                }
            )
            continue

        input_schema = json.dumps(block.input_schema.jsonschema())
        output_schema = json.dumps(block.output_schema.jsonschema())
        if (
            block.id != existing_block.id
            or block.name != existing_block.name
            or input_schema != existing_block.inputSchema
            or output_schema != existing_block.outputSchema
        ):
            await AgentBlock.prisma().update(
                where={""id"": existing_block.id},
                data={
                    ""id"": block.id,
                    ""name"": block.name,
                    ""inputSchema"": input_schema,
                    ""outputSchema"": output_schema,
                },
            )


def get_block(block_id: str) -> Block | None:
    cls = get_blocks().get(block_id)
    return cls() if cls else None
","import inspect
from abc import ABC, abstractmethod
from enum import Enum
from typing import (
    Any,
    ClassVar,
    Generator,
    Generic,
    Optional,
    Type,
    TypeVar,
    cast,
    get_origin,
)

import jsonref
import jsonschema
from prisma.models import AgentBlock
from pydantic import BaseModel

from backend.util import json
from backend.util.settings import Config

from .model import (
    CREDENTIALS_FIELD_NAME,
    ContributorDetails,
    Credentials,
    CredentialsMetaInput,
)

app_config = Config()

BlockData = tuple[str, Any]  # Input & Output data should be a tuple of (name, data).
BlockInput = dict[str, Any]  # Input: 1 input pin consumes 1 data.
BlockOutput = Generator[BlockData, None, None]  # Output: 1 output pin produces n data.
CompletedBlockOutput = dict[str, list[Any]]  # Completed stream, collected as a dict.

class BlockType(Enum):
    STANDARD = ""Standard""
    INPUT = ""Input""
    OUTPUT = ""Output""
    NOTE = ""Note""
    WEBHOOK = ""Webhook""
    WEBHOOK_MANUAL = ""Webhook (manual)""
    AGENT = ""Agent""

class BlockCategory(Enum):
    AI = ""Block that leverages AI to perform a task.""
    SOCIAL = ""Block that interacts with social media platforms.""
    TEXT = ""Block that processes text data.""
    SEARCH = ""Block that searches or extracts information from the internet.""
    BASIC = ""Block that performs basic operations.""
    INPUT = ""Block that interacts with input of the graph.""
    OUTPUT = ""Block that interacts with output of the graph.""
    LOGIC = ""Programming logic to control the flow of your agent""
    COMMUNICATION = ""Block that interacts with communication platforms.""
    DEVELOPER_TOOLS = ""Developer tools such as GitHub blocks.""
    DATA = ""Block that interacts with structured data.""
    HARDWARE = ""Block that interacts with hardware.""
    AGENT = ""Block that interacts with other agents.""
    CRM = ""Block that interacts with CRM services.""
    
    def dict(self) -> dict[str, str]:
        return {""category"": self.name, ""description"": self.value}

class BlockSchema(BaseModel):
    cached_jsonschema: ClassVar[dict[str, Any]]

    @classmethod
    def jsonschema(cls) -> dict[str, Any]:
        if cls.cached_jsonschema:
            return cls.cached_jsonschema
        
        model = jsonref.replace_refs(cls.model_json_schema(), merge_props=True)

        def ref_to_dict(obj):
            if isinstance(obj, dict):
                # OpenAPI <3.1 does not support sibling fields that has a $ref key
                # So sometimes, the schema has an ""allOf""/""anyOf""/""oneOf"" with 1 item.
                keys = {""allOf"", ""anyOf"", ""oneOf""}
                one_key = next((k for k in keys if k in obj and len(obj[k]) == 1), None)
                if one_key:
                    obj.update(obj[one_key][0])

                return {
                    key: ref_to_dict(value)
                    for key, value in obj.items()
                    if not key.startswith(""$"") and key != one_key
                }
            elif isinstance(obj, list):
                return [ref_to_dict(item) for item in obj]

            return obj

        cls.cached_jsonschema = cast(dict[str, Any], ref_to_dict(model))

        for field in cls.cached_jsonschema.get(""properties"", {}).values():
            if isinstance(field, dict) and ""advanced"" not in field:
                field[""advanced""] = True

        return cls.cached_jsonschema

    @classmethod
    def validate_data(cls, data: BlockInput) -> str | None:
        return json.validate_with_jsonschema(schema=cls.jsonschema(), data=data)

    @classmethod
    def validate_field(cls, field_name: str, data: BlockInput) -> str | None:
        """"""
        Validate the data against a specific property (one of the input/output name).
        Returns the validation error message if the data does not match the schema.
        """"""
        model_schema = cls.jsonschema().get(""properties"", {})
        if not model_schema:
            return f""Invalid model schema {cls}""

        property_schema = model_schema.get(field_name)
        if not property_schema:
            return f""Invalid property name {field_name}""

        try:
            jsonschema.validate(json.to_dict(data), property_schema)
            return None
        except jsonschema.ValidationError as e:
            return str(e)

    @classmethod
    def get_fields(cls) -> set[str]:
        return set(cls.model_fields.keys())

    @classmethod
    def get_required_fields(cls) -> set[str]:
        return {
            field
            for field, field_info in cls.model_fields.items()
            if field_info.is_required()
        }

    @classmethod
    def __pydantic_init_subclass__(cls, **kwargs):
        """"""Validates the schema definition. Rules:
        - Only one `CredentialsMetaInput` field may be present.
          - This field MUST be called `credentials`.
        - A field that is called `credentials` MUST be a `CredentialsMetaInput`.
        """"""
        super().__pydantic_init_subclass__(**kwargs)

        # Reset cached JSON schema to prevent inheriting it from parent class
        cls.cached_jsonschema = {}

        credentials_fields = [
            field_name
            for field_name, info in cls.model_fields.items()
            if (
                inspect.isclass(info.annotation)
                and issubclass(
                    get_origin(info.annotation) or info.annotation,
                    CredentialsMetaInput,
                )
            )
        ]

        if len(credentials_fields) > 1:
            raise ValueError(
                f""{cls.__qualname__} can only have one CredentialsMetaInput field""
            )

        elif (
            len(credentials_fields) == 1
            and credentials_fields[0] != CREDENTIALS_FIELD_NAME
        ):
            raise ValueError(
                f""CredentialsMetaInput field on {cls.__qualname__} ""
                ""must be named 'credentials'""
            )

        elif (
            len(credentials_fields) == 0
            and CREDENTIALS_FIELD_NAME in cls.model_fields.keys()
        ):
            raise TypeError(
                f""Field 'credentials' on {cls.__qualname__} ""
                f""must be of type {CredentialsMetaInput.__name__}""
            )

        if credentials_field := cls.model_fields.get(CREDENTIALS_FIELD_NAME):
            credentials_input_type = cast(
                CredentialsMetaInput, credentials_field.annotation
            )
            credentials_input_type.validate_credentials_field_schema(cls)

class EmptySchema(BlockSchema):
    pass

class BlockManualWebhookConfig(BaseModel):
    """"""
    Configuration model for webhook-triggered blocks on which
    the user has to manually set up the webhook at the provider.
    """"""

    provider: str
    """"""The service provider that the webhook connects to""""""

    webhook_type: str
    """"""
    Identifier for the webhook type. E.g. GitHub has repo and organization level hooks.

    Only for use in the corresponding `WebhooksManager`.
    """"""

    event_filter_input: str = """"
    """"""
    Name of the block's event filter input.
    Leave empty if the corresponding webhook doesn't have distinct event/payload types.
    """"""

    event_format: str = ""{event}""
    """"""
    Template string for the event(s) that a block instance subscribes to.
    Applied individually to each event selected in the event filter input.

    Example: `""pull_request.{event}""` -> `""pull_request.opened""`
    """"""

class BlockWebhookConfig(BlockManualWebhookConfig):
    """"""
    Configuration model for webhook-triggered blocks for which
    the webhook can be automatically set up through the provider's API.
    """"""

    resource_format: str
    """"""
    Template string for the resource that a block instance subscribes to.
    Fields will be filled from the block's inputs (except `payload`).

    Example: `f""{repo}/pull_requests""` (note: not how it's actually implemented)

    Only for use in the corresponding `WebhooksManager`.
    """"""
    # --8<-- [end:BlockWebhookConfig]

class Block(ABC, Generic[BlockSchemaInputType, BlockSchemaOutputType]):
    def __init__(
        self,
        id: str = """",
        description: str = """",
        contributors: list[ContributorDetails] = [],
        categories: set[BlockCategory] | None = None,
        input_schema: Type[BlockSchemaInputType] = EmptySchema,
        output_schema: Type[BlockSchemaOutputType] = EmptySchema,
        test_input: BlockInput | list[BlockInput] | None = None,
        test_output: BlockData | list[BlockData] | None = None,
        test_mock: dict[str, Any] | None = None,
        test_credentials: Optional[Credentials] = None,
        disabled: bool = False,
        static_output: bool = False,
        block_type: BlockType = BlockType.STANDARD,
        webhook_config: Optional[BlockWebhookConfig | BlockManualWebhookConfig] = None,
    ):
        """"""
        Initialize the block with the given schema.

        Args:
            id: The unique identifier for the block, this value will be persisted in the
                DB. So it should be a unique and constant across the application run.
                Use the UUID format for the ID.
            description: The description of the block, explaining what the block does.
            contributors: The list of contributors who contributed to the block.
            input_schema: The schema, defined as a Pydantic model, for the input data.
            output_schema: The schema, defined as a Pydantic model, for the output data.
            test_input: The list or single sample input data for the block, for testing.
            test_output: The list or single expected output if the test_input is run.
            test_mock: function names on the block implementation to mock on test run.
            disabled: If the block is disabled, it will not be available for execution.
            static_output: Whether the output links of the block are static by default.
        """"""
        self.id = id
        self.input_schema = input_schema
        self.output_schema = output_schema
        self.test_input = test_input
        self.test_output = test_output
        self.test_mock = test_mock
        self.test_credentials = test_credentials
        self.description = description
        self.categories = categories or set()
        self.contributors = contributors or set()
        self.disabled = disabled
        self.static_output = static_output
        self.block_type = block_type
        self.webhook_config = webhook_config
        self.execution_stats = {}

        if self.webhook_config:
            if isinstance(self.webhook_config, BlockWebhookConfig):
                # Enforce presence of credentials field on auto-setup webhook blocks
                if CREDENTIALS_FIELD_NAME not in self.input_schema.model_fields:
                    raise TypeError(
                        ""credentials field is required on auto-setup webhook blocks""
                    )
                self.block_type = BlockType.WEBHOOK
            else:
                self.block_type = BlockType.WEBHOOK_MANUAL

            # Enforce shape of webhook event filter, if present
            if self.webhook_config.event_filter_input:
                event_filter_field = self.input_schema.model_fields[
                    self.webhook_config.event_filter_input
                ]
                if not (
                    isinstance(event_filter_field.annotation, type)
                    and issubclass(event_filter_field.annotation, BaseModel)
                    and all(
                        field.annotation is bool
                        for field in event_filter_field.annotation.model_fields.values()
                    )
                ):
                    raise NotImplementedError(
                        f""{self.name} has an invalid webhook event selector: ""
                        ""field must be a BaseModel and all its fields must be boolean""
                    )

            # Enforce presence of 'payload' input
            if ""payload"" not in self.input_schema.model_fields:
                raise TypeError(
                    f""{self.name} is webhook-triggered but has no 'payload' input""
                )

            # Disable webhook-triggered block if webhook functionality not available
            if not app_config.platform_base_url:
                self.disabled = True

    @classmethod
    def create(cls: Type[""Block""]) -> ""Block"":
        return cls()

    @abstractmethod
    def run(self, input_data: BlockSchemaInputType, **kwargs) -> BlockOutput:
        """"""
        Run the block with the given input data.
        Args:
            input_data: The input data with the structure of input_schema.
        Returns:
            A Generator that yields (output_name, output_data).
            output_name: One of the output name defined in Block's output_schema.
            output_data: The data for the output_name, matching the defined schema.
        """"""
        pass

    def run_once(self, input_data: BlockSchemaInputType, output: str, **kwargs) -> Any:
        for name, data in self.run(input_data, **kwargs):
            if name == output:
                return data
        raise ValueError(f""{self.name} did not produce any output for {output}"")

    def merge_stats(self, stats: dict[str, Any]) -> dict[str, Any]:
        for key, value in stats.items():
            if isinstance(value, dict):
                self.execution_stats.setdefault(key, {}).update(value)
            elif isinstance(value, (int, float)):
                self.execution_stats.setdefault(key, 0)
                self.execution_stats[key] += value
            elif isinstance(value, list):
                self.execution_stats.setdefault(key, [])
                self.execution_stats[key].extend(value)
            else:
                self.execution_stats[key] = value
        return self.execution_stats

    @property
    def name(self):
        return self.__class__.__name__

    def to_dict(self):
        return {
            ""id"": self.id,
            ""name"": self.name,
            ""inputSchema"": self.input_schema.jsonschema(),
            ""outputSchema"": self.output_schema.jsonschema(),
            ""description"": self.description,
            ""categories"": [category.dict() for category in self.categories],
            ""contributors"": [
                contributor.model_dump() for contributor in self.contributors
            ],
            ""staticOutput"": self.static_output,
            ""uiType"": self.block_type.value,
        }

def get_blocks() -> dict[str, Type[Block]]:
    from backend.blocks import AVAILABLE_BLOCKS  # noqa: E402
    return AVAILABLE_BLOCKS

async def initialize_blocks() -> None:
    for cls in get_blocks().values():
        block = cls()
        existing_block = await AgentBlock.prisma().find_first(
            where={""OR"": [{""id"": block.id}, {""name"": block.name}]}
        )
        if not existing_block:
            await AgentBlock.prisma().create(
                data={
                    ""id"": block.id,
                    ""name"": block.name,
                    ""inputSchema"": json.dumps(block.input_schema.jsonschema()),
                    ""outputSchema"": json.dumps(block.output_schema.jsonschema()),
                }
            )
            continue
        input_schema = json.dumps(block.input_schema.jsonschema())
        output_schema = json.dumps(block.output_schema.jsonschema())
        if (
            block.id != existing_block.id
            or block.name != existing_block.name
            or input_schema != existing_block.inputSchema
            or output_schema != existing_block.outputSchema
        ):
            await AgentBlock.prisma().update(
                where={""id"": existing_block.id},
                data={
                    ""id"": block.id,
                    ""name"": block.name,
                    ""inputSchema"": input_schema,
                    ""outputSchema"": output_schema,
                },
            )

def get_block(block_id: str) -> Block | None:
    cls = get_blocks().get(block_id)
    return cls() if cls else None","{'final_score': 96.26, 'length_similarity': 93.9, 'token_similarity': 96.06, 'structure_similarity': 94.88, 'function_similarity': 100, 'complexity_similarity': 93.75, 'variable_similarity': 100}",
121,"import time

from backend.util.decorator import error_logged, time_measured


@time_measured
def example_function(a: int, b: int, c: int) -> int:
    time.sleep(0.5)
    return a + b + c


@error_logged
def example_function_with_error(a: int, b: int, c: int) -> int:
    raise ValueError(""This is a test error"")


def test_timer_decorator():
    info, res = example_function(1, 2, 3)
    assert info.cpu_time >= 0
    assert info.wall_time >= 0.4
    assert res == 6


def test_error_decorator():
    res = example_function_with_error(1, 2, 3)
    assert res is None
","import time
from backend.util.decorator import error_logged, time_measured

@time_measured
def example_function(a: int, b: int, c: int) -> int:
    time.sleep(0.5)
    return a + b + c

@error_logged
def example_function_with_error(a: int, b: int, c: int) -> int:
    raise ValueError(""This is a test error"")

def test_timer_decorator():
    info, res = example_function(1, 2, 3)
    assert info.cpu_time >= 0
    assert info.wall_time >= 0.4
    assert res == 6

def test_error_decorator():
    res = example_function_with_error(1, 2, 3)
    assert res is None","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
122,"from collections import defaultdict
from datetime import datetime, timezone
from multiprocessing import Manager
from typing import Any, AsyncGenerator, Generator, Generic, TypeVar

from prisma.enums import AgentExecutionStatus
from prisma.models import (
    AgentGraphExecution,
    AgentNodeExecution,
    AgentNodeExecutionInputOutput,
)
from pydantic import BaseModel

from backend.data.block import BlockData, BlockInput, CompletedBlockOutput
from backend.data.includes import EXECUTION_RESULT_INCLUDE, GRAPH_EXECUTION_INCLUDE
from backend.data.queue import AsyncRedisEventBus, RedisEventBus
from backend.util import json, mock
from backend.util.settings import Config


class GraphExecutionEntry(BaseModel):
    user_id: str
    graph_exec_id: str
    graph_id: str
    start_node_execs: list[""NodeExecutionEntry""]


class NodeExecutionEntry(BaseModel):
    user_id: str
    graph_exec_id: str
    graph_id: str
    node_exec_id: str
    node_id: str
    data: BlockInput


ExecutionStatus = AgentExecutionStatus

T = TypeVar(""T"")


class ExecutionQueue(Generic[T]):
    """"""
    Queue for managing the execution of agents.
    This will be shared between different processes
    """"""

    def __init__(self):
        self.queue = Manager().Queue()

    def add(self, execution: T) -> T:
        self.queue.put(execution)
        return execution

    def get(self) -> T:
        return self.queue.get()

    def empty(self) -> bool:
        return self.queue.empty()


class ExecutionResult(BaseModel):
    graph_id: str
    graph_version: int
    graph_exec_id: str
    node_exec_id: str
    node_id: str
    block_id: str
    status: ExecutionStatus
    input_data: BlockInput
    output_data: CompletedBlockOutput
    add_time: datetime
    queue_time: datetime | None
    start_time: datetime | None
    end_time: datetime | None

    @staticmethod
    def from_graph(graph: AgentGraphExecution):
        return ExecutionResult(
            graph_id=graph.agentGraphId,
            graph_version=graph.agentGraphVersion,
            graph_exec_id=graph.id,
            node_exec_id="""",
            node_id="""",
            block_id="""",
            status=graph.executionStatus,
            # TODO: Populate input_data & output_data from AgentNodeExecutions
            #       Input & Output comes AgentInputBlock & AgentOutputBlock.
            input_data={},
            output_data={},
            add_time=graph.createdAt,
            queue_time=graph.createdAt,
            start_time=graph.startedAt,
            end_time=graph.updatedAt,
        )

    @staticmethod
    def from_db(execution: AgentNodeExecution):
        if execution.executionData:
            # Execution that has been queued for execution will persist its data.
            input_data = json.loads(execution.executionData, target_type=dict[str, Any])
        else:
            # For incomplete execution, executionData will not be yet available.
            input_data: BlockInput = defaultdict()
            for data in execution.Input or []:
                input_data[data.name] = json.loads(data.data)

        output_data: CompletedBlockOutput = defaultdict(list)
        for data in execution.Output or []:
            output_data[data.name].append(json.loads(data.data))

        graph_execution: AgentGraphExecution | None = execution.AgentGraphExecution

        return ExecutionResult(
            graph_id=graph_execution.agentGraphId if graph_execution else """",
            graph_version=graph_execution.agentGraphVersion if graph_execution else 0,
            graph_exec_id=execution.agentGraphExecutionId,
            block_id=execution.AgentNode.agentBlockId if execution.AgentNode else """",
            node_exec_id=execution.id,
            node_id=execution.agentNodeId,
            status=execution.executionStatus,
            input_data=input_data,
            output_data=output_data,
            add_time=execution.addedTime,
            queue_time=execution.queuedTime,
            start_time=execution.startedTime,
            end_time=execution.endedTime,
        )


# --------------------- Model functions --------------------- #


async def create_graph_execution(
    graph_id: str,
    graph_version: int,
    nodes_input: list[tuple[str, BlockInput]],
    user_id: str,
) -> tuple[str, list[ExecutionResult]]:
    """"""
    Create a new AgentGraphExecution record.
    Returns:
        The id of the AgentGraphExecution and the list of ExecutionResult for each node.
    """"""
    result = await AgentGraphExecution.prisma().create(
        data={
            ""agentGraphId"": graph_id,
            ""agentGraphVersion"": graph_version,
            ""executionStatus"": ExecutionStatus.QUEUED,
            ""AgentNodeExecutions"": {
                ""create"": [  # type: ignore
                    {
                        ""agentNodeId"": node_id,
                        ""executionStatus"": ExecutionStatus.INCOMPLETE,
                        ""Input"": {
                            ""create"": [
                                {""name"": name, ""data"": json.dumps(data)}
                                for name, data in node_input.items()
                            ]
                        },
                    }
                    for node_id, node_input in nodes_input
                ]
            },
            ""userId"": user_id,
        },
        include=GRAPH_EXECUTION_INCLUDE,
    )

    return result.id, [
        ExecutionResult.from_db(execution)
        for execution in result.AgentNodeExecutions or []
    ]


async def upsert_execution_input(
    node_id: str,
    graph_exec_id: str,
    input_name: str,
    input_data: Any,
    node_exec_id: str | None = None,
) -> tuple[str, BlockInput]:
    """"""
    Insert AgentNodeExecutionInputOutput record for as one of AgentNodeExecution.Input.
    If there is no AgentNodeExecution that has no `input_name` as input, create new one.

    Args:
        node_id: The id of the AgentNode.
        graph_exec_id: The id of the AgentGraphExecution.
        input_name: The name of the input data.
        input_data: The input data to be inserted.
        node_exec_id: [Optional] The id of the AgentNodeExecution that has no `input_name` as input. If not provided, it will find the eligible incomplete AgentNodeExecution or create a new one.

    Returns:
        * The id of the created or existing AgentNodeExecution.
        * Dict of node input data, key is the input name, value is the input data.
    """"""
    existing_execution = await AgentNodeExecution.prisma().find_first(
        where={  # type: ignore
            **({""id"": node_exec_id} if node_exec_id else {}),
            ""agentNodeId"": node_id,
            ""agentGraphExecutionId"": graph_exec_id,
            ""executionStatus"": ExecutionStatus.INCOMPLETE,
            ""Input"": {""every"": {""name"": {""not"": input_name}}},
        },
        order={""addedTime"": ""asc""},
        include={""Input"": True},
    )
    json_input_data = json.dumps(input_data)

    if existing_execution:
        await AgentNodeExecutionInputOutput.prisma().create(
            data={
                ""name"": input_name,
                ""data"": json_input_data,
                ""referencedByInputExecId"": existing_execution.id,
            }
        )
        return existing_execution.id, {
            **{
                input_data.name: json.loads(input_data.data)
                for input_data in existing_execution.Input or []
            },
            input_name: input_data,
        }

    elif not node_exec_id:
        result = await AgentNodeExecution.prisma().create(
            data={
                ""agentNodeId"": node_id,
                ""agentGraphExecutionId"": graph_exec_id,
                ""executionStatus"": ExecutionStatus.INCOMPLETE,
                ""Input"": {""create"": {""name"": input_name, ""data"": json_input_data}},
            }
        )
        return result.id, {input_name: input_data}

    else:
        raise ValueError(
            f""NodeExecution {node_exec_id} not found or already has input {input_name}.""
        )


async def upsert_execution_output(
    node_exec_id: str,
    output_name: str,
    output_data: Any,
) -> None:
    """"""
    Insert AgentNodeExecutionInputOutput record for as one of AgentNodeExecution.Output.
    """"""
    await AgentNodeExecutionInputOutput.prisma().create(
        data={
            ""name"": output_name,
            ""data"": json.dumps(output_data),
            ""referencedByOutputExecId"": node_exec_id,
        }
    )


async def update_graph_execution_start_time(graph_exec_id: str):
    await AgentGraphExecution.prisma().update(
        where={""id"": graph_exec_id},
        data={
            ""executionStatus"": ExecutionStatus.RUNNING,
            ""startedAt"": datetime.now(tz=timezone.utc),
        },
    )


async def update_graph_execution_stats(
    graph_exec_id: str,
    stats: dict[str, Any],
) -> ExecutionResult:
    status = ExecutionStatus.FAILED if stats.get(""error"") else ExecutionStatus.COMPLETED
    res = await AgentGraphExecution.prisma().update(
        where={""id"": graph_exec_id},
        data={
            ""executionStatus"": status,
            ""stats"": json.dumps(stats),
        },
    )
    if not res:
        raise ValueError(f""Execution {graph_exec_id} not found."")

    return ExecutionResult.from_graph(res)


async def update_node_execution_stats(node_exec_id: str, stats: dict[str, Any]):
    await AgentNodeExecution.prisma().update(
        where={""id"": node_exec_id},
        data={""stats"": json.dumps(stats)},
    )


async def update_execution_status(
    node_exec_id: str,
    status: ExecutionStatus,
    execution_data: BlockInput | None = None,
    stats: dict[str, Any] | None = None,
) -> ExecutionResult:
    if status == ExecutionStatus.QUEUED and execution_data is None:
        raise ValueError(""Execution data must be provided when queuing an execution."")

    now = datetime.now(tz=timezone.utc)
    data = {
        **({""executionStatus"": status}),
        **({""queuedTime"": now} if status == ExecutionStatus.QUEUED else {}),
        **({""startedTime"": now} if status == ExecutionStatus.RUNNING else {}),
        **({""endedTime"": now} if status == ExecutionStatus.FAILED else {}),
        **({""endedTime"": now} if status == ExecutionStatus.COMPLETED else {}),
        **({""executionData"": json.dumps(execution_data)} if execution_data else {}),
        **({""stats"": json.dumps(stats)} if stats else {}),
    }

    res = await AgentNodeExecution.prisma().update(
        where={""id"": node_exec_id},
        data=data,  # type: ignore
        include=EXECUTION_RESULT_INCLUDE,
    )
    if not res:
        raise ValueError(f""Execution {node_exec_id} not found."")

    return ExecutionResult.from_db(res)


async def get_execution_results(graph_exec_id: str) -> list[ExecutionResult]:
    executions = await AgentNodeExecution.prisma().find_many(
        where={""agentGraphExecutionId"": graph_exec_id},
        include=EXECUTION_RESULT_INCLUDE,
        order=[
            {""queuedTime"": ""asc""},
            {""addedTime"": ""asc""},  # Fallback: Incomplete execs has no queuedTime.
        ],
    )
    res = [ExecutionResult.from_db(execution) for execution in executions]
    return res


LIST_SPLIT = ""_$_""
DICT_SPLIT = ""_#_""
OBJC_SPLIT = ""_@_""


def parse_execution_output(output: BlockData, name: str) -> Any | None:
    # Allow extracting partial output data by name.
    output_name, output_data = output

    if name == output_name:
        return output_data

    if name.startswith(f""{output_name}{LIST_SPLIT}""):
        index = int(name.split(LIST_SPLIT)[1])
        if not isinstance(output_data, list) or len(output_data) <= index:
            return None
        return output_data[int(name.split(LIST_SPLIT)[1])]

    if name.startswith(f""{output_name}{DICT_SPLIT}""):
        index = name.split(DICT_SPLIT)[1]
        if not isinstance(output_data, dict) or index not in output_data:
            return None
        return output_data[index]

    if name.startswith(f""{output_name}{OBJC_SPLIT}""):
        index = name.split(OBJC_SPLIT)[1]
        if isinstance(output_data, object) and hasattr(output_data, index):
            return getattr(output_data, index)
        return None

    return None


def merge_execution_input(data: BlockInput) -> BlockInput:
    """"""
    Merge all dynamic input pins which described by the following pattern:
    - <input_name>_$_<index> for list input.
    - <input_name>_#_<index> for dict input.
    - <input_name>_@_<index> for object input.
    This function will construct pins with the same name into a single list/dict/object.
    """"""

    # Merge all input with <input_name>_$_<index> into a single list.
    items = list(data.items())

    for key, value in items:
        if LIST_SPLIT not in key:
            continue
        name, index = key.split(LIST_SPLIT)
        if not index.isdigit():
            raise ValueError(f""Invalid key: {key}, #{index} index must be an integer."")

        data[name] = data.get(name, [])
        if int(index) >= len(data[name]):
            # Pad list with empty string on missing indices.
            data[name].extend([""""] * (int(index) - len(data[name]) + 1))
        data[name][int(index)] = value

    # Merge all input with <input_name>_#_<index> into a single dict.
    for key, value in items:
        if DICT_SPLIT not in key:
            continue
        name, index = key.split(DICT_SPLIT)
        data[name] = data.get(name, {})
        data[name][index] = value

    # Merge all input with <input_name>_@_<index> into a single object.
    for key, value in items:
        if OBJC_SPLIT not in key:
            continue
        name, index = key.split(OBJC_SPLIT)
        if name not in data or not isinstance(data[name], object):
            data[name] = mock.MockObject()
        setattr(data[name], index, value)

    return data


async def get_latest_execution(node_id: str, graph_eid: str) -> ExecutionResult | None:
    execution = await AgentNodeExecution.prisma().find_first(
        where={
            ""agentNodeId"": node_id,
            ""agentGraphExecutionId"": graph_eid,
            ""executionStatus"": {""not"": ExecutionStatus.INCOMPLETE},
            ""executionData"": {""not"": None},  # type: ignore
        },
        order={""queuedTime"": ""desc""},
        include=EXECUTION_RESULT_INCLUDE,
    )
    if not execution:
        return None
    return ExecutionResult.from_db(execution)


async def get_incomplete_executions(
    node_id: str, graph_eid: str
) -> list[ExecutionResult]:
    executions = await AgentNodeExecution.prisma().find_many(
        where={
            ""agentNodeId"": node_id,
            ""agentGraphExecutionId"": graph_eid,
            ""executionStatus"": ExecutionStatus.INCOMPLETE,
        },
        include=EXECUTION_RESULT_INCLUDE,
    )
    return [ExecutionResult.from_db(execution) for execution in executions]


# --------------------- Event Bus --------------------- #

config = Config()


class RedisExecutionEventBus(RedisEventBus[ExecutionResult]):
    Model = ExecutionResult

    @property
    def event_bus_name(self) -> str:
        return config.execution_event_bus_name

    def publish(self, res: ExecutionResult):
        self.publish_event(res, f""{res.graph_id}/{res.graph_exec_id}"")

    def listen(
        self, graph_id: str = ""*"", graph_exec_id: str = ""*""
    ) -> Generator[ExecutionResult, None, None]:
        for execution_result in self.listen_events(f""{graph_id}/{graph_exec_id}""):
            yield execution_result


class AsyncRedisExecutionEventBus(AsyncRedisEventBus[ExecutionResult]):
    Model = ExecutionResult

    @property
    def event_bus_name(self) -> str:
        return config.execution_event_bus_name

    async def publish(self, res: ExecutionResult):
        await self.publish_event(res, f""{res.graph_id}/{res.graph_exec_id}"")

    async def listen(
        self, graph_id: str = ""*"", graph_exec_id: str = ""*""
    ) -> AsyncGenerator[ExecutionResult, None]:
        async for execution_result in self.listen_events(f""{graph_id}/{graph_exec_id}""):
            yield execution_result
","from collections import defaultdict
from datetime import datetime, timezone
from multiprocessing import Manager
from typing import Any, AsyncGenerator, Generator, Generic, TypeVar

from prisma.enums import AgentExecutionStatus
from prisma.models import (
    AgentGraphExecution,
    AgentNodeExecution,
    AgentNodeExecutionInputOutput,
)

from pydantic import BaseModel

from backend.data.block import BlockData, BlockInput, CompletedBlockOutput
from backend.data.includes import EXECUTION_RESULT_INCLUDE, GRAPH_EXECUTION_INCLUDE
from backend.data.queue import AsyncRedisEventBus, RedisEventBus
from backend.util import json, mock
from backend.util.settings import Config


class GraphExecutionEntry(BaseModel):
    user_id: str
    graph_exec_id: str
    graph_id: str
    start_node_execs: list[""NodeExecutionEntry""]


class NodeExecutionEntry(BaseModel):
    user_id: str
    graph_exec_id: str
    graph_id: str
    node_exec_id: str
    node_id: str
    data: BlockInput


T = TypeVar('T')


class ExecutionQueue(Generic[T]):
    """"""
    Queue for managing the execution of agents.
    This will be shared between different processes
    """"""

    def __init__(self):
        self.queue = Manager().Queue()

    def add(self, execution: T) -> T:
        self.queue.put(execution)
        return execution

    def get(self) -> T:
        return self.queue.get()

    def empty(self) -> bool:
        return self.queue.empty()


class ExecutionResult(BaseModel):
    graph_id: str
    graph_version: int
    graph_exec_id: str
    node_exec_id: str
    node_id: str
    block_id: str
    status: ExecutionStatus
    input_data: BlockInput
    output_data: CompletedBlockOutput
    add_time: datetime
    queue_time: datetime | None
    start_time: datetime | None
    end_time: datetime | None

    @staticmethod
    def from_graph(graph: AgentGraphExecution):
        return ExecutionResult(
            graph_id=graph.agentGraphId,
            graph_version=graph.agentGraphVersion,
            graph_exec_id=graph.id,
            node_exec_id="""",
            node_id="""",
            block_id="""",
            status=graph.executionStatus,
            # TODO: Populate input_data & output_data from AgentNodeExecutions
            # Input & Output comes AgentInputBlock & AgentOutputBlock.
            input_data={},
            output_data={},
            add_time=graph.createdAt,
            queue_time=graph.createdAt,
            start_time=graph.startedAt,
            end_time=graph.updatedAt,
        )

    @staticmethod
    def from_db(execution: AgentNodeExecution):
        if execution.executionData:
            # Execution that has been queued for execution will persist its data.
            input_data = json.loads(execution.executionData, target_type=dict[str, Any])
        else:
            # For incomplete execution, executionData will not be yet available.
            input_data: BlockInput = defaultdict()
            for data in execution.Input or []:
                input_data[data.name] = json.loads(data.data)

        output_data: CompletedBlockOutput = defaultdict(list)
        for data in execution.Output or []:
            output_data[data.name].append(json.loads(data.data))

        graph_execution: AgentGraphExecution | None = execution.AgentGraphExecution

        return ExecutionResult(
            graph_id=graph_execution.agentGraphId if graph_execution else """",
            graph_version=graph_execution.agentGraphVersion if graph_execution else 0,
            graph_exec_id=execution.agentGraphExecutionId,
            block_id=execution.AgentNode.agentBlockId if execution.AgentNode else """",
            node_exec_id=execution.id,
            node_id=execution.agentNodeId,
            status=execution.executionStatus,
            input_data=input_data,
            output_data=output_data,
            add_time=execution.addedTime,
            queue_time=execution.queuedTime,
            start_time=execution.startedTime,
            end_time=execution.endedTime,
        )


async def create_graph_execution(
    graph_id: str,
    graph_version: int,
    nodes_input: list[tuple[str, BlockInput]],
    user_id: str,
) -> tuple[str, list[ExecutionResult]]:
    """"""
    Create a new AgentGraphExecution record.
    Returns:
        The id of the AgentGraphExecution and the list of ExecutionResult for each node.
    """"""
    result = await AgentGraphExecution.prisma().create(
        data={
            ""agentGraphId"": graph_id,
            ""agentGraphVersion"": graph_version,
            ""executionStatus"": ExecutionStatus.QUEUED,
            ""AgentNodeExecutions"": {
                ""create"": [  # type: ignore
                    {
                        ""agentNodeId"": node_id,
                        ""executionStatus"": ExecutionStatus.INCOMPLETE,
                        ""Input"": {
                            ""create"": [
                                {""name"": name, ""data"": json.dumps(data)}
                                for name, data in node_input.items()
                            ]
                        },
                    }
                    for node_id, node_input in nodes_input
                ]
            },
            ""userId"": user_id,
        },
        include=GRAPH_EXECUTION_INCLUDE,
    )
    return result.id, [
        ExecutionResult.from_db(execution)
        for execution in result.AgentNodeExecutions or []
    ]


async def upsert_execution_input(
    node_id: str,
    graph_exec_id: str,
    input_name: str,
    input_data: Any,
    node_exec_id: str | None = None,
) -> tuple[str, BlockInput]:
    """"""
    Insert AgentNodeExecutionInputOutput record for as one of AgentNodeExecution.Input.
    If there is no AgentNodeExecution that has no `input_name` as input, create new one.

    Args:
        node_id: The id of the AgentNode.
        graph_exec_id: The id of the AgentGraphExecution.
        input_name: The name of the input data.
        input_data: The input data to be inserted.
        node_exec_id: [Optional] The id of the AgentNodeExecution that has no `input_name` as input. If not provided, it will find the eligible incomplete AgentNodeExecution or create a new one.

    Returns:
        * The id of the created or existing AgentNodeExecution.
        * Dict of node input data, key is the input name, value is the input data.
    """"""
    existing_execution = await AgentNodeExecution.prisma().find_first(
        where={  # type: ignore
            **({""id"": node_exec_id} if node_exec_id else {}),
            ""agentNodeId"": node_id,
            ""agentGraphExecutionId"": graph_exec_id,
            ""executionStatus"": ExecutionStatus.INCOMPLETE,
            ""Input"": {""every"": {""name"": {""not"": input_name}}},
        },
        order={""addedTime"": ""asc""},
        include={""Input"": True},
    )

    json_input_data = json.dumps(input_data)

    if existing_execution:
        await AgentNodeExecutionInputOutput.prisma().create(
            data={
                ""name"": input_name,
                ""data"": json_input_data,
                ""referencedByInputExecId"": existing_execution.id,
                }
        )
        return existing_execution.id, {
            **{
                input_data.name: json.loads(input_data.data)
                for input_data in existing_execution.Input or []
            },
            input_name: input_data,
        }

    elif not node_exec_id:
        result = await AgentNodeExecution.prisma().create(
            data={
                ""agentNodeId"": node_id,
                ""agentGraphExecutionId"": graph_exec_id,
                ""executionStatus"": ExecutionStatus.INCOMPLETE,
                ""Input"": {""create"": {""name"": input_name, ""data"": json_input_data}},
            }
        )
        return result.id, {input_name: input_data}

    else:
        raise ValueError(
            f""NodeExecution {node_exec_id} not found or already has input {input_name}.""
        )


async def upsert_execution_output(
    node_exec_id: str,
    output_name: str,
    output_data: Any,
) -> None:
    """"""
    Insert AgentNodeExecutionInputOutput record for as one of AgentNodeExecution.Output.
    """"""
    await AgentNodeExecutionInputOutput.prisma().create(
        data={
            ""name"": output_name,
            ""data"": json.dumps(output_data),
            ""referencedByOutputExecId"": node_exec_id,
        }
    )


async def update_graph_execution_start_time(graph_exec_id: str):
    await AgentGraphExecution.prisma().update(
        where={""id"": graph_exec_id},
        data={
            ""executionStatus"": ExecutionStatus.RUNNING,
            ""startedAt"": datetime.now(tz=timezone.utc),
        },
    )


async def update_graph_execution_stats(
    graph_exec_id: str,
    stats: dict[str, Any],
) -> ExecutionResult:
    status = ExecutionStatus.FAILED if stats.get(""error"") else ExecutionStatus.COMPLETED

    res = await AgentGraphExecution.prisma().update(
        where={""id"": graph_exec_id},
        data={
            ""executionStatus"": status,
            ""stats"": json.dumps(stats),
        },
    )

    if not res:
        raise ValueError(f""Execution {graph_exec_id} not found."")

    return ExecutionResult.from_graph(res)


async def update_node_execution_stats(node_exec_id: str, stats: dict[str, Any]):
    await AgentNodeExecution.prisma().update(
        where={""id"": node_exec_id},
        data={""stats"": json.dumps(stats)},
    )


async def update_execution_status(
    node_exec_id: str,
    status: ExecutionStatus,
    execution_data: BlockInput | None = None,
    stats: dict[str, Any] | None = None,
) -> ExecutionResult:
    if status == ExecutionStatus.QUEUED and execution_data is None:
        raise ValueError(""Execution data must be provided when queuing an execution."")

    now = datetime.now(tz=timezone.utc)
    data = {
        **({""executionStatus"": status}),
        **({""queuedTime"": now} if status == ExecutionStatus.QUEUED else {}),
        **({""startedTime"": now} if status == ExecutionStatus.RUNNING else {}),
        **({""endedTime"": now} if status == ExecutionStatus.FAILED else {}),
        **({""endedTime"": now} if status == ExecutionStatus.COMPLETED else {}),
        **({""executionData"": json.dumps(execution_data)} if execution_data else {}),
        **({""stats"": json.dumps(stats)} if stats else {}),
    }

    res = await AgentNodeExecution.prisma().update(
        where={""id"": node_exec_id},
        data=data,  # type: ignore
        include=EXECUTION_RESULT_INCLUDE,
    )

    if not res:
        raise ValueError(f""Execution {node_exec_id} not found."")

    return ExecutionResult.from_db(res)


async def get_execution_results(graph_exec_id: str) -> list[ExecutionResult]:
    executions = await AgentNodeExecution.prisma().find_many(
        where={""agentGraphExecutionId"": graph_exec_id},
        include=EXECUTION_RESULT_INCLUDE,
        order=[
            {""queuedTime"": ""asc""},
            {""addedTime"": ""asc""},  # Fallback: Incomplete execs has no queuedTime.
        ],
    )
    res = [ExecutionResult.from_db(execution) for execution in executions]
    return res


def parse_execution_output(output: BlockData, name: str) -> Any | None:
    # Allow extracting partial output data by name.
    output_name, output_data = output

    if name == output_name:
        return output_data

    if name.startswith(f""{output_name}{LIST_SPLIT}""):
        index = int(name.split(LIST_SPLIT)[1])
        if not isinstance(output_data, list) or len(output_data) <= index:
            return None
        return output_data[int(name.split(LIST_SPLIT)[1])]

    if name.startswith(f""{output_name}{DICT_SPLIT}""):
        index = name.split(DICT_SPLIT)[1]
        if not isinstance(output_data, dict) or index not in output_data:
            return None
        return output_data[index]

    if name.startswith(f""{output_name}{OBJC_SPLIT}""):
        index = name.split(OBJC_SPLIT)[1]
        if isinstance(output_data, object) and hasattr(output_data, index):
            return getattr(output_data, index)
        return None

    return None


def merge_execution_input(data: BlockInput) -> BlockInput:
    """"""
    Merge all dynamic input pins which described by the following pattern:
    - <input_name>_$_<index> for list input.
    - <input_name>_#_<index> for dict input.
    - <input_name>_@_<index> for object input.
    This function will construct pins with the same name into a single list/dict/object.
    """"""
    # Merge all input with <input_name>_$_<index> into a single list.
    items = list(data.items())

    for key, value in items:
        if LIST_SPLIT not in key:
            continue

        name, index = key.split(LIST_SPLIT)
        if not index.isdigit():
            raise ValueError(f""Invalid key: {key}, #{index} index must be an integer."")

        data[name] = data.get(name, [])
        if int(index) >= len(data[name]):
            # Pad list with empty string on missing indices.
            data[name].extend([""""] * (int(index) - len(data[name]) + 1))
        data[name][int(index)] = value

    # Merge all input with <input_name>_#_<index> into a single dict.
    for key, value in items:
        if DICT_SPLIT not in key:
            continue

        name, index = key.split(DICT_SPLIT)
        data[name] = data.get(name, {})
        data[name][index] = value

    # Merge all input with <input_name>_@_<index> into a single object.
    for key, value in items:
        if OBJC_SPLIT not in key:
            continue

        name, index = key.split(OBJC_SPLIT)
        if name not in data or not isinstance(data[name], object):
            data[name] = mock.MockObject()
        setattr(data[name], index, value)

    return data


async def get_latest_execution(node_id: str, graph_eid: str) -> ExecutionResult | None:
    execution = await AgentNodeExecution.prisma().find_first(
        where={
            ""agentNodeId"": node_id,
            ""agentGraphExecutionId"": graph_eid,
            ""executionStatus"": {""not"": ExecutionStatus.INCOMPLETE},
            ""executionData"": {""not"": None},  # type: ignore
        },
        order={""queuedTime"": ""desc""},
        include=EXECUTION_RESULT_INCLUDE,
    )
    if not execution:
        return None
    return ExecutionResult.from_db(execution)


async def get_incomplete_executions(
    node_id: str, graph_eid: str
) -> list[ExecutionResult]:
    executions = await AgentNodeExecution.prisma().find_many(
        where={
            ""agentNodeId"": node_id,
            ""agentGraphExecutionId"": graph_eid,
            ""executionStatus"": ExecutionStatus.INCOMPLETE,
        },
        include=EXECUTION_RESULT_INCLUDE,
    )
    return [ExecutionResult.from_db(execution) for execution in executions]


class RedisExecutionEventBus(RedisEventBus[ExecutionResult]):
    Model = ExecutionResult

    @property
    def event_bus_name(self) -> str:
        return config.execution_event_bus_name

    def publish(self, res: ExecutionResult):
        self.publish_event(res, f""{res.graph_id}/{res.graph_exec_id}"")

    def listen(
        self, graph_id: str = ""*"", graph_exec_id: str = ""*""
    ) -> Generator[ExecutionResult, None, None]:
        for execution_result in self.listen_events(f""{graph_id}/{graph_exec_id}""):
            yield execution_result


class AsyncRedisExecutionEventBus(AsyncRedisEventBus[ExecutionResult]):
    Model = ExecutionResult

    @property
    def event_bus_name(self) -> str:
        return config.execution_event_bus_name

    async def publish(self, res: ExecutionResult):
        await self.publish_event(res, f""{res.graph_id}/{res.graph_exec_id}"")

    async def listen(
        self, graph_id: str = ""*"", graph_exec_id: str = ""*""
    ) -> AsyncGenerator[ExecutionResult, None]:
        async for execution_result in self.listen_events(f""{graph_id}/{graph_exec_id}""):
            yield execution_result","{'final_score': 98.79, 'length_similarity': 98.52, 'token_similarity': 98.04, 'structure_similarity': 98.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
123,"import uuid
from typing import List

import requests as baserequests

from backend.data.block import BlockOutput, BlockSchema
from backend.data.model import APIKeyCredentials, SchemaField
from backend.util import settings
from backend.util.settings import BehaveAs

from ._api import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    CustomerDetails,
    OrderItem,
    Slant3DCredentialsField,
    Slant3DCredentialsInput,
)
from .base import Slant3DBlockBase


class Slant3DCreateOrderBlock(Slant3DBlockBase):
    """"""Block for creating new orders""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        order_number: str = SchemaField(
            description=""Your custom order number (or leave blank for a random one)"",
            default_factory=lambda: str(uuid.uuid4()),
        )
        customer: CustomerDetails = SchemaField(
            description=""Customer details for where to ship the item"",
            advanced=False,
        )
        items: List[OrderItem] = SchemaField(
            description=""List of items to print"",
            advanced=False,
        )

    class Output(BlockSchema):
        order_id: str = SchemaField(description=""Slant3D order ID"")
        error: str = SchemaField(description=""Error message if order failed"")

    def __init__(self):
        super().__init__(
            id=""f73007d6-f48f-4aaf-9e6b-6883998a09b4"",
            description=""Create a new print order"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""order_number"": ""TEST-001"",
                ""customer"": {
                    ""name"": ""John Doe"",
                    ""email"": ""john@example.com"",
                    ""phone"": ""123-456-7890"",
                    ""address"": ""123 Test St"",
                    ""city"": ""Test City"",
                    ""state"": ""TS"",
                    ""zip"": ""12345"",
                },
                ""items"": [
                    {
                        ""file_url"": ""https://example.com/model.stl"",
                        ""quantity"": ""1"",
                        ""color"": ""black"",
                        ""profile"": ""PLA"",
                    }
                ],
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""order_id"", ""314144241"")],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {""orderId"": ""314144241""},
                ""_convert_to_color"": lambda *args, **kwargs: ""black"",
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            order_data = self._format_order_data(
                input_data.customer,
                input_data.order_number,
                input_data.items,
                credentials.api_key.get_secret_value(),
            )
            result = self._make_request(
                ""POST"", ""order"", credentials.api_key.get_secret_value(), json=order_data
            )
            yield ""order_id"", result[""orderId""]
        except Exception as e:
            yield ""error"", str(e)
            raise


class Slant3DEstimateOrderBlock(Slant3DBlockBase):
    """"""Block for getting order cost estimates""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        order_number: str = SchemaField(
            description=""Your custom order number (or leave blank for a random one)"",
            default_factory=lambda: str(uuid.uuid4()),
        )
        customer: CustomerDetails = SchemaField(
            description=""Customer details for where to ship the item"",
            advanced=False,
        )
        items: List[OrderItem] = SchemaField(
            description=""List of items to print"",
            advanced=False,
        )

    class Output(BlockSchema):
        total_price: float = SchemaField(description=""Total price in USD"")
        shipping_cost: float = SchemaField(description=""Shipping cost"")
        printing_cost: float = SchemaField(description=""Printing cost"")
        error: str = SchemaField(description=""Error message if estimation failed"")

    def __init__(self):
        super().__init__(
            id=""bf8823d6-b42a-48c7-b558-d7c117f2ae85"",
            description=""Get order cost estimate"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""order_number"": ""TEST-001"",
                ""customer"": {
                    ""name"": ""John Doe"",
                    ""email"": ""john@example.com"",
                    ""phone"": ""123-456-7890"",
                    ""address"": ""123 Test St"",
                    ""city"": ""Test City"",
                    ""state"": ""TS"",
                    ""zip"": ""12345"",
                },
                ""items"": [
                    {
                        ""file_url"": ""https://example.com/model.stl"",
                        ""quantity"": ""1"",
                        ""color"": ""black"",
                        ""profile"": ""PLA"",
                    }
                ],
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""total_price"", 9.31),
                (""shipping_cost"", 5.56),
                (""printing_cost"", 3.75),
            ],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""totalPrice"": 9.31,
                    ""shippingCost"": 5.56,
                    ""printingCost"": 3.75,
                },
                ""_convert_to_color"": lambda *args, **kwargs: ""black"",
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        order_data = self._format_order_data(
            input_data.customer,
            input_data.order_number,
            input_data.items,
            credentials.api_key.get_secret_value(),
        )
        try:
            result = self._make_request(
                ""POST"",
                ""order/estimate"",
                credentials.api_key.get_secret_value(),
                json=order_data,
            )
            yield ""total_price"", result[""totalPrice""]
            yield ""shipping_cost"", result[""shippingCost""]
            yield ""printing_cost"", result[""printingCost""]
        except baserequests.HTTPError as e:
            yield ""error"", str(f""Error estimating order: {e} {e.response.text}"")
            raise


class Slant3DEstimateShippingBlock(Slant3DBlockBase):
    """"""Block for getting shipping cost estimates""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        order_number: str = SchemaField(
            description=""Your custom order number (or leave blank for a random one)"",
            default_factory=lambda: str(uuid.uuid4()),
        )
        customer: CustomerDetails = SchemaField(
            description=""Customer details for where to ship the item""
        )
        items: List[OrderItem] = SchemaField(
            description=""List of items to print"",
            advanced=False,
        )

    class Output(BlockSchema):
        shipping_cost: float = SchemaField(description=""Estimated shipping cost"")
        currency_code: str = SchemaField(description=""Currency code (e.g., 'usd')"")
        error: str = SchemaField(description=""Error message if estimation failed"")

    def __init__(self):
        super().__init__(
            id=""00aae2a1-caf6-4a74-8175-39a0615d44e1"",
            description=""Get shipping cost estimate"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""order_number"": ""TEST-001"",
                ""customer"": {
                    ""name"": ""John Doe"",
                    ""email"": ""john@example.com"",
                    ""phone"": ""123-456-7890"",
                    ""address"": ""123 Test St"",
                    ""city"": ""Test City"",
                    ""state"": ""TS"",
                    ""zip"": ""12345"",
                },
                ""items"": [
                    {
                        ""file_url"": ""https://example.com/model.stl"",
                        ""quantity"": ""1"",
                        ""color"": ""black"",
                        ""profile"": ""PLA"",
                    }
                ],
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""shipping_cost"", 4.81), (""currency_code"", ""usd"")],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""shippingCost"": 4.81,
                    ""currencyCode"": ""usd"",
                },
                ""_convert_to_color"": lambda *args, **kwargs: ""black"",
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            order_data = self._format_order_data(
                input_data.customer,
                input_data.order_number,
                input_data.items,
                credentials.api_key.get_secret_value(),
            )
            result = self._make_request(
                ""POST"",
                ""order/estimateShipping"",
                credentials.api_key.get_secret_value(),
                json=order_data,
            )
            yield ""shipping_cost"", result[""shippingCost""]
            yield ""currency_code"", result[""currencyCode""]
        except Exception as e:
            yield ""error"", str(e)
            raise


class Slant3DGetOrdersBlock(Slant3DBlockBase):
    """"""Block for retrieving all orders""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()

    class Output(BlockSchema):
        orders: List[str] = SchemaField(description=""List of orders with their details"")
        error: str = SchemaField(description=""Error message if request failed"")

    def __init__(self):
        super().__init__(
            id=""42283bf5-8a32-4fb4-92a2-60a9ea48e105"",
            description=""Get all orders for the account"",
            input_schema=self.Input,
            output_schema=self.Output,
            # This block is disabled for cloud hosted because it allows access to all orders for the account
            disabled=settings.Settings().config.behave_as == BehaveAs.CLOUD,
            test_input={""credentials"": TEST_CREDENTIALS_INPUT},
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""orders"",
                    [
                        ""1234567890"",
                    ],
                )
            ],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""ordersData"": [
                        {
                            ""orderId"": 1234567890,
                            ""orderTimestamp"": {
                                ""_seconds"": 1719510986,
                                ""_nanoseconds"": 710000000,
                            },
                        }
                    ]
                }
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            result = self._make_request(
                ""GET"", ""order"", credentials.api_key.get_secret_value()
            )
            yield ""orders"", [str(order[""orderId""]) for order in result[""ordersData""]]
        except Exception as e:
            yield ""error"", str(e)
            raise


class Slant3DTrackingBlock(Slant3DBlockBase):
    """"""Block for tracking order status and shipping""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        order_id: str = SchemaField(description=""Slant3D order ID to track"")

    class Output(BlockSchema):
        status: str = SchemaField(description=""Order status"")
        tracking_numbers: List[str] = SchemaField(
            description=""List of tracking numbers""
        )
        error: str = SchemaField(description=""Error message if tracking failed"")

    def __init__(self):
        super().__init__(
            id=""dd7c0293-c5af-4551-ba3e-fc162fb1fb89"",
            description=""Track order status and shipping"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""order_id"": ""314144241"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""awaiting_shipment""), (""tracking_numbers"", [])],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""status"": ""awaiting_shipment"",
                    ""trackingNumbers"": [],
                }
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            result = self._make_request(
                ""GET"",
                f""order/{input_data.order_id}/get-tracking"",
                credentials.api_key.get_secret_value(),
            )
            yield ""status"", result[""status""]
            yield ""tracking_numbers"", result[""trackingNumbers""]
        except Exception as e:
            yield ""error"", str(e)
            raise


class Slant3DCancelOrderBlock(Slant3DBlockBase):
    """"""Block for canceling orders""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        order_id: str = SchemaField(description=""Slant3D order ID to cancel"")

    class Output(BlockSchema):
        status: str = SchemaField(description=""Cancellation status message"")
        error: str = SchemaField(description=""Error message if cancellation failed"")

    def __init__(self):
        super().__init__(
            id=""54de35e1-407f-450b-b5fa-3b5e2eba8185"",
            description=""Cancel an existing order"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""order_id"": ""314144241"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Order cancelled"")],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {""status"": ""Order cancelled""}
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            result = self._make_request(
                ""DELETE"",
                f""order/{input_data.order_id}"",
                credentials.api_key.get_secret_value(),
            )
            yield ""status"", result[""status""]
        except Exception as e:
            yield ""error"", str(e)
            raise
","import uuid
from typing import List

import requests as baserequests

from backend.data.block import BlockOutput, BlockSchema
from backend.data.model import APIKeyCredentials, SchemaField
from backend.util import settings
from backend.util.settings import BehaveAs

from ._api import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    CustomerDetails,
    OrderItem,
    Slant3DCredentialsField,
    Slant3DCredentialsInput,
)

from .base import Slant3DBlockBase


class Slant3DCreateOrderBlock(Slant3DBlockBase):
    """"""Block for creating new orders""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        order_number: str = SchemaField(
            description=""Your custom order number (or leave blank for a random one)"",
            default_factory=lambda: str(uuid.uuid4()),
        )
        customer: CustomerDetails = SchemaField(
            description=""Customer details for where to ship the item"",
            advanced=False,
        )
        items: List[OrderItem] = SchemaField(
            description=""List of items to print"",
            advanced=False,
        )

    class Output(BlockSchema):
        order_id: str = SchemaField(description=""Slant3D order ID"")
        error: str = SchemaField(description=""Error message if order failed"")

    def __init__(self):
        super().__init__(
            id=""f73007d6-f48f-4aaf-9e6b-6883998a09b4"",
            description=""Create a new print order"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""order_number"": ""TEST-001"",
                ""customer"": {
                    ""name"": ""John Doe"",
                    ""email"": ""john@example.com"",
                    ""phone"": ""123-456-7890"",
                    ""address"": ""123 Test St"",
                    ""city"": ""Test City"",
                    ""state"": ""TS"",
                    ""zip"": ""12345"",
                },
                ""items"": [
                    {
                        ""file_url"": ""https://example.com/model.stl"",
                        ""quantity"": ""1"",
                        ""color"": ""black"",
                        ""profile"": ""PLA"",
                    }
                ],
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""order_id"", ""314144241"")],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {""orderId"": ""314144241""},
                ""_convert_to_color"": lambda *args, **kwargs: ""black"",
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            order_data = self._format_order_data(
                input_data.customer,
                input_data.order_number,
                input_data.items,
                credentials.api_key.get_secret_value(),
            )
            result = self._make_request(
                ""POST"", ""order"", credentials.api_key.get_secret_value(), json=order_data
            )
            yield ""order_id"", result[""orderId""]
        except Exception as e:
            yield ""error"", str(e)
            raise


class Slant3DEstimateOrderBlock(Slant3DBlockBase):
    """"""Block for getting order cost estimates""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        order_number: str = SchemaField(
            description=""Your custom order number (or leave blank for a random one)"",
            default_factory=lambda: str(uuid.uuid4()),
        )
        customer: CustomerDetails = SchemaField(
            description=""Customer details for where to ship the item"",
            advanced=False,
        )
        items: List[OrderItem] = SchemaField(
            description=""List of items to print"",
            advanced=False,
        )

    class Output(BlockSchema):
        total_price: float = SchemaField(description=""Total price in USD"")
        shipping_cost: float = SchemaField(description=""Shipping cost"")
        printing_cost: float = SchemaField(description=""Printing cost"")
        error: str = SchemaField(description=""Error message if estimation failed"")

    def __init__(self):
        super().__init__(
            id=""bf8823d6-b42a-48c7-b558-d7c117f2ae85"",
            description=""Get order cost estimate"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""order_number"": ""TEST-001"",
                ""customer"": {
                    ""name"": ""John Doe"",
                    ""email"": ""john@example.com"",
                    ""phone"": ""123-456-7890"",
                    ""address"": ""123 Test St"",
                    ""city"": ""Test City"",
                    ""state"": ""TS"",
                    ""zip"": ""12345"",
                },
                ""items"": [
                    {
                        ""file_url"": ""https://example.com/model.stl"",
                        ""quantity"": ""1"",
                        ""color"": ""black"",
                        ""profile"": ""PLA"",
                    }
                ],
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""total_price"", 9.31),
                (""shipping_cost"", 5.56),
                (""printing_cost"", 3.75),
            ],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""totalPrice"": 9.31,
                    ""shippingCost"": 5.56,
                    ""printingCost"": 3.75,
                },
                ""_convert_to_color"": lambda *args, **kwargs: ""black"",
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        order_data = self._format_order_data(
            input_data.customer,
            input_data.order_number,
            input_data.items,
            credentials.api_key.get_secret_value(),
        )
        try:
            result = self._make_request(
                ""POST"",
                ""order/estimate"",
                credentials.api_key.get_secret_value(),
                json=order_data,
            )
            yield ""total_price"", result[""totalPrice""]
            yield ""shipping_cost"", result[""shippingCost""]
            yield ""printing_cost"", result[""printingCost""]
        except baserequests.HTTPError as e:
            yield ""error"", str(f""Error estimating order: {e} {e.response.text}"")
            raise


class Slant3DEstimateShippingBlock(Slant3DBlockBase):
    """"""Block for getting shipping cost estimates""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        order_number: str = SchemaField(
            description=""Your custom order number (or leave blank for a random one)"",
            default_factory=lambda: str(uuid.uuid4()),
        )
        customer: CustomerDetails = SchemaField(
            description=""Customer details for where to ship the item""
        )
        items: List[OrderItem] = SchemaField(
            description=""List of items to print"",
            advanced=False,
        )

    class Output(BlockSchema):
        shipping_cost: float = SchemaField(description=""Estimated shipping cost"")
        currency_code: str = SchemaField(description=""Currency code (e.g., 'usd')"")
        error: str = SchemaField(description=""Error message if estimation failed"")

    def __init__(self):
        super().__init__(
            id=""00aae2a1-caf6-4a74-8175-39a0615d44e1"",
            description=""Get shipping cost estimate"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""order_number"": ""TEST-001"",
                ""customer"": {
                    ""name"": ""John Doe"",
                    ""email"": ""john@example.com"",
                    ""phone"": ""123-456-7890"",
                    ""address"": ""123 Test St"",
                    ""city"": ""Test City"",
                    ""state"": ""TS"",
                    ""zip"": ""12345"",
                },
                ""items"": [
                    {
                        ""file_url"": ""https://example.com/model.stl"",
                        ""quantity"": ""1"",
                        ""color"": ""black"",
                        ""profile"": ""PLA"",
                    }
                ],
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""shipping_cost"", 4.81), (""currency_code"", ""usd"")],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""shippingCost"": 4.81,
                    ""currencyCode"": ""usd"",
                },
                ""_convert_to_color"": lambda *args, **kwargs: ""black"",
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            order_data = self._format_order_data(
                input_data.customer,
                input_data.order_number,
                input_data.items,
                credentials.api_key.get_secret_value(),
            )
            result = self._make_request(
                ""POST"",
                ""order/estimateShipping"",
                credentials.api_key.get_secret_value(),
                json=order_data,
            )
            yield ""shipping_cost"", result[""shippingCost""]
            yield ""currency_code"", result[""currencyCode""]
        except Exception as e:
            yield ""error"", str(e)
            raise


class Slant3DGetOrdersBlock(Slant3DBlockBase):
    """"""Block for retrieving all orders""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()

    class Output(BlockSchema):
        orders: List[str] = SchemaField(description=""List of orders with their details"")
        error: str = SchemaField(description=""Error message if request failed"")

    def __init__(self):
        super().__init__(
            id=""42283bf5-8a32-4fb4-92a2-60a9ea48e105"",
            description=""Get all orders for the account"",
            input_schema=self.Input,
            output_schema=self.Output,
            # This block is disabled for cloud hosted because it allows access to all orders for the account
            disabled=settings.Settings().config.behave_as == BehaveAs.CLOUD,
            test_input={""credentials"": TEST_CREDENTIALS_INPUT},
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""orders"",
                    [
                        ""1234567890"",
                    ],
                )
            ],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""ordersData"": [
                        {
                            ""orderId"": 1234567890,
                            ""orderTimestamp"": {
                                ""_seconds"": 1719510986,
                                ""_nanoseconds"": 710000000,
                            },
                        }
                    ]
                }
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            result = self._make_request(
                ""GET"", ""order"", credentials.api_key.get_secret_value()
            )
            yield ""orders"", [str(order[""orderId""]) for order in result[""ordersData""]]
        except Exception as e:
            yield ""error"", str(e)
            raise


class Slant3DTrackingBlock(Slant3DBlockBase):
    """"""Block for tracking order status and shipping""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        order_id: str = SchemaField(description=""Slant3D order ID to track"")

    class Output(BlockSchema):
        status: str = SchemaField(description=""Order status"")
        tracking_numbers: List[str] = SchemaField(
            description=""List of tracking numbers""
        )
        error: str = SchemaField(description=""Error message if tracking failed"")

    def __init__(self):
        super().__init__(
            id=""dd7c0293-c5af-4551-ba3e-fc162fb1fb89"",
            description=""Track order status and shipping"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""order_id"": ""314144241"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""awaiting_shipment""), (""tracking_numbers"", [])],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""status"": ""awaiting_shipment"",
                    ""trackingNumbers"": [],
                }
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            result = self._make_request(
                ""GET"",
                f""order/{input_data.order_id}/get-tracking"",
                credentials.api_key.get_secret_value(),
            )
            yield ""status"", result[""status""]
            yield ""tracking_numbers"", result[""trackingNumbers""]
        except Exception as e:
            yield ""error"", str(e)
            raise


class Slant3DCancelOrderBlock(Slant3DBlockBase):
    """"""Block for canceling orders""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        order_id: str = SchemaField(description=""Slant3D order ID to cancel"")

    class Output(BlockSchema):
        status: str = SchemaField(description=""Cancellation status message"")
        error: str = SchemaField(description=""Error message if cancellation failed"")

    def __init__(self):
        super().__init__(
            id=""54de35e1-407f-450b-b5fa-3b5e2eba8185"",
            description=""Cancel an existing order"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""order_id"": ""314144241"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Order cancelled"")],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {""status"": ""Order cancelled""}
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            result = self._make_request(
                ""DELETE"",
                f""order/{input_data.order_id}"",
                credentials.api_key.get_secret_value(),
            )
            yield ""status"", result[""status""]
        except Exception as e:
            yield ""error"", str(e)
            raise","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
124,"import hashlib
import hmac
import logging

import requests
from fastapi import HTTPException, Request
from strenum import StrEnum

from backend.data import integrations
from backend.data.model import Credentials
from backend.integrations.providers import ProviderName

from ._base import BaseWebhooksManager

logger = logging.getLogger(__name__)


# --8<-- [start:GithubWebhooksManager]
class GithubWebhookType(StrEnum):
    REPO = ""repo""


class GithubWebhooksManager(BaseWebhooksManager):
    PROVIDER_NAME = ProviderName.GITHUB

    WebhookType = GithubWebhookType

    GITHUB_API_URL = ""https://api.github.com""
    GITHUB_API_DEFAULT_HEADERS = {""Accept"": ""application/vnd.github.v3+json""}

    @classmethod
    async def validate_payload(
        cls, webhook: integrations.Webhook, request: Request
    ) -> tuple[dict, str]:
        if not (event_type := request.headers.get(""X-GitHub-Event"")):
            raise HTTPException(
                status_code=400, detail=""X-GitHub-Event header is missing!""
            )

        if not (signature_header := request.headers.get(""X-Hub-Signature-256"")):
            raise HTTPException(
                status_code=403, detail=""X-Hub-Signature-256 header is missing!""
            )

        payload_body = await request.body()
        hash_object = hmac.new(
            webhook.secret.encode(""utf-8""), msg=payload_body, digestmod=hashlib.sha256
        )
        expected_signature = ""sha256="" + hash_object.hexdigest()

        if not hmac.compare_digest(expected_signature, signature_header):
            raise HTTPException(
                status_code=403, detail=""Request signatures didn't match!""
            )

        payload = await request.json()
        if action := payload.get(""action""):
            event_type += f"".{action}""

        return payload, event_type

    async def trigger_ping(
        self, webhook: integrations.Webhook, credentials: Credentials | None
    ) -> None:
        if not credentials:
            raise ValueError(""Credentials are required but were not passed"")

        headers = {
            **self.GITHUB_API_DEFAULT_HEADERS,
            ""Authorization"": credentials.bearer(),
        }

        repo, github_hook_id = webhook.resource, webhook.provider_webhook_id
        ping_url = f""{self.GITHUB_API_URL}/repos/{repo}/hooks/{github_hook_id}/pings""

        response = requests.post(ping_url, headers=headers)

        if response.status_code != 204:
            error_msg = extract_github_error_msg(response)
            raise ValueError(f""Failed to ping GitHub webhook: {error_msg}"")

    async def _register_webhook(
        self,
        credentials: Credentials,
        webhook_type: GithubWebhookType,
        resource: str,
        events: list[str],
        ingress_url: str,
        secret: str,
    ) -> tuple[str, dict]:
        if webhook_type == self.WebhookType.REPO and resource.count(""/"") > 1:
            raise ValueError(""Invalid repo format: expected 'owner/repo'"")

        # Extract main event, e.g. `pull_request.opened` -> `pull_request`
        github_events = list({event.split(""."")[0] for event in events})

        headers = {
            **self.GITHUB_API_DEFAULT_HEADERS,
            ""Authorization"": credentials.bearer(),
        }
        webhook_data = {
            ""name"": ""web"",
            ""active"": True,
            ""events"": github_events,
            ""config"": {
                ""url"": ingress_url,
                ""content_type"": ""json"",
                ""insecure_ssl"": ""0"",
                ""secret"": secret,
            },
        }

        response = requests.post(
            f""{self.GITHUB_API_URL}/repos/{resource}/hooks"",
            headers=headers,
            json=webhook_data,
        )

        if response.status_code != 201:
            error_msg = extract_github_error_msg(response)
            if ""not found"" in error_msg.lower():
                error_msg = (
                    f""{error_msg} ""
                    ""(Make sure the GitHub account or API key has 'repo' or ""
                    f""webhook create permissions to '{resource}')""
                )
            raise ValueError(f""Failed to create GitHub webhook: {error_msg}"")

        webhook_id = response.json()[""id""]
        config = response.json()[""config""]

        return str(webhook_id), config

    async def _deregister_webhook(
        self, webhook: integrations.Webhook, credentials: Credentials
    ) -> None:
        webhook_type = self.WebhookType(webhook.webhook_type)
        if webhook.credentials_id != credentials.id:
            raise ValueError(
                f""Webhook #{webhook.id} does not belong to credentials {credentials.id}""
            )

        headers = {
            **self.GITHUB_API_DEFAULT_HEADERS,
            ""Authorization"": credentials.bearer(),
        }

        if webhook_type == self.WebhookType.REPO:
            repo = webhook.resource
            delete_url = f""{self.GITHUB_API_URL}/repos/{repo}/hooks/{webhook.provider_webhook_id}""  # noqa
        else:
            raise NotImplementedError(
                f""Unsupported webhook type '{webhook.webhook_type}'""
            )

        response = requests.delete(delete_url, headers=headers)

        if response.status_code not in [204, 404]:
            # 204 means successful deletion, 404 means the webhook was already deleted
            error_msg = extract_github_error_msg(response)
            raise ValueError(f""Failed to delete GitHub webhook: {error_msg}"")

        # If we reach here, the webhook was successfully deleted or didn't exist


# --8<-- [end:GithubWebhooksManager]


def extract_github_error_msg(response: requests.Response) -> str:
    error_msgs = []
    resp = response.json()
    if resp.get(""message""):
        error_msgs.append(resp[""message""])
    if resp.get(""errors""):
        error_msgs.extend(f""* {err.get('message', err)}"" for err in resp[""errors""])
    if resp.get(""error""):
        if isinstance(resp[""error""], dict):
            error_msgs.append(resp[""error""].get(""message"", resp[""error""]))
        else:
            error_msgs.append(resp[""error""])
    return ""\n"".join(error_msgs)
","import hashlib
import hmac
import logging

import requests
from fastapi import HTTPException, Request
from strenum import StrEnum

from backend.data import integrations
from backend.data.model import Credentials
from backend.integrations.providers import ProviderName

from ._base import BaseWebhooksManager

logger = logging.getLogger(__name__)

class GithubWebhookType(StrEnum):
    REPO = ""repo""

class GithubWebhooksManager(BaseWebhooksManager):
    PROVIDER_NAME = ProviderName.GITHUB

    WebhookType = GithubWebhookType

    GITHUB_API_URL = ""https://api.github.com""
    GITHUB_API_DEFAULT_HEADERS = {""Accept"": ""application/vnd.github.v3+json""}

    @classmethod
    async def validate_payload(
        cls, webhook: integrations.Webhook, request: Request
    ) -> tuple[dict, str]:
        if not (event_type := request.headers.get(""X-GitHub-Event"")):
            raise HTTPException(
                status_code=400, detail=""X-GitHub-Event header is missing!""
            )
        if not (signature_header := request.headers.get(""X-Hub-Signature-256"")):
            raise HTTPException(
                status_code=403, detail=""X-Hub-Signature-256 header is missing!""
            )
            
        payload_body = await request.body()
        hash_object = hmac.new(
            webhook.secret.encode(""utf-8""), msg=payload_body, digestmod=hashlib.sha256
        )
        expected_signature = ""sha256="" + hash_object.hexdigest()
        
        if not hmac.compare_digest(expected_signature, signature_header):
            raise HTTPException(
                status_code=403, detail=""Request signatures didn't match!""
            )
        
        payload = await request.json()
        if action := payload.get(""action""):
            event_type += f"".{action}""
        
        return payload, event_type
    
    async def trigger_ping(
        self, webhook: integrations.Webhook, credentials: Credentials | None
    ) -> None:
        if not credentials:
            raise ValueError(""Credentials are required but were not passed"")
        
        headers = {
            **self.GITHUB_API_DEFAULT_HEADERS,
            ""Authorization"": credentials.bearer(),
        }
        
        repo, github_hook_id = webhook.resource, webhook.provider_webhook_id
        ping_url = f""{self.GITHUB_API_URL}/repos/{repo}/hooks/{github_hook_id}/pings""
        
        response = requests.post(ping_url, headers=headers)
        
        if response.status_code != 204:
            error_msg = extract_github_error_msg(response)
            raise ValueError(f""Failed to ping GitHub webhook: {error_msg}"")

    async def _register_webhook(
        self,
        credentials: Credentials,
        webhook_type: GithubWebhookType,
        resource: str,
        events: list[str],
        ingress_url: str,
        secret: str,
    ) -> tuple[str, dict]:
        if webhook_type == self.WebhookType.REPO and resource.count(""/"") > 1:
            raise ValueError(""Invalid repo format: expected 'owner/repo'"")
        
        # Extract main event, e.g. `pull_request.opened` -> `pull_request`
        github_events = list({event.split(""."")[0] for event in events})
        
        headers = {
            **self.GITHUB_API_DEFAULT_HEADERS,
            ""Authorization"": credentials.bearer(),
        }
        
        webhook_data = {
            ""name"": ""web"",
            ""active"": True,
            ""events"": github_events,
            ""config"": {
                ""url"": ingress_url,
                ""content_type"": ""json"",
                ""insecure_ssl"": ""0"",
                ""secret"": secret,
            },
        }
        
        response = requests.post(
            f""{self.GITHUB_API_URL}/repos/{resource}/hooks"",
            headers=headers,
            json=webhook_data,
        )
        
        if response.status_code != 201:
            error_msg = extract_github_error_msg(response)
            if ""not found"" in error_msg.lower():
                error_msg = (
                    f""{error_msg} ""
                    ""(Make sure the GitHub account or API key has 'repo' or ""
                    f""webhook create permissions to '{resource}')""
                )
            raise ValueError(f""Failed to create GitHub webhook: {error_msg}"")
        
        webhook_id = response.json()[""id""]
        config = response.json()[""config""]
        
        return str(webhook_id), config
    
    async def _deregister_webhook(
        self, webhook: integrations.Webhook, credentials: Credentials
    ) -> None:
        webhook_type = self.WebhookType(webhook.webhook_type)
        
        if webhook.credentials_id != credentials.id:
            raise ValueError(
                f""Webhook #{webhook.id} does not belong to credentials {credentials.id}""
            )
        
        headers = {
            **self.GITHUB_API_DEFAULT_HEADERS,
            ""Authorization"": credentials.bearer(),
        }
        
        if webhook_type == self.WebhookType.REPO:
            repo = webhook.resource
            delete_url = f""{self.GITHUB_API_URL}/repos/{repo}/hooks/{webhook.provider_webhook_id}""  # noqa
        else:
            raise NotImplementedError(
                f""Unsupported webhook type '{webhook.webhook_type}'""
            )
        
        response = requests.delete(delete_url, headers=headers)
        
        if response.status_code not in [204, 404]:
            # 204 means successful deletion, 404 means the webhook was already deleted
            error_msg = extract_github_error_msg(response)
            raise ValueError(f""Failed to delete GitHub webhook: {error_msg}"")

def extract_github_error_msg(response: requests.Response) -> str:
    error_msgs = []
    resp = response.json()
    
    if resp.get(""message""):
        error_msgs.append(resp[""message""])
    
    if resp.get(""errors""):
        error_msgs.extend(f""* {err.get('message', err)}"" for err in resp[""errors""])
    
    if resp.get(""error""):
        if isinstance(resp[""error""], dict):
            error_msgs.append(resp[""error""].get(""message"", resp[""error""]))
        else:
            error_msgs.append(resp[""error""])
    
    return ""\n"".join(error_msgs)
","{'final_score': 98.27, 'length_similarity': 97.42, 'token_similarity': 96.91, 'structure_similarity': 97.9, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
125,"import logging
import time
from enum import Enum
from typing import Any

import httpx

from backend.blocks.fal._auth import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    FalCredentials,
    FalCredentialsField,
    FalCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

logger = logging.getLogger(__name__)


class FalModel(str, Enum):
    MOCHI = ""fal-ai/mochi-v1""
    LUMA = ""fal-ai/luma-dream-machine""


class AIVideoGeneratorBlock(Block):
    class Input(BlockSchema):
        prompt: str = SchemaField(
            description=""Description of the video to generate."",
            placeholder=""A dog running in a field."",
        )
        model: FalModel = SchemaField(
            title=""FAL Model"",
            default=FalModel.MOCHI,
            description=""The FAL model to use for video generation."",
        )
        credentials: FalCredentialsInput = FalCredentialsField()

    class Output(BlockSchema):
        video_url: str = SchemaField(description=""The URL of the generated video."")
        error: str = SchemaField(
            description=""Error message if video generation failed.""
        )
        logs: list[str] = SchemaField(
            description=""Generation progress logs."", optional=True
        )

    def __init__(self):
        super().__init__(
            id=""530cf046-2ce0-4854-ae2c-659db17c7a46"",
            description=""Generate videos using FAL AI models."",
            categories={BlockCategory.AI},
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""prompt"": ""A dog running in a field."",
                ""model"": FalModel.MOCHI,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""video_url"", ""https://fal.media/files/example/video.mp4"")],
            test_mock={
                ""generate_video"": lambda *args, **kwargs: ""https://fal.media/files/example/video.mp4""
            },
        )

    def _get_headers(self, api_key: str) -> dict[str, str]:
        """"""Get headers for FAL API requests.""""""
        return {
            ""Authorization"": f""Key {api_key}"",
            ""Content-Type"": ""application/json"",
        }

    def _submit_request(
        self, url: str, headers: dict[str, str], data: dict[str, Any]
    ) -> dict[str, Any]:
        """"""Submit a request to the FAL API.""""""
        try:
            response = httpx.post(url, headers=headers, json=data)
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            logger.error(f""FAL API request failed: {str(e)}"")
            raise RuntimeError(f""Failed to submit request: {str(e)}"")

    def _poll_status(self, status_url: str, headers: dict[str, str]) -> dict[str, Any]:
        """"""Poll the status endpoint until completion or failure.""""""
        try:
            response = httpx.get(status_url, headers=headers)
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            logger.error(f""Failed to get status: {str(e)}"")
            raise RuntimeError(f""Failed to get status: {str(e)}"")

    def generate_video(self, input_data: Input, credentials: FalCredentials) -> str:
        """"""Generate video using the specified FAL model.""""""
        base_url = ""https://queue.fal.run""
        api_key = credentials.api_key.get_secret_value()
        headers = self._get_headers(api_key)

        # Submit generation request
        submit_url = f""{base_url}/{input_data.model.value}""
        submit_data = {""prompt"": input_data.prompt}

        seen_logs = set()

        try:
            # Submit request to queue
            submit_response = httpx.post(submit_url, headers=headers, json=submit_data)
            submit_response.raise_for_status()
            request_data = submit_response.json()

            # Get request_id and urls from initial response
            request_id = request_data.get(""request_id"")
            status_url = request_data.get(""status_url"")
            result_url = request_data.get(""response_url"")

            if not all([request_id, status_url, result_url]):
                raise ValueError(""Missing required data in submission response"")

            # Poll for status with exponential backoff
            max_attempts = 30
            attempt = 0
            base_wait_time = 5

            while attempt < max_attempts:
                status_response = httpx.get(f""{status_url}?logs=1"", headers=headers)
                status_response.raise_for_status()
                status_data = status_response.json()

                # Process new logs only
                logs = status_data.get(""logs"", [])
                if logs and isinstance(logs, list):
                    for log in logs:
                        if isinstance(log, dict):
                            # Create a unique key for this log entry
                            log_key = (
                                f""{log.get('timestamp', '')}-{log.get('message', '')}""
                            )
                            if log_key not in seen_logs:
                                seen_logs.add(log_key)
                                message = log.get(""message"", """")
                                if message:
                                    logger.debug(
                                        f""[FAL Generation] [{log.get('level', 'INFO')}] [{log.get('source', '')}] [{log.get('timestamp', '')}] {message}""
                                    )

                status = status_data.get(""status"")
                if status == ""COMPLETED"":
                    # Get the final result
                    result_response = httpx.get(result_url, headers=headers)
                    result_response.raise_for_status()
                    result_data = result_response.json()

                    if ""video"" not in result_data or not isinstance(
                        result_data[""video""], dict
                    ):
                        raise ValueError(""Invalid response format - missing video data"")

                    video_url = result_data[""video""].get(""url"")
                    if not video_url:
                        raise ValueError(""No video URL in response"")

                    return video_url

                elif status == ""FAILED"":
                    error_msg = status_data.get(""error"", ""No error details provided"")
                    raise RuntimeError(f""Video generation failed: {error_msg}"")
                elif status == ""IN_QUEUE"":
                    position = status_data.get(""queue_position"", ""unknown"")
                    logger.debug(
                        f""[FAL Generation] Status: In queue, position: {position}""
                    )
                elif status == ""IN_PROGRESS"":
                    logger.debug(
                        ""[FAL Generation] Status: Request is being processed...""
                    )
                else:
                    logger.info(f""[FAL Generation] Status: Unknown status: {status}"")

                wait_time = min(base_wait_time * (2**attempt), 60)  # Cap at 60 seconds
                time.sleep(wait_time)
                attempt += 1

            raise RuntimeError(""Maximum polling attempts reached"")

        except httpx.HTTPError as e:
            raise RuntimeError(f""API request failed: {str(e)}"")

    def run(
        self, input_data: Input, *, credentials: FalCredentials, **kwargs
    ) -> BlockOutput:
        try:
            video_url = self.generate_video(input_data, credentials)
            yield ""video_url"", video_url
        except Exception as e:
            error_message = str(e)
            yield ""error"", error_message
","import logging
import time
from enum import Enum
from typing import Any
import httpx
from backend.blocks.fal._auth import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    FalCredentials,
    FalCredentialsField,
    FalCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

logger = logging.getLogger(__name__)

class FalModel(str, Enum):
    MOCHI = ""fal-ai/mochi-v1""
    LUMA = ""fal-ai/luma-dream-machine""

class AIVideoGeneratorBlock(Block):
    class Input(BlockSchema):
        prompt: str = SchemaField(
            description=""Description of the video to generate."",
            placeholder=""A dog running in a field."",
        )
        model: FalModel = SchemaField(
            title=""FAL Model"",
            default=FalModel.MOCHI,
            description=""The FAL model to use for video generation."",
        )
        credentials: FalCredentialsInput = FalCredentialsField()

    class Output(BlockSchema):
        video_url: str = SchemaField(description=""The URL of the generated video."")
        error: str = SchemaField(
            description=""Error message if video generation failed.""
        )
        logs: list[str] = SchemaField(
            description=""Generation progress logs."", optional=True
        )

    def __init__(self):
        super().__init__(
            id=""530cf046-2ce0-4854-ae2c-659db17c7a46"",
            description=""Generate videos using FAL AI models."",
            categories={BlockCategory.AI},
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={
                ""prompt"": ""A dog running in a field."",
                ""model"": FalModel.MOCHI,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""video_url"", ""https://fal.media/files/example/video.mp4"")],
            test_mock={
                ""generate_video"": lambda *args, **kwargs: ""https://fal.media/files/example/video.mp4""
            },
        )

    def _get_headers(self, api_key: str) -> dict[str, str]:
        """"""Get headers for FAL API requests.""""""
        return {
            ""Authorization"": f""Key {api_key}"",
            ""Content-Type"": ""application/json"",
        }

    def _submit_request(
        self, url: str, headers: dict[str, str], data: dict[str, Any]
    ) -> dict[str, Any]:
        """"""Submit a request to the FAL API.""""""
        try:
            response = httpx.post(url, headers=headers, json=data)
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            logger.error(f""FAL API request failed: {str(e)}"")
            raise RuntimeError(f""Failed to submit request: {str(e)}"")

    def _poll_status(self, status_url: str, headers: dict[str, str]) -> dict[str, Any]:
        """"""Poll the status endpoint until completion or failure.""""""
        try:
            response = httpx.get(status_url, headers=headers)
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            logger.error(f""Failed to get status: {str(e)}"")
            raise RuntimeError(f""Failed to get status: {str(e)}"")

    def generate_video(self, input_data: Input, credentials: FalCredentials) -> str:
        """"""Generate video using the specified FAL model.""""""
        base_url = ""https://queue.fal.run""
        api_key = credentials.api_key.get_secret_value()
        headers = self._get_headers(api_key)

        # Submit generation request
        submit_url = f""{base_url}/{input_data.model.value}""
        submit_data = {""prompt"": input_data.prompt}
        seen_logs = set()

        try:
            # Submit request to queue
            submit_response = httpx.post(submit_url, headers=headers, json=submit_data)
            submit_response.raise_for_status()
            request_data = submit_response.json()

            # Get request_id and urls from initial response
            request_id = request_data.get(""request_id"")
            status_url = request_data.get(""status_url"")
            result_url = request_data.get(""response_url"")

            if not all([request_id, status_url, result_url]):
                raise ValueError(""Missing required data in submission response"")

            # Poll for status with exponential backoff
            max_attempts = 30
            attempt = 0
            base_wait_time = 5

            while attempt < max_attempts:
                status_response = httpx.get(f""{status_url}?logs=1"", headers=headers)
                status_response.raise_for_status()
                status_data = status_response.json()

                # Process new logs only
                logs = status_data.get(""logs"", [])
                if logs and isinstance(logs, list):
                    for log in logs:
                        if isinstance(log, dict):
                            # Create a unique key for this log entry
                            log_key = (
                                f""{log.get('timestamp', '')}-{log.get('message', '')}""
                            )
                            if log_key not in seen_logs:
                                seen_logs.add(log_key)
                                message = log.get(""message"", """")
                                if message:
                                    logger.debug(
                                        f""[FAL Generation] [{log.get('level', 'INFO')}] [{log.get('source', '')}] [{log.get('timestamp', '')}] {message}""
                                    )

                status = status_data.get(""status"")
                if status == ""COMPLETED"":
                    # Get the final result
                    result_response = httpx.get(result_url, headers=headers)
                    result_response.raise_for_status()
                    result_data = result_response.json()

                    if ""video"" not in result_data or not isinstance(
                        result_data[""video""], dict
                    ):
                        raise ValueError(""Invalid response format - missing video data"")

                    video_url = result_data[""video""].get(""url"")
                    if not video_url:
                        raise ValueError(""No video URL in response"")

                    return video_url

                elif status == ""FAILED"":
                    error_msg = status_data.get(""error"", ""No error details provided"")
                    raise RuntimeError(f""Video generation failed: {error_msg}"")

                elif status == ""IN_QUEUE"":
                    position = status_data.get(""queue_position"", ""unknown"")
                    logger.debug(
                        f""[FAL Generation] Status: In queue, position: {position}""
                    )

                elif status == ""IN_PROGRESS"":
                    logger.debug(
                        ""[FAL Generation] Status: Request is being processed...""
                    )

                else:
                    logger.info(f""[FAL Generation] Status: Unknown status: {status}"")

                wait_time = min(base_wait_time * (2**attempt), 60)  # Cap at 60 seconds
                time.sleep(wait_time)
                attempt += 1

            raise RuntimeError(""Maximum polling attempts reached"")

        except httpx.HTTPError as e:
            raise RuntimeError(f""API request failed: {str(e)}"")

    def run(
        self, input_data: Input, *, credentials: FalCredentials, **kwargs
    ) -> BlockOutput:
        try:
            video_url = self.generate_video(input_data, credentials)
            yield ""video_url"", video_url
        except Exception as e:
            error_message = str(e)
            yield ""error"", error_message","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
126,"def find_previous_power_of_two(number: int) -> int:
    """"""
    Find the largest power of two that is less than or equal to a given integer.
    https://stackoverflow.com/questions/1322510

    >>> [find_previous_power_of_two(i) for i in range(18)]
    [0, 1, 2, 2, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 16, 16]
    >>> find_previous_power_of_two(-5)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    >>> find_previous_power_of_two(10.5)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    """"""
    if not isinstance(number, int) or number < 0:
        raise ValueError(""Input must be a non-negative integer"")
    if number == 0:
        return 0
    power = 1
    while power <= number:
        power <<= 1  # Equivalent to multiplying by 2
    return power >> 1 if number > 1 else 1


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","def find_previous_power_of_two(number: int) -> int:
    """"""
    Find the largest power of two that is less than or equal to a given integer.
    https://stackoverflow.com/questions/1322510

    >>> [find_previous_power_of_two(i) for i in range(18)]
    [0, 1, 2, 2, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 16, 16]
    >>> find_previous_power_of_two(-5)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    >>> find_previous_power_of_two(10.5)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    """"""
    if not isinstance(number, int) or number < 0:
        raise ValueError(""Input must be a non-negative integer"")
    if number == 0:
        return 0
    power = 1
    while power <= number:
        power <<= 1  # Equivalent to multiplying by 2
    return power >> 1 if number > 1 else 1

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
127,"from __future__ import annotations


def solve_maze(
    maze: list[list[int]],
    source_row: int,
    source_column: int,
    destination_row: int,
    destination_column: int,
) -> list[list[int]]:
    """"""
    This method solves the ""rat in maze"" problem.
    Parameters :
        - maze: A two dimensional matrix of zeros and ones.
        - source_row: The row index of the starting point.
        - source_column: The column index of the starting point.
        - destination_row: The row index of the destination point.
        - destination_column: The column index of the destination point.
    Returns:
        - solution: A 2D matrix representing the solution path if it exists.
    Raises:
        - ValueError: If no solution exists or if the source or
            destination coordinates are invalid.
    Description:
        This method navigates through a maze represented as an n by n matrix,
        starting from a specified source cell and
        aiming to reach a destination cell.
        The maze consists of walls (1s) and open paths (0s).
        By providing custom row and column values, the source and destination
        cells can be adjusted.
    >>> maze = [[0, 1, 0, 1, 1],
    ...         [0, 0, 0, 0, 0],
    ...         [1, 0, 1, 0, 1],
    ...         [0, 0, 1, 0, 0],
    ...         [1, 0, 0, 1, 0]]
    >>> solve_maze(maze,0,0,len(maze)-1,len(maze)-1)    # doctest: +NORMALIZE_WHITESPACE
    [[0, 1, 1, 1, 1],
    [0, 0, 0, 0, 1],
    [1, 1, 1, 0, 1],
    [1, 1, 1, 0, 0],
    [1, 1, 1, 1, 0]]

    Note:
        In the output maze, the zeros (0s) represent one of the possible
        paths from the source to the destination.

    >>> maze = [[0, 1, 0, 1, 1],
    ...         [0, 0, 0, 0, 0],
    ...         [0, 0, 0, 0, 1],
    ...         [0, 0, 0, 0, 0],
    ...         [0, 0, 0, 0, 0]]
    >>> solve_maze(maze,0,0,len(maze)-1,len(maze)-1)    # doctest: +NORMALIZE_WHITESPACE
    [[0, 1, 1, 1, 1],
    [0, 1, 1, 1, 1],
    [0, 1, 1, 1, 1],
    [0, 1, 1, 1, 1],
    [0, 0, 0, 0, 0]]

    >>> maze = [[0, 0, 0],
    ...         [0, 1, 0],
    ...         [1, 0, 0]]
    >>> solve_maze(maze,0,0,len(maze)-1,len(maze)-1)    # doctest: +NORMALIZE_WHITESPACE
    [[0, 0, 0],
    [1, 1, 0],
    [1, 1, 0]]

    >>> maze = [[1, 0, 0],
    ...         [0, 1, 0],
    ...         [1, 0, 0]]
    >>> solve_maze(maze,0,1,len(maze)-1,len(maze)-1)    # doctest: +NORMALIZE_WHITESPACE
    [[1, 0, 0],
    [1, 1, 0],
    [1, 1, 0]]

    >>> maze = [[1, 1, 0, 0, 1, 0, 0, 1],
    ...         [1, 0, 1, 0, 0, 1, 1, 1],
    ...         [0, 1, 0, 1, 0, 0, 1, 0],
    ...         [1, 1, 1, 0, 0, 1, 0, 1],
    ...         [0, 1, 0, 0, 1, 0, 1, 1],
    ...         [0, 0, 0, 1, 1, 1, 0, 1],
    ...         [0, 1, 0, 1, 0, 1, 1, 1],
    ...         [1, 1, 0, 0, 0, 0, 0, 1]]
    >>> solve_maze(maze,0,2,len(maze)-1,2)  # doctest: +NORMALIZE_WHITESPACE
    [[1, 1, 0, 0, 1, 1, 1, 1],
    [1, 1, 1, 0, 0, 1, 1, 1],
    [1, 1, 1, 1, 0, 1, 1, 1],
    [1, 1, 1, 0, 0, 1, 1, 1],
    [1, 1, 0, 0, 1, 1, 1, 1],
    [1, 1, 0, 1, 1, 1, 1, 1],
    [1, 1, 0, 1, 1, 1, 1, 1],
    [1, 1, 0, 1, 1, 1, 1, 1]]
    >>> maze = [[1, 0, 0],
    ...         [0, 1, 1],
    ...         [1, 0, 1]]
    >>> solve_maze(maze,0,1,len(maze)-1,len(maze)-1)
    Traceback (most recent call last):
        ...
    ValueError: No solution exists!

    >>> maze = [[0, 0],
    ...         [1, 1]]
    >>> solve_maze(maze,0,0,len(maze)-1,len(maze)-1)
    Traceback (most recent call last):
        ...
    ValueError: No solution exists!

    >>> maze = [[0, 1],
    ...         [1, 0]]
    >>> solve_maze(maze,2,0,len(maze)-1,len(maze)-1)
    Traceback (most recent call last):
        ...
    ValueError: Invalid source or destination coordinates

    >>> maze = [[1, 0, 0],
    ...         [0, 1, 0],
    ...         [1, 0, 0]]
    >>> solve_maze(maze,0,1,len(maze),len(maze)-1)
    Traceback (most recent call last):
        ...
    ValueError: Invalid source or destination coordinates
    """"""
    size = len(maze)
    # Check if source and destination coordinates are Invalid.
    if not (0 <= source_row <= size - 1 and 0 <= source_column <= size - 1) or (
        not (0 <= destination_row <= size - 1 and 0 <= destination_column <= size - 1)
    ):
        raise ValueError(""Invalid source or destination coordinates"")
    # We need to create solution object to save path.
    solutions = [[1 for _ in range(size)] for _ in range(size)]
    solved = run_maze(
        maze, source_row, source_column, destination_row, destination_column, solutions
    )
    if solved:
        return solutions
    else:
        raise ValueError(""No solution exists!"")


def run_maze(
    maze: list[list[int]],
    i: int,
    j: int,
    destination_row: int,
    destination_column: int,
    solutions: list[list[int]],
) -> bool:
    """"""
    This method is recursive starting from (i, j) and going in one of four directions:
    up, down, left, right.
    If a path is found to destination it returns True otherwise it returns False.
    Parameters
        maze: A two dimensional matrix of zeros and ones.
        i, j : coordinates of matrix
        solutions: A two dimensional matrix of solutions.
    Returns:
        Boolean if path is found True, Otherwise False.
    """"""
    size = len(maze)
    # Final check point.
    if i == destination_row and j == destination_column and maze[i][j] == 0:
        solutions[i][j] = 0
        return True

    lower_flag = (not i < 0) and (not j < 0)  # Check lower bounds
    upper_flag = (i < size) and (j < size)  # Check upper bounds

    if lower_flag and upper_flag:
        # check for already visited and block points.
        block_flag = (solutions[i][j]) and (not maze[i][j])
        if block_flag:
            # check visited
            solutions[i][j] = 0

            # check for directions
            if (
                run_maze(maze, i + 1, j, destination_row, destination_column, solutions)
                or run_maze(
                    maze, i, j + 1, destination_row, destination_column, solutions
                )
                or run_maze(
                    maze, i - 1, j, destination_row, destination_column, solutions
                )
                or run_maze(
                    maze, i, j - 1, destination_row, destination_column, solutions
                )
            ):
                return True

            solutions[i][j] = 1
            return False
    return False


if __name__ == ""__main__"":
    import doctest

    doctest.testmod(optionflags=doctest.NORMALIZE_WHITESPACE)
","from __future__ import annotations

def solve_maze(
    maze: list[list[int]],
    source_row: int,
    source_column: int,
    destination_row: int,
    destination_column: int,
) -> list[list[int]]:
    """"""
    This method solves the ""rat in maze"" problem.
    Parameters :
        - maze: A two dimensional matrix of zeros and ones.
        - source_row: The row index of the starting point.
        - source_column: The column index of the starting point.
        - destination_row: The row index of the destination point.
        - destination_column: The column index of the destination point.
    Returns:
        - solution: A 2D matrix representing the solution path if it exists.
    Raises:
        - ValueError: If no solution exists or if the source or
            destination coordinates are invalid.
    Description:
        This method navigates through a maze represented as an n by n matrix,
        starting from a specified source cell and
        aiming to reach a destination cell.
        The maze consists of walls (1s) and open paths (0s).
        By providing custom row and column values, the source and destination
        cells can be adjusted.
    >>> maze = [[0, 1, 0, 1, 1],
    ...         [0, 0, 0, 0, 0],
    ...         [1, 0, 1, 0, 1],
    ...         [0, 0, 1, 0, 0],
    ...         [1, 0, 0, 1, 0]]
    >>> solve_maze(maze,0,0,len(maze)-1,len(maze)-1)    # doctest: +NORMALIZE_WHITESPACE
    [[0, 1, 1, 1, 1],
    [0, 0, 0, 0, 1],
    [1, 1, 1, 0, 1],
    [1, 1, 1, 0, 0],
    [1, 1, 1, 1, 0]]

    Note:
        In the output maze, the zeros (0s) represent one of the possible
        paths from the source to the destination.

    >>> maze = [[0, 1, 0, 1, 1],
    ...         [0, 0, 0, 0, 0],
    ...         [0, 0, 0, 0, 1],
    ...         [0, 0, 0, 0, 0],
    ...         [0, 0, 0, 0, 0]]
    >>> solve_maze(maze,0,0,len(maze)-1,len(maze)-1)    # doctest: +NORMALIZE_WHITESPACE
    [[0, 1, 1, 1, 1],
    [0, 1, 1, 1, 1],
    [0, 1, 1, 1, 1],
    [0, 1, 1, 1, 1],
    [0, 0, 0, 0, 0]]

    >>> maze = [[0, 0, 0],
    ...         [0, 1, 0],
    ...         [1, 0, 0]]
    >>> solve_maze(maze,0,0,len(maze)-1,len(maze)-1)    # doctest: +NORMALIZE_WHITESPACE
    [[0, 0, 0],
    [1, 1, 0],
    [1, 1, 0]]

    >>> maze = [[1, 0, 0],
    ...         [0, 1, 0],
    ...         [1, 0, 0]]
    >>> solve_maze(maze,0,1,len(maze)-1,len(maze)-1)    # doctest: +NORMALIZE_WHITESPACE
    [[1, 0, 0],
    [1, 1, 0],
    [1, 1, 0]]

    >>> maze = [[1, 1, 0, 0, 1, 0, 0, 1],
    ...         [1, 0, 1, 0, 0, 1, 1, 1],
    ...         [0, 1, 0, 1, 0, 0, 1, 0],
    ...         [1, 1, 1, 0, 0, 1, 0, 1],
    ...         [0, 1, 0, 0, 1, 0, 1, 1],
    ...         [0, 0, 0, 1, 1, 1, 0, 1],
    ...         [0, 1, 0, 1, 0, 1, 1, 1],
    ...         [1, 1, 0, 0, 0, 0, 0, 1]]
    >>> solve_maze(maze,0,2,len(maze)-1,2)  # doctest: +NORMALIZE_WHITESPACE
    [[1, 1, 0, 0, 1, 1, 1, 1],
    [1, 1, 1, 0, 0, 1, 1, 1],
    [1, 1, 1, 1, 0, 1, 1, 1],
    [1, 1, 1, 0, 0, 1, 1, 1],
    [1, 1, 0, 0, 1, 1, 1, 1],
    [1, 1, 0, 1, 1, 1, 1, 1],
    [1, 1, 0, 1, 1, 1, 1, 1],
    [1, 1, 0, 1, 1, 1, 1, 1]]
    >>> maze = [[1, 0, 0],
    ...         [0, 1, 1],
    ...         [1, 0, 1]]
    >>> solve_maze(maze,0,1,len(maze)-1,len(maze)-1)
    Traceback (most recent call last):
        ...
    ValueError: No solution exists!

    >>> maze = [[0, 0],
    ...         [1, 1]]
    >>> solve_maze(maze,0,0,len(maze)-1,len(maze)-1)
    Traceback (most recent call last):
        ...
    ValueError: No solution exists!

    >>> maze = [[0, 1],
    ...         [1, 0]]
    >>> solve_maze(maze,2,0,len(maze)-1,len(maze)-1)
    Traceback (most recent call last):
        ...
    ValueError: Invalid source or destination coordinates

    >>> maze = [[1, 0, 0],
    ...         [0, 1, 0],
    ...         [1, 0, 0]]
    >>> solve_maze(maze,0,1,len(maze),len(maze)-1)
    Traceback (most recent call last):
        ...
    ValueError: Invalid source or destination coordinates
    """"""
    size = len(maze)

    # Check if source and destination coordinates are Invalid.
    if not (0 <= source_row <= size - 1 and 0 <= source_column <= size - 1) or (
        not (0 <= destination_row <= size - 1 and 0 <= destination_column <= size - 1)
    ):
        raise ValueError(""Invalid source or destination coordinates"")

    # We need to create solution object to save path.
    solutions = [[1 for _ in range(size)] for _ in range(size)]

    solved = run_maze(
        maze, source_row, source_column, destination_row, destination_column, solutions
    )

    if solved:
        return solutions
    else:
        raise ValueError(""No solution exists!"")

def run_maze(
    maze: list[list[int]],
    i: int,
    j: int,
    destination_row: int,
    destination_column: int,
    solutions: list[list[int]],
) -> bool:
    """"""
    This method is recursive starting from (i, j) and going in one of four directions:
    up, down, left, right.
    If a path is found to destination it returns True otherwise it returns False.
    Parameters
        maze: A two dimensional matrix of zeros and ones.
        i, j : coordinates of matrix
        solutions: A two dimensional matrix of solutions.
    Returns:
        Boolean if path is found True, Otherwise False.
    """"""
    size = len(maze)

    # Final check point.
    if i == destination_row and j == destination_column and maze[i][j] == 0:
        solutions[i][j] = 0
        return True

    lower_flag = (not i < 0) and (not j < 0)  # Check lower bounds
    upper_flag = (i < size) and (j < size)  # Check upper bounds

    if lower_flag and upper_flag:
        # check for already visited and block points.
        block_flag = (solutions[i][j]) and (not maze[i][j])
        if block_flag:
            # check visited
            solutions[i][j] = 0

            # check for directions
            if (
                run_maze(maze, i + 1, j, destination_row, destination_column, solutions)
                or run_maze(
                    maze, i, j + 1, destination_row, destination_column, solutions
                )
                or run_maze(
                    maze, i - 1, j, destination_row, destination_column, solutions
                )
                or run_maze(
                    maze, i, j - 1, destination_row, destination_column, solutions
                )
            ):
                return True

            solutions[i][j] = 1
            return False
    return False

if __name__ == ""__main__"":
    import doctest

    doctest.testmod(optionflags=doctest.NORMALIZE_WHITESPACE)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
128,"import logging
import time
from abc import ABC, abstractmethod
from typing import ClassVar

from backend.data.model import OAuth2Credentials
from backend.integrations.providers import ProviderName

logger = logging.getLogger(__name__)


class BaseOAuthHandler(ABC):
    # --8<-- [start:BaseOAuthHandler1]
    PROVIDER_NAME: ClassVar[ProviderName]
    DEFAULT_SCOPES: ClassVar[list[str]] = []
    # --8<-- [end:BaseOAuthHandler1]

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler2]
    def __init__(self, client_id: str, client_secret: str, redirect_uri: str): ...

    # --8<-- [end:BaseOAuthHandler2]

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler3]
    def get_login_url(self, scopes: list[str], state: str) -> str:
        # --8<-- [end:BaseOAuthHandler3]
        """"""Constructs a login URL that the user can be redirected to""""""
        ...

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler4]
    def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        # --8<-- [end:BaseOAuthHandler4]
        """"""Exchanges the acquired authorization code from login for a set of tokens""""""
        ...

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler5]
    def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # --8<-- [end:BaseOAuthHandler5]
        """"""Implements the token refresh mechanism""""""
        ...

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler6]
    def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        # --8<-- [end:BaseOAuthHandler6]
        """"""Revokes the given token at provider,
        returns False provider does not support it""""""
        ...

    def refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        if credentials.provider != self.PROVIDER_NAME:
            raise ValueError(
                f""{self.__class__.__name__} can not refresh tokens ""
                f""for other provider '{credentials.provider}'""
            )
        return self._refresh_tokens(credentials)

    def get_access_token(self, credentials: OAuth2Credentials) -> str:
        """"""Returns a valid access token, refreshing it first if needed""""""
        if self.needs_refresh(credentials):
            credentials = self.refresh_tokens(credentials)
        return credentials.access_token.get_secret_value()

    def needs_refresh(self, credentials: OAuth2Credentials) -> bool:
        """"""Indicates whether the given tokens need to be refreshed""""""
        return (
            credentials.access_token_expires_at is not None
            and credentials.access_token_expires_at < int(time.time()) + 300
        )

    def handle_default_scopes(self, scopes: list[str]) -> list[str]:
        """"""Handles the default scopes for the provider""""""
        # If scopes are empty, use the default scopes for the provider
        if not scopes:
            logger.debug(
                f""Using default scopes for provider {self.PROVIDER_NAME.value}""
            )
            scopes = self.DEFAULT_SCOPES
        return scopes
","import logging
import time
from abc import ABC, abstractmethod
from typing import ClassVar
from backend.data.model import OAuth2Credentials
from backend.integrations.providers import ProviderName

logger = logging.getLogger(__name__)

class BaseOAuthHandler(ABC):
    # --8<-- [start:BaseOAuthHandler1]
    PROVIDER_NAME: ClassVar[ProviderName]
    DEFAULT_SCOPES: ClassVar[list[str]] = []
    # --8<-- [end:BaseOAuthHandler1]

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler2]
    def __init__(self, client_id: str, client_secret: str, redirect_uri: str): ...
    # --8<-- [end:BaseOAuthHandler2]

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler3]
    def get_login_url(self, scopes: list[str], state: str) -> str:
        # --8<-- [end:BaseOAuthHandler3]
        """"""Constructs a login URL that the user can be redirected to""""""
        ...

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler4]
    def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        # --8<-- [end:BaseOAuthHandler4]
        """"""Exchanges the acquired authorization code from login for a set of tokens""""""
        ...

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler5]
    def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # --8<-- [end:BaseOAuthHandler5]
        """"""Implements the token refresh mechanism""""""
        ...

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler6]
    def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        # --8<-- [end:BaseOAuthHandler6]
        """"""Revokes the given token at provider,
        returns False provider does not support it""""""
        ...

    def refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        if credentials.provider != self.PROVIDER_NAME:
            raise ValueError(
                f""{self.__class__.__name__} can not refresh tokens ""
                f""for other provider '{credentials.provider}'""
            )
        return self._refresh_tokens(credentials)

    def get_access_token(self, credentials: OAuth2Credentials) -> str:
        """"""Returns a valid access token, refreshing it first if needed""""""
        if self.needs_refresh(credentials):
            credentials = self.refresh_tokens(credentials)
        return credentials.access_token.get_secret_value()

    def needs_refresh(self, credentials: OAuth2Credentials) -> bool:
        """"""Indicates whether the given tokens need to be refreshed""""""
        return (
            credentials.access_token_expires_at is not None
            and credentials.access_token_expires_at < int(time.time()) + 300
        )

    def handle_default_scopes(self, scopes: list[str]) -> list[str]:
        """"""Handles the default scopes for the provider""""""
        if not scopes:
            logger.debug(
                f""Using default scopes for provider {self.PROVIDER_NAME.value}""
            )
            scopes = self.DEFAULT_SCOPES
        return scopes","{'final_score': 97.62, 'length_similarity': 97.69, 'token_similarity': 97.52, 'structure_similarity': 98.57, 'function_similarity': 100, 'complexity_similarity': 90.0, 'variable_similarity': 100}",
129,"import datetime

import prisma.enums

import backend.server.v2.store.model


def test_pagination():
    pagination = backend.server.v2.store.model.Pagination(
        total_items=100, total_pages=5, current_page=2, page_size=20
    )
    assert pagination.total_items == 100
    assert pagination.total_pages == 5
    assert pagination.current_page == 2
    assert pagination.page_size == 20


def test_store_agent():
    agent = backend.server.v2.store.model.StoreAgent(
        slug=""test-agent"",
        agent_name=""Test Agent"",
        agent_image=""test.jpg"",
        creator=""creator1"",
        creator_avatar=""avatar.jpg"",
        sub_heading=""Test subheading"",
        description=""Test description"",
        runs=50,
        rating=4.5,
    )
    assert agent.slug == ""test-agent""
    assert agent.agent_name == ""Test Agent""
    assert agent.runs == 50
    assert agent.rating == 4.5


def test_store_agents_response():
    response = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""test-agent"",
                agent_name=""Test Agent"",
                agent_image=""test.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar.jpg"",
                sub_heading=""Test subheading"",
                description=""Test description"",
                runs=50,
                rating=4.5,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            total_items=1, total_pages=1, current_page=1, page_size=20
        ),
    )
    assert len(response.agents) == 1
    assert response.pagination.total_items == 1


def test_store_agent_details():
    details = backend.server.v2.store.model.StoreAgentDetails(
        store_listing_version_id=""version123"",
        slug=""test-agent"",
        agent_name=""Test Agent"",
        agent_video=""video.mp4"",
        agent_image=[""image1.jpg"", ""image2.jpg""],
        creator=""creator1"",
        creator_avatar=""avatar.jpg"",
        sub_heading=""Test subheading"",
        description=""Test description"",
        categories=[""cat1"", ""cat2""],
        runs=50,
        rating=4.5,
        versions=[""1.0"", ""2.0""],
        last_updated=datetime.datetime.now(),
    )
    assert details.slug == ""test-agent""
    assert len(details.agent_image) == 2
    assert len(details.categories) == 2
    assert len(details.versions) == 2


def test_creator():
    creator = backend.server.v2.store.model.Creator(
        agent_rating=4.8,
        agent_runs=1000,
        name=""Test Creator"",
        username=""creator1"",
        description=""Test description"",
        avatar_url=""avatar.jpg"",
        num_agents=5,
        is_featured=False,
    )
    assert creator.name == ""Test Creator""
    assert creator.num_agents == 5


def test_creators_response():
    response = backend.server.v2.store.model.CreatorsResponse(
        creators=[
            backend.server.v2.store.model.Creator(
                agent_rating=4.8,
                agent_runs=1000,
                name=""Test Creator"",
                username=""creator1"",
                description=""Test description"",
                avatar_url=""avatar.jpg"",
                num_agents=5,
                is_featured=False,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            total_items=1, total_pages=1, current_page=1, page_size=20
        ),
    )
    assert len(response.creators) == 1
    assert response.pagination.total_items == 1


def test_creator_details():
    details = backend.server.v2.store.model.CreatorDetails(
        name=""Test Creator"",
        username=""creator1"",
        description=""Test description"",
        links=[""link1.com"", ""link2.com""],
        avatar_url=""avatar.jpg"",
        agent_rating=4.8,
        agent_runs=1000,
        top_categories=[""cat1"", ""cat2""],
    )
    assert details.name == ""Test Creator""
    assert len(details.links) == 2
    assert details.agent_rating == 4.8
    assert len(details.top_categories) == 2


def test_store_submission():
    submission = backend.server.v2.store.model.StoreSubmission(
        agent_id=""agent123"",
        agent_version=1,
        sub_heading=""Test subheading"",
        name=""Test Agent"",
        slug=""test-agent"",
        description=""Test description"",
        image_urls=[""image1.jpg"", ""image2.jpg""],
        date_submitted=datetime.datetime(2023, 1, 1),
        status=prisma.enums.SubmissionStatus.PENDING,
        runs=50,
        rating=4.5,
    )
    assert submission.name == ""Test Agent""
    assert len(submission.image_urls) == 2
    assert submission.status == prisma.enums.SubmissionStatus.PENDING


def test_store_submissions_response():
    response = backend.server.v2.store.model.StoreSubmissionsResponse(
        submissions=[
            backend.server.v2.store.model.StoreSubmission(
                agent_id=""agent123"",
                agent_version=1,
                sub_heading=""Test subheading"",
                name=""Test Agent"",
                slug=""test-agent"",
                description=""Test description"",
                image_urls=[""image1.jpg""],
                date_submitted=datetime.datetime(2023, 1, 1),
                status=prisma.enums.SubmissionStatus.PENDING,
                runs=50,
                rating=4.5,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            total_items=1, total_pages=1, current_page=1, page_size=20
        ),
    )
    assert len(response.submissions) == 1
    assert response.pagination.total_items == 1


def test_store_submission_request():
    request = backend.server.v2.store.model.StoreSubmissionRequest(
        agent_id=""agent123"",
        agent_version=1,
        slug=""test-agent"",
        name=""Test Agent"",
        sub_heading=""Test subheading"",
        video_url=""video.mp4"",
        image_urls=[""image1.jpg"", ""image2.jpg""],
        description=""Test description"",
        categories=[""cat1"", ""cat2""],
    )
    assert request.agent_id == ""agent123""
    assert request.agent_version == 1
    assert len(request.image_urls) == 2
    assert len(request.categories) == 2
","import datetime
import prisma.enums
import backend.server.v2.store.model

def test_pagination():
    pagination = backend.server.v2.store.model.Pagination(
        total_items=100, total_pages=5, current_page=2, page_size=20
    )
    assert pagination.total_items == 100
    assert pagination.total_pages == 5
    assert pagination.current_page == 2
    assert pagination.page_size == 20

def test_store_agent():
    agent = backend.server.v2.store.model.StoreAgent(
        slug=""test-agent"",
        agent_name=""Test Agent"",
        agent_image=""test.jpg"",
        creator=""creator1"",
        creator_avatar=""avatar.jpg"",
        sub_heading=""Test subheading"",
        description=""Test description"",
        runs=50,
        rating=4.5,
    )
    assert agent.slug == ""test-agent""
    assert agent.agent_name == ""Test Agent""
    assert agent.runs == 50
    assert agent.rating == 4.5

def test_store_agents_response():
    response = backend.server.v2.store.model.StoreAgentsResponse(
        agents=[
            backend.server.v2.store.model.StoreAgent(
                slug=""test-agent"",
                agent_name=""Test Agent"",
                agent_image=""test.jpg"",
                creator=""creator1"",
                creator_avatar=""avatar.jpg"",
                sub_heading=""Test subheading"",
                description=""Test description"",
                runs=50,
                rating=4.5,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            total_items=1, total_pages=1, current_page=1, page_size=20
        ),
    )
    assert len(response.agents) == 1
    assert response.pagination.total_items == 1

def test_store_agent_details():
    details = backend.server.v2.store.model.StoreAgentDetails(
        store_listing_version_id=""version123"",
        slug=""test-agent"",
        agent_name=""Test Agent"",
        agent_video=""video.mp4"",
        agent_image=[""image1.jpg"", ""image2.jpg""],
        creator=""creator1"",
        creator_avatar=""avatar.jpg"",
        sub_heading=""Test subheading"",
        description=""Test description"",
        categories=[""cat1"", ""cat2""],
        runs=50,
        rating=4.5,
        versions=[""1.0"", ""2.0""],
        last_updated=datetime.datetime.now(),
    )
    assert details.slug == ""test-agent""
    assert len(details.agent_image) == 2
    assert len(details.categories) == 2
    assert len(details.versions) == 2

def test_creator():
    creator = backend.server.v2.store.model.Creator(
        agent_rating=4.8,
        agent_runs=1000,
        name=""Test Creator"",
        username=""creator1"",
        description=""Test description"",
        avatar_url=""avatar.jpg"",
        num_agents=5,
        is_featured=False,
    )
    assert creator.name == ""Test Creator""
    assert creator.num_agents == 5

def test_creators_response():
    response = backend.server.v2.store.model.CreatorsResponse(
        creators=[
            backend.server.v2.store.model.Creator(
                agent_rating=4.8,
                agent_runs=1000,
                name=""Test Creator"",
                username=""creator1"",
                description=""Test description"",
                avatar_url=""avatar.jpg"",
                num_agents=5,
                is_featured=False,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            total_items=1, total_pages=1, current_page=1, page_size=20
        ),
    )
    assert len(response.creators) == 1
    assert response.pagination.total_items == 1

def test_creator_details():
    details = backend.server.v2.store.model.CreatorDetails(
        name=""Test Creator"",
        username=""creator1"",
        description=""Test description"",
        links=[""link1.com"", ""link2.com""],
        avatar_url=""avatar.jpg"",
        agent_rating=4.8,
        agent_runs=1000,
        top_categories=[""cat1"", ""cat2""],
    )
    assert details.name == ""Test Creator""
    assert len(details.links) == 2
    assert details.agent_rating == 4.8
    assert len(details.top_categories) == 2

def test_store_submission():
    submission = backend.server.v2.store.model.StoreSubmission(
        agent_id=""agent123"",
        agent_version=1,
        sub_heading=""Test subheading"",
        name=""Test Agent"",
        slug=""test-agent"",
        description=""Test description"",
        image_urls=[""image1.jpg"", ""image2.jpg""],
        date_submitted=datetime.datetime(2023, 1, 1),
        status=prisma.enums.SubmissionStatus.PENDING,
        runs=50,
        rating=4.5,
    )
    assert submission.name == ""Test Agent""
    assert len(submission.image_urls) == 2
    assert submission.status == prisma.enums.SubmissionStatus.PENDING

def test_store_submissions_response():
    response = backend.server.v2.store.model.StoreSubmissionsResponse(
        submissions=[
            backend.server.v2.store.model.StoreSubmission(
                agent_id=""agent123"",
                agent_version=1,
                sub_heading=""Test subheading"",
                name=""Test Agent"",
                slug=""test-agent"",
                description=""Test description"",
                image_urls=[""image1.jpg""],
                date_submitted=datetime.datetime(2023, 1, 1),
                status=prisma.enums.SubmissionStatus.PENDING,
                runs=50,
                rating=4.5,
            )
        ],
        pagination=backend.server.v2.store.model.Pagination(
            total_items=1, total_pages=1, current_page=1, page_size=20
        ),
    )
    assert len(response.submissions) == 1
    assert response.pagination.total_items == 1

def test_store_submission_request():
    request = backend.server.v2.store.model.StoreSubmissionRequest(
        agent_id=""agent123"",
        agent_version=1,
        slug=""test-agent"",
        name=""Test Agent"",
        sub_heading=""Test subheading"",
        video_url=""video.mp4"",
        image_urls=[""image1.jpg"", ""image2.jpg""],
        description=""Test description"",
        categories=[""cat1"", ""cat2""],
    )
    assert request.agent_id == ""agent123""
    assert request.agent_version == 1
    assert len(request.image_urls) == 2
    assert len(request.categories) == 2","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
130,"from typing import List, Optional

from pydantic import BaseModel

from backend.blocks.exa._auth import (
    ExaCredentials,
    ExaCredentialsField,
    ExaCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class ContentRetrievalSettings(BaseModel):
    text: Optional[dict] = SchemaField(
        description=""Text content settings"",
        default={""maxCharacters"": 1000, ""includeHtmlTags"": False},
        advanced=True,
    )
    highlights: Optional[dict] = SchemaField(
        description=""Highlight settings"",
        default={
            ""numSentences"": 3,
            ""highlightsPerUrl"": 3,
            ""query"": """",
        },
        advanced=True,
    )
    summary: Optional[dict] = SchemaField(
        description=""Summary settings"",
        default={""query"": """"},
        advanced=True,
    )


class ExaContentsBlock(Block):
    class Input(BlockSchema):
        credentials: ExaCredentialsInput = ExaCredentialsField()
        ids: List[str] = SchemaField(
            description=""Array of document IDs obtained from searches"",
        )
        contents: ContentRetrievalSettings = SchemaField(
            description=""Content retrieval settings"",
            default=ContentRetrievalSettings(),
            advanced=True,
        )

    class Output(BlockSchema):
        results: list = SchemaField(
            description=""List of document contents"",
            default=[],
        )

    def __init__(self):
        super().__init__(
            id=""c52be83f-f8cd-4180-b243-af35f986b461"",
            description=""Retrieves document contents using Exa's contents API"",
            categories={BlockCategory.SEARCH},
            input_schema=ExaContentsBlock.Input,
            output_schema=ExaContentsBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: ExaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://api.exa.ai/contents""
        headers = {
            ""Content-Type"": ""application/json"",
            ""x-api-key"": credentials.api_key.get_secret_value(),
        }

        payload = {
            ""ids"": input_data.ids,
            ""text"": input_data.contents.text,
            ""highlights"": input_data.contents.highlights,
            ""summary"": input_data.contents.summary,
        }

        try:
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            yield ""results"", data.get(""results"", [])
        except Exception as e:
            yield ""error"", str(e)
            yield ""results"", []
","from typing import List, Optional
from pydantic import BaseModel
from backend.blocks.exa._auth import (
    ExaCredentials,
    ExaCredentialsField,
    ExaCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests

class ContentRetrievalSettings(BaseModel):
    text: Optional[dict] = SchemaField(
        description=""Text content settings"",
        default={""maxCharacters"": 1000, ""includeHtmlTags"": False},
        advanced=True,
    )
    highlights: Optional[dict] = SchemaField(
        description=""Highlight settings"",
        default={
            ""numSentences"": 3,
            ""highlightsPerUrl"": 3,
            ""query"": """",
        },
        advanced=True,
    )
    summary: Optional[dict] = SchemaField(
        description=""Summary settings"",
        default={""query"": """"},
        advanced=True,
    )

class ExaContentsBlock(Block):
    class Input(BlockSchema):
        credentials: ExaCredentialsInput = ExaCredentialsField()
        ids: List[str] = SchemaField(
            description=""Array of document IDs obtained from searches"",
        )
        contents: ContentRetrievalSettings = SchemaField(
            description=""Content retrieval settings"",
            default=ContentRetrievalSettings(),
            advanced=True,
        )

    class Output(BlockSchema):
        results: list = SchemaField(
            description=""List of document contents"",
            default=[],
        )

    def __init__(self):
        super().__init__(
            id=""c52be83f-f8cd-4180-b243-af35f986b461"",
            description=""Retrieves document contents using Exa's contents API"",
            categories={BlockCategory.SEARCH},
            input_schema=ExaContentsBlock.Input,
            output_schema=ExaContentsBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: ExaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://api.exa.ai/contents""
        headers = {
            ""Content-Type"": ""application/json"",
            ""x-api-key"": credentials.api_key.get_secret_value(),
        }
        payload = {
            ""ids"": input_data.ids,
            ""text"": input_data.contents.text,
            ""highlights"": input_data.contents.highlights,
            ""summary"": input_data.contents.summary,
        }
        try:
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            yield ""results"", data.get(""results"", [])
        except Exception as e:
            yield ""error"", str(e)
            yield ""results"", []","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
131,"import logging
from typing import TYPE_CHECKING, AsyncGenerator, Optional

from prisma import Json
from prisma.models import IntegrationWebhook
from pydantic import Field, computed_field

from backend.data.includes import INTEGRATION_WEBHOOK_INCLUDE
from backend.data.queue import AsyncRedisEventBus
from backend.integrations.providers import ProviderName
from backend.integrations.webhooks.utils import webhook_ingress_url

from .db import BaseDbModel

if TYPE_CHECKING:
    from .graph import NodeModel

logger = logging.getLogger(__name__)


class Webhook(BaseDbModel):
    user_id: str
    provider: ProviderName
    credentials_id: str
    webhook_type: str
    resource: str
    events: list[str]
    config: dict = Field(default_factory=dict)
    secret: str

    provider_webhook_id: str

    attached_nodes: Optional[list[""NodeModel""]] = None

    @computed_field
    @property
    def url(self) -> str:
        return webhook_ingress_url(self.provider.value, self.id)

    @staticmethod
    def from_db(webhook: IntegrationWebhook):
        from .graph import NodeModel

        return Webhook(
            id=webhook.id,
            user_id=webhook.userId,
            provider=ProviderName(webhook.provider),
            credentials_id=webhook.credentialsId,
            webhook_type=webhook.webhookType,
            resource=webhook.resource,
            events=webhook.events,
            config=dict(webhook.config),
            secret=webhook.secret,
            provider_webhook_id=webhook.providerWebhookId,
            attached_nodes=(
                [NodeModel.from_db(node) for node in webhook.AgentNodes]
                if webhook.AgentNodes is not None
                else None
            ),
        )


# --------------------- CRUD functions --------------------- #


async def create_webhook(webhook: Webhook) -> Webhook:
    created_webhook = await IntegrationWebhook.prisma().create(
        data={
            ""id"": webhook.id,
            ""userId"": webhook.user_id,
            ""provider"": webhook.provider.value,
            ""credentialsId"": webhook.credentials_id,
            ""webhookType"": webhook.webhook_type,
            ""resource"": webhook.resource,
            ""events"": webhook.events,
            ""config"": Json(webhook.config),
            ""secret"": webhook.secret,
            ""providerWebhookId"": webhook.provider_webhook_id,
        }
    )
    return Webhook.from_db(created_webhook)


async def get_webhook(webhook_id: str) -> Webhook:
    """"""⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    webhook = await IntegrationWebhook.prisma().find_unique_or_raise(
        where={""id"": webhook_id},
        include=INTEGRATION_WEBHOOK_INCLUDE,
    )
    return Webhook.from_db(webhook)


async def get_all_webhooks_by_creds(credentials_id: str) -> list[Webhook]:
    """"""⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    if not credentials_id:
        raise ValueError(""credentials_id must not be empty"")
    webhooks = await IntegrationWebhook.prisma().find_many(
        where={""credentialsId"": credentials_id},
        include=INTEGRATION_WEBHOOK_INCLUDE,
    )
    return [Webhook.from_db(webhook) for webhook in webhooks]


async def find_webhook_by_credentials_and_props(
    credentials_id: str, webhook_type: str, resource: str, events: list[str]
) -> Webhook | None:
    """"""⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    webhook = await IntegrationWebhook.prisma().find_first(
        where={
            ""credentialsId"": credentials_id,
            ""webhookType"": webhook_type,
            ""resource"": resource,
            ""events"": {""has_every"": events},
        },
        include=INTEGRATION_WEBHOOK_INCLUDE,
    )
    return Webhook.from_db(webhook) if webhook else None


async def find_webhook_by_graph_and_props(
    graph_id: str, provider: str, webhook_type: str, events: list[str]
) -> Webhook | None:
    """"""⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    webhook = await IntegrationWebhook.prisma().find_first(
        where={
            ""provider"": provider,
            ""webhookType"": webhook_type,
            ""events"": {""has_every"": events},
            ""AgentNodes"": {""some"": {""agentGraphId"": graph_id}},
        },
        include=INTEGRATION_WEBHOOK_INCLUDE,
    )
    return Webhook.from_db(webhook) if webhook else None


async def update_webhook_config(webhook_id: str, updated_config: dict) -> Webhook:
    """"""⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    _updated_webhook = await IntegrationWebhook.prisma().update(
        where={""id"": webhook_id},
        data={""config"": Json(updated_config)},
        include=INTEGRATION_WEBHOOK_INCLUDE,
    )
    if _updated_webhook is None:
        raise ValueError(f""Webhook #{webhook_id} not found"")
    return Webhook.from_db(_updated_webhook)


async def delete_webhook(webhook_id: str) -> None:
    """"""⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    deleted = await IntegrationWebhook.prisma().delete(where={""id"": webhook_id})
    if not deleted:
        raise ValueError(f""Webhook #{webhook_id} not found"")


# --------------------- WEBHOOK EVENTS --------------------- #


class WebhookEvent(BaseDbModel):
    provider: str
    webhook_id: str
    event_type: str
    payload: dict


class WebhookEventBus(AsyncRedisEventBus[WebhookEvent]):
    Model = WebhookEvent

    @property
    def event_bus_name(self) -> str:
        return ""webhooks""


_webhook_event_bus = WebhookEventBus()


async def publish_webhook_event(event: WebhookEvent):
    await _webhook_event_bus.publish_event(
        event, f""{event.webhook_id}/{event.event_type}""
    )


async def listen_for_webhook_events(
    webhook_id: str, event_type: Optional[str] = None
) -> AsyncGenerator[WebhookEvent, None]:
    async for event in _webhook_event_bus.listen_events(
        f""{webhook_id}/{event_type or '*'}""
    ):
        yield event


async def wait_for_webhook_event(
    webhook_id: str, event_type: Optional[str] = None, timeout: Optional[float] = None
) -> WebhookEvent | None:
    return await _webhook_event_bus.wait_for_event(
        f""{webhook_id}/{event_type or '*'}"", timeout
    )
","import logging
from typing import TYPE_CHECKING, AsyncGenerator, Optional
from prisma import Json
from prisma.models import IntegrationWebhook
from pydantic import Field, computed_field
from backend.data.includes import INTEGRATION_WEBHOOK_INCLUDE
from backend.data.queue import AsyncRedisEventBus
from backend.integrations.providers import ProviderName
from backend.integrations.webhooks.utils import webhook_ingress_url
from .db import BaseDbModel

if TYPE_CHECKING:
    from .graph import NodeModel

logger = logging.getLogger(__name__)

class Webhook(BaseDbModel):
    user_id: str
    provider: ProviderName
    credentials_id: str
    webhook_type: str
    resource: str
    events: list[str]
    config: dict = Field(default_factory=dict)
    secret: str

    provider_webhook_id: str

    attached_nodes: Optional[list[""NodeModel""]] = None

    @computed_field
    @property
    def url(self) -> str:
        return webhook_ingress_url(self.provider.value, self.id)

    @staticmethod
    def from_db(webhook: IntegrationWebhook):
        from .graph import NodeModel

        return Webhook(
            id=webhook.id,
            user_id=webhook.userId,
            provider=ProviderName(webhook.provider),
            credentials_id=webhook.credentialsId,
            webhook_type=webhook.webhookType,
            resource=webhook.resource,
            events=webhook.events,
            config=dict(webhook.config),
            secret=webhook.secret,
            provider_webhook_id=webhook.providerWebhookId,
            attached_nodes=(
                [NodeModel.from_db(node) for node in webhook.AgentNodes]
                if webhook.AgentNodes is not None
                else None
            ),
        )

async def create_webhook(webhook: Webhook) -> Webhook:
    created_webhook = await IntegrationWebhook.prisma().create(
        data={
            ""id"": webhook.id,
            ""userId"": webhook.user_id,
            ""provider"": webhook.provider.value,
            ""credentialsId"": webhook.credentials_id,
            ""webhookType"": webhook.webhook_type,
            ""resource"": webhook.resource,
            ""events"": webhook.events,
            ""config"": Json(webhook.config),
            ""secret"": webhook.secret,
            ""providerWebhookId"": webhook.provider_webhook_id,
        }
    )
    return Webhook.from_db(created_webhook)

async def get_webhook(webhook_id: str) -> Webhook:
    """"""
    ⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    webhook = await IntegrationWebhook.prisma().find_unique_or_raise(
        where={""id"": webhook_id},
        include=INTEGRATION_WEBHOOK_INCLUDE,
    )
    return Webhook.from_db(webhook)

async def get_all_webhooks_by_creds(credentials_id: str) -> list[Webhook]:
    """"""
    ⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    if not credentials_id:
        raise ValueError(""credentials_id must not be empty"")
    webhooks = await IntegrationWebhook.prisma().find_many(
        where={""credentialsId"": credentials_id},
        include=INTEGRATION_WEBHOOK_INCLUDE,
    )
    return [Webhook.from_db(webhook) for webhook in webhooks]

async def find_webhook_by_credentials_and_props(
    credentials_id: str, webhook_type: str, resource: str, events: list[str]
) -> Webhook | None:
    """"""
    ⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    webhook = await IntegrationWebhook.prisma().find_first(
        where={
            ""credentialsId"": credentials_id,
            ""webhookType"": webhook_type,
            ""resource"": resource,
            ""events"": {""has_every"": events},
        },
        include=INTEGRATION_WEBHOOK_INCLUDE,
    )
    return Webhook.from_db(webhook) if webhook else None

async def find_webhook_by_graph_and_props(
    graph_id: str, provider: str, webhook_type: str, events: list[str]
) -> Webhook | None:
    """"""
    ⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    webhook = await IntegrationWebhook.prisma().find_first(
        where={
            ""provider"": provider,
            ""webhookType"": webhook_type,
            ""events"": {""has_every"": events},
            ""AgentNodes"": {""some"": {""agentGraphId"": graph_id}},
        },
        include=INTEGRATION_WEBHOOK_INCLUDE,
    )
    return Webhook.from_db(webhook) if webhook else None

async def update_webhook_config(webhook_id: str, updated_config: dict) -> Webhook:
    """"""
    ⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    _updated_webhook = await IntegrationWebhook.prisma().update(
        where={""id"": webhook_id},
        data={""config"": Json(updated_config)},
        include=INTEGRATION_WEBHOOK_INCLUDE,
    )
    if _updated_webhook is None:
        raise ValueError(f""Webhook #{webhook_id} not found"")
    return Webhook.from_db(_updated_webhook)

async def delete_webhook(webhook_id: str) -> None:
    """"""
    ⚠️ No `user_id` check: DO NOT USE without check in user-facing endpoints.""""""
    deleted = await IntegrationWebhook.prisma().delete(where={""id"": webhook_id})
    if not deleted:
        raise ValueError(f""Webhook #{webhook_id} not found"")

class WebhookEvent(BaseDbModel):
    provider: str
    webhook_id: str
    event_type: str
    payload: dict

class WebhookEventBus(AsyncRedisEventBus[WebhookEvent]):
    Model = WebhookEvent

    @property
    def event_bus_name(self) -> str:
        return ""webhooks""

async def publish_webhook_event(event: WebhookEvent):
    await _webhook_event_bus.publish_event(
        event, f""{event.webhook_id}/{event.event_type}""
    )

async def listen_for_webhook_events(
    webhook_id: str, event_type: Optional[str] = None
) -> AsyncGenerator[WebhookEvent, None]:
    async for event in _webhook_event_bus.listen_events(
        f""{webhook_id}/{event_type or '*'}""
    ):
        yield event

async def wait_for_webhook_event(
    webhook_id: str, event_type: Optional[str] = None, timeout: Optional[float] = None
) -> WebhookEvent | None:
    return await _webhook_event_bus.wait_for_event(
        f""{webhook_id}/{event_type or '*'}"", timeout
    )","{'final_score': 97.31, 'length_similarity': 97.84, 'token_similarity': 95.95, 'structure_similarity': 94.23, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
132,"def get_1s_count(number: int) -> int:
    """"""
    Count the number of set bits in a 32 bit integer using Brian Kernighan's way.
    Ref - https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetKernighan
    >>> get_1s_count(25)
    3
    >>> get_1s_count(37)
    3
    >>> get_1s_count(21)
    3
    >>> get_1s_count(58)
    4
    >>> get_1s_count(0)
    0
    >>> get_1s_count(256)
    1
    >>> get_1s_count(-1)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    >>> get_1s_count(0.8)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    >>> get_1s_count(""25"")
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    """"""
    if not isinstance(number, int) or number < 0:
        raise ValueError(""Input must be a non-negative integer"")

    count = 0
    while number:
        # This way we arrive at next set bit (next 1) instead of looping
        # through each bit and checking for 1s hence the
        # loop won't run 32 times it will only run the number of `1` times
        number &= number - 1
        count += 1
    return count


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","def get_1s_count(number: int) -> int:
    """"""
    Count the number of set bits in a 32 bit integer using Brian Kernighan's way.
    Ref - https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetKernighan
    >>> get_1s_count(25)
    3
    >>> get_1s_count(37)
    3
    >>> get_1s_count(21)
    3
    >>> get_1s_count(58)
    4
    >>> get_1s_count(0)
    0
    >>> get_1s_count(256)
    1
    >>> get_1s_count(-1)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    >>> get_1s_count(0.8)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    >>> get_1s_count(""25"")
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    """"""
    if not isinstance(number, int) or number < 0:
        raise ValueError(""Input must be a non-negative integer"")
    
    count = 0
    while number:
        # This way we arrive at next set bit (next 1) instead of looping
        # through each bit and checking for 1s hence the
        # loop won't run 32 times it will only run the number of `1` times
        number &= number - 1
        count += 1
    
    return count

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
133,"import asyncio
import random
from datetime import datetime

import prisma.enums
from faker import Faker
from prisma import Prisma

faker = Faker()

# Constants for data generation limits

# Base entities
NUM_USERS = 100  # Creates 100 user records
NUM_AGENT_BLOCKS = 100  # Creates 100 agent block templates

# Per-user entities
MIN_GRAPHS_PER_USER = 1  # Each user will have between 1-5 graphs
MAX_GRAPHS_PER_USER = 5  # Total graphs: 500-2500 (NUM_USERS * MIN/MAX_GRAPHS)

# Per-graph entities
MIN_NODES_PER_GRAPH = 2  # Each graph will have between 2-5 nodes
MAX_NODES_PER_GRAPH = (
    5  # Total nodes: 1000-2500 (GRAPHS_PER_USER * NUM_USERS * MIN/MAX_NODES)
)

# Additional per-user entities
MIN_PRESETS_PER_USER = 1  # Each user will have between 1-2 presets
MAX_PRESETS_PER_USER = 5  # Total presets: 500-2500 (NUM_USERS * MIN/MAX_PRESETS)
MIN_AGENTS_PER_USER = 1  # Each user will have between 1-2 agents
MAX_AGENTS_PER_USER = 10  # Total agents: 500-5000 (NUM_USERS * MIN/MAX_AGENTS)

# Execution and review records
MIN_EXECUTIONS_PER_GRAPH = 1  # Each graph will have between 1-5 execution records
MAX_EXECUTIONS_PER_GRAPH = (
    20  # Total executions: 1000-5000 (TOTAL_GRAPHS * MIN/MAX_EXECUTIONS)
)
MIN_REVIEWS_PER_VERSION = 1  # Each version will have between 1-3 reviews
MAX_REVIEWS_PER_VERSION = 5  # Total reviews depends on number of versions created


def get_image():
    url = faker.image_url()
    while ""placekitten.com"" in url:
        url = faker.image_url()
    return url


async def main():
    db = Prisma()
    await db.connect()

    # Insert Users
    print(f""Inserting {NUM_USERS} users"")
    users = []
    for _ in range(NUM_USERS):
        user = await db.user.create(
            data={
                ""id"": str(faker.uuid4()),
                ""email"": faker.unique.email(),
                ""name"": faker.name(),
                ""metadata"": prisma.Json({}),
                ""integrations"": """",
            }
        )
        users.append(user)

    # Insert AgentBlocks
    agent_blocks = []
    print(f""Inserting {NUM_AGENT_BLOCKS} agent blocks"")
    for _ in range(NUM_AGENT_BLOCKS):
        block = await db.agentblock.create(
            data={
                ""name"": f""{faker.word()}_{str(faker.uuid4())[:8]}"",
                ""inputSchema"": ""{}"",
                ""outputSchema"": ""{}"",
            }
        )
        agent_blocks.append(block)

    # Insert AgentGraphs
    agent_graphs = []
    print(f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER} agent graphs"")
    for user in users:
        for _ in range(
            random.randint(MIN_GRAPHS_PER_USER, MAX_GRAPHS_PER_USER)
        ):  # Adjust the range to create more graphs per user if desired
            graph = await db.agentgraph.create(
                data={
                    ""name"": faker.sentence(nb_words=3),
                    ""description"": faker.text(max_nb_chars=200),
                    ""userId"": user.id,
                    ""isActive"": True,
                    ""isTemplate"": False,
                }
            )
            agent_graphs.append(graph)

    # Insert AgentNodes
    agent_nodes = []
    print(
        f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER * MAX_NODES_PER_GRAPH} agent nodes""
    )
    for graph in agent_graphs:
        num_nodes = random.randint(MIN_NODES_PER_GRAPH, MAX_NODES_PER_GRAPH)
        for _ in range(num_nodes):  # Create 5 AgentNodes per graph
            block = random.choice(agent_blocks)
            node = await db.agentnode.create(
                data={
                    ""agentBlockId"": block.id,
                    ""agentGraphId"": graph.id,
                    ""agentGraphVersion"": graph.version,
                    ""constantInput"": ""{}"",
                    ""metadata"": ""{}"",
                }
            )
            agent_nodes.append(node)

    # Insert AgentPresets
    agent_presets = []
    print(f""Inserting {NUM_USERS * MAX_PRESETS_PER_USER} agent presets"")
    for user in users:
        num_presets = random.randint(MIN_PRESETS_PER_USER, MAX_PRESETS_PER_USER)
        for _ in range(num_presets):  # Create 1 AgentPreset per user
            graph = random.choice(agent_graphs)
            preset = await db.agentpreset.create(
                data={
                    ""name"": faker.sentence(nb_words=3),
                    ""description"": faker.text(max_nb_chars=200),
                    ""userId"": user.id,
                    ""agentId"": graph.id,
                    ""agentVersion"": graph.version,
                    ""isActive"": True,
                }
            )
            agent_presets.append(preset)

    # Insert UserAgents
    user_agents = []
    print(f""Inserting {NUM_USERS * MAX_AGENTS_PER_USER} user agents"")
    for user in users:
        num_agents = random.randint(MIN_AGENTS_PER_USER, MAX_AGENTS_PER_USER)
        for _ in range(num_agents):  # Create 1 UserAgent per user
            graph = random.choice(agent_graphs)
            preset = random.choice(agent_presets)
            user_agent = await db.useragent.create(
                data={
                    ""userId"": user.id,
                    ""agentId"": graph.id,
                    ""agentVersion"": graph.version,
                    ""agentPresetId"": preset.id,
                    ""isFavorite"": random.choice([True, False]),
                    ""isCreatedByUser"": random.choice([True, False]),
                    ""isArchived"": random.choice([True, False]),
                    ""isDeleted"": random.choice([True, False]),
                }
            )
            user_agents.append(user_agent)

    # Insert AgentGraphExecutions
    # Insert AgentGraphExecutions
    agent_graph_executions = []
    print(
        f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER * MAX_EXECUTIONS_PER_GRAPH} agent graph executions""
    )
    graph_execution_data = []
    for graph in agent_graphs:
        user = random.choice(users)
        num_executions = random.randint(
            MIN_EXECUTIONS_PER_GRAPH, MAX_EXECUTIONS_PER_GRAPH
        )
        for _ in range(num_executions):
            matching_presets = [p for p in agent_presets if p.agentId == graph.id]
            preset = (
                random.choice(matching_presets)
                if matching_presets and random.random() < 0.5
                else None
            )

            graph_execution_data.append(
                {
                    ""agentGraphId"": graph.id,
                    ""agentGraphVersion"": graph.version,
                    ""userId"": user.id,
                    ""executionStatus"": prisma.enums.AgentExecutionStatus.COMPLETED,
                    ""startedAt"": faker.date_time_this_year(),
                    ""agentPresetId"": preset.id if preset else None,
                }
            )

    agent_graph_executions = await db.agentgraphexecution.create_many(
        data=graph_execution_data
    )
    # Need to fetch the created records since create_many doesn't return them
    agent_graph_executions = await db.agentgraphexecution.find_many()

    # Insert AgentNodeExecutions
    print(
        f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER * MAX_EXECUTIONS_PER_GRAPH} agent node executions""
    )
    node_execution_data = []
    for execution in agent_graph_executions:
        nodes = [
            node for node in agent_nodes if node.agentGraphId == execution.agentGraphId
        ]
        for node in nodes:
            node_execution_data.append(
                {
                    ""agentGraphExecutionId"": execution.id,
                    ""agentNodeId"": node.id,
                    ""executionStatus"": prisma.enums.AgentExecutionStatus.COMPLETED,
                    ""addedTime"": datetime.now(),
                }
            )

    agent_node_executions = await db.agentnodeexecution.create_many(
        data=node_execution_data
    )
    # Need to fetch the created records since create_many doesn't return them
    agent_node_executions = await db.agentnodeexecution.find_many()

    # Insert AgentNodeExecutionInputOutput
    print(
        f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER * MAX_EXECUTIONS_PER_GRAPH} agent node execution input/outputs""
    )
    input_output_data = []
    for node_execution in agent_node_executions:
        # Input data
        input_output_data.append(
            {
                ""name"": ""input1"",
                ""data"": ""{}"",
                ""time"": datetime.now(),
                ""referencedByInputExecId"": node_execution.id,
            }
        )
        # Output data
        input_output_data.append(
            {
                ""name"": ""output1"",
                ""data"": ""{}"",
                ""time"": datetime.now(),
                ""referencedByOutputExecId"": node_execution.id,
            }
        )

    await db.agentnodeexecutioninputoutput.create_many(data=input_output_data)

    # Insert AgentNodeLinks
    print(f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER} agent node links"")
    for graph in agent_graphs:
        nodes = [node for node in agent_nodes if node.agentGraphId == graph.id]
        if len(nodes) >= 2:
            source_node = nodes[0]
            sink_node = nodes[1]
            await db.agentnodelink.create(
                data={
                    ""agentNodeSourceId"": source_node.id,
                    ""sourceName"": ""output1"",
                    ""agentNodeSinkId"": sink_node.id,
                    ""sinkName"": ""input1"",
                    ""isStatic"": False,
                }
            )

    # Insert AnalyticsDetails
    print(f""Inserting {NUM_USERS} analytics details"")
    for user in users:
        for _ in range(1):
            await db.analyticsdetails.create(
                data={
                    ""userId"": user.id,
                    ""type"": faker.word(),
                    ""data"": prisma.Json({}),
                    ""dataIndex"": faker.word(),
                }
            )

    # Insert AnalyticsMetrics
    print(f""Inserting {NUM_USERS} analytics metrics"")
    for user in users:
        for _ in range(1):
            await db.analyticsmetrics.create(
                data={
                    ""userId"": user.id,
                    ""analyticMetric"": faker.word(),
                    ""value"": random.uniform(0, 100),
                    ""dataString"": faker.word(),
                }
            )

    # Insert CreditTransaction (formerly UserBlockCredit)
    print(f""Inserting {NUM_USERS} credit transactions"")
    for user in users:
        for _ in range(1):
            block = random.choice(agent_blocks)
            await db.credittransaction.create(
                data={
                    ""transactionKey"": str(faker.uuid4()),
                    ""userId"": user.id,
                    ""blockId"": block.id,
                    ""amount"": random.randint(1, 100),
                    ""type"": (
                        prisma.enums.CreditTransactionType.TOP_UP
                        if random.random() < 0.5
                        else prisma.enums.CreditTransactionType.USAGE
                    ),
                    ""metadata"": prisma.Json({}),
                }
            )

    # Insert Profiles
    profiles = []
    print(f""Inserting {NUM_USERS} profiles"")
    for user in users:
        profile = await db.profile.create(
            data={
                ""userId"": user.id,
                ""name"": user.name or faker.name(),
                ""username"": faker.unique.user_name(),
                ""description"": faker.text(),
                ""links"": [faker.url() for _ in range(3)],
                ""avatarUrl"": get_image(),
            }
        )
        profiles.append(profile)

    # Insert StoreListings
    store_listings = []
    print(f""Inserting {NUM_USERS} store listings"")
    for graph in agent_graphs:
        user = random.choice(users)
        listing = await db.storelisting.create(
            data={
                ""agentId"": graph.id,
                ""agentVersion"": graph.version,
                ""owningUserId"": user.id,
                ""isApproved"": random.choice([True, False]),
            }
        )
        store_listings.append(listing)

    # Insert StoreListingVersions
    store_listing_versions = []
    print(f""Inserting {NUM_USERS} store listing versions"")
    for listing in store_listings:
        graph = [g for g in agent_graphs if g.id == listing.agentId][0]
        version = await db.storelistingversion.create(
            data={
                ""agentId"": graph.id,
                ""agentVersion"": graph.version,
                ""slug"": faker.slug(),
                ""name"": graph.name or faker.sentence(nb_words=3),
                ""subHeading"": faker.sentence(),
                ""videoUrl"": faker.url(),
                ""imageUrls"": [get_image() for _ in range(3)],
                ""description"": faker.text(),
                ""categories"": [faker.word() for _ in range(3)],
                ""isFeatured"": random.choice([True, False]),
                ""isAvailable"": True,
                ""isApproved"": random.choice([True, False]),
                ""storeListingId"": listing.id,
            }
        )
        store_listing_versions.append(version)

    # Insert StoreListingReviews
    print(f""Inserting {NUM_USERS * MAX_REVIEWS_PER_VERSION} store listing reviews"")
    for version in store_listing_versions:
        # Create a copy of users list and shuffle it to avoid duplicates
        available_reviewers = users.copy()
        random.shuffle(available_reviewers)

        # Limit number of reviews to available unique reviewers
        num_reviews = min(
            random.randint(MIN_REVIEWS_PER_VERSION, MAX_REVIEWS_PER_VERSION),
            len(available_reviewers),
        )

        # Take only the first num_reviews reviewers
        for reviewer in available_reviewers[:num_reviews]:
            await db.storelistingreview.create(
                data={
                    ""storeListingVersionId"": version.id,
                    ""reviewByUserId"": reviewer.id,
                    ""score"": random.randint(1, 5),
                    ""comments"": faker.text(),
                }
            )

    # Insert StoreListingSubmissions
    print(f""Inserting {NUM_USERS} store listing submissions"")
    for listing in store_listings:
        version = random.choice(store_listing_versions)
        reviewer = random.choice(users)
        status: prisma.enums.SubmissionStatus = random.choice(
            [
                prisma.enums.SubmissionStatus.PENDING,
                prisma.enums.SubmissionStatus.APPROVED,
                prisma.enums.SubmissionStatus.REJECTED,
            ]
        )
        await db.storelistingsubmission.create(
            data={
                ""storeListingId"": listing.id,
                ""storeListingVersionId"": version.id,
                ""reviewerId"": reviewer.id,
                ""Status"": status,
                ""reviewComments"": faker.text(),
            }
        )

    # Insert APIKeys
    print(f""Inserting {NUM_USERS} api keys"")
    for user in users:
        await db.apikey.create(
            data={
                ""name"": faker.word(),
                ""prefix"": str(faker.uuid4())[:8],
                ""postfix"": str(faker.uuid4())[-8:],
                ""key"": str(faker.sha256()),
                ""status"": prisma.enums.APIKeyStatus.ACTIVE,
                ""permissions"": [
                    prisma.enums.APIKeyPermission.EXECUTE_GRAPH,
                    prisma.enums.APIKeyPermission.READ_GRAPH,
                ],
                ""description"": faker.text(),
                ""userId"": user.id,
            }
        )

    await db.disconnect()


if __name__ == ""__main__"":
    asyncio.run(main())
","import asyncio
import random
from datetime import datetime

import prisma.enums
from faker import Faker
from prisma import Prisma

faker = Faker()

# Constants for data generation limits

# Base entities
NUM_USERS = 100  # Creates 100 user records
NUM_AGENT_BLOCKS = 100  # Creates 100 agent block templates

# Per-user entities
MIN_GRAPHS_PER_USER = 1  # Each user will have between 1-5 graphs
MAX_GRAPHS_PER_USER = 5  # Total graphs: 500-2500 (NUM_USERS * MIN/MAX_GRAPHS)

# Per-graph entities
MIN_NODES_PER_GRAPH = 2  # Each graph will have between 2-5 nodes
MAX_NODES_PER_GRAPH = (
    5  # Total nodes: 1000-2500 (GRAPHS_PER_USER * NUM_USERS * MIN/MAX_NODES)
)


# Additional per-user entities
MIN_PRESETS_PER_USER = 1  # Each user will have between 1-2 presets
MAX_PRESETS_PER_USER = 5  # Total presets: 500-2500 (NUM_USERS * MIN/MAX_PRESETS)
MIN_AGENTS_PER_USER = 1  # Each user will have between 1-2 agents
MAX_AGENTS_PER_USER = 10  # Total agents: 500-5000 (NUM_USERS * MIN/MAX_AGENTS)

# Execution and review records
MIN_EXECUTIONS_PER_GRAPH = 1  # Each graph will have between 1-5 execution records
MAX_EXECUTIONS_PER_GRAPH = (
    20  # Total executions: 1000-5000 (TOTAL_GRAPHS * MIN/MAX_EXECUTIONS)
)
MIN_REVIEWS_PER_VERSION = 1  # Each version will have between 1-3 reviews
MAX_REVIEWS_PER_VERSION = 5  # Total reviews depends on number of versions created


def get_image():
    url = faker.image_url()
    while ""placekitten.com"" in url:
        url = faker.image_url()
    return url


async def main():
    db = Prisma()
    await db.connect()
    
    print(f""Inserting {NUM_USERS} users"")
    users = []
    for _ in range(NUM_USERS):
        user = await db.user.create(
            data={
                ""id"": str(faker.uuid4()),
                ""email"": faker.unique.email(),
                ""name"": faker.name(),
                ""metadata"": prisma.Json({}),
                ""integrations"": """",
            }
        )
        users.append(user)
    
    agent_blocks = []
    print(f""Inserting {NUM_AGENT_BLOCKS} agent blocks"")
    for _ in range(NUM_AGENT_BLOCKS):
        block = await db.agentblock.create(
            data={
                ""name"": f""{faker.word()}_{str(faker.uuid4())[:8]}"",
                ""inputSchema"": ""{}"",
                ""outputSchema"": ""{}"",
            }
        )
        agent_blocks.append(block)
    
    agent_graphs = []
    print(f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER} agent graphs"")
    for user in users:
        for _ in range(
            random.randint(MIN_GRAPHS_PER_USER, MAX_GRAPHS_PER_USER)
        ):  # Adjust the range to create more graphs per user if desired
            graph = await db.agentgraph.create(
                data={
                    ""name"": faker.sentence(nb_words=3),
                    ""description"": faker.text(max_nb_chars=200),
                    ""userId"": user.id,
                    ""isActive"": True,
                    ""isTemplate"": False,
                }
            )
            agent_graphs.append(graph)
    
    agent_nodes = []
    print(
        f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER * MAX_NODES_PER_GRAPH} agent nodes""
    )
    for graph in agent_graphs:
        num_nodes = random.randint(MIN_NODES_PER_GRAPH, MAX_NODES_PER_GRAPH)
        for _ in range(num_nodes):  # Create 5 AgentNodes per graph
            block = random.choice(agent_blocks)
            node = await db.agentnode.create(
                data={
                    ""agentBlockId"": block.id,
                    ""agentGraphId"": graph.id,
                    ""agentGraphVersion"": graph.version,
                    ""constantInput"": ""{}"",
                    ""metadata"": ""{}"",
                }
            )
            agent_nodes.append(node)
    
    agent_presets = []
    print(f""Inserting {NUM_USERS * MAX_PRESETS_PER_USER} agent presets"")
    for user in users:
        num_presets = random.randint(MIN_PRESETS_PER_USER, MAX_PRESETS_PER_USER)
        for _ in range(num_presets):  # Create 1 AgentPreset per user
            graph = random.choice(agent_graphs)
            preset = await db.agentpreset.create(
                data={
                    ""name"": faker.sentence(nb_words=3),
                    ""description"": faker.text(max_nb_chars=200),
                    ""userId"": user.id,
                    ""agentId"": graph.id,
                    ""agentVersion"": graph.version,
                    ""isActive"": True,
                }
            )
            agent_presets.append(preset)
    
    user_agents = []
    print(f""Inserting {NUM_USERS * MAX_AGENTS_PER_USER} user agents"")
    for user in users:
        num_agents = random.randint(MIN_AGENTS_PER_USER, MAX_AGENTS_PER_USER)
        for _ in range(num_agents):  # Create 1 UserAgent per user
            graph = random.choice(agent_graphs)
            preset = random.choice(agent_presets)
            user_agent = await db.useragent.create(
                data={
                    ""userId"": user.id,
                    ""agentId"": graph.id,
                    ""agentVersion"": graph.version,
                    ""agentPresetId"": preset.id,
                    ""isFavorite"": random.choice([True, False]),
                    ""isCreatedByUser"": random.choice([True, False]),
                    ""isArchived"": random.choice([True, False]),
                    ""isDeleted"": random.choice([True, False]),
                }
            )
            user_agents.append(user_agent)
    
    # Insert AgentGraphExecutions
    agent_graph_executions = []
    print(
        f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER * MAX_EXECUTIONS_PER_GRAPH} agent graph executions""
    )
    graph_execution_data = []
    for graph in agent_graphs:
        user = random.choice(users)
        num_executions = random.randint(
            MIN_EXECUTIONS_PER_GRAPH, MAX_EXECUTIONS_PER_GRAPH
        )
        for _ in range(num_executions):
            matching_presets = [p for p in agent_presets if p.agentId == graph.id]
            preset = (
                random.choice(matching_presets)
                if matching_presets and random.random() < 0.5
                else None
            )

            graph_execution_data.append(
                {
                    ""agentGraphId"": graph.id,
                    ""agentGraphVersion"": graph.version,
                    ""userId"": user.id,
                    ""executionStatus"": prisma.enums.AgentExecutionStatus.COMPLETED,
                    ""startedAt"": faker.date_time_this_year(),
                    ""agentPresetId"": preset.id if preset else None,
                }
            )

    agent_graph_executions = await db.agentgraphexecution.create_many(
        data=graph_execution_data
    )
    
    # Need to fetch the created records since create_many doesn't return them
    agent_graph_executions = await db.agentgraphexecution.find_many()
    
    print(
        f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER * MAX_EXECUTIONS_PER_GRAPH} agent node executions""
    )
    node_execution_data = []
    for execution in agent_graph_executions:
        nodes = [
            node for node in agent_nodes if node.agentGraphId == execution.agentGraphId
        ]
        for node in nodes:
            node_execution_data.append(
                {
                    ""agentGraphExecutionId"": execution.id,
                    ""agentNodeId"": node.id,
                    ""executionStatus"": prisma.enums.AgentExecutionStatus.COMPLETED,
                    ""addedTime"": datetime.now(),
                }
            )

    agent_node_executions = await db.agentnodeexecution.create_many(
        data=node_execution_data
    )
    
    # Need to fetch the created records since create_many doesn't return them
    agent_node_executions = await db.agentnodeexecution.find_many()
    
    print(
        f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER * MAX_EXECUTIONS_PER_GRAPH} agent node execution input/outputs""
    )
    input_output_data = []
    for node_execution in agent_node_executions:
        # Input data
        input_output_data.append(
            {
                ""name"": ""input1"",
                ""data"": ""{}"",
                ""time"": datetime.now(),
                ""referencedByInputExecId"": node_execution.id,
            }
        )
        # Output data
        input_output_data.append(
            {
                ""name"": ""output1"",
                ""data"": ""{}"",
                ""time"": datetime.now(),
                ""referencedByOutputExecId"": node_execution.id,
            }
        )

    await db.agentnodeexecutioninputoutput.create_many(data=input_output_data)
    
    print(f""Inserting {NUM_USERS * MAX_GRAPHS_PER_USER} agent node links"")
    for graph in agent_graphs:
        nodes = [node for node in agent_nodes if node.agentGraphId == graph.id]
        if len(nodes) >= 2:
            source_node = nodes[0]
            sink_node = nodes[1]
            await db.agentnodelink.create(
                data={
                    ""agentNodeSourceId"": source_node.id,
                    ""sourceName"": ""output1"",
                    ""agentNodeSinkId"": sink_node.id,
                    ""sinkName"": ""input1"",
                    ""isStatic"": False,
                }
            )

    print(f""Inserting {NUM_USERS} analytics details"")
    for user in users:
        for _ in range(1):
            await db.analyticsdetails.create(
                data={
                    ""userId"": user.id,
                    ""type"": faker.word(),
                    ""data"": prisma.Json({}),
                    ""dataIndex"": faker.word(),
                }
            )

    print(f""Inserting {NUM_USERS} analytics metrics"")
    for user in users:
        for _ in range(1):
            await db.analyticsmetrics.create(
                data={
                    ""userId"": user.id,
                    ""analyticMetric"": faker.word(),
                    ""value"": random.uniform(0, 100),
                    ""dataString"": faker.word(),
                }
            )

    print(f""Inserting {NUM_USERS} credit transactions"")
    for user in users:
        for _ in range(1):
            block = random.choice(agent_blocks)
            await db.credittransaction.create(
                data={
                    ""transactionKey"": str(faker.uuid4()),
                    ""userId"": user.id,
                    ""blockId"": block.id,
                    ""amount"": random.randint(1, 100),
                    ""type"": (
                        prisma.enums.CreditTransactionType.TOP_UP
                        if random.random() < 0.5
                        else prisma.enums.CreditTransactionType.USAGE
                    ),
                    ""metadata"": prisma.Json({}),
                }
            )

    profiles = []
    print(f""Inserting {NUM_USERS} profiles"")
    for user in users:
        profile = await db.profile.create(
            data={
                ""userId"": user.id,
                ""name"": user.name or faker.name(),
                ""username"": faker.unique.user_name(),
                ""description"": faker.text(),
                ""links"": [faker.url() for _ in range(3)],
                ""avatarUrl"": get_image(),
            }
        )
        profiles.append(profile)
    
    # Insert StoreListings
    store_listings = []
    print(f""Inserting {NUM_USERS} store listings"")
    for graph in agent_graphs:
        user = random.choice(users)
        listing = await db.storelisting.create(
            data={
                ""agentId"": graph.id,
                ""agentVersion"": graph.version,
                ""owningUserId"": user.id,
                ""isApproved"": random.choice([True, False]),
            }
        )
        store_listings.append(listing)
    
    store_listing_versions = []
    print(f""Inserting {NUM_USERS} store listing versions"")
    for listing in store_listings:
        graph = [g for g in agent_graphs if g.id == listing.agentId][0]
        version = await db.storelistingversion.create(
            data={
                ""agentId"": graph.id,
                ""agentVersion"": graph.version,
                ""slug"": faker.slug(),
                ""name"": graph.name or faker.sentence(nb_words=3),
                ""subHeading"": faker.sentence(),
                ""videoUrl"": faker.url(),
                ""imageUrls"": [get_image() for _ in range(3)],
                ""description"": faker.text(),
                ""categories"": [faker.word() for _ in range(3)],
                ""isFeatured"": random.choice([True, False]),
                ""isAvailable"": True,
                ""isApproved"": random.choice([True, False]),
                ""storeListingId"": listing.id,
            }
        )
        store_listing_versions.append(version)
    
    print(f""Inserting {NUM_USERS * MAX_REVIEWS_PER_VERSION} store listing reviews"")
    for version in store_listing_versions:
        # Create a copy of users list and shuffle it to avoid duplicates
        available_reviewers = users.copy()
        random.shuffle(available_reviewers)

        # Limit number of reviews to available unique reviewers
        num_reviews = min(
            random.randint(MIN_REVIEWS_PER_VERSION, MAX_REVIEWS_PER_VERSION),
            len(available_reviewers),
        )

        # Take only the first num_reviews reviewers
        for reviewer in available_reviewers[:num_reviews]:
            await db.storelistingreview.create(
                data={
                    ""storeListingVersionId"": version.id,
                    ""reviewByUserId"": reviewer.id,
                    ""score"": random.randint(1, 5),
                    ""comments"": faker.text(),
                }
            )

    print(f""Inserting {NUM_USERS} store listing submissions"")
    for listing in store_listings:
        version = random.choice(store_listing_versions)
        reviewer = random.choice(users)
        status: prisma.enums.SubmissionStatus = random.choice(
            [
                prisma.enums.SubmissionStatus.PENDING,
                prisma.enums.SubmissionStatus.APPROVED,
                prisma.enums.SubmissionStatus.REJECTED,
            ]
        )
        await db.storelistingsubmission.create(
            data={
                ""storeListingId"": listing.id,
                ""storeListingVersionId"": version.id,
                ""reviewerId"": reviewer.id,
                ""Status"": status,
                ""reviewComments"": faker.text(),
            }
        )
    
    print(f""Inserting {NUM_USERS} api keys"")
    for user in users:
        await db.apikey.create(
            data={
                ""name"": faker.word(),
                ""prefix"": str(faker.uuid4())[:8],
                ""postfix"": str(faker.uuid4())[-8:],
                ""key"": str(faker.sha256()),
                ""status"": prisma.enums.APIKeyStatus.ACTIVE,
                ""permissions"": [
                    prisma.enums.APIKeyPermission.EXECUTE_GRAPH,
                    prisma.enums.APIKeyPermission.READ_GRAPH,
                ],
                ""description"": faker.text(),
                ""userId"": user.id,
            }
        )
    
    await db.disconnect()


if __name__ == ""__main__"":
    asyncio.run(main())","{'final_score': 97.35, 'length_similarity': 96.5, 'token_similarity': 95.94, 'structure_similarity': 95.45, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
134,"import json
import logging
from pathlib import Path

from pydantic import BaseModel

from backend.data.block import (
    Block,
    BlockCategory,
    BlockOutput,
    BlockSchema,
    BlockWebhookConfig,
)
from backend.data.model import SchemaField

from ._auth import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GithubCredentialsField,
    GithubCredentialsInput,
)

logger = logging.getLogger(__name__)


# --8<-- [start:GithubTriggerExample]
class GitHubTriggerBase:
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo: str = SchemaField(
            description=(
                ""Repository to subscribe to.\n\n""
                ""**Note:** Make sure your GitHub credentials have permissions ""
                ""to create webhooks on this repo.""
            ),
            placeholder=""{owner}/{repo}"",
        )
        # --8<-- [start:example-payload-field]
        payload: dict = SchemaField(hidden=True, default={})
        # --8<-- [end:example-payload-field]

    class Output(BlockSchema):
        payload: dict = SchemaField(
            description=""The complete webhook payload that was received from GitHub. ""
            ""Includes information about the affected resource (e.g. pull request), ""
            ""the event, and the user who triggered the event.""
        )
        triggered_by_user: dict = SchemaField(
            description=""Object representing the GitHub user who triggered the event""
        )
        error: str = SchemaField(
            description=""Error message if the payload could not be processed""
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""payload"", input_data.payload
        yield ""triggered_by_user"", input_data.payload[""sender""]


class GithubPullRequestTriggerBlock(GitHubTriggerBase, Block):
    EXAMPLE_PAYLOAD_FILE = (
        Path(__file__).parent / ""example_payloads"" / ""pull_request.synchronize.json""
    )

    # --8<-- [start:example-event-filter]
    class Input(GitHubTriggerBase.Input):
        class EventsFilter(BaseModel):
            """"""
            https://docs.github.com/en/webhooks/webhook-events-and-payloads#pull_request
            """"""

            opened: bool = False
            edited: bool = False
            closed: bool = False
            reopened: bool = False
            synchronize: bool = False
            assigned: bool = False
            unassigned: bool = False
            labeled: bool = False
            unlabeled: bool = False
            converted_to_draft: bool = False
            locked: bool = False
            unlocked: bool = False
            enqueued: bool = False
            dequeued: bool = False
            milestoned: bool = False
            demilestoned: bool = False
            ready_for_review: bool = False
            review_requested: bool = False
            review_request_removed: bool = False
            auto_merge_enabled: bool = False
            auto_merge_disabled: bool = False

        events: EventsFilter = SchemaField(
            title=""Events"", description=""The events to subscribe to""
        )
        # --8<-- [end:example-event-filter]

    class Output(GitHubTriggerBase.Output):
        event: str = SchemaField(
            description=""The PR event that triggered the webhook (e.g. 'opened')""
        )
        number: int = SchemaField(description=""The number of the affected pull request"")
        pull_request: dict = SchemaField(
            description=""Object representing the affected pull request""
        )
        pull_request_url: str = SchemaField(
            description=""The URL of the affected pull request""
        )

    def __init__(self):
        from backend.integrations.webhooks.github import GithubWebhookType

        example_payload = json.loads(
            self.EXAMPLE_PAYLOAD_FILE.read_text(encoding=""utf-8"")
        )

        super().__init__(
            id=""6c60ec01-8128-419e-988f-96a063ee2fea"",
            description=""This block triggers on pull request events and outputs the event type and payload."",
            categories={BlockCategory.DEVELOPER_TOOLS, BlockCategory.INPUT},
            input_schema=GithubPullRequestTriggerBlock.Input,
            output_schema=GithubPullRequestTriggerBlock.Output,
            # --8<-- [start:example-webhook_config]
            webhook_config=BlockWebhookConfig(
                provider=""github"",
                webhook_type=GithubWebhookType.REPO,
                resource_format=""{repo}"",
                event_filter_input=""events"",
                event_format=""pull_request.{event}"",
            ),
            # --8<-- [end:example-webhook_config]
            test_input={
                ""repo"": ""Significant-Gravitas/AutoGPT"",
                ""events"": {""opened"": True, ""synchronize"": True},
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""payload"": example_payload,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""payload"", example_payload),
                (""triggered_by_user"", example_payload[""sender""]),
                (""event"", example_payload[""action""]),
                (""number"", example_payload[""number""]),
                (""pull_request"", example_payload[""pull_request""]),
                (""pull_request_url"", example_payload[""pull_request""][""html_url""]),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:  # type: ignore
        yield from super().run(input_data, **kwargs)
        yield ""event"", input_data.payload[""action""]
        yield ""number"", input_data.payload[""number""]
        yield ""pull_request"", input_data.payload[""pull_request""]
        yield ""pull_request_url"", input_data.payload[""pull_request""][""html_url""]


# --8<-- [end:GithubTriggerExample]
","import json
import logging
from pathlib import Path

from pydantic import BaseModel
from backend.data.block import (
    Block,
    BlockCategory,
    BlockOutput,
    BlockSchema,
    BlockWebhookConfig,
)
from backend.data.model import SchemaField
from ._auth import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GithubCredentialsField,
    GithubCredentialsInput,
)

logger = logging.getLogger(__name__)

class GitHubTriggerBase:
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo: str = SchemaField(
            description=(
                ""Repository to subscribe to.\n\n""
                ""**Note:** Make sure your GitHub credentials have permissions ""
                ""to create webhooks on this repo.""
            ),
            placeholder=""{owner}/{repo}"",
        )
        # --8<-- [start:example-payload-field]
        payload: dict = SchemaField(hidden=True, default={})
        # --8<-- [end:example-payload-field]

    class Output(BlockSchema):
        payload: dict = SchemaField(
            description=""The complete webhook payload that was received from GitHub. ""
            ""Includes information about the affected resource (e.g. pull request), ""
            ""the event, and the user who triggered the event.""
        )
        triggered_by_user: dict = SchemaField(
            description=""Object representing the GitHub user who triggered the event""
        )
        error: str = SchemaField(
            description=""Error message if the payload could not be processed""
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""payload"", input_data.payload
        yield ""triggered_by_user"", input_data.payload[""sender""]


class GithubPullRequestTriggerBlock(GitHubTriggerBase, Block):
    EXAMPLE_PAYLOAD_FILE = (
        Path(__file__).parent / ""example_payloads"" / ""pull_request.synchronize.json""
    )

    # --8<-- [start:example-event-filter]
    class Input(GitHubTriggerBase.Input):
        class EventsFilter(BaseModel):
            """"""
            https://docs.github.com/en/webhooks/webhook-events-and-payloads#pull_request
            """"""

            opened: bool = False
            edited: bool = False
            closed: bool = False
            reopened: bool = False
            synchronize: bool = False
            assigned: bool = False
            unassigned: bool = False
            labeled: bool = False
            unlabeled: bool = False
            converted_to_draft: bool = False
            locked: bool = False
            unlocked: bool = False
            enqueued: bool = False
            dequeued: bool = False
            milestoned: bool = False
            demilestoned: bool = False
            ready_for_review: bool = False
            review_requested: bool = False
            review_request_removed: bool = False
            auto_merge_enabled: bool = False
            auto_merge_disabled: bool = False

        events: EventsFilter = SchemaField(
            title=""Events"", description=""The events to subscribe to""
        )
        # --8<-- [end:example-event-filter]

    class Output(GitHubTriggerBase.Output):
        event: str = SchemaField(
            description=""The PR event that triggered the webhook (e.g. 'opened')""
        )
        number: int = SchemaField(description=""The number of the affected pull request"")
        pull_request: dict = SchemaField(
            description=""Object representing the affected pull request""
        )
        pull_request_url: str = SchemaField(
            description=""The URL of the affected pull request""
        )

    def __init__(self):
        from backend.integrations.webhooks.github import GithubWebhookType

        example_payload = json.loads(
            self.EXAMPLE_PAYLOAD_FILE.read_text(encoding=""utf-8"")
        )

        super().__init__(
            id=""6c60ec01-8128-419e-988f-96a063ee2fea"",
            description=""This block triggers on pull request events and outputs the event type and payload."",
            categories={BlockCategory.DEVELOPER_TOOLS, BlockCategory.INPUT},
            input_schema=GithubPullRequestTriggerBlock.Input,
            output_schema=GithubPullRequestTriggerBlock.Output,
            # --8<-- [start:example-webhook_config]
            webhook_config=BlockWebhookConfig(
                provider=""github"",
                webhook_type=GithubWebhookType.REPO,
                resource_format=""{repo}"",
                event_filter_input=""events"",
                event_format=""pull_request.{event}"",
            ),
            # --8<-- [end:example-webhook_config]
            test_input={
                ""repo"": ""Significant-Gravitas/AutoGPT"",
                ""events"": {""opened"": True, ""synchronize"": True},
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""payload"": example_payload,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""payload"", example_payload),
                (""triggered_by_user"", example_payload[""sender""]),
                (""event"", example_payload[""action""]),
                (""number"", example_payload[""number""]),
                (""pull_request"", example_payload[""pull_request""]),
                (""pull_request_url"", example_payload[""pull_request""][""html_url""]),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:  # type: ignore
        yield from super().run(input_data, **kwargs)
        yield ""event"", input_data.payload[""action""]
        yield ""number"", input_data.payload[""number""]
        yield ""pull_request"", input_data.payload[""pull_request""]
        yield ""pull_request_url"", input_data.payload[""pull_request""][""html_url""]","{'final_score': 99.27, 'length_similarity': 98.72, 'token_similarity': 99.17, 'structure_similarity': 98.55, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
135,"# -*- coding: utf-8 -*-

from mrjob.job import MRJob


class HitCounts(MRJob):

    def extract_url(self, line):
        """"""Extract the generated url from the log line.""""""
        pass

    def extract_year_month(self, line):
        """"""Return the year and month portions of the timestamp.""""""
        pass

    def mapper(self, _, line):
        """"""Parse each log line, extract and transform relevant lines.

        Emit key value pairs of the form:

        (2016-01, url0), 1
        (2016-01, url0), 1
        (2016-01, url1), 1
        """"""
        url = self.extract_url(line)
        period = self.extract_year_month(line)
        yield (period, url), 1

    def reducer(self, key, values):
        """"""Sum values for each key.

        (2016-01, url0), 2
        (2016-01, url1), 1
        """"""
        yield key, sum(values)

    def steps(self):
        """"""Run the map and reduce steps.""""""
        return [
            self.mr(mapper=self.mapper,
                    reducer=self.reducer)
        ]


if __name__ == '__main__':
    HitCounts.run()
","# -*- coding: utf-8 -*-
from mrjob.job import MRJob

class HitCounts(MRJob):

    def extract_url(self, line):
        """"""Extract the generated url from the log line.""""""
        pass

    def extract_year_month(self, line):
        """"""Return the year and month portions of the timestamp.""""""
        pass

    def mapper(self, _, line):
        """"""Parse each log line, extract and transform relevant lines.

        Emit key value pairs of the form:

        (2016-01, url0), 1
        (2016-01, url0), 1
        (2016-01, url1), 1
        """"""
        url = self.extract_url(line)
        period = self.extract_year_month(line)
        yield (period, url), 1

    def reducer(self, key, values):
        """"""Sum values for each key.

        (2016-01, url0), 2
        (2016-01, url1), 1
        """"""
        yield key, sum(values)

    def steps(self):
        """"""Run the map and reduce steps.""""""
        return [
            self.mr(mapper=self.mapper,
                    reducer=self.reducer)
        ]

if __name__ == '__main__':
    HitCounts.run()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
136,"import logging

import requests
from fastapi import Request

from backend.data import integrations
from backend.data.model import APIKeyCredentials, Credentials
from backend.integrations.providers import ProviderName
from backend.integrations.webhooks._base import BaseWebhooksManager

logger = logging.getLogger(__name__)


class Slant3DWebhooksManager(BaseWebhooksManager):
    """"""Manager for Slant3D webhooks""""""

    PROVIDER_NAME = ProviderName.SLANT3D
    BASE_URL = ""https://www.slant3dapi.com/api""

    async def _register_webhook(
        self,
        credentials: Credentials,
        webhook_type: str,
        resource: str,
        events: list[str],
        ingress_url: str,
        secret: str,
    ) -> tuple[str, dict]:
        """"""Register a new webhook with Slant3D""""""

        if not isinstance(credentials, APIKeyCredentials):
            raise ValueError(""API key is required to register a webhook"")

        headers = {
            ""api-key"": credentials.api_key.get_secret_value(),
            ""Content-Type"": ""application/json"",
        }

        # Slant3D's API doesn't use events list, just register for all order updates
        payload = {""endPoint"": ingress_url}

        response = requests.post(
            f""{self.BASE_URL}/customer/webhookSubscribe"", headers=headers, json=payload
        )

        if not response.ok:
            error = response.json().get(""error"", ""Unknown error"")
            raise RuntimeError(f""Failed to register webhook: {error}"")

        webhook_config = {
            ""endpoint"": ingress_url,
            ""provider"": self.PROVIDER_NAME,
            ""events"": [""order.shipped""],  # Currently the only supported event
            ""type"": webhook_type,
        }

        return """", webhook_config

    @classmethod
    async def validate_payload(
        cls, webhook: integrations.Webhook, request: Request
    ) -> tuple[dict, str]:
        """"""Validate incoming webhook payload from Slant3D""""""

        payload = await request.json()

        # Validate required fields from Slant3D API spec
        required_fields = [""orderId"", ""status"", ""trackingNumber"", ""carrierCode""]
        missing_fields = [field for field in required_fields if field not in payload]

        if missing_fields:
            raise ValueError(f""Missing required fields: {', '.join(missing_fields)}"")

        # Normalize payload structure
        normalized_payload = {
            ""orderId"": payload[""orderId""],
            ""status"": payload[""status""],
            ""trackingNumber"": payload[""trackingNumber""],
            ""carrierCode"": payload[""carrierCode""],
        }

        # Currently Slant3D only sends shipping notifications
        # Convert status to lowercase for event format compatibility
        event_type = f""order.{payload['status'].lower()}""

        return normalized_payload, event_type

    async def _deregister_webhook(
        self, webhook: integrations.Webhook, credentials: Credentials
    ) -> None:
        """"""
        Note: Slant3D API currently doesn't provide a deregistration endpoint.
        This would need to be handled through support.
        """"""
        # Log warning since we can't properly deregister
        logger.warning(
            f""Warning: Manual deregistration required for webhook {webhook.id}""
        )
        pass
","import logging
import requests
from fastapi import Request
from backend.data import integrations
from backend.data.model import APIKeyCredentials, Credentials
from backend.integrations.providers import ProviderName
from backend.integrations.webhooks._base import BaseWebhooksManager

logger = logging.getLogger(__name__)

class Slant3DWebhooksManager(BaseWebhooksManager):
    """"""Manager for Slant3D webhooks""""""

    PROVIDER_NAME = ProviderName.SLANT3D
    BASE_URL = ""https://www.slant3dapi.com/api""

    async def _register_webhook(
        self,
        credentials: Credentials,
        webhook_type: str,
        resource: str,
        events: list[str],
        ingress_url: str,
        secret: str,
    ) -> tuple[str, dict]:
        """"""Register a new webhook with Slant3D""""""

        if not isinstance(credentials, APIKeyCredentials):
            raise ValueError(""API key is required to register a webhook"")

        headers = {
            ""api-key"": credentials.api_key.get_secret_value(),
            ""Content-Type"": ""application/json"",
        }

        # Slant3D's API doesn't use events list, just register for all order updates
        payload = {""endPoint"": ingress_url}

        response = requests.post(
            f""{self.BASE_URL}/customer/webhookSubscribe"", headers=headers, json=payload
        )

        if not response.ok:
            error = response.json().get(""error"", ""Unknown error"")
            raise RuntimeError(f""Failed to register webhook: {error}"")

        webhook_config = {
            ""endpoint"": ingress_url,
            ""provider"": self.PROVIDER_NAME,
            ""events"": [""order.shipped""],  # Currently the only supported event
            ""type"": webhook_type,
        }

        return """", webhook_config

    @classmethod
    async def validate_payload(
        cls, webhook: integrations.Webhook, request: Request
    ) -> tuple[dict, str]:
        """"""Validate incoming webhook payload from Slant3D""""""

        payload = await request.json()

        # Validate required fields from Slant3D API spec
        required_fields = [""orderId"", ""status"", ""trackingNumber"", ""carrierCode""]
        missing_fields = [field for field in required_fields if field not in payload]

        if missing_fields:
            raise ValueError(f""Missing required fields: {', '.join(missing_fields)}"")

        # Normalize payload structure
        normalized_payload = {
            ""orderId"": payload[""orderId""],
            ""status"": payload[""status""],
            ""trackingNumber"": payload[""trackingNumber""],
            ""carrierCode"": payload[""carrierCode""],
        }

        # Currently Slant3D only sends shipping notifications
        # Convert status to lowercase for event format compatibility
        event_type = f""order.{payload['status'].lower()}""

        return normalized_payload, event_type

    async def _deregister_webhook(
        self, webhook: integrations.Webhook, credentials: Credentials
    ) -> None:
        """"""
        Note: Slant3D API currently doesn't provide a deregistration endpoint.
        This would need to be handled through support.
        """"""
        # Log warning since we can't properly deregister
        logger.warning(
            f""Warning: Manual deregistration required for webhook {webhook.id}""
        )
        pass","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
137,"import logging
import os
import uuid

import fastapi
from google.cloud import storage

import backend.server.v2.store.exceptions
from backend.util.settings import Settings

logger = logging.getLogger(__name__)

ALLOWED_IMAGE_TYPES = {""image/jpeg"", ""image/png"", ""image/gif"", ""image/webp""}
ALLOWED_VIDEO_TYPES = {""video/mp4"", ""video/webm""}
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB


async def check_media_exists(user_id: str, filename: str) -> str | None:
    """"""
    Check if a media file exists in storage for the given user.
    Tries both images and videos directories.

    Args:
        user_id (str): ID of the user who uploaded the file
        filename (str): Name of the file to check

    Returns:
        str | None: URL of the blob if it exists, None otherwise
    """"""
    try:
        settings = Settings()
        storage_client = storage.Client()
        bucket = storage_client.bucket(settings.config.media_gcs_bucket_name)

        # Check images
        image_path = f""users/{user_id}/images/{filename}""
        image_blob = bucket.blob(image_path)
        if image_blob.exists():
            return image_blob.public_url

        # Check videos
        video_path = f""users/{user_id}/videos/{filename}""

        video_blob = bucket.blob(video_path)
        if video_blob.exists():
            return video_blob.public_url

        return None
    except Exception as e:
        logger.error(f""Error checking if media file exists: {str(e)}"")
        return None


async def upload_media(
    user_id: str, file: fastapi.UploadFile, use_file_name: bool = False
) -> str:

    # Get file content for deeper validation
    try:
        content = await file.read(1024)  # Read first 1KB for validation
        await file.seek(0)  # Reset file pointer
    except Exception as e:
        logger.error(f""Error reading file content: {str(e)}"")
        raise backend.server.v2.store.exceptions.FileReadError(
            ""Failed to read file content""
        ) from e

    # Validate file signature/magic bytes
    if file.content_type in ALLOWED_IMAGE_TYPES:
        # Check image file signatures
        if content.startswith(b""\xFF\xD8\xFF""):  # JPEG
            if file.content_type != ""image/jpeg"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        elif content.startswith(b""\x89PNG\r\n\x1a\n""):  # PNG
            if file.content_type != ""image/png"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        elif content.startswith(b""GIF87a"") or content.startswith(b""GIF89a""):  # GIF
            if file.content_type != ""image/gif"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        elif content.startswith(b""RIFF"") and content[8:12] == b""WEBP"":  # WebP
            if file.content_type != ""image/webp"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        else:
            raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                ""Invalid image file signature""
            )

    elif file.content_type in ALLOWED_VIDEO_TYPES:
        # Check video file signatures
        if content.startswith(b""\x00\x00\x00"") and (content[4:8] == b""ftyp""):  # MP4
            if file.content_type != ""video/mp4"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        elif content.startswith(b""\x1a\x45\xdf\xa3""):  # WebM
            if file.content_type != ""video/webm"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        else:
            raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                ""Invalid video file signature""
            )

    settings = Settings()

    # Check required settings first before doing any file processing
    if not settings.config.media_gcs_bucket_name:
        logger.error(""Missing GCS bucket name setting"")
        raise backend.server.v2.store.exceptions.StorageConfigError(
            ""Missing storage bucket configuration""
        )

    try:
        # Validate file type
        content_type = file.content_type
        if content_type is None:
            content_type = ""image/jpeg""

        if (
            content_type not in ALLOWED_IMAGE_TYPES
            and content_type not in ALLOWED_VIDEO_TYPES
        ):
            logger.warning(f""Invalid file type attempted: {content_type}"")
            raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                f""File type not supported. Must be jpeg, png, gif, webp, mp4 or webm. Content type: {content_type}""
            )

        # Validate file size
        file_size = 0
        chunk_size = 8192  # 8KB chunks

        try:
            while chunk := await file.read(chunk_size):
                file_size += len(chunk)
                if file_size > MAX_FILE_SIZE:
                    logger.warning(f""File size too large: {file_size} bytes"")
                    raise backend.server.v2.store.exceptions.FileSizeTooLargeError(
                        ""File too large. Maximum size is 50MB""
                    )
        except backend.server.v2.store.exceptions.FileSizeTooLargeError:
            raise
        except Exception as e:
            logger.error(f""Error reading file chunks: {str(e)}"")
            raise backend.server.v2.store.exceptions.FileReadError(
                ""Failed to read uploaded file""
            ) from e

        # Reset file pointer
        await file.seek(0)

        # Generate unique filename
        filename = file.filename or """"
        file_ext = os.path.splitext(filename)[1].lower()
        if use_file_name:
            unique_filename = filename
        else:
            unique_filename = f""{uuid.uuid4()}{file_ext}""

        # Construct storage path
        media_type = ""images"" if content_type in ALLOWED_IMAGE_TYPES else ""videos""
        storage_path = f""users/{user_id}/{media_type}/{unique_filename}""

        try:
            storage_client = storage.Client()
            bucket = storage_client.bucket(settings.config.media_gcs_bucket_name)
            blob = bucket.blob(storage_path)
            blob.content_type = content_type

            file_bytes = await file.read()
            blob.upload_from_string(file_bytes, content_type=content_type)

            public_url = blob.public_url

            logger.info(f""Successfully uploaded file to: {storage_path}"")
            return public_url

        except Exception as e:
            logger.error(f""GCS storage error: {str(e)}"")
            raise backend.server.v2.store.exceptions.StorageUploadError(
                ""Failed to upload file to storage""
            ) from e

    except backend.server.v2.store.exceptions.MediaUploadError:
        raise
    except Exception as e:
        logger.exception(""Unexpected error in upload_media"")
        raise backend.server.v2.store.exceptions.MediaUploadError(
            ""Unexpected error during media upload""
        ) from e
","import logging
import os
import uuid

import fastapi
from google.cloud import storage

import backend.server.v2.store.exceptions
from backend.util.settings import Settings

logger = logging.getLogger(__name__)

ALLOWED_IMAGE_TYPES = {""image/jpeg"", ""image/png"", ""image/gif"", ""image/webp""}
ALLOWED_VIDEO_TYPES = {""video/mp4"", ""video/webm""}
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

async def check_media_exists(user_id: str, filename: str) -> str | None:
    """"""
    Check if a media file exists in storage for the given user.
    Tries both images and videos directories.

    Args:
        user_id (str): ID of the user who uploaded the file
        filename (str): Name of the file to check

    Returns:
        str | None: URL of the blob if it exists, None otherwise
    """"""
    try:
        settings = Settings()
        storage_client = storage.Client()
        bucket = storage_client.bucket(settings.config.media_gcs_bucket_name)

        # Check images
        image_path = f""users/{user_id}/images/{filename}""
        image_blob = bucket.blob(image_path)
        if image_blob.exists():
            return image_blob.public_url

        # Check videos
        video_path = f""users/{user_id}/videos/{filename}""
        video_blob = bucket.blob(video_path)
        if video_blob.exists():
            return video_blob.public_url

        return None
    except Exception as e:
        logger.error(f""Error checking if media file exists: {str(e)}"")
        return None

async def upload_media(user_id: str, file: fastapi.UploadFile, use_file_name: bool = False) -> str:
    # Get file content for deeper validation
    try:
        content = await file.read(1024)  # Read first 1KB for validation
        await file.seek(0)  # Reset file pointer
    except Exception as e:
        logger.error(f""Error reading file content: {str(e)}"")
        raise backend.server.v2.store.exceptions.FileReadError(
            ""Failed to read file content""
        ) from e

    # Validate file signature/magic bytes
    if file.content_type in ALLOWED_IMAGE_TYPES:
        # Check image file signatures
        if content.startswith(b""\xFF\xD8\xFF""):  # JPEG
            if file.content_type != ""image/jpeg"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        elif content.startswith(b""\x89PNG\r\n\x1a\n""):  # PNG
            if file.content_type != ""image/png"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        elif content.startswith(b""GIF87a"") or content.startswith(b""GIF89a""):  # GIF
            if file.content_type != ""image/gif"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        elif content.startswith(b""RIFF"") and content[8:12] == b""WEBP"":  # WebP
            if file.content_type != ""image/webp"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        else:
            raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                ""Invalid image file signature""
            )
    elif file.content_type in ALLOWED_VIDEO_TYPES:
        # Check video file signatures
        if content.startswith(b""\x00\x00\x00"") and (content[4:8] == b""ftyp""):  # MP4
            if file.content_type != ""video/mp4"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        elif content.startswith(b""\x1a\x45\xdf\xa3""):  # WebM
            if file.content_type != ""video/webm"":
                raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                    ""File signature does not match content type""
                )
        else:
            raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                ""Invalid video file signature""
            )

    settings = Settings()

    # Check required settings first before doing any file processing
    if not settings.config.media_gcs_bucket_name:
        logger.error(""Missing GCS bucket name setting"")
        raise backend.server.v2.store.exceptions.StorageConfigError(
            ""Missing storage bucket configuration""
        )

    try:
        # Validate file type
        content_type = file.content_type
        if content_type is None:
            content_type = ""image/jpeg""

        if (
            content_type not in ALLOWED_IMAGE_TYPES
            and content_type not in ALLOWED_VIDEO_TYPES
        ):
            logger.warning(f""Invalid file type attempted: {content_type}"")
            raise backend.server.v2.store.exceptions.InvalidFileTypeError(
                f""File type not supported. Must be jpeg, png, gif, webp, mp4 or webm. Content type: {content_type}""
            )

        # Validate file size
        file_size = 0
        chunk_size = 8192  # 8KB chunks

        try:
            while chunk := await file.read(chunk_size):
                file_size += len(chunk)
                if file_size > MAX_FILE_SIZE:
                    logger.warning(f""File size too large: {file_size} bytes"")
                    raise backend.server.v2.store.exceptions.FileSizeTooLargeError(
                        ""File too large. Maximum size is 50MB""
                    )
        except backend.server.v2.store.exceptions.FileSizeTooLargeError:
            raise
        except Exception as e:
            logger.error(f""Error reading file chunks: {str(e)}"")
            raise backend.server.v2.store.exceptions.FileReadError(
                ""Failed to read uploaded file""
            ) from e

        # Reset file pointer
        await file.seek(0)

        # Generate unique filename
        filename = file.filename or """"
        file_ext = os.path.splitext(filename)[1].lower()
        if use_file_name:
            unique_filename = filename
        else:
            unique_filename = f""{uuid.uuid4()}{file_ext}""

        # Construct storage path
        media_type = ""images"" if content_type in ALLOWED_IMAGE_TYPES else ""videos""
        storage_path = f""users/{user_id}/{media_type}/{unique_filename}""

        try:
            storage_client = storage.Client()
            bucket = storage_client.bucket(settings.config.media_gcs_bucket_name)
            blob = bucket.blob(storage_path)
            blob.content_type = content_type

            file_bytes = await file.read()
            blob.upload_from_string(file_bytes, content_type=content_type)

            public_url = blob.public_url

            logger.info(f""Successfully uploaded file to: {storage_path}"")
            return public_url
        except Exception as e:
            logger.error(f""GCS storage error: {str(e)}"")
            raise backend.server.v2.store.exceptions.StorageUploadError(
                ""Failed to upload file to storage""
            ) from e
    except backend.server.v2.store.exceptions.MediaUploadError:
        raise
    except Exception as e:
        logger.exception(""Unexpected error in upload_media"")
        raise backend.server.v2.store.exceptions.MediaUploadError(
            ""Unexpected error during media upload""
        ) from e","{'final_score': 99.75, 'length_similarity': 99.92, 'token_similarity': 100.0, 'structure_similarity': 98.8, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
138,"import time
from typing import Literal

from pydantic import SecretStr

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName
from backend.util.request import requests

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""d_id"",
    api_key=SecretStr(""mock-d-id-api-key""),
    title=""Mock D-ID API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class CreateTalkingAvatarVideoBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.D_ID], Literal[""api_key""]
        ] = CredentialsField(
            description=""The D-ID integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        script_input: str = SchemaField(
            description=""The text input for the script"",
            placeholder=""Welcome to AutoGPT"",
        )
        provider: Literal[""microsoft"", ""elevenlabs"", ""amazon""] = SchemaField(
            description=""The voice provider to use"", default=""microsoft""
        )
        voice_id: str = SchemaField(
            description=""The voice ID to use, get list of voices [here](https://docs.agpt.co/server/d_id)"",
            default=""en-US-JennyNeural"",
        )
        presenter_id: str = SchemaField(
            description=""The presenter ID to use"", default=""amy-Aq6OmGZnMt""
        )
        driver_id: str = SchemaField(
            description=""The driver ID to use"", default=""Vcq0R4a8F0""
        )
        result_format: Literal[""mp4"", ""gif"", ""wav""] = SchemaField(
            description=""The desired result format"", default=""mp4""
        )
        crop_type: Literal[""wide"", ""square"", ""vertical""] = SchemaField(
            description=""The crop type for the presenter"", default=""wide""
        )
        subtitles: bool = SchemaField(
            description=""Whether to include subtitles"", default=False
        )
        ssml: bool = SchemaField(description=""Whether the input is SSML"", default=False)
        max_polling_attempts: int = SchemaField(
            description=""Maximum number of polling attempts"", default=30, ge=5
        )
        polling_interval: int = SchemaField(
            description=""Interval between polling attempts in seconds"", default=10, ge=5
        )

    class Output(BlockSchema):
        video_url: str = SchemaField(description=""The URL of the created video"")
        error: str = SchemaField(description=""Error message if the request failed"")

    def __init__(self):
        super().__init__(
            id=""98c6f503-8c47-4b1c-a96d-351fc7c87dab"",
            description=""This block integrates with D-ID to create video clips and retrieve their URLs."",
            categories={BlockCategory.AI},
            input_schema=CreateTalkingAvatarVideoBlock.Input,
            output_schema=CreateTalkingAvatarVideoBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""script_input"": ""Welcome to AutoGPT"",
                ""voice_id"": ""en-US-JennyNeural"",
                ""presenter_id"": ""amy-Aq6OmGZnMt"",
                ""driver_id"": ""Vcq0R4a8F0"",
                ""result_format"": ""mp4"",
                ""crop_type"": ""wide"",
                ""subtitles"": False,
                ""ssml"": False,
                ""max_polling_attempts"": 5,
                ""polling_interval"": 5,
            },
            test_output=[
                (
                    ""video_url"",
                    ""https://d-id.com/api/clips/abcd1234-5678-efgh-ijkl-mnopqrstuvwx/video"",
                ),
            ],
            test_mock={
                ""create_clip"": lambda *args, **kwargs: {
                    ""id"": ""abcd1234-5678-efgh-ijkl-mnopqrstuvwx"",
                    ""status"": ""created"",
                },
                ""get_clip_status"": lambda *args, **kwargs: {
                    ""status"": ""done"",
                    ""result_url"": ""https://d-id.com/api/clips/abcd1234-5678-efgh-ijkl-mnopqrstuvwx/video"",
                },
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def create_clip(self, api_key: SecretStr, payload: dict) -> dict:
        url = ""https://api.d-id.com/clips""
        headers = {
            ""accept"": ""application/json"",
            ""content-type"": ""application/json"",
            ""authorization"": f""Basic {api_key.get_secret_value()}"",
        }
        response = requests.post(url, json=payload, headers=headers)
        return response.json()

    def get_clip_status(self, api_key: SecretStr, clip_id: str) -> dict:
        url = f""https://api.d-id.com/clips/{clip_id}""
        headers = {
            ""accept"": ""application/json"",
            ""authorization"": f""Basic {api_key.get_secret_value()}"",
        }
        response = requests.get(url, headers=headers)
        return response.json()

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # Create the clip
        payload = {
            ""script"": {
                ""type"": ""text"",
                ""subtitles"": str(input_data.subtitles).lower(),
                ""provider"": {
                    ""type"": input_data.provider,
                    ""voice_id"": input_data.voice_id,
                },
                ""ssml"": str(input_data.ssml).lower(),
                ""input"": input_data.script_input,
            },
            ""config"": {""result_format"": input_data.result_format},
            ""presenter_config"": {""crop"": {""type"": input_data.crop_type}},
            ""presenter_id"": input_data.presenter_id,
            ""driver_id"": input_data.driver_id,
        }

        response = self.create_clip(credentials.api_key, payload)
        clip_id = response[""id""]

        # Poll for clip status
        for _ in range(input_data.max_polling_attempts):
            status_response = self.get_clip_status(credentials.api_key, clip_id)
            if status_response[""status""] == ""done"":
                yield ""video_url"", status_response[""result_url""]
                return
            elif status_response[""status""] == ""error"":
                raise RuntimeError(
                    f""Clip creation failed: {status_response.get('error', 'Unknown error')}""
                )

            time.sleep(input_data.polling_interval)

        raise TimeoutError(""Clip creation timed out"")
","import time
from typing import Literal
from pydantic import SecretStr
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName
from backend.util.request import requests

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""d_id"",
    api_key=SecretStr(""mock-d-id-api-key""),
    title=""Mock D-ID API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


class CreateTalkingAvatarVideoBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.D_ID], Literal[""api_key""]
        ] = CredentialsField(
            description=""The D-ID integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        script_input: str = SchemaField(
            description=""The text input for the script"",
            placeholder=""Welcome to AutoGPT"",
        )
        provider: Literal[""microsoft"", ""elevenlabs"", ""amazon""] = SchemaField(
            description=""The voice provider to use"", default=""microsoft""
        )
        voice_id: str = SchemaField(
            description=""The voice ID to use, get list of voices [here](https://docs.agpt.co/server/d_id)"",
            default=""en-US-JennyNeural"",
        )
        presenter_id: str = SchemaField(
            description=""The presenter ID to use"", default=""amy-Aq6OmGZnMt""
        )
        driver_id: str = SchemaField(
            description=""The driver ID to use"", default=""Vcq0R4a8F0""
        )
        result_format: Literal[""mp4"", ""gif"", ""wav""] = SchemaField(
            description=""The desired result format"", default=""mp4""
        )
        crop_type: Literal[""wide"", ""square"", ""vertical""] = SchemaField(
            description=""The crop type for the presenter"", default=""wide""
        )
        subtitles: bool = SchemaField(
            description=""Whether to include subtitles"", default=False
        )
        ssml: bool = SchemaField(description=""Whether the input is SSML"", default=False)
        max_polling_attempts: int = SchemaField(
            description=""Maximum number of polling attempts"", default=30, ge=5
        )
        polling_interval: int = SchemaField(
            description=""Interval between polling attempts in seconds"", default=10, ge=5
        )

    class Output(BlockSchema):
        video_url: str = SchemaField(description=""The URL of the created video"")
        error: str = SchemaField(description=""Error message if the request failed"")

    def __init__(self):
        super().__init__(
            id=""98c6f503-8c47-4b1c-a96d-351fc7c87dab"",
            description=""This block integrates with D-ID to create video clips and retrieve their URLs."",
            categories={BlockCategory.AI},
            input_schema=CreateTalkingAvatarVideoBlock.Input,
            output_schema=CreateTalkingAvatarVideoBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""script_input"": ""Welcome to AutoGPT"",
                ""voice_id"": ""en-US-JennyNeural"",
                ""presenter_id"": ""amy-Aq6OmGZnMt"",
                ""driver_id"": ""Vcq0R4a8F0"",
                ""result_format"": ""mp4"",
                ""crop_type"": ""wide"",
                ""subtitles"": False,
                ""ssml"": False,
                ""max_polling_attempts"": 5,
                ""polling_interval"": 5,
            },
            test_output=[
                (
                    ""video_url"",
                    ""https://d-id.com/api/clips/abcd1234-5678-efgh-ijkl-mnopqrstuvwx/video"",
                ),
            ],
            test_mock={
                ""create_clip"": lambda *args, **kwargs: {
                    ""id"": ""abcd1234-5678-efgh-ijkl-mnopqrstuvwx"",
                    ""status"": ""created"",
                },
                ""get_clip_status"": lambda *args, **kwargs: {
                    ""status"": ""done"",
                    ""result_url"": ""https://d-id.com/api/clips/abcd1234-5678-efgh-ijkl-mnopqrstuvwx/video"",
                },
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def create_clip(self, api_key: SecretStr, payload: dict) -> dict:
        url = ""https://api.d-id.com/clips""
        headers = {
            ""accept"": ""application/json"",
            ""content-type"": ""application/json"",
            ""authorization"": f""Basic {api_key.get_secret_value()}"",
        }
        response = requests.post(url, json=payload, headers=headers)
        return response.json()

    def get_clip_status(self, api_key: SecretStr, clip_id: str) -> dict:
        url = f""https://api.d-id.com/clips/{clip_id}""
        headers = {
            ""accept"": ""application/json"",
            ""authorization"": f""Basic {api_key.get_secret_value()}"",
        }
        response = requests.get(url, headers=headers)
        return response.json()

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # Create the clip
        payload = {
            ""script"": {
                ""type"": ""text"",
                ""subtitles"": str(input_data.subtitles).lower(),
                ""provider"": {
                    ""type"": input_data.provider,
                    ""voice_id"": input_data.voice_id,
                },
                ""ssml"": str(input_data.ssml).lower(),
                ""input"": input_data.script_input,
            },
            ""config"": {""result_format"": input_data.result_format},
            ""presenter_config"": {""crop"": {""type"": input_data.crop_type}},
            ""presenter_id"": input_data.presenter_id,
            ""driver_id"": input_data.driver_id,
        }
        response = self.create_clip(credentials.api_key, payload)
        clip_id = response[""id""]

        # Poll for clip status
        for _ in range(input_data.max_polling_attempts):
            status_response = self.get_clip_status(credentials.api_key, clip_id)
            if status_response[""status""] == ""done"":
                yield ""video_url"", status_response[""result_url""]
                return
            elif status_response[""status""] == ""error"":
                raise RuntimeError(
                    f""Clip creation failed: {status_response.get('error', 'Unknown error')}""
                )

            time.sleep(input_data.polling_interval)

        raise TimeoutError(""Clip creation timed out"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
139,"from pydantic import BaseModel

from backend.data.block import (
    Block,
    BlockCategory,
    BlockOutput,
    BlockSchema,
    BlockWebhookConfig,
)
from backend.data.model import SchemaField
from backend.util import settings
from backend.util.settings import AppEnvironment, BehaveAs

from ._api import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    Slant3DCredentialsField,
    Slant3DCredentialsInput,
)


class Slant3DTriggerBase:
    """"""Base class for Slant3D webhook triggers""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        # Webhook URL is handled by the webhook system
        payload: dict = SchemaField(hidden=True, default={})

    class Output(BlockSchema):
        payload: dict = SchemaField(
            description=""The complete webhook payload received from Slant3D""
        )
        order_id: str = SchemaField(description=""The ID of the affected order"")
        error: str = SchemaField(
            description=""Error message if payload processing failed""
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""payload"", input_data.payload
        yield ""order_id"", input_data.payload[""orderId""]


class Slant3DOrderWebhookBlock(Slant3DTriggerBase, Block):
    """"""Block for handling Slant3D order webhooks""""""

    class Input(Slant3DTriggerBase.Input):
        class EventsFilter(BaseModel):
            """"""
            Currently Slant3D only supports 'SHIPPED' status updates
            Could be expanded in the future with more status types
            """"""

            shipped: bool = True

        events: EventsFilter = SchemaField(
            title=""Events"",
            description=""Order status events to subscribe to"",
            default=EventsFilter(shipped=True),
        )

    class Output(Slant3DTriggerBase.Output):
        status: str = SchemaField(description=""The new status of the order"")
        tracking_number: str = SchemaField(
            description=""The tracking number for the shipment""
        )
        carrier_code: str = SchemaField(description=""The carrier code (e.g., 'usps')"")

    def __init__(self):
        super().__init__(
            id=""8a74c2ad-0104-4640-962f-26c6b69e58cd"",
            description=(
                ""This block triggers on Slant3D order status updates and outputs ""
                ""the event details, including tracking information when orders are shipped.""
            ),
            # All webhooks are currently subscribed to for all orders. This works for self hosted, but not for cloud hosted prod
            disabled=(
                settings.Settings().config.behave_as == BehaveAs.CLOUD
                and settings.Settings().config.app_env != AppEnvironment.LOCAL
            ),
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=self.Input,
            output_schema=self.Output,
            webhook_config=BlockWebhookConfig(
                provider=""slant3d"",
                webhook_type=""orders"",  # Only one type for now
                resource_format="""",  # No resource format needed
                event_filter_input=""events"",
                event_format=""order.{event}"",
            ),
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""events"": {""shipped"": True},
                ""payload"": {
                    ""orderId"": ""1234567890"",
                    ""status"": ""SHIPPED"",
                    ""trackingNumber"": ""ABCDEF123456"",
                    ""carrierCode"": ""usps"",
                },
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""payload"",
                    {
                        ""orderId"": ""1234567890"",
                        ""status"": ""SHIPPED"",
                        ""trackingNumber"": ""ABCDEF123456"",
                        ""carrierCode"": ""usps"",
                    },
                ),
                (""order_id"", ""1234567890""),
                (""status"", ""SHIPPED""),
                (""tracking_number"", ""ABCDEF123456""),
                (""carrier_code"", ""usps""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:  # type: ignore
        yield from super().run(input_data, **kwargs)

        # Extract and normalize values from the payload
        yield ""status"", input_data.payload[""status""]
        yield ""tracking_number"", input_data.payload[""trackingNumber""]
        yield ""carrier_code"", input_data.payload[""carrierCode""]
","from pydantic import BaseModel
from backend.data.block import (
    Block,
    BlockCategory,
    BlockOutput,
    BlockSchema,
    BlockWebhookConfig,
)
from backend.data.model import SchemaField
from backend.util import settings
from backend.util.settings import AppEnvironment, BehaveAs
from ._api import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    Slant3DCredentialsField,
    Slant3DCredentialsInput,
)

class Slant3DTriggerBase:
    """"""Base class for Slant3D webhook triggers""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()
        # Webhook URL is handled by the webhook system
        payload: dict = SchemaField(hidden=True, default={})

    class Output(BlockSchema):
        payload: dict = SchemaField(
            description=""The complete webhook payload received from Slant3D""
        )
        order_id: str = SchemaField(description=""The ID of the affected order"")
        error: str = SchemaField(
            description=""Error message if payload processing failed""
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""payload"", input_data.payload
        yield ""order_id"", input_data.payload[""orderId""]

class Slant3DOrderWebhookBlock(Slant3DTriggerBase, Block):
    """"""Block for handling Slant3D order webhooks""""""

    class Input(Slant3DTriggerBase.Input):
        class EventsFilter(BaseModel):
            """"""
            Currently Slant3D only supports 'SHIPPED' status updates
            Could be expanded in the future with more status types
            """"""

            shipped: bool = True

        events: EventsFilter = SchemaField(
            title=""Events"",
            description=""Order status events to subscribe to"",
            default=EventsFilter(shipped=True),
        )

    class Output(Slant3DTriggerBase.Output):
        status: str = SchemaField(description=""The new status of the order"")
        tracking_number: str = SchemaField(
            description=""The tracking number for the shipment""
        )
        carrier_code: str = SchemaField(description=""The carrier code (e.g., 'usps')"")

    def __init__(self):
        super().__init__(
            id=""8a74c2ad-0104-4640-962f-26c6b69e58cd"",
            description=(
                ""This block triggers on Slant3D order status updates and outputs ""
                ""the event details, including tracking information when orders are shipped.""
            ),
            # All webhooks are currently subscribed to for all orders. This works for self hosted, but not for cloud hosted prod
            disabled=(
                settings.Settings().config.behave_as == BehaveAs.CLOUD
                and settings.Settings().config.app_env != AppEnvironment.LOCAL
            ),
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=self.Input,
            output_schema=self.Output,
            webhook_config=BlockWebhookConfig(
                provider=""slant3d"",
                webhook_type=""orders"",  # Only one type for now
                resource_format="""",  # No resource format needed
                event_filter_input=""events"",
                event_format=""order.{event}"",
            ),
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""events"": {""shipped"": True},
                ""payload"": {
                    ""orderId"": ""1234567890"",
                    ""status"": ""SHIPPED"",
                    ""trackingNumber"": ""ABCDEF123456"",
                    ""carrierCode"": ""usps"",
                },
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""payload"",
                    {
                        ""orderId"": ""1234567890"",
                        ""status"": ""SHIPPED"",
                        ""trackingNumber"": ""ABCDEF123456"",
                        ""carrierCode"": ""usps"",
                    },
                ),
                (""order_id"", ""1234567890""),
                (""status"", ""SHIPPED""),
                (""tracking_number"", ""ABCDEF123456""),
                (""carrier_code"", ""usps""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:  # type: ignore
        yield from super().run(input_data, **kwargs)
        # Extract and normalize values from the payload
        yield ""status"", input_data.payload[""status""]
        yield ""tracking_number"", input_data.payload[""trackingNumber""]
        yield ""carrier_code"", input_data.payload[""carrierCode""]","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
140,"import json
from typing import Any, Type, TypeVar, cast, get_args, get_origin


class ConversionError(ValueError):
    pass


def __convert_list(value: Any) -> list:
    if isinstance(value, (list, tuple, set)):
        return list(value)
    elif isinstance(value, dict):
        return list(value.items())
    elif isinstance(value, str):
        value = value.strip()
        if value.startswith(""["") and value.endswith(""]""):
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return [value]
        else:
            return [value]
    else:
        return [value]


def __convert_dict(value: Any) -> dict:
    if isinstance(value, str):
        try:
            result = json.loads(value)
            if isinstance(result, dict):
                return result
            else:
                return {""value"": result}
        except json.JSONDecodeError:
            return {""value"": value}  # Fallback conversion
    elif isinstance(value, list):
        return {i: value[i] for i in range(len(value))}
    elif isinstance(value, tuple):
        return {i: value[i] for i in range(len(value))}
    elif isinstance(value, dict):
        return value
    else:
        return {""value"": value}


def __convert_tuple(value: Any) -> tuple:
    if isinstance(value, (str, list, set)):
        return tuple(value)
    elif isinstance(value, dict):
        return tuple(value.items())
    elif isinstance(value, (int, float, bool)):
        return (value,)
    elif isinstance(value, tuple):
        return value
    else:
        return (value,)


def __convert_set(value: Any) -> set:
    if isinstance(value, (str, list, tuple)):
        return set(value)
    elif isinstance(value, dict):
        return set(value.items())
    elif isinstance(value, set):
        return value
    else:
        return {value}


def __convert_str(value: Any) -> str:
    if isinstance(value, str):
        return value
    else:
        return json.dumps(value)


NUM = TypeVar(""NUM"", int, float)


def __convert_num(value: Any, num_type: Type[NUM]) -> NUM:
    if isinstance(value, (list, dict, tuple, set)):
        return num_type(len(value))
    elif isinstance(value, num_type):
        return value
    else:
        try:
            return num_type(float(value))
        except (ValueError, TypeError):
            return num_type(0)  # Fallback conversion


def __convert_bool(value: Any) -> bool:
    if isinstance(value, bool):
        return value
    elif isinstance(value, str):
        if value.lower() in [""true"", ""1""]:
            return True
        else:
            return False
    else:
        return bool(value)


def _try_convert(value: Any, target_type: Type, raise_on_mismatch: bool) -> Any:
    origin = get_origin(target_type)
    args = get_args(target_type)
    if origin is None:
        origin = target_type
    if origin not in [list, dict, tuple, str, set, int, float, bool]:
        return value

    # Handle the case when value is already of the target type
    if isinstance(value, origin):
        if not args:
            return value
        else:
            # Need to convert elements
            if origin is list:
                return [convert(v, args[0]) for v in value]
            elif origin is tuple:
                # Tuples can have multiple types
                if len(args) == 1:
                    return tuple(convert(v, args[0]) for v in value)
                else:
                    return tuple(convert(v, t) for v, t in zip(value, args))
            elif origin is dict:
                key_type, val_type = args
                return {
                    convert(k, key_type): convert(v, val_type) for k, v in value.items()
                }
            elif origin is set:
                return {convert(v, args[0]) for v in value}
            else:
                return value
    elif raise_on_mismatch:
        raise TypeError(f""Value {value} is not of expected type {target_type}"")
    else:
        # Need to convert value to the origin type
        if origin is list:
            value = __convert_list(value)
            if args:
                return [convert(v, args[0]) for v in value]
            else:
                return value
        elif origin is dict:
            value = __convert_dict(value)
            if args:
                key_type, val_type = args
                return {
                    convert(k, key_type): convert(v, val_type) for k, v in value.items()
                }
            else:
                return value
        elif origin is tuple:
            value = __convert_tuple(value)
            if args:
                if len(args) == 1:
                    return tuple(convert(v, args[0]) for v in value)
                else:
                    return tuple(convert(v, t) for v, t in zip(value, args))
            else:
                return value
        elif origin is str:
            return __convert_str(value)
        elif origin is set:
            value = __convert_set(value)
            if args:
                return {convert(v, args[0]) for v in value}
            else:
                return value
        elif origin is int:
            return __convert_num(value, int)
        elif origin is float:
            return __convert_num(value, float)
        elif origin is bool:
            return __convert_bool(value)
        else:
            return value


T = TypeVar(""T"")


def type_match(value: Any, target_type: Type[T]) -> T:
    return cast(T, _try_convert(value, target_type, raise_on_mismatch=True))


def convert(value: Any, target_type: Type[T]) -> T:
    try:
        return cast(T, _try_convert(value, target_type, raise_on_mismatch=False))
    except Exception as e:
        raise ConversionError(f""Failed to convert {value} to {target_type}"") from e
","import json
from typing import Any, Type, TypeVar, cast, get_args, get_origin

class ConversionError(ValueError):
    pass

def __convert_list(value: Any) -> list:
    if isinstance(value, (list, tuple, set)):
        return list(value)
    elif isinstance(value, dict):
        return list(value.items())
    elif isinstance(value, str):
        value = value.strip()
        if value.startswith(""["") and value.endswith(""]""):
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return [value]
        else:
            return [value]
    else:
        return [value]

def __convert_dict(value: Any) -> dict:
    if isinstance(value, str):
        try:
            result = json.loads(value)
            if isinstance(result, dict):
                return result
            else:
                return {""value"": result}
        except json.JSONDecodeError:
            return {""value"": value}  # Fallback conversion
    elif isinstance(value, list):
        return {i: value[i] for i in range(len(value))}
    elif isinstance(value, tuple):
        return {i: value[i] for i in range(len(value))}
    elif isinstance(value, dict):
        return value
    else:
        return {""value"": value}

def __convert_tuple(value: Any) -> tuple:
    if isinstance(value, (str, list, set)):
        return tuple(value)
    elif isinstance(value, dict):
        return tuple(value.items())
    elif isinstance(value, (int, float, bool)):
        return (value,)
    elif isinstance(value, tuple):
        return value
    else:
        return (value,)

def __convert_set(value: Any) -> set:
    if isinstance(value, (str, list, tuple)):
        return set(value)
    elif isinstance(value, dict):
        return set(value.items())
    elif isinstance(value, set):
        return value
    else:
        return {value}

def __convert_str(value: Any) -> str:
    if isinstance(value, str):
        return value
    else:
        return json.dumps(value)

NUM = TypeVar(""NUM"", int, float)

def __convert_num(value: Any, num_type: Type[NUM]) -> NUM:
    if isinstance(value, (list, dict, tuple, set)):
        return num_type(len(value))
    elif isinstance(value, num_type):
        return value
    else:
        try:
            return num_type(float(value))
        except (ValueError, TypeError):
            return num_type(0)  # Fallback conversion

def __convert_bool(value: Any) -> bool:
    if isinstance(value, bool):
        return value
    elif isinstance(value, str):
        if value.lower() in [""true"", ""1""]:
            return True
        else:
            return False
    else:
        return bool(value)

def _try_convert(value: Any, target_type: Type, raise_on_mismatch: bool) -> Any:
    origin = get_origin(target_type)
    args = get_args(target_type)
    if origin is None:
        origin = target_type
    if origin not in [list, dict, tuple, str, set, int, float, bool]:
        return value
    if isinstance(value, origin):
        if not args:
            return value
        else:
            # Need to convert elements
            if origin is list:
                return [convert(v, args[0]) for v in value]
            elif origin is tuple:
                # Tuples can have multiple types
                if len(args) == 1:
                    return tuple(convert(v, args[0]) for v in value)
                else:
                    return tuple(convert(v, t) for v, t in zip(value, args))
            elif origin is dict:
                key_type, val_type = args
                return {
                    convert(k, key_type): convert(v, val_type) for k, v in value.items()
                }
            elif origin is set:
                return {convert(v, args[0]) for v in value}
            else:
                return value
    elif raise_on_mismatch:
        raise TypeError(f""Value {value} is not of expected type {target_type}"")
    else:
        # Need to convert value to the origin type
        if origin is list:
            value = __convert_list(value)
            if args:
                return [convert(v, args[0]) for v in value]
            else:
                return value
        elif origin is dict:
            value = __convert_dict(value)
            if args:
                key_type, val_type = args
                return {
                    convert(k, key_type): convert(v, val_type) for k, v in value.items()
                }
            else:
                return value
        elif origin is tuple:
            value = __convert_tuple(value)
            if args:
                if len(args) == 1:
                    return tuple(convert(v, args[0]) for v in value)
                else:
                    return tuple(convert(v, t) for v, t in zip(value, args))
            else:
                return value
        elif origin is str:
            return __convert_str(value)
        elif origin is set:
            value = __convert_set(value)
            if args:
                return {convert(v, args[0]) for v in value}
            else:
                return value
        elif origin is int:
            return __convert_num(value, int)
        elif origin is float:
            return __convert_num(value, float)
        elif origin is bool:
            return __convert_bool(value)
        else:
            return value

T = TypeVar(""T"")

def type_match(value: Any, target_type: Type[T]) -> T:
    return cast(T, _try_convert(value, target_type, raise_on_mismatch=True))

def convert(value: Any, target_type: Type[T]) -> T:
    try:
        return cast(T, _try_convert(value, target_type, raise_on_mismatch=False))
    except Exception as e:
        raise ConversionError(f""Failed to convert {value} to {target_type}"") from e","{'final_score': 98.81, 'length_similarity': 98.92, 'token_similarity': 96.97, 'structure_similarity': 99.4, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
141,"import re

from typing_extensions import TypedDict

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

from ._api import get_api
from ._auth import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GithubCredentials,
    GithubCredentialsField,
    GithubCredentialsInput,
)


class GithubListPullRequestsBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class PRItem(TypedDict):
            title: str
            url: str

        pull_request: PRItem = SchemaField(
            title=""Pull Request"", description=""PRs with their title and URL""
        )
        error: str = SchemaField(description=""Error message if listing issues failed"")

    def __init__(self):
        super().__init__(
            id=""ffef3c4c-6cd0-48dd-817d-459f975219f4"",
            description=""This block lists all pull requests for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListPullRequestsBlock.Input,
            output_schema=GithubListPullRequestsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""pull_request"",
                    {
                        ""title"": ""Pull request 1"",
                        ""url"": ""https://github.com/owner/repo/pull/1"",
                    },
                )
            ],
            test_mock={
                ""list_prs"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Pull request 1"",
                        ""url"": ""https://github.com/owner/repo/pull/1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_prs(credentials: GithubCredentials, repo_url: str) -> list[Output.PRItem]:
        api = get_api(credentials)
        pulls_url = repo_url + ""/pulls""
        response = api.get(pulls_url)
        data = response.json()
        pull_requests: list[GithubListPullRequestsBlock.Output.PRItem] = [
            {""title"": pr[""title""], ""url"": pr[""html_url""]} for pr in data
        ]
        return pull_requests

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        pull_requests = self.list_prs(
            credentials,
            input_data.repo_url,
        )
        yield from ((""pull_request"", pr) for pr in pull_requests)


class GithubMakePullRequestBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        title: str = SchemaField(
            description=""Title of the pull request"",
            placeholder=""Enter the pull request title"",
        )
        body: str = SchemaField(
            description=""Body of the pull request"",
            placeholder=""Enter the pull request body"",
        )
        head: str = SchemaField(
            description=(
                ""The name of the branch where your changes are implemented. ""
                ""For cross-repository pull requests in the same network, ""
                ""namespace head with a user like this: username:branch.""
            ),
            placeholder=""Enter the head branch"",
        )
        base: str = SchemaField(
            description=""The name of the branch you want the changes pulled into."",
            placeholder=""Enter the base branch"",
        )

    class Output(BlockSchema):
        number: int = SchemaField(description=""Number of the created pull request"")
        url: str = SchemaField(description=""URL of the created pull request"")
        error: str = SchemaField(
            description=""Error message if the pull request creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""dfb987f8-f197-4b2e-bf19-111812afd692"",
            description=""This block creates a new pull request on a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakePullRequestBlock.Input,
            output_schema=GithubMakePullRequestBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""title"": ""Test Pull Request"",
                ""body"": ""This is a test pull request."",
                ""head"": ""feature-branch"",
                ""base"": ""main"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""number"", 1),
                (""url"", ""https://github.com/owner/repo/pull/1""),
            ],
            test_mock={
                ""create_pr"": lambda *args, **kwargs: (
                    1,
                    ""https://github.com/owner/repo/pull/1"",
                )
            },
        )

    @staticmethod
    def create_pr(
        credentials: GithubCredentials,
        repo_url: str,
        title: str,
        body: str,
        head: str,
        base: str,
    ) -> tuple[int, str]:
        api = get_api(credentials)
        pulls_url = repo_url + ""/pulls""
        data = {""title"": title, ""body"": body, ""head"": head, ""base"": base}
        response = api.post(pulls_url, json=data)
        pr_data = response.json()
        return pr_data[""number""], pr_data[""html_url""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            number, url = self.create_pr(
                credentials,
                input_data.repo_url,
                input_data.title,
                input_data.body,
                input_data.head,
                input_data.base,
            )
            yield ""number"", number
            yield ""url"", url
        except Exception as e:
            yield ""error"", str(e)


class GithubReadPullRequestBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        include_pr_changes: bool = SchemaField(
            description=""Whether to include the changes made in the pull request"",
            default=False,
        )

    class Output(BlockSchema):
        title: str = SchemaField(description=""Title of the pull request"")
        body: str = SchemaField(description=""Body of the pull request"")
        author: str = SchemaField(description=""User who created the pull request"")
        changes: str = SchemaField(description=""Changes made in the pull request"")
        error: str = SchemaField(
            description=""Error message if reading the pull request failed""
        )

    def __init__(self):
        super().__init__(
            id=""bf94b2a4-1a30-4600-a783-a8a44ee31301"",
            description=""This block reads the body, title, user, and changes of a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadPullRequestBlock.Input,
            output_schema=GithubReadPullRequestBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""include_pr_changes"": True,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""title"", ""Title of the pull request""),
                (""body"", ""This is the body of the pull request.""),
                (""author"", ""username""),
                (""changes"", ""List of changes made in the pull request.""),
            ],
            test_mock={
                ""read_pr"": lambda *args, **kwargs: (
                    ""Title of the pull request"",
                    ""This is the body of the pull request."",
                    ""username"",
                ),
                ""read_pr_changes"": lambda *args, **kwargs: ""List of changes made in the pull request."",
            },
        )

    @staticmethod
    def read_pr(credentials: GithubCredentials, pr_url: str) -> tuple[str, str, str]:
        api = get_api(credentials)
        # Adjust the URL to access the issue endpoint for PR metadata
        issue_url = pr_url.replace(""/pull/"", ""/issues/"")
        response = api.get(issue_url)
        data = response.json()
        title = data.get(""title"", ""No title found"")
        body = data.get(""body"", ""No body content found"")
        author = data.get(""user"", {}).get(""login"", ""No user found"")
        return title, body, author

    @staticmethod
    def read_pr_changes(credentials: GithubCredentials, pr_url: str) -> str:
        api = get_api(credentials)
        files_url = prepare_pr_api_url(pr_url=pr_url, path=""files"")
        response = api.get(files_url)
        files = response.json()
        changes = []
        for file in files:
            filename = file.get(""filename"")
            patch = file.get(""patch"")
            if filename and patch:
                changes.append(f""File: {filename}\n{patch}"")
        return ""\n\n"".join(changes)

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        title, body, author = self.read_pr(
            credentials,
            input_data.pr_url,
        )
        yield ""title"", title
        yield ""body"", body
        yield ""author"", author

        if input_data.include_pr_changes:
            changes = self.read_pr_changes(
                credentials,
                input_data.pr_url,
            )
            yield ""changes"", changes


class GithubAssignPRReviewerBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        reviewer: str = SchemaField(
            description=""Username of the reviewer to assign"",
            placeholder=""Enter the reviewer's username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the reviewer assignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the reviewer assignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""c0d22c5e-e688-43e3-ba43-d5faba7927fd"",
            description=""This block assigns a reviewer to a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAssignPRReviewerBlock.Input,
            output_schema=GithubAssignPRReviewerBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""reviewer"": ""reviewer_username"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Reviewer assigned successfully"")],
            test_mock={
                ""assign_reviewer"": lambda *args, **kwargs: ""Reviewer assigned successfully""
            },
        )

    @staticmethod
    def assign_reviewer(
        credentials: GithubCredentials, pr_url: str, reviewer: str
    ) -> str:
        api = get_api(credentials)
        reviewers_url = prepare_pr_api_url(pr_url=pr_url, path=""requested_reviewers"")
        data = {""reviewers"": [reviewer]}
        api.post(reviewers_url, json=data)
        return ""Reviewer assigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            status = self.assign_reviewer(
                credentials,
                input_data.pr_url,
                input_data.reviewer,
            )
            yield ""status"", status
        except Exception as e:
            yield ""error"", str(e)


class GithubUnassignPRReviewerBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        reviewer: str = SchemaField(
            description=""Username of the reviewer to unassign"",
            placeholder=""Enter the reviewer's username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the reviewer unassignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the reviewer unassignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""9637945d-c602-4875-899a-9c22f8fd30de"",
            description=""This block unassigns a reviewer from a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubUnassignPRReviewerBlock.Input,
            output_schema=GithubUnassignPRReviewerBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""reviewer"": ""reviewer_username"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Reviewer unassigned successfully"")],
            test_mock={
                ""unassign_reviewer"": lambda *args, **kwargs: ""Reviewer unassigned successfully""
            },
        )

    @staticmethod
    def unassign_reviewer(
        credentials: GithubCredentials, pr_url: str, reviewer: str
    ) -> str:
        api = get_api(credentials)
        reviewers_url = prepare_pr_api_url(pr_url=pr_url, path=""requested_reviewers"")
        data = {""reviewers"": [reviewer]}
        api.delete(reviewers_url, json=data)
        return ""Reviewer unassigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            status = self.unassign_reviewer(
                credentials,
                input_data.pr_url,
                input_data.reviewer,
            )
            yield ""status"", status
        except Exception as e:
            yield ""error"", str(e)


class GithubListPRReviewersBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )

    class Output(BlockSchema):
        class ReviewerItem(TypedDict):
            username: str
            url: str

        reviewer: ReviewerItem = SchemaField(
            title=""Reviewer"",
            description=""Reviewers with their username and profile URL"",
        )
        error: str = SchemaField(
            description=""Error message if listing reviewers failed""
        )

    def __init__(self):
        super().__init__(
            id=""2646956e-96d5-4754-a3df-034017e7ed96"",
            description=""This block lists all reviewers for a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListPRReviewersBlock.Input,
            output_schema=GithubListPRReviewersBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""reviewer"",
                    {
                        ""username"": ""reviewer1"",
                        ""url"": ""https://github.com/reviewer1"",
                    },
                )
            ],
            test_mock={
                ""list_reviewers"": lambda *args, **kwargs: [
                    {
                        ""username"": ""reviewer1"",
                        ""url"": ""https://github.com/reviewer1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_reviewers(
        credentials: GithubCredentials, pr_url: str
    ) -> list[Output.ReviewerItem]:
        api = get_api(credentials)
        reviewers_url = prepare_pr_api_url(pr_url=pr_url, path=""requested_reviewers"")
        response = api.get(reviewers_url)
        data = response.json()
        reviewers: list[GithubListPRReviewersBlock.Output.ReviewerItem] = [
            {""username"": reviewer[""login""], ""url"": reviewer[""html_url""]}
            for reviewer in data.get(""users"", [])
        ]
        return reviewers

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        reviewers = self.list_reviewers(
            credentials,
            input_data.pr_url,
        )
        yield from ((""reviewer"", reviewer) for reviewer in reviewers)


def prepare_pr_api_url(pr_url: str, path: str) -> str:
    # Pattern to capture the base repository URL and the pull request number
    pattern = r""^(?:https?://)?([^/]+/[^/]+/[^/]+)/pull/(\d+)""
    match = re.match(pattern, pr_url)
    if not match:
        return pr_url

    base_url, pr_number = match.groups()
    return f""{base_url}/pulls/{pr_number}/{path}""
","import re
from typing_extensions import TypedDict
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from ._api import get_api
from ._auth import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GithubCredentials,
    GithubCredentialsField,
    GithubCredentialsInput,
)

class GithubListPullRequestsBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class PRItem(TypedDict):
            title: str
            url: str

        pull_request: PRItem = SchemaField(
            title=""Pull Request"", description=""PRs with their title and URL""
        )
        error: str = SchemaField(description=""Error message if listing issues failed"")

    def __init__(self):
        super().__init__(
            id=""ffef3c4c-6cd0-48dd-817d-459f975219f4"",
            description=""This block lists all pull requests for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListPullRequestsBlock.Input,
            output_schema=GithubListPullRequestsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""pull_request"",
                    {
                        ""title"": ""Pull request 1"",
                        ""url"": ""https://github.com/owner/repo/pull/1"",
                    },
                )
            ],
            test_mock={
                ""list_prs"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Pull request 1"",
                        ""url"": ""https://github.com/owner/repo/pull/1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_prs(credentials: GithubCredentials, repo_url: str) -> list[Output.PRItem]:
        api = get_api(credentials)
        pulls_url = repo_url + ""/pulls""
        response = api.get(pulls_url)
        data = response.json()
        pull_requests: list[GithubListPullRequestsBlock.Output.PRItem] = [
            {""title"": pr[""title""], ""url"": pr[""html_url""]} for pr in data
        ]
        return pull_requests

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        pull_requests = self.list_prs(
            credentials,
            input_data.repo_url,
        )
        yield from ((""pull_request"", pr) for pr in pull_requests)


class GithubMakePullRequestBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        title: str = SchemaField(
            description=""Title of the pull request"",
            placeholder=""Enter the pull request title"",
        )
        body: str = SchemaField(
            description=""Body of the pull request"",
            placeholder=""Enter the pull request body"",
        )
        head: str = SchemaField(
            description=(
                ""The name of the branch where your changes are implemented. ""
                ""For cross-repository pull requests in the same network, ""
                ""namespace head with a user like this: username:branch.""
            ),
            placeholder=""Enter the head branch"",
        )
        base: str = SchemaField(
            description=""The name of the branch you want the changes pulled into."",
            placeholder=""Enter the base branch"",
        )

    class Output(BlockSchema):
        number: int = SchemaField(description=""Number of the created pull request"")
        url: str = SchemaField(description=""URL of the created pull request"")
        error: str = SchemaField(
            description=""Error message if the pull request creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""dfb987f8-f197-4b2e-bf19-111812afd692"",
            description=""This block creates a new pull request on a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakePullRequestBlock.Input,
            output_schema=GithubMakePullRequestBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""title"": ""Test Pull Request"",
                ""body"": ""This is a test pull request."",
                ""head"": ""feature-branch"",
                ""base"": ""main"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""number"", 1),
                (""url"", ""https://github.com/owner/repo/pull/1""),
            ],
            test_mock={
                ""create_pr"": lambda *args, **kwargs: (
                    1,
                    ""https://github.com/owner/repo/pull/1"",
                )
            },
        )

    @staticmethod
    def create_pr(
        credentials: GithubCredentials,
        repo_url: str,
        title: str,
        body: str,
        head: str,
        base: str,
    ) -> tuple[int, str]:
        api = get_api(credentials)
        pulls_url = repo_url + ""/pulls""
        data = {""title"": title, ""body"": body, ""head"": head, ""base"": base}
        response = api.post(pulls_url, json=data)
        pr_data = response.json()
        return pr_data[""number""], pr_data[""html_url""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            number, url = self.create_pr(
                credentials,
                input_data.repo_url,
                input_data.title,
                input_data.body,
                input_data.head,
                input_data.base,
            )
            yield ""number"", number
            yield ""url"", url
        except Exception as e:
            yield ""error"", str(e)


class GithubReadPullRequestBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        include_pr_changes: bool = SchemaField(
            description=""Whether to include the changes made in the pull request"",
            default=False,
        )

    class Output(BlockSchema):
        title: str = SchemaField(description=""Title of the pull request"")
        body: str = SchemaField(description=""Body of the pull request"")
        author: str = SchemaField(description=""User who created the pull request"")
        changes: str = SchemaField(description=""Changes made in the pull request"")
        error: str = SchemaField(
            description=""Error message if reading the pull request failed""
        )

    def __init__(self):
        super().__init__(
            id=""bf94b2a4-1a30-4600-a783-a8a44ee31301"",
            description=""This block reads the body, title, user, and changes of a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadPullRequestBlock.Input,
            output_schema=GithubReadPullRequestBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""include_pr_changes"": True,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""title"", ""Title of the pull request""),
                (""body"", ""This is the body of the pull request.""),
                (""author"", ""username""),
                (""changes"", ""List of changes made in the pull request.""),
            ],
            test_mock={
                ""read_pr"": lambda *args, **kwargs: (
                    ""Title of the pull request"",
                    ""This is the body of the pull request."",
                    ""username"",
                ),
                ""read_pr_changes"": lambda *args, **kwargs: ""List of changes made in the pull request."",
            },
        )

    @staticmethod
    def read_pr(credentials: GithubCredentials, pr_url: str) -> tuple[str, str, str]:
        api = get_api(credentials)
        # Adjust the URL to access the issue endpoint for PR metadata
        issue_url = pr_url.replace(""/pull/"", ""/issues/"")
        response = api.get(issue_url)
        data = response.json()
        title = data.get(""title"", ""No title found"")
        body = data.get(""body"", ""No body content found"")
        author = data.get(""user"", {}).get(""login"", ""No user found"")
        return title, body, author

    @staticmethod
    def read_pr_changes(credentials: GithubCredentials, pr_url: str) -> str:
        api = get_api(credentials)
        files_url = prepare_pr_api_url(pr_url=pr_url, path=""files"")
        response = api.get(files_url)
        files = response.json()
        changes = []
        for file in files:
            filename = file.get(""filename"")
            patch = file.get(""patch"")
            if filename and patch:
                changes.append(f""File: {filename}\n{patch}"")
        return ""\n\n"".join(changes)

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        title, body, author = self.read_pr(
            credentials,
            input_data.pr_url,
        )
        yield ""title"", title
        yield ""body"", body
        yield ""author"", author

        if input_data.include_pr_changes:
            changes = self.read_pr_changes(
                credentials,
                input_data.pr_url,
            )
            yield ""changes"", changes


class GithubAssignPRReviewerBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        reviewer: str = SchemaField(
            description=""Username of the reviewer to assign"",
            placeholder=""Enter the reviewer's username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the reviewer assignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the reviewer assignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""c0d22c5e-e688-43e3-ba43-d5faba7927fd"",
            description=""This block assigns a reviewer to a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAssignPRReviewerBlock.Input,
            output_schema=GithubAssignPRReviewerBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""reviewer"": ""reviewer_username"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Reviewer assigned successfully"")],
            test_mock={
                ""assign_reviewer"": lambda *args, **kwargs: ""Reviewer assigned successfully""
            },
        )

    @staticmethod
    def assign_reviewer(
        credentials: GithubCredentials, pr_url: str, reviewer: str
    ) -> str:
        api = get_api(credentials)
        reviewers_url = prepare_pr_api_url(pr_url=pr_url, path=""requested_reviewers"")
        data = {""reviewers"": [reviewer]}
        api.post(reviewers_url, json=data)
        return ""Reviewer assigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            status = self.assign_reviewer(
                credentials,
                input_data.pr_url,
                input_data.reviewer,
            )
            yield ""status"", status
        except Exception as e:
            yield ""error"", str(e)


class GithubUnassignPRReviewerBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        reviewer: str = SchemaField(
            description=""Username of the reviewer to unassign"",
            placeholder=""Enter the reviewer's username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the reviewer unassignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the reviewer unassignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""9637945d-c602-4875-899a-9c22f8fd30de"",
            description=""This block unassigns a reviewer from a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubUnassignPRReviewerBlock.Input,
            output_schema=GithubUnassignPRReviewerBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""reviewer"": ""reviewer_username"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Reviewer unassigned successfully"")],
            test_mock={
                ""unassign_reviewer"": lambda *args, **kwargs: ""Reviewer unassigned successfully""
            },
        )

    @staticmethod
    def unassign_reviewer(
        credentials: GithubCredentials, pr_url: str, reviewer: str
    ) -> str:
        api = get_api(credentials)
        reviewers_url = prepare_pr_api_url(pr_url=pr_url, path=""requested_reviewers"")
        data = {""reviewers"": [reviewer]}
        api.delete(reviewers_url, json=data)
        return ""Reviewer unassigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            status = self.unassign_reviewer(
                credentials,
                input_data.pr_url,
                input_data.reviewer,
            )
            yield ""status"", status
        except Exception as e:
            yield ""error"", str(e)


class GithubListPRReviewersBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )

    class Output(BlockSchema):
        class ReviewerItem(TypedDict):
            username: str
            url: str

        reviewer: ReviewerItem = SchemaField(
            title=""Reviewer"",
            description=""Reviewers with their username and profile URL"",
        )
        error: str = SchemaField(
            description=""Error message if listing reviewers failed""
        )

    def __init__(self):
        super().__init__(
            id=""2646956e-96d5-4754-a3df-034017e7ed96"",
            description=""This block lists all reviewers for a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListPRReviewersBlock.Input,
            output_schema=GithubListPRReviewersBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""reviewer"",
                    {
                        ""username"": ""reviewer1"",
                        ""url"": ""https://github.com/reviewer1"",
                    },
                )
            ],
            test_mock={
                ""list_reviewers"": lambda *args, **kwargs: [
                    {
                        ""username"": ""reviewer1"",
                        ""url"": ""https://github.com/reviewer1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_reviewers(
        credentials: GithubCredentials, pr_url: str
    ) -> list[Output.ReviewerItem]:
        api = get_api(credentials)
        reviewers_url = prepare_pr_api_url(pr_url=pr_url, path=""requested_reviewers"")
        response = api.get(reviewers_url)
        data = response.json()
        reviewers: list[GithubListPRReviewersBlock.Output.ReviewerItem] = [
            {""username"": reviewer[""login""], ""url"": reviewer[""html_url""]}
            for reviewer in data.get(""users"", [])
        ]
        return reviewers

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        reviewers = self.list_reviewers(
            credentials,
            input_data.pr_url,
        )
        yield from ((""reviewer"", reviewer) for reviewer in reviewers)


def prepare_pr_api_url(pr_url: str, path: str) -> str:
    # Pattern to capture the base repository URL and the pull request number
    pattern = r""^(?:https?://)?([^/]+/[^/]+/[^/]+)/pull/(\d+)""
    match = re.match(pattern, pr_url)
    if not match:
        return pr_url
    base_url, pr_number = match.groups()
    return f""{base_url}/pulls/{pr_number}/{path}""","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
142,"import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

from pydantic import BaseModel, ConfigDict

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import BlockSecret, SchemaField, SecretField


class EmailCredentials(BaseModel):
    smtp_server: str = SchemaField(
        default=""smtp.gmail.com"", description=""SMTP server address""
    )
    smtp_port: int = SchemaField(default=25, description=""SMTP port number"")
    smtp_username: BlockSecret = SecretField(key=""smtp_username"")
    smtp_password: BlockSecret = SecretField(key=""smtp_password"")

    model_config = ConfigDict(title=""Email Credentials"")


class SendEmailBlock(Block):
    class Input(BlockSchema):
        to_email: str = SchemaField(
            description=""Recipient email address"", placeholder=""recipient@example.com""
        )
        subject: str = SchemaField(
            description=""Subject of the email"", placeholder=""Enter the email subject""
        )
        body: str = SchemaField(
            description=""Body of the email"", placeholder=""Enter the email body""
        )
        creds: EmailCredentials = SchemaField(
            description=""SMTP credentials"",
            default=EmailCredentials(),
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the email sending operation"")
        error: str = SchemaField(
            description=""Error message if the email sending failed""
        )

    def __init__(self):
        super().__init__(
            disabled=True,
            id=""4335878a-394e-4e67-adf2-919877ff49ae"",
            description=""This block sends an email using the provided SMTP credentials."",
            categories={BlockCategory.OUTPUT},
            input_schema=SendEmailBlock.Input,
            output_schema=SendEmailBlock.Output,
            test_input={
                ""to_email"": ""recipient@example.com"",
                ""subject"": ""Test Email"",
                ""body"": ""This is a test email."",
                ""creds"": {
                    ""smtp_server"": ""smtp.gmail.com"",
                    ""smtp_port"": 25,
                    ""smtp_username"": ""your-email@gmail.com"",
                    ""smtp_password"": ""your-gmail-password"",
                },
            },
            test_output=[(""status"", ""Email sent successfully"")],
            test_mock={""send_email"": lambda *args, **kwargs: ""Email sent successfully""},
        )

    @staticmethod
    def send_email(
        creds: EmailCredentials, to_email: str, subject: str, body: str
    ) -> str:
        smtp_server = creds.smtp_server
        smtp_port = creds.smtp_port
        smtp_username = creds.smtp_username.get_secret_value()
        smtp_password = creds.smtp_password.get_secret_value()

        msg = MIMEMultipart()
        msg[""From""] = smtp_username
        msg[""To""] = to_email
        msg[""Subject""] = subject
        msg.attach(MIMEText(body, ""plain""))

        with smtplib.SMTP(smtp_server, smtp_port) as server:
            server.starttls()
            server.login(smtp_username, smtp_password)
            server.sendmail(smtp_username, to_email, msg.as_string())

        return ""Email sent successfully""

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""status"", self.send_email(
            input_data.creds,
            input_data.to_email,
            input_data.subject,
            input_data.body,
        )
","import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from pydantic import BaseModel, ConfigDict
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import BlockSecret, SchemaField, SecretField

class EmailCredentials(BaseModel):
    smtp_server: str = SchemaField(
        default=""smtp.gmail.com"", description=""SMTP server address""
    )
    smtp_port: int = SchemaField(default=25, description=""SMTP port number"")
    smtp_username: BlockSecret = SecretField(key=""smtp_username"")
    smtp_password: BlockSecret = SecretField(key=""smtp_password"")
    model_config = ConfigDict(title=""Email Credentials"")

class SendEmailBlock(Block):
    class Input(BlockSchema):
        to_email: str = SchemaField(
            description=""Recipient email address"", placeholder=""recipient@example.com""
        )
        subject: str = SchemaField(
            description=""Subject of the email"", placeholder=""Enter the email subject""
        )
        body: str = SchemaField(
            description=""Body of the email"", placeholder=""Enter the email body""
        )
        creds: EmailCredentials = SchemaField(
            description=""SMTP credentials"",
            default=EmailCredentials(),
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the email sending operation"")
        error: str = SchemaField(
            description=""Error message if the email sending failed""
        )

    def __init__(self):
        super().__init__(
            disabled=True,
            id=""4335878a-394e-4e67-adf2-919877ff49ae"",
            description=""This block sends an email using the provided SMTP credentials."",
            categories={BlockCategory.OUTPUT},
            input_schema=SendEmailBlock.Input,
            output_schema=SendEmailBlock.Output,
            test_input={
                ""to_email"": ""recipient@example.com"",
                ""subject"": ""Test Email"",
                ""body"": ""This is a test email."",
                ""creds"": {
                    ""smtp_server"": ""smtp.gmail.com"",
                    ""smtp_port"": 25,
                    ""smtp_username"": ""your-email@gmail.com"",
                    ""smtp_password"": ""your-gmail-password"",
                },
            },
            test_output=[(""status"", ""Email sent successfully"")],
            test_mock={""send_email"": lambda *args, **kwargs: ""Email sent successfully""},
        )

    @staticmethod
    def send_email(
        creds: EmailCredentials, to_email: str, subject: str, body: str
    ) -> str:
        smtp_server = creds.smtp_server
        smtp_port = creds.smtp_port
        smtp_username = creds.smtp_username.get_secret_value()
        smtp_password = creds.smtp_password.get_secret_value()

        msg = MIMEMultipart()
        msg[""From""] = smtp_username
        msg[""To""] = to_email
        msg[""Subject""] = subject
        msg.attach(MIMEText(body, ""plain""))

        with smtplib.SMTP(smtp_server, smtp_port) as server:
            server.starttls()
            server.login(smtp_username, smtp_password)
            server.sendmail(smtp_username, to_email, msg.as_string())

        return ""Email sent successfully""

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""status"", self.send_email(
            input_data.creds,
            input_data.to_email,
            input_data.subject,
            input_data.body,
        )","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
143,"import contextlib
import logging
import typing

import fastapi
import fastapi.responses
import starlette.middleware.cors
import uvicorn
from autogpt_libs.feature_flag.client import (
    initialize_launchdarkly,
    shutdown_launchdarkly,
)

import backend.data.block
import backend.data.db
import backend.data.graph
import backend.data.user
import backend.server.routers.v1
import backend.server.v2.library.routes
import backend.server.v2.store.routes
import backend.util.service
import backend.util.settings

settings = backend.util.settings.Settings()
logger = logging.getLogger(__name__)

logging.getLogger(""autogpt_libs"").setLevel(logging.INFO)


@contextlib.contextmanager
def launch_darkly_context():
    if settings.config.app_env != backend.util.settings.AppEnvironment.LOCAL:
        initialize_launchdarkly()
        try:
            yield
        finally:
            shutdown_launchdarkly()
    else:
        yield


@contextlib.asynccontextmanager
async def lifespan_context(app: fastapi.FastAPI):
    await backend.data.db.connect()
    await backend.data.block.initialize_blocks()
    await backend.data.user.migrate_and_encrypt_user_integrations()
    await backend.data.graph.fix_llm_provider_credentials()
    with launch_darkly_context():
        yield
    await backend.data.db.disconnect()


docs_url = (
    ""/docs""
    if settings.config.app_env == backend.util.settings.AppEnvironment.LOCAL
    else None
)

app = fastapi.FastAPI(
    title=""AutoGPT Agent Server"",
    description=(
        ""This server is used to execute agents that are created by the ""
        ""AutoGPT system.""
    ),
    summary=""AutoGPT Agent Server"",
    version=""0.1"",
    lifespan=lifespan_context,
    docs_url=docs_url,
)


def handle_internal_http_error(status_code: int = 500, log_error: bool = True):
    def handler(request: fastapi.Request, exc: Exception):
        if log_error:
            logger.exception(f""{request.method} {request.url.path} failed: {exc}"")
        return fastapi.responses.JSONResponse(
            content={
                ""message"": f""{request.method} {request.url.path} failed"",
                ""detail"": str(exc),
            },
            status_code=status_code,
        )

    return handler


app.add_exception_handler(ValueError, handle_internal_http_error(400))
app.add_exception_handler(Exception, handle_internal_http_error(500))
app.include_router(backend.server.routers.v1.v1_router, tags=[""v1""], prefix=""/api"")
app.include_router(
    backend.server.v2.store.routes.router, tags=[""v2""], prefix=""/api/store""
)
app.include_router(
    backend.server.v2.library.routes.router, tags=[""v2""], prefix=""/api/library""
)


@app.get(path=""/health"", tags=[""health""], dependencies=[])
async def health():
    return {""status"": ""healthy""}


class AgentServer(backend.util.service.AppProcess):
    def run(self):
        server_app = starlette.middleware.cors.CORSMiddleware(
            app=app,
            allow_origins=settings.config.backend_cors_allow_origins,
            allow_credentials=True,
            allow_methods=[""*""],  # Allows all methods
            allow_headers=[""*""],  # Allows all headers
        )
        uvicorn.run(
            server_app,
            host=backend.util.settings.Config().agent_api_host,
            port=backend.util.settings.Config().agent_api_port,
        )

    @staticmethod
    async def test_execute_graph(
        graph_id: str, node_input: dict[typing.Any, typing.Any], user_id: str
    ):
        return backend.server.routers.v1.execute_graph(graph_id, node_input, user_id)

    @staticmethod
    async def test_create_graph(
        create_graph: backend.server.routers.v1.CreateGraph,
        user_id: str,
    ):
        return await backend.server.routers.v1.create_new_graph(create_graph, user_id)

    @staticmethod
    async def test_get_graph_run_status(graph_exec_id: str, user_id: str):
        execution = await backend.data.graph.get_execution(
            user_id=user_id, execution_id=graph_exec_id
        )
        if not execution:
            raise ValueError(f""Execution {graph_exec_id} not found"")
        return execution.status

    @staticmethod
    async def test_get_graph_run_node_execution_results(
        graph_id: str, graph_exec_id: str, user_id: str
    ):
        return await backend.server.routers.v1.get_graph_run_node_execution_results(
            graph_id, graph_exec_id, user_id
        )

    @staticmethod
    async def test_delete_graph(graph_id: str, user_id: str):
        return await backend.server.routers.v1.delete_graph(graph_id, user_id)

    def set_test_dependency_overrides(self, overrides: dict):
        app.dependency_overrides.update(overrides)
","import contextlib
import logging
import typing

import fastapi
import fastapi.responses
import starlette.middleware.cors
import uvicorn

from autogpt_libs.feature_flag.client import (
    initialize_launchdarkly,
    shutdown_launchdarkly,
)

import backend.data.block
import backend.data.db
import backend.data.graph
import backend.data.user
import backend.server.routers.v1
import backend.server.v2.library.routes
import backend.server.v2.store.routes
import backend.util.service
import backend.util.settings

settings = backend.util.settings.Settings()
logger = logging.getLogger(__name__)

logging.getLogger(""autogpt_libs"").setLevel(logging.INFO)

@contextlib.contextmanager
def launch_darkly_context():
    if settings.config.app_env != backend.util.settings.AppEnvironment.LOCAL:
        initialize_launchdarkly()
        try:
            yield
        finally:
            shutdown_launchdarkly()
    else:
        yield

@contextlib.asynccontextmanager
async def lifespan_context(app: fastapi.FastAPI):
    await backend.data.db.connect()
    await backend.data.block.initialize_blocks()
    await backend.data.user.migrate_and_encrypt_user_integrations()
    await backend.data.graph.fix_llm_provider_credentials()
    with launch_darkly_context():
        yield
    await backend.data.db.disconnect()

docs_url = (
    ""/docs""
    if settings.config.app_env == backend.util.settings.AppEnvironment.LOCAL
    else None
)

app = fastapi.FastAPI(
    title=""AutoGPT Agent Server"",
    description=(
        ""This server is used to execute agents that are created by the ""
        ""AutoGPT system.""
    ),
    summary=""AutoGPT Agent Server"",
    version=""0.1"",
    lifespan=lifespan_context,
    docs_url=docs_url,
)

def handle_internal_http_error(status_code: int = 500, log_error: bool = True):
    def handler(request: fastapi.Request, exc: Exception):
        if log_error:
            logger.exception(f""{request.method} {request.url.path} failed: {exc}"")
        return fastapi.responses.JSONResponse(
            content={
                ""message"": f""{request.method} {request.url.path} failed"",
                ""detail"": str(exc),
            },
            status_code=status_code,
        )
    return handler

app.add_exception_handler(ValueError, handle_internal_http_error(400))
app.add_exception_handler(Exception, handle_internal_http_error(500))

app.include_router(backend.server.routers.v1.v1_router, tags=[""v1""], prefix=""/api"")
app.include_router(
    backend.server.v2.store.routes.router, tags=[""v2""], prefix=""/api/store""
)
app.include_router(
    backend.server.v2.library.routes.router, tags=[""v2""], prefix=""/api/library""
)

@app.get(path=""/health"", tags=[""health""], dependencies=[])
async def health():
    return {""status"": ""healthy""}

class AgentServer(backend.util.service.AppProcess):
    def run(self):
        server_app = starlette.middleware.cors.CORSMiddleware(
            app=app,
            allow_origins=settings.config.backend_cors_allow_origins,
            allow_credentials=True,
            allow_methods=[""*""],  # Allows all methods
            allow_headers=[""*""],  # Allows all headers
        )
        uvicorn.run(
            server_app,
            host=backend.util.settings.Config().agent_api_host,
            port=backend.util.settings.Config().agent_api_port,
        )

    @staticmethod
    async def test_execute_graph(
        graph_id: str, node_input: dict[typing.Any, typing.Any], user_id: str
    ):
        return backend.server.routers.v1.execute_graph(graph_id, node_input, user_id)
    
    @staticmethod
    async def test_create_graph(
        create_graph: backend.server.routers.v1.CreateGraph,
        user_id: str,
    ):
        return await backend.server.routers.v1.create_new_graph(create_graph, user_id)

    @staticmethod
    async def test_get_graph_run_status(graph_exec_id: str, user_id: str):
        execution = await backend.data.graph.get_execution(
            user_id=user_id, execution_id=graph_exec_id
        )
        if not execution:
            raise ValueError(f""Execution {graph_exec_id} not found"")
        return execution.status

    @staticmethod
    async def test_get_graph_run_node_execution_results(
        graph_id: str, graph_exec_id: str, user_id: str
    ):
        return await backend.server.routers.v1.get_graph_run_node_execution_results(
            graph_id, graph_exec_id, user_id
        )

    @staticmethod
    async def test_delete_graph(graph_id: str, user_id: str):
        return await backend.server.routers.v1.delete_graph(graph_id, user_id)

    def set_test_dependency_overrides(self, overrides: dict):
        app.dependency_overrides.update(overrides)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
144,"from __future__ import annotations

import json
import logging


class JsonFileHandler(logging.FileHandler):
    def format(self, record: logging.LogRecord) -> str:
        record.json_data = json.loads(record.getMessage())
        return json.dumps(getattr(record, ""json_data""), ensure_ascii=False, indent=4)

    def emit(self, record: logging.LogRecord) -> None:
        with open(self.baseFilename, ""w"", encoding=""utf-8"") as f:
            f.write(self.format(record))
","from __future__ import annotations
import json
import logging

class JsonFileHandler(logging.FileHandler):
    def format(self, record: logging.LogRecord) -> str:
        record.json_data = json.loads(record.getMessage())
        return json.dumps(getattr(record, ""json_data""), ensure_ascii=False, indent=4)

    def emit(self, record: logging.LogRecord) -> None:
        with open(self.baseFilename, ""w"", encoding=""utf-8"") as f:
            f.write(self.format(record))","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
145,"from abc import ABCMeta, abstractmethod
from enum import Enum
import sys


class Suit(Enum):

    HEART = 0
    DIAMOND = 1
    CLUBS = 2
    SPADE = 3


class Card(metaclass=ABCMeta):

    def __init__(self, value, suit):
        self.value = value
        self.suit = suit
        self.is_available = True

    @property
    @abstractmethod
    def value(self):
        pass

    @value.setter
    @abstractmethod
    def value(self, other):
        pass


class BlackJackCard(Card):

    def __init__(self, value, suit):
        super(BlackJackCard, self).__init__(value, suit)

    def is_ace(self):
        return True if self._value == 1 else False

    def is_face_card(self):
        """"""Jack = 11, Queen = 12, King = 13""""""
        return True if 10 < self._value <= 13 else False

    @property
    def value(self):
        if self.is_ace() == 1:
            return 1
        elif self.is_face_card():
            return 10
        else:
            return self._value

    @value.setter
    def value(self, new_value):
        if 1 <= new_value <= 13:
            self._value = new_value
        else:
            raise ValueError('Invalid card value: {}'.format(new_value))


class Hand(object):

    def __init__(self, cards):
        self.cards = cards

    def add_card(self, card):
        self.cards.append(card)

    def score(self):
        total_value = 0
        for card in self.cards:
            total_value += card.value
        return total_value


class BlackJackHand(Hand):

    BLACKJACK = 21

    def __init__(self, cards):
        super(BlackJackHand, self).__init__(cards)

    def score(self):
        min_over = sys.MAXSIZE
        max_under = -sys.MAXSIZE
        for score in self.possible_scores():
            if self.BLACKJACK < score < min_over:
                min_over = score
            elif max_under < score <= self.BLACKJACK:
                max_under = score
        return max_under if max_under != -sys.MAXSIZE else min_over

    def possible_scores(self):
        """"""Return a list of possible scores, taking Aces into account.""""""
        pass


class Deck(object):

    def __init__(self, cards):
        self.cards = cards
        self.deal_index = 0

    def remaining_cards(self):
        return len(self.cards) - self.deal_index

    def deal_card(self):
        try:
            card = self.cards[self.deal_index]
            card.is_available = False
            self.deal_index += 1
        except IndexError:
            return None
        return card

    def shuffle(self):
        pass
","from abc import ABCMeta, abstractmethod
from enum import Enum
import sys


class Suit(Enum):
    HEART = 0
    DIAMOND = 1
    CLUBS = 2
    SPADE = 3



class Card(metaclass=ABCMeta):
    def __init__(self, value, suit):
        self.value = value
        self.suit = suit
        self.is_available = True

    @property
    @abstractmethod
    def value(self):
        pass

    @value.setter
    @abstractmethod
    def value(self, other):
        pass


class BlackJackCard(Card):
    def __init__(self, value, suit):
        super(BlackJackCard, self).__init__(value, suit)

    def is_ace(self):
        return True if self._value == 1 else False

    def is_face_card(self):
        """"""Jack = 11, Queen = 12, King = 13""""""
        return True if 10 < self._value <= 13 else False

    @property
    def value(self):
        if self.is_ace() == 1:
            return 1
        elif self.is_face_card():
            return 10
        else:
            return self._value

    @value.setter
    def value(self, new_value):
        if 1 <= new_value <= 13:
            self._value = new_value
        else:
            raise ValueError('Invalid card value: {}'.format(new_value))


class Hand(object):
    def __init__(self, cards):
        self.cards = cards

    def add_card(self, card):
        self.cards.append(card)

    def score(self):
        total_value = 0
        for card in self.cards:
            total_value += card.value
        return total_value


class BlackJackHand(Hand):
    BLACKJACK = 21

    def __init__(self, cards):
        super(BlackJackHand, self).__init__(cards)

    def score(self):
        min_over = sys.maxsize
        max_under = -sys.maxsize
        for score in self.possible_scores():
            if self.BLACKJACK < score < min_over:
                min_over = score
            elif max_under < score <= self.BLACKJACK:
                max_under = score
        return max_under if max_under != -sys.maxsize else min_over

    def possible_scores(self):
        """"""Return a list of possible scores, taking Aces into account.""""""
        pass


class Deck(object):
    def __init__(self, cards):
        self.cards = cards
        self.deal_index = 0

    def remaining_cards(self):
        return len(self.cards) - self.deal_index

    def deal_card(self):
        try:
            card = self.cards[self.deal_index]
            card.is_available = False
            self.deal_index += 1
        except IndexError:
            return None
        return card

    def shuffle(self):
        pass","{'final_score': 99.07, 'length_similarity': 100.0, 'token_similarity': 96.9, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
146,"import logging
import typing
import urllib.parse

import autogpt_libs.auth.depends
import autogpt_libs.auth.middleware
import fastapi
import fastapi.responses

import backend.data.graph
import backend.server.v2.store.db
import backend.server.v2.store.image_gen
import backend.server.v2.store.media
import backend.server.v2.store.model

logger = logging.getLogger(__name__)

router = fastapi.APIRouter()


##############################################
############### Profile Endpoints ############
##############################################


@router.get(
    ""/profile"",
    tags=[""store"", ""private""],
    response_model=backend.server.v2.store.model.ProfileDetails,
)
async def get_profile(
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ]
):
    """"""
    Get the profile details for the authenticated user.
    """"""
    try:
        profile = await backend.server.v2.store.db.get_user_profile(user_id)
        return profile
    except Exception:
        logger.exception(""Exception occurred whilst getting user profile"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while retrieving the user profile""},
        )


@router.post(
    ""/profile"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=backend.server.v2.store.model.CreatorDetails,
)
async def update_or_create_profile(
    profile: backend.server.v2.store.model.Profile,
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
):
    """"""
    Update the store profile for the authenticated user.

    Args:
        profile (Profile): The updated profile details
        user_id (str): ID of the authenticated user

    Returns:
        CreatorDetails: The updated profile

    Raises:
        HTTPException: If there is an error updating the profile
    """"""
    try:
        updated_profile = await backend.server.v2.store.db.update_or_create_profile(
            user_id=user_id, profile=profile
        )
        return updated_profile
    except Exception:
        logger.exception(""Exception occurred whilst updating profile"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while updating the user profile""},
        )


##############################################
############### Agent Endpoints ##############
##############################################


@router.get(
    ""/agents"",
    tags=[""store"", ""public""],
    response_model=backend.server.v2.store.model.StoreAgentsResponse,
)
async def get_agents(
    featured: bool = False,
    creator: str | None = None,
    sorted_by: str | None = None,
    search_query: str | None = None,
    category: str | None = None,
    page: int = 1,
    page_size: int = 20,
):
    """"""
    Get a paginated list of agents from the store with optional filtering and sorting.

    Args:
        featured (bool, optional): Filter to only show featured agents. Defaults to False.
        creator (str | None, optional): Filter agents by creator username. Defaults to None.
        sorted_by (str | None, optional): Sort agents by ""runs"" or ""rating"". Defaults to None.
        search_query (str | None, optional): Search agents by name, subheading and description. Defaults to None.
        category (str | None, optional): Filter agents by category. Defaults to None.
        page (int, optional): Page number for pagination. Defaults to 1.
        page_size (int, optional): Number of agents per page. Defaults to 20.

    Returns:
        StoreAgentsResponse: Paginated list of agents matching the filters

    Raises:
        HTTPException: If page or page_size are less than 1

    Used for:
    - Home Page Featured Agents
    - Home Page Top Agents
    - Search Results
    - Agent Details - Other Agents By Creator
    - Agent Details - Similar Agents
    - Creator Details - Agents By Creator
    """"""
    if page < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page must be greater than 0""
        )

    if page_size < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page size must be greater than 0""
        )

    try:
        agents = await backend.server.v2.store.db.get_store_agents(
            featured=featured,
            creator=creator,
            sorted_by=sorted_by,
            search_query=search_query,
            category=category,
            page=page,
            page_size=page_size,
        )
        return agents
    except Exception:
        logger.exception(""Exception occured whilst getting store agents"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while retrieving the store agents""},
        )


@router.get(
    ""/agents/{username}/{agent_name}"",
    tags=[""store"", ""public""],
    response_model=backend.server.v2.store.model.StoreAgentDetails,
)
async def get_agent(username: str, agent_name: str):
    """"""
    This is only used on the AgentDetails Page

    It returns the store listing agents details.
    """"""
    try:
        username = urllib.parse.unquote(username).lower()
        # URL decode the agent name since it comes from the URL path
        agent_name = urllib.parse.unquote(agent_name).lower()
        agent = await backend.server.v2.store.db.get_store_agent_details(
            username=username, agent_name=agent_name
        )
        return agent
    except Exception:
        logger.exception(""Exception occurred whilst getting store agent details"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={
                ""detail"": ""An error occurred while retrieving the store agent details""
            },
        )


@router.post(
    ""/agents/{username}/{agent_name}/review"",
    tags=[""store""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=backend.server.v2.store.model.StoreReview,
)
async def create_review(
    username: str,
    agent_name: str,
    review: backend.server.v2.store.model.StoreReviewCreate,
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
):
    """"""
    Create a review for a store agent.

    Args:
        username: Creator's username
        agent_name: Name/slug of the agent
        review: Review details including score and optional comments
        user_id: ID of authenticated user creating the review

    Returns:
        The created review
    """"""
    try:
        username = urllib.parse.unquote(username).lower()
        agent_name = urllib.parse.unquote(agent_name)
        # Create the review
        created_review = await backend.server.v2.store.db.create_store_review(
            user_id=user_id,
            store_listing_version_id=review.store_listing_version_id,
            score=review.score,
            comments=review.comments,
        )

        return created_review
    except Exception:
        logger.exception(""Exception occurred whilst creating store review"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while creating the store review""},
        )


##############################################
############# Creator Endpoints #############
##############################################


@router.get(
    ""/creators"",
    tags=[""store"", ""public""],
    response_model=backend.server.v2.store.model.CreatorsResponse,
)
async def get_creators(
    featured: bool = False,
    search_query: str | None = None,
    sorted_by: str | None = None,
    page: int = 1,
    page_size: int = 20,
):
    """"""
    This is needed for:
    - Home Page Featured Creators
    - Search Results Page

    ---

    To support this functionality we need:
    - featured: bool - to limit the list to just featured agents
    - search_query: str - vector search based on the creators profile description.
    - sorted_by: [agent_rating, agent_runs] -
    """"""
    if page < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page must be greater than 0""
        )

    if page_size < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page size must be greater than 0""
        )

    try:
        creators = await backend.server.v2.store.db.get_store_creators(
            featured=featured,
            search_query=search_query,
            sorted_by=sorted_by,
            page=page,
            page_size=page_size,
        )
        return creators
    except Exception:
        logger.exception(""Exception occurred whilst getting store creators"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while retrieving the store creators""},
        )


@router.get(
    ""/creator/{username}"",
    tags=[""store"", ""public""],
    response_model=backend.server.v2.store.model.CreatorDetails,
)
async def get_creator(
    username: str,
):
    """"""
    Get the details of a creator
    - Creator Details Page
    """"""
    try:
        username = urllib.parse.unquote(username).lower()
        creator = await backend.server.v2.store.db.get_store_creator_details(
            username=username.lower()
        )
        return creator
    except Exception:
        logger.exception(""Exception occurred whilst getting creator details"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={
                ""detail"": ""An error occurred while retrieving the creator details""
            },
        )


############################################
############# Store Submissions ###############
############################################
@router.get(
    ""/myagents"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=backend.server.v2.store.model.MyAgentsResponse,
)
async def get_my_agents(
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ]
):
    try:
        agents = await backend.server.v2.store.db.get_my_agents(user_id)
        return agents
    except Exception:
        logger.exception(""Exception occurred whilst getting my agents"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while retrieving the my agents""},
        )


@router.delete(
    ""/submissions/{submission_id}"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=bool,
)
async def delete_submission(
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
    submission_id: str,
):
    """"""
    Delete a store listing submission.

    Args:
        user_id (str): ID of the authenticated user
        submission_id (str): ID of the submission to be deleted

    Returns:
        bool: True if the submission was successfully deleted, False otherwise
    """"""
    try:
        result = await backend.server.v2.store.db.delete_store_submission(
            user_id=user_id,
            submission_id=submission_id,
        )
        return result
    except Exception:
        logger.exception(""Exception occurred whilst deleting store submission"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while deleting the store submission""},
        )


@router.get(
    ""/submissions"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=backend.server.v2.store.model.StoreSubmissionsResponse,
)
async def get_submissions(
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
    page: int = 1,
    page_size: int = 20,
):
    """"""
    Get a paginated list of store submissions for the authenticated user.

    Args:
        user_id (str): ID of the authenticated user
        page (int, optional): Page number for pagination. Defaults to 1.
        page_size (int, optional): Number of submissions per page. Defaults to 20.

    Returns:
        StoreListingsResponse: Paginated list of store submissions

    Raises:
        HTTPException: If page or page_size are less than 1
    """"""
    if page < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page must be greater than 0""
        )

    if page_size < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page size must be greater than 0""
        )
    try:
        listings = await backend.server.v2.store.db.get_store_submissions(
            user_id=user_id,
            page=page,
            page_size=page_size,
        )
        return listings
    except Exception:
        logger.exception(""Exception occurred whilst getting store submissions"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={
                ""detail"": ""An error occurred while retrieving the store submissions""
            },
        )


@router.post(
    ""/submissions"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=backend.server.v2.store.model.StoreSubmission,
)
async def create_submission(
    submission_request: backend.server.v2.store.model.StoreSubmissionRequest,
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
):
    """"""
    Create a new store listing submission.

    Args:
        submission_request (StoreSubmissionRequest): The submission details
        user_id (str): ID of the authenticated user submitting the listing

    Returns:
        StoreSubmission: The created store submission

    Raises:
        HTTPException: If there is an error creating the submission
    """"""
    try:
        submission = await backend.server.v2.store.db.create_store_submission(
            user_id=user_id,
            agent_id=submission_request.agent_id,
            agent_version=submission_request.agent_version,
            slug=submission_request.slug,
            name=submission_request.name,
            video_url=submission_request.video_url,
            image_urls=submission_request.image_urls,
            description=submission_request.description,
            sub_heading=submission_request.sub_heading,
            categories=submission_request.categories,
        )
        return submission
    except Exception:
        logger.exception(""Exception occurred whilst creating store submission"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while creating the store submission""},
        )


@router.post(
    ""/submissions/media"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
)
async def upload_submission_media(
    file: fastapi.UploadFile,
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
):
    """"""
    Upload media (images/videos) for a store listing submission.

    Args:
        file (UploadFile): The media file to upload
        user_id (str): ID of the authenticated user uploading the media

    Returns:
        str: URL of the uploaded media file

    Raises:
        HTTPException: If there is an error uploading the media
    """"""
    try:
        media_url = await backend.server.v2.store.media.upload_media(
            user_id=user_id, file=file
        )
        return media_url
    except Exception:
        logger.exception(""Exception occurred whilst uploading submission media"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while uploading the media file""},
        )


@router.post(
    ""/submissions/generate_image"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
)
async def generate_image(
    agent_id: str,
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
) -> fastapi.responses.Response:
    """"""
    Generate an image for a store listing submission.

    Args:
        agent_id (str): ID of the agent to generate an image for
        user_id (str): ID of the authenticated user

    Returns:
        JSONResponse: JSON containing the URL of the generated image
    """"""
    try:
        agent = await backend.data.graph.get_graph(agent_id, user_id=user_id)

        if not agent:
            raise fastapi.HTTPException(
                status_code=404, detail=f""Agent with ID {agent_id} not found""
            )
        # Use .jpeg here since we are generating JPEG images
        filename = f""agent_{agent_id}.jpeg""

        existing_url = await backend.server.v2.store.media.check_media_exists(
            user_id, filename
        )
        if existing_url:
            logger.info(f""Using existing image for agent {agent_id}"")
            return fastapi.responses.JSONResponse(content={""image_url"": existing_url})
        # Generate agent image as JPEG
        image = await backend.server.v2.store.image_gen.generate_agent_image(
            agent=agent
        )

        # Create UploadFile with the correct filename and content_type
        image_file = fastapi.UploadFile(
            file=image,
            filename=filename,
        )

        image_url = await backend.server.v2.store.media.upload_media(
            user_id=user_id, file=image_file, use_file_name=True
        )

        return fastapi.responses.JSONResponse(content={""image_url"": image_url})
    except Exception:
        logger.exception(""Exception occurred whilst generating submission image"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while generating the image""},
        )
","import logging
import typing
import urllib.parse

import autogpt_libs.auth.depends
import autogpt_libs.auth.middleware
import fastapi
import fastapi.responses

import backend.data.graph
import backend.server.v2.store.db
import backend.server.v2.store.image_gen
import backend.server.v2.store.media
import backend.server.v2.store.model

logger = logging.getLogger(__name__)

router = fastapi.APIRouter()

@router.get(
    ""/profile"",
    tags=[""store"", ""private""],
    response_model=backend.server.v2.store.model.ProfileDetails,
)
async def get_profile(
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ]
):
    """"""
    Get the profile details for the authenticated user.
    """"""
    try:
        profile = await backend.server.v2.store.db.get_user_profile(user_id)
        return profile
    except Exception:
        logger.exception(""Exception occurred whilst getting user profile"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while retrieving the user profile""},
        )

@router.post(
    ""/profile"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=backend.server.v2.store.model.CreatorDetails,
)
async def update_or_create_profile(
    profile: backend.server.v2.store.model.Profile,
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
):
    """"""
    Update the store profile for the authenticated user.

    Args:
        profile (Profile): The updated profile details
        user_id (str): ID of the authenticated user

    Returns:
        CreatorDetails: The updated profile

    Raises:
        HTTPException: If there is an error updating the profile
    """"""
    try:
        updated_profile = await backend.server.v2.store.db.update_or_create_profile(
            user_id=user_id, profile=profile
        )
        return updated_profile
    except Exception:
        logger.exception(""Exception occurred whilst updating profile"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while updating the user profile""},
        )

@router.get(
    ""/agents"",
    tags=[""store"", ""public""],
    response_model=backend.server.v2.store.model.StoreAgentsResponse,
)
async def get_agents(
    featured: bool = False,
    creator: str | None = None,
    sorted_by: str | None = None,
    search_query: str | None = None,
    category: str | None = None,
    page: int = 1,
    page_size: int = 20,
):
    """"""
    Get a paginated list of agents from the store with optional filtering and sorting.

    Args:
        featured (bool, optional): Filter to only show featured agents. Defaults to False.
        creator (str | None, optional): Filter agents by creator username. Defaults to None.
        sorted_by (str | None, optional): Sort agents by ""runs"" or ""rating"". Defaults to None.
        search_query (str | None, optional): Search agents by name, subheading and description. Defaults to None.
        category (str | None, optional): Filter agents by category. Defaults to None.
        page (int, optional): Page number for pagination. Defaults to 1.
        page_size (int, optional): Number of agents per page. Defaults to 20.

    Returns:
        StoreAgentsResponse: Paginated list of agents matching the filters

    Raises:
        HTTPException: If page or page_size are less than 1

    Used for:
    - Home Page Featured Agents
    - Home Page Top Agents
    - Search Results
    - Agent Details - Other Agents By Creator
    - Agent Details - Similar Agents
    - Creator Details - Agents By Creator
    """"""
    if page < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page must be greater than 0""
        )
    if page_size < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page size must be greater than 0""
        )
    try:
        agents = await backend.server.v2.store.db.get_store_agents(
            featured=featured,
            creator=creator,
            sorted_by=sorted_by,
            search_query=search_query,
            category=category,
            page=page,
            page_size=page_size,
        )
        return agents
    except Exception:
        logger.exception(""Exception occured whilst getting store agents"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while retrieving the store agents""},
        )

@router.get(
    ""/agents/{username}/{agent_name}"",
    tags=[""store"", ""public""],
    response_model=backend.server.v2.store.model.StoreAgentDetails,
)
async def get_agent(username: str, agent_name: str):
    """"""
    This is only used on the AgentDetails Page

    It returns the store listing agents details.
    """"""
    try:
        username = urllib.parse.unquote(username).lower()
        # URL decode the agent name since it comes from the URL path
        agent_name = urllib.parse.unquote(agent_name).lower()
        agent = await backend.server.v2.store.db.get_store_agent_details(
            username=username, agent_name=agent_name
        )
        return agent
    except Exception:
        logger.exception(""Exception occurred whilst getting store agent details"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={
                ""detail"": ""An error occurred while retrieving the store agent details""
            },
        )

@router.post(
    ""/agents/{username}/{agent_name}/review"",
    tags=[""store""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=backend.server.v2.store.model.StoreReview,
)
async def create_review(
    username: str,
    agent_name: str,
    review: backend.server.v2.store.model.StoreReviewCreate,
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
):
    """"""
    Create a review for a store agent.

    Args:
        username: Creator's username
        agent_name: Name/slug of the agent
        review: Review details including score and optional comments
        user_id: ID of authenticated user creating the review

    Returns:
        The created review
    """"""
    try:
        username = urllib.parse.unquote(username).lower()
        agent_name = urllib.parse.unquote(agent_name)
        # Create the review
        created_review = await backend.server.v2.store.db.create_store_review(
            user_id=user_id,
            store_listing_version_id=review.store_listing_version_id,
            score=review.score,
            comments=review.comments,
        )
        return created_review
    except Exception:
        logger.exception(""Exception occurred whilst creating store review"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while creating the store review""},
        )

@router.get(
    ""/creators"",
    tags=[""store"", ""public""],
    response_model=backend.server.v2.store.model.CreatorsResponse,
)
async def get_creators(
    featured: bool = False,
    search_query: str | None = None,
    sorted_by: str | None = None,
    page: int = 1,
    page_size: int = 20,
):
    """"""
    This is needed for:
    - Home Page Featured Creators
    - Search Results Page

    ---

    To support this functionality we need:
    - featured: bool - to limit the list to just featured agents
    - search_query: str - vector search based on the creators profile description.
    - sorted_by: [agent_rating, agent_runs] -
    """"""
    if page < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page must be greater than 0""
        )
    if page_size < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page size must be greater than 0""
        )
    try:
        creators = await backend.server.v2.store.db.get_store_creators(
            featured=featured,
            search_query=search_query,
            sorted_by=sorted_by,
            page=page,
            page_size=page_size,
        )
        return creators
    except Exception:
        logger.exception(""Exception occurred whilst getting store creators"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while retrieving the store creators""},
        )

@router.get(
    ""/creator/{username}"",
    tags=[""store"", ""public""],
    response_model=backend.server.v2.store.model.CreatorDetails,
)
async def get_creator(
    username: str,
):
    """"""
    Get the details of a creator
    - Creator Details Page
    """"""
    try:
        username = urllib.parse.unquote(username).lower()
        creator = await backend.server.v2.store.db.get_store_creator_details(
            username=username.lower()
        )
        return creator
    except Exception:
        logger.exception(""Exception occurred whilst getting creator details"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={
                ""detail"": ""An error occurred while retrieving the creator details""
            },
        )

@router.get(
    ""/myagents"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=backend.server.v2.store.model.MyAgentsResponse,
)
async def get_my_agents(
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ]
):
    try:
        agents = await backend.server.v2.store.db.get_my_agents(user_id)
        return agents
    except Exception:
        logger.exception(""Exception occurred whilst getting my agents"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while retrieving the my agents""},
        )

@router.delete(
    ""/submissions/{submission_id}"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=bool,
)
async def delete_submission(
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
    submission_id: str,
):
    """"""
    Delete a store listing submission.

    Args:
        user_id (str): ID of the authenticated user
        submission_id (str): ID of the submission to be deleted

    Returns:
        bool: True if the submission was successfully deleted, False otherwise
    """"""
    try:
        result = await backend.server.v2.store.db.delete_store_submission(
            user_id=user_id,
            submission_id=submission_id,
        )
        return result
    except Exception:
        logger.exception(""Exception occurred whilst deleting store submission"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while deleting the store submission""},
        )

@router.get(
    ""/submissions"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=backend.server.v2.store.model.StoreSubmissionsResponse,
)
async def get_submissions(
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
    page: int = 1,
    page_size: int = 20,
):
    """"""
    Get a paginated list of store submissions for the authenticated user.

    Args:
        user_id (str): ID of the authenticated user
        page (int, optional): Page number for pagination. Defaults to 1.
        page_size (int, optional): Number of submissions per page. Defaults to 20.

    Returns:
        StoreListingsResponse: Paginated list of store submissions

    Raises:
        HTTPException: If page or page_size are less than 1
    """"""
    if page < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page must be greater than 0""
        )
    if page_size < 1:
        raise fastapi.HTTPException(
            status_code=422, detail=""Page size must be greater than 0""
        )
    try:
        listings = await backend.server.v2.store.db.get_store_submissions(
            user_id=user_id,
            page=page,
            page_size=page_size,
        )
        return listings
    except Exception:
        logger.exception(""Exception occurred whilst getting store submissions"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={
                ""detail"": ""An error occurred while retrieving the store submissions""
            },
        )

@router.post(
    ""/submissions"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
    response_model=backend.server.v2.store.model.StoreSubmission,
)
async def create_submission(
    submission_request: backend.server.v2.store.model.StoreSubmissionRequest,
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
):
    """"""
    Create a new store listing submission.

    Args:
        submission_request (StoreSubmissionRequest): The submission details
        user_id (str): ID of the authenticated user submitting the listing

    Returns:
        StoreSubmission: The created store submission

    Raises:
        HTTPException: If there is an error creating the submission
    """"""
    try:
        submission = await backend.server.v2.store.db.create_store_submission(
            user_id=user_id,
            agent_id=submission_request.agent_id,
            agent_version=submission_request.agent_version,
            slug=submission_request.slug,
            name=submission_request.name,
            video_url=submission_request.video_url,
            image_urls=submission_request.image_urls,
            description=submission_request.description,
            sub_heading=submission_request.sub_heading,
            categories=submission_request.categories,
        )
        return submission
    except Exception:
        logger.exception(""Exception occurred whilst creating store submission"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while creating the store submission""},
        )

@router.post(
    ""/submissions/media"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
)
async def upload_submission_media(
    file: fastapi.UploadFile,
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
):
    """"""
    Upload media (images/videos) for a store listing submission.

    Args:
        file (UploadFile): The media file to upload
        user_id (str): ID of the authenticated user uploading the media

    Returns:
        str: URL of the uploaded media file

    Raises:
        HTTPException: If there is an error uploading the media
    """"""
    try:
        media_url = await backend.server.v2.store.media.upload_media(
            user_id=user_id, file=file
        )
        return media_url
    except Exception:
        logger.exception(""Exception occurred whilst uploading submission media"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while uploading the media file""},
        )

@router.post(
    ""/submissions/generate_image"",
    tags=[""store"", ""private""],
    dependencies=[fastapi.Depends(autogpt_libs.auth.middleware.auth_middleware)],
)
async def generate_image(
    agent_id: str,
    user_id: typing.Annotated[
        str, fastapi.Depends(autogpt_libs.auth.depends.get_user_id)
    ],
) -> fastapi.responses.Response:
    """"""
    Generate an image for a store listing submission.

    Args:
        agent_id (str): ID of the agent to generate an image for
        user_id (str): ID of the authenticated user

    Returns:
        JSONResponse: JSON containing the URL of the generated image
    """"""
    try:
        agent = await backend.data.graph.get_graph(agent_id, user_id=user_id)
        if not agent:
            raise fastapi.HTTPException(
                status_code=404, detail=f""Agent with ID {agent_id} not found""
            )
        # Use .jpeg here since we are generating JPEG images
        filename = f""agent_{agent_id}.jpeg""
        existing_url = await backend.server.v2.store.media.check_media_exists(
            user_id, filename
        )
        if existing_url:
            logger.info(f""Using existing image for agent {agent_id}"")
            return fastapi.responses.JSONResponse(content={""image_url"": existing_url})
        image = await backend.server.v2.store.image_gen.generate_agent_image(
            agent=agent
        )
        # Create UploadFile with the correct filename and content_type
        image_file = fastapi.UploadFile(
            file=image,
            filename=filename,
        )
        image_url = await backend.server.v2.store.media.upload_media(
            user_id=user_id, file=image_file, use_file_name=True
        )
        return fastapi.responses.JSONResponse(content={""image_url"": image_url})
    except Exception:
        logger.exception(""Exception occurred whilst generating submission image"")
        return fastapi.responses.JSONResponse(
            status_code=500,
            content={""detail"": ""An error occurred while generating the image""},
        )","{'final_score': 98.28, 'length_similarity': 96.72, 'token_similarity': 97.63, 'structure_similarity': 97.43, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
147,"# -*- coding: utf-8 -*-


class QueryApi(object):

    def __init__(self, memory_cache, reverse_index_cluster):
        self.memory_cache = memory_cache
        self.reverse_index_cluster = reverse_index_cluster

    def parse_query(self, query):
        """"""Remove markup, break text into terms, deal with typos,
        normalize capitalization, convert to use boolean operations.
        """"""
        ...

    def process_query(self, query):
        query = self.parse_query(query)
        results = self.memory_cache.get(query)
        if results is None:
            results = self.reverse_index_cluster.process_search(query)
            self.memory_cache.set(query, results)
        return results


class Node(object):

    def __init__(self, query, results):
        self.query = query
        self.results = results


class LinkedList(object):

    def __init__(self):
        self.head = None
        self.tail = None

    def move_to_front(self, node):
        ...

    def append_to_front(self, node):
        ...

    def remove_from_tail(self):
        ...


class Cache(object):

    def __init__(self, MAX_SIZE):
        self.MAX_SIZE = MAX_SIZE
        self.size = 0
        self.lookup = {}
        self.linked_list = LinkedList()

    def get(self, query):
        """"""Get the stored query result from the cache.

        Accessing a node updates its position to the front of the LRU list.
        """"""
        node = self.lookup[query]
        if node is None:
            return None
        self.linked_list.move_to_front(node)
        return node.results

    def set(self, results, query):
        """"""Set the result for the given query key in the cache.

        When updating an entry, updates its position to the front of the LRU list.
        If the entry is new and the cache is at capacity, removes the oldest entry
        before the new entry is added.
        """"""
        node = self.map[query]
        if node is not None:
            # Key exists in cache, update the value
            node.results = results
            self.linked_list.move_to_front(node)
        else:
            # Key does not exist in cache
            if self.size == self.MAX_SIZE:
                # Remove the oldest entry from the linked list and lookup
                self.lookup.pop(self.linked_list.tail.query, None)
                self.linked_list.remove_from_tail()
            else:
                self.size += 1
            # Add the new key and value
            new_node = Node(query, results)
            self.linked_list.append_to_front(new_node)
            self.lookup[query] = new_node
","# -*- coding: utf-8 -*-



class QueryApi(object):

    def __init__(self, memory_cache, reverse_index_cluster):
        self.memory_cache = memory_cache
        self.reverse_index_cluster = reverse_index_cluster

    def parse_query(self, query):
        """"""Remove markup, break text into terms, deal with typos,
        normalize capitalization, convert to use boolean operations.
        """"""
        ...

    def process_query(self, query):
        query = self.parse_query(query)
        results = self.memory_cache.get(query)
        if results is None:
            results = self.reverse_index_cluster.process_search(query)
            self.memory_cache.set(query, results)
        return results


class Node(object):

    def __init__(self, query, results):
        self.query = query
        self.results = results



class LinkedList(object):

    def __init__(self):
        self.head = None
        self.tail = None

    def move_to_front(self, node):
        ...

    def append_to_front(self, node):
        ...

    def remove_from_tail(self):
        ...


class Cache(object):

    def __init__(self, MAX_SIZE):
        self.MAX_SIZE = MAX_SIZE
        self.size = 0
        self.lookup = {}
        self.linked_list = LinkedList()

    def get(self, query):
        """"""Get the stored query result from the cache.

        Accessing a node updates its position to the front of the LRU list.
        """"""
        node = self.lookup[query]
        if node is None:
            return None
        self.linked_list.move_to_front(node)
        return node.results

    def set(self, results, query):
        """"""Set the result for the given query key in the cache.

        When updating an entry, updates its position to the front of the LRU list.
        If the entry is new and the cache is at capacity, removes the oldest entry
        before the new entry is added.
        """"""
        node = self.map[query]
        if node is not None:
            # Key exists in cache, update the value
            node.results = results
            self.linked_list.move_to_front(node)
        else:
            # Key does not exist in cache
            if self.size == self.MAX_SIZE:
                # Remove the oldest entry from the linked list and lookup
                self.lookup.pop(self.linked_list.tail.query, None)
                self.linked_list.remove_from_tail()
            else:
                self.size += 1
            # Add the new key and value
            new_node = Node(query, results)
            self.linked_list.append_to_front(new_node)
            self.lookup[query] = new_node","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
148,"from abc import ABCMeta
from enum import Enum


class UserService(object):

    def __init__(self):
        self.users_by_id = {}  # key: user id, value: User

    def add_user(self, user_id, name, pass_hash):
        pass

    def remove_user(self, user_id):
        pass

    def add_friend_request(self, from_user_id, to_user_id):
        pass

    def approve_friend_request(self, from_user_id, to_user_id):
        pass

    def reject_friend_request(self, from_user_id, to_user_id):
        pass


class User(object):

    def __init__(self, user_id, name, pass_hash):
        self.user_id = user_id
        self.name = name
        self.pass_hash = pass_hash
        self.friends_by_id = {}  # key: friend id, value: User
        self.friend_ids_to_private_chats = {}  # key: friend id, value: private chats
        self.group_chats_by_id = {}  # key: chat id, value: GroupChat
        self.received_friend_requests_by_friend_id = {}  # key: friend id, value: AddRequest
        self.sent_friend_requests_by_friend_id = {}  # key: friend id, value: AddRequest

    def message_user(self, friend_id, message):
        pass

    def message_group(self, group_id, message):
        pass

    def send_friend_request(self, friend_id):
        pass

    def receive_friend_request(self, friend_id):
        pass

    def approve_friend_request(self, friend_id):
        pass

    def reject_friend_request(self, friend_id):
        pass


class Chat(metaclass=ABCMeta):

    def __init__(self, chat_id):
        self.chat_id = chat_id
        self.users = []
        self.messages = []


class PrivateChat(Chat):

    def __init__(self, first_user, second_user):
        super(PrivateChat, self).__init__()
        self.users.append(first_user)
        self.users.append(second_user)


class GroupChat(Chat):

    def add_user(self, user):
        pass

    def remove_user(self, user):
        pass


class Message(object):

    def __init__(self, message_id, message, timestamp):
        self.message_id = message_id
        self.message = message
        self.timestamp = timestamp


class AddRequest(object):

    def __init__(self, from_user_id, to_user_id, request_status, timestamp):
        self.from_user_id = from_user_id
        self.to_user_id = to_user_id
        self.request_status = request_status
        self.timestamp = timestamp


class RequestStatus(Enum):

    UNREAD = 0
    READ = 1
    ACCEPTED = 2
    REJECTED = 3
","from abc import ABCMeta
from enum import Enum

class UserService(object):

    def __init__(self):
        self.users_by_id = {}  # key: user id, value: User

    def add_user(self, user_id, name, pass_hash):
        pass

    def remove_user(self, user_id):
        pass

    def add_friend_request(self, from_user_id, to_user_id):
        pass

    def approve_friend_request(self, from_user_id, to_user_id):
        pass

    def reject_friend_request(self, from_user_id, to_user_id):
        pass


class User(object):

    def __init__(self, user_id, name, pass_hash):
        self.user_id = user_id
        self.name = name
        self.pass_hash = pass_hash
        self.friends_by_id = {}  # key: friend id, value: User
        self.friend_ids_to_private_chats = {}  # key: friend id, value: private chats
        self.group_chats_by_id = {}  # key: chat id, value: GroupChat
        self.received_friend_requests_by_friend_id = {}  # key: friend id, value: AddRequest
        self.sent_friend_requests_by_friend_id = {}  # key: friend id, value: AddRequest

    def message_user(self, friend_id, message):
        pass

    def message_group(self, group_id, message):
        pass

    def send_friend_request(self, friend_id):
        pass

    def receive_friend_request(self, friend_id):
        pass

    def approve_friend_request(self, friend_id):
        pass

    def reject_friend_request(self, friend_id):
        pass


class Chat(metaclass=ABCMeta):

    def __init__(self, chat_id):
        self.chat_id = chat_id
        self.users = []
        self.messages = []


class PrivateChat(Chat):

    def __init__(self, first_user, second_user):
        super(PrivateChat, self).__init__()
        self.users.append(first_user)
        self.users.append(second_user)


class GroupChat(Chat):

    def add_user(self, user):
        pass

    def remove_user(self, user):
        pass


class Message(object):

    def __init__(self, message_id, message, timestamp):
        self.message_id = message_id
        self.message = message
        self.timestamp = timestamp


class AddRequest(object):

    def __init__(self, from_user_id, to_user_id, request_status, timestamp):
        self.from_user_id = from_user_id
        self.to_user_id = to_user_id
        self.request_status = request_status
        self.timestamp = timestamp


class RequestStatus(Enum):

    UNREAD = 0
    READ = 1
    ACCEPTED = 2
    REJECTED = 3","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
149,"import logging

from fastapi import Request
from strenum import StrEnum

from backend.data import integrations
from backend.integrations.providers import ProviderName

from ._manual_base import ManualWebhookManagerBase

logger = logging.getLogger(__name__)


class CompassWebhookType(StrEnum):
    TRANSCRIPTION = ""transcription""
    TASK = ""task""


class CompassWebhookManager(ManualWebhookManagerBase):
    PROVIDER_NAME = ProviderName.COMPASS
    WebhookType = CompassWebhookType

    @classmethod
    async def validate_payload(
        cls, webhook: integrations.Webhook, request: Request
    ) -> tuple[dict, str]:
        payload = await request.json()
        event_type = CompassWebhookType.TRANSCRIPTION  # currently the only type

        return payload, event_type
","import logging
from fastapi import Request
from strenum import StrEnum
from backend.data import integrations
from backend.integrations.providers import ProviderName
from ._manual_base import ManualWebhookManagerBase

logger = logging.getLogger(__name__)

class CompassWebhookType(StrEnum):
    TRANSCRIPTION = ""transcription""
    TASK = ""task""

class CompassWebhookManager(ManualWebhookManagerBase):
    PROVIDER_NAME = ProviderName.COMPASS
    WebhookType = CompassWebhookType

    @classmethod
    async def validate_payload(
        cls, webhook: integrations.Webhook, request: Request
    ) -> tuple[dict, str]:
        payload = await request.json()
        event_type = CompassWebhookType.TRANSCRIPTION  # currently the only type
        return payload, event_type","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
150,"r""""""
Problem:

The n queens problem is: placing N queens on a N * N chess board such that no queen
can attack any other queens placed on that chess board.  This means that one queen
cannot have any other queen on its horizontal, vertical and diagonal lines.

Solution:

To solve this problem we will use simple math. First we know the queen can move in all
the possible ways, we can simplify it in this: vertical, horizontal, diagonal left and
 diagonal right.

We can visualize it like this:

left diagonal = \
right diagonal = /

On a chessboard vertical movement could be the rows and horizontal movement could be
the columns.

In programming we can use an array, and in this array each index could be the rows and
each value in the array could be the column. For example:

    . Q . .     We have this chessboard with one queen in each column and each queen
    . . . Q     can't attack to each other.
    Q . . .     The array for this example would look like this: [1, 3, 0, 2]
    . . Q .

So if we use an array and we verify that each value in the array is different to each
other we know that at least the queens can't attack each other in horizontal and
vertical.

At this point we have it halfway completed and we will treat the chessboard as a
Cartesian plane.  Hereinafter we are going to remember basic math, so in the school we
learned this formula:

    Slope of a line:

           y2 - y1
     m = ----------
          x2 - x1

This formula allow us to get the slope. For the angles 45º (right diagonal) and 135º
(left diagonal) this formula gives us m = 1, and m = -1 respectively.

See::
https://www.enotes.com/homework-help/write-equation-line-that-hits-origin-45-degree-1474860

Then we have this other formula:

Slope intercept:

y = mx + b

b is where the line crosses the Y axis (to get more information see:
https://www.mathsisfun.com/y_intercept.html), if we change the formula to solve for b
we would have:

y - mx = b

And since we already have the m values for the angles 45º and 135º, this formula would
look like this:

45º: y - (1)x = b
45º: y - x = b

135º: y - (-1)x = b
135º: y + x = b

y = row
x = column

Applying these two formulas we can check if a queen in some position is being attacked
for another one or vice versa.

""""""

from __future__ import annotations


def depth_first_search(
    possible_board: list[int],
    diagonal_right_collisions: list[int],
    diagonal_left_collisions: list[int],
    boards: list[list[str]],
    n: int,
) -> None:
    """"""
    >>> boards = []
    >>> depth_first_search([], [], [], boards, 4)
    >>> for board in boards:
    ...     print(board)
    ['. Q . . ', '. . . Q ', 'Q . . . ', '. . Q . ']
    ['. . Q . ', 'Q . . . ', '. . . Q ', '. Q . . ']
    """"""

    # Get next row in the current board (possible_board) to fill it with a queen
    row = len(possible_board)

    # If row is equal to the size of the board it means there are a queen in each row in
    # the current board (possible_board)
    if row == n:
        # We convert the variable possible_board that looks like this: [1, 3, 0, 2] to
        # this: ['. Q . . ', '. . . Q ', 'Q . . . ', '. . Q . ']
        boards.append(["". "" * i + ""Q "" + "". "" * (n - 1 - i) for i in possible_board])
        return

    # We iterate each column in the row to find all possible results in each row
    for col in range(n):
        # We apply that we learned previously. First we check that in the current board
        # (possible_board) there are not other same value because if there is it means
        # that there are a collision in vertical. Then we apply the two formulas we
        # learned before:
        #
        # 45º: y - x = b or 45: row - col = b
        # 135º: y + x = b or row + col = b.
        #
        # And we verify if the results of this two formulas not exist in their variables
        # respectively.  (diagonal_right_collisions, diagonal_left_collisions)
        #
        # If any or these are True it means there is a collision so we continue to the
        # next value in the for loop.
        if (
            col in possible_board
            or row - col in diagonal_right_collisions
            or row + col in diagonal_left_collisions
        ):
            continue

        # If it is False we call dfs function again and we update the inputs
        depth_first_search(
            [*possible_board, col],
            [*diagonal_right_collisions, row - col],
            [*diagonal_left_collisions, row + col],
            boards,
            n,
        )


def n_queens_solution(n: int) -> None:
    boards: list[list[str]] = []
    depth_first_search([], [], [], boards, n)

    # Print all the boards
    for board in boards:
        for column in board:
            print(column)
        print("""")

    print(len(boards), ""solutions were found."")


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
    n_queens_solution(4)
","r""""""
Problem:

The n queens problem is: placing N queens on a N * N chess board such that no queen
can attack any other queens placed on that chess board.  This means that one queen
cannot have any other queen on its horizontal, vertical and diagonal lines.

Solution:

To solve this problem we will use simple math. First we know the queen can move in all
the possible ways, we can simplify it in this: vertical, horizontal, diagonal left and
 diagonal right.

We can visualize it like this:

left diagonal = \
right diagonal = /

On a chessboard vertical movement could be the rows and horizontal movement could be
the columns.

In programming we can use an array, and in this array each index could be the rows and
each value in the array could be the column. For example:

    . Q . .     We have this chessboard with one queen in each column and each queen
    . . . Q     can't attack to each other.
    Q . . .     The array for this example would look like this: [1, 3, 0, 2]
    . . Q .

So if we use an array and we verify that each value in the array is different to each
other we know that at least the queens can't attack each other in horizontal and
vertical.

At this point we have it halfway completed and we will treat the chessboard as a
Cartesian plane.  Hereinafter we are going to remember basic math, so in the school we
learned this formula:

    Slope of a line:

           y2 - y1
     m = ----------
          x2 - x1

This formula allow us to get the slope. For the angles 45º (right diagonal) and 135º
(left diagonal) this formula gives us m = 1, and m = -1 respectively.

See::
https://www.enotes.com/homework-help/write-equation-line-that-hits-origin-45-degree-1474860

Then we have this other formula:

Slope intercept:

y = mx + b

b is where the line crosses the Y axis (to get more information see:
https://www.mathsisfun.com/y_intercept.html), if we change the formula to solve for b
we would have:

y - mx = b

And since we already have the m values for the angles 45º and 135º, this formula would
look like this:

45º: y - (1)x = b
45º: y - x = b

135º: y - (-1)x = b
135º: y + x = b

y = row
x = column

Applying these two formulas we can check if a queen in some position is being attacked
for another one or vice versa.

""""""

from __future__ import annotations


def depth_first_search(
    possible_board: list[int],
    diagonal_right_collisions: list[int],
    diagonal_left_collisions: list[int],
    boards: list[list[str]],
    n: int,
) -> None:
    """"""
    >>> boards = []
    >>> depth_first_search([], [], [], boards, 4)
    >>> for board in boards:
    ...     print(board)
    ['. Q . . ', '. . . Q ', 'Q . . . ', '. . Q . ']
    ['. . Q . ', 'Q . . . ', '. . . Q ', '. Q . . ']
    """"""
    # Get next row in the current board (possible_board) to fill it with a queen
    row = len(possible_board)
    # If row is equal to the size of the board it means there are a queen in each row in
    # the current board (possible_board)
    if row == n:
        # We convert the variable possible_board that looks like this: [1, 3, 0, 2] to
        # this: ['. Q . . ', '. . . Q ', 'Q . . . ', '. . Q . ']
        boards.append(["". "" * i + ""Q "" + "". "" * (n - 1 - i) for i in possible_board])
        return
    # We iterate each column in the row to find all possible results in each row
    for col in range(n):
        # We apply that we learned previously. First we check that in the current board
        # (possible_board) there are not other same value because if there is it means
        # that there are a collision in vertical. Then we apply the two formulas we
        # learned before:
        #
        # 45º: y - x = b or 45: row - col = b
        # 135º: y + x = b or row + col = b.
        #
        # And we verify if the results of this two formulas not exist in their variables
        # respectively.  (diagonal_right_collisions, diagonal_left_collisions)
        #
        # If any or these are True it means there is a collision so we continue to the
        # next value in the for loop.
        if (
            col in possible_board
            or row - col in diagonal_right_collisions
            or row + col in diagonal_left_collisions
        ):
            continue
        # If it is False we call dfs function again and we update the inputs
        depth_first_search(
            [*possible_board, col],
            [*diagonal_right_collisions, row - col],
            [*diagonal_left_collisions, row + col],
            boards,
            n,
        )


def n_queens_solution(n: int) -> None:
    boards: list[list[str]] = []
    depth_first_search([], [], [], boards, n)
    for board in boards:
        for column in board:
            print(column)
        print("""")
    print(len(boards), ""solutions were found."")


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
    n_queens_solution(4)","{'final_score': 99.64, 'length_similarity': 99.43, 'token_similarity': 99.66, 'structure_similarity': 99.16, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
151,"from typing import Any, List

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema, BlockType
from backend.data.model import SchemaField
from backend.util.mock import MockObject
from backend.util.text import TextFormatter

formatter = TextFormatter()


class StoreValueBlock(Block):
    """"""
    This block allows you to provide a constant value as a block, in a stateless manner.
    The common use-case is simply pass the `input` data, it will `output` the same data.
    The block output will be static, the output can be consumed multiple times.
    """"""

    class Input(BlockSchema):
        input: Any = SchemaField(
            description=""Trigger the block to produce the output. ""
            ""The value is only used when `data` is None.""
        )
        data: Any = SchemaField(
            description=""The constant data to be retained in the block. ""
            ""This value is passed as `output`."",
            default=None,
        )

    class Output(BlockSchema):
        output: Any = SchemaField(description=""The stored data retained in the block."")

    def __init__(self):
        super().__init__(
            id=""1ff065e9-88e8-4358-9d82-8dc91f622ba9"",
            description=""This block forwards an input value as output, allowing reuse without change."",
            categories={BlockCategory.BASIC},
            input_schema=StoreValueBlock.Input,
            output_schema=StoreValueBlock.Output,
            test_input=[
                {""input"": ""Hello, World!""},
                {""input"": ""Hello, World!"", ""data"": ""Existing Data""},
            ],
            test_output=[
                (""output"", ""Hello, World!""),  # No data provided, so trigger is returned
                (""output"", ""Existing Data""),  # Data is provided, so data is returned.
            ],
            static_output=True,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""output"", input_data.data or input_data.input


class PrintToConsoleBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(description=""The text to print to the console."")

    class Output(BlockSchema):
        status: str = SchemaField(description=""The status of the print operation."")

    def __init__(self):
        super().__init__(
            id=""f3b1c1b2-4c4f-4f0d-8d2f-4c4f0d8d2f4c"",
            description=""Print the given text to the console, this is used for a debugging purpose."",
            categories={BlockCategory.BASIC},
            input_schema=PrintToConsoleBlock.Input,
            output_schema=PrintToConsoleBlock.Output,
            test_input={""text"": ""Hello, World!""},
            test_output=(""status"", ""printed""),
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        print("">>>>> Print: "", input_data.text)
        yield ""status"", ""printed""


class FindInDictionaryBlock(Block):
    class Input(BlockSchema):
        input: Any = SchemaField(description=""Dictionary to lookup from"")
        key: str | int = SchemaField(description=""Key to lookup in the dictionary"")

    class Output(BlockSchema):
        output: Any = SchemaField(description=""Value found for the given key"")
        missing: Any = SchemaField(
            description=""Value of the input that missing the key""
        )

    def __init__(self):
        super().__init__(
            id=""0e50422c-6dee-4145-83d6-3a5a392f65de"",
            description=""Lookup the given key in the input dictionary/object/list and return the value."",
            input_schema=FindInDictionaryBlock.Input,
            output_schema=FindInDictionaryBlock.Output,
            test_input=[
                {""input"": {""apple"": 1, ""banana"": 2, ""cherry"": 3}, ""key"": ""banana""},
                {""input"": {""x"": 10, ""y"": 20, ""z"": 30}, ""key"": ""w""},
                {""input"": [1, 2, 3], ""key"": 1},
                {""input"": [1, 2, 3], ""key"": 3},
                {""input"": MockObject(value=""!!"", key=""key""), ""key"": ""key""},
                {""input"": [{""k1"": ""v1""}, {""k2"": ""v2""}, {""k1"": ""v3""}], ""key"": ""k1""},
            ],
            test_output=[
                (""output"", 2),
                (""missing"", {""x"": 10, ""y"": 20, ""z"": 30}),
                (""output"", 2),
                (""missing"", [1, 2, 3]),
                (""output"", ""key""),
                (""output"", [""v1"", ""v3""]),
            ],
            categories={BlockCategory.BASIC},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        obj = input_data.input
        key = input_data.key

        if isinstance(obj, dict) and key in obj:
            yield ""output"", obj[key]
        elif isinstance(obj, list) and isinstance(key, int) and 0 <= key < len(obj):
            yield ""output"", obj[key]
        elif isinstance(obj, list) and isinstance(key, str):
            if len(obj) == 0:
                yield ""output"", []
            elif isinstance(obj[0], dict) and key in obj[0]:
                yield ""output"", [item[key] for item in obj if key in item]
            else:
                yield ""output"", [getattr(val, key) for val in obj if hasattr(val, key)]
        elif isinstance(obj, object) and isinstance(key, str) and hasattr(obj, key):
            yield ""output"", getattr(obj, key)
        else:
            yield ""missing"", input_data.input


class AgentInputBlock(Block):
    """"""
    This block is used to provide input to the graph.

    It takes in a value, name, description, default values list and bool to limit selection to default values.

    It Outputs the value passed as input.
    """"""

    class Input(BlockSchema):
        name: str = SchemaField(description=""The name of the input."")
        value: Any = SchemaField(
            description=""The value to be passed as input."",
            default=None,
        )
        title: str | None = SchemaField(
            description=""The title of the input."", default=None, advanced=True
        )
        description: str | None = SchemaField(
            description=""The description of the input."",
            default=None,
            advanced=True,
        )
        placeholder_values: List[Any] = SchemaField(
            description=""The placeholder values to be passed as input."",
            default=[],
            advanced=True,
        )
        limit_to_placeholder_values: bool = SchemaField(
            description=""Whether to limit the selection to placeholder values."",
            default=False,
            advanced=True,
        )
        advanced: bool = SchemaField(
            description=""Whether to show the input in the advanced section, if the field is not required."",
            default=False,
            advanced=True,
        )
        secret: bool = SchemaField(
            description=""Whether the input should be treated as a secret."",
            default=False,
            advanced=True,
        )

    class Output(BlockSchema):
        result: Any = SchemaField(description=""The value passed as input."")

    def __init__(self):
        super().__init__(
            id=""c0a8e994-ebf1-4a9c-a4d8-89d09c86741b"",
            description=""This block is used to provide input to the graph."",
            input_schema=AgentInputBlock.Input,
            output_schema=AgentInputBlock.Output,
            test_input=[
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""input_1"",
                    ""description"": ""This is a test input."",
                    ""placeholder_values"": [],
                    ""limit_to_placeholder_values"": False,
                },
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""input_2"",
                    ""description"": ""This is a test input."",
                    ""placeholder_values"": [""Hello, World!""],
                    ""limit_to_placeholder_values"": True,
                },
            ],
            test_output=[
                (""result"", ""Hello, World!""),
                (""result"", ""Hello, World!""),
            ],
            categories={BlockCategory.INPUT, BlockCategory.BASIC},
            block_type=BlockType.INPUT,
            static_output=True,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""result"", input_data.value


class AgentOutputBlock(Block):
    """"""
    Records the output of the graph for users to see.

    Behavior:
        If `format` is provided and the `value` is of a type that can be formatted,
        the block attempts to format the recorded_value using the `format`.
        If formatting fails or no `format` is provided, the raw `value` is output.
    """"""

    class Input(BlockSchema):
        value: Any = SchemaField(
            description=""The value to be recorded as output."",
            default=None,
            advanced=False,
        )
        name: str = SchemaField(description=""The name of the output."")
        title: str | None = SchemaField(
            description=""The title of the output."",
            default=None,
            advanced=True,
        )
        description: str | None = SchemaField(
            description=""The description of the output."",
            default=None,
            advanced=True,
        )
        format: str = SchemaField(
            description=""The format string to be used to format the recorded_value."",
            default="""",
            advanced=True,
        )
        advanced: bool = SchemaField(
            description=""Whether to treat the output as advanced."",
            default=False,
            advanced=True,
        )
        secret: bool = SchemaField(
            description=""Whether the output should be treated as a secret."",
            default=False,
            advanced=True,
        )

    class Output(BlockSchema):
        output: Any = SchemaField(description=""The value recorded as output."")

    def __init__(self):
        super().__init__(
            id=""363ae599-353e-4804-937e-b2ee3cef3da4"",
            description=""Stores the output of the graph for users to see."",
            input_schema=AgentOutputBlock.Input,
            output_schema=AgentOutputBlock.Output,
            test_input=[
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""output_1"",
                    ""description"": ""This is a test output."",
                    ""format"": ""{{ output_1 }}!!"",
                },
                {
                    ""value"": ""42"",
                    ""name"": ""output_2"",
                    ""description"": ""This is another test output."",
                    ""format"": ""{{ output_2 }}"",
                },
                {
                    ""value"": MockObject(value=""!!"", key=""key""),
                    ""name"": ""output_3"",
                    ""description"": ""This is a test output with a mock object."",
                    ""format"": ""{{ output_3 }}"",
                },
            ],
            test_output=[
                (""output"", ""Hello, World!!!""),
                (""output"", ""42""),
                (""output"", MockObject(value=""!!"", key=""key"")),
            ],
            categories={BlockCategory.OUTPUT, BlockCategory.BASIC},
            block_type=BlockType.OUTPUT,
            static_output=True,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        """"""
        Attempts to format the recorded_value using the fmt_string if provided.
        If formatting fails or no fmt_string is given, returns the original recorded_value.
        """"""
        if input_data.format:
            try:
                yield ""output"", formatter.format_string(
                    input_data.format, {input_data.name: input_data.value}
                )
            except Exception as e:
                yield ""output"", f""Error: {e}, {input_data.value}""
        else:
            yield ""output"", input_data.value


class AddToDictionaryBlock(Block):
    class Input(BlockSchema):
        dictionary: dict[Any, Any] = SchemaField(
            default={},
            description=""The dictionary to add the entry to. If not provided, a new dictionary will be created."",
        )
        key: str = SchemaField(
            default="""",
            description=""The key for the new entry."",
            placeholder=""new_key"",
            advanced=False,
        )
        value: Any = SchemaField(
            default=None,
            description=""The value for the new entry."",
            placeholder=""new_value"",
            advanced=False,
        )
        entries: dict[Any, Any] = SchemaField(
            default={},
            description=""The entries to add to the dictionary. This is the batch version of the `key` and `value` fields."",
            advanced=True,
        )

    class Output(BlockSchema):
        updated_dictionary: dict = SchemaField(
            description=""The dictionary with the new entry added.""
        )
        error: str = SchemaField(description=""Error message if the operation failed."")

    def __init__(self):
        super().__init__(
            id=""31d1064e-7446-4693-a7d4-65e5ca1180d1"",
            description=""Adds a new key-value pair to a dictionary. If no dictionary is provided, a new one is created."",
            categories={BlockCategory.BASIC},
            input_schema=AddToDictionaryBlock.Input,
            output_schema=AddToDictionaryBlock.Output,
            test_input=[
                {
                    ""dictionary"": {""existing_key"": ""existing_value""},
                    ""key"": ""new_key"",
                    ""value"": ""new_value"",
                },
                {""key"": ""first_key"", ""value"": ""first_value""},
                {
                    ""dictionary"": {""existing_key"": ""existing_value""},
                    ""entries"": {""new_key"": ""new_value"", ""first_key"": ""first_value""},
                },
            ],
            test_output=[
                (
                    ""updated_dictionary"",
                    {""existing_key"": ""existing_value"", ""new_key"": ""new_value""},
                ),
                (""updated_dictionary"", {""first_key"": ""first_value""}),
                (
                    ""updated_dictionary"",
                    {
                        ""existing_key"": ""existing_value"",
                        ""new_key"": ""new_value"",
                        ""first_key"": ""first_value"",
                    },
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        updated_dict = input_data.dictionary.copy()

        if input_data.value is not None and input_data.key:
            updated_dict[input_data.key] = input_data.value

        for key, value in input_data.entries.items():
            updated_dict[key] = value

        yield ""updated_dictionary"", updated_dict


class AddToListBlock(Block):
    class Input(BlockSchema):
        list: List[Any] = SchemaField(
            default=[],
            advanced=False,
            description=""The list to add the entry to. If not provided, a new list will be created."",
        )
        entry: Any = SchemaField(
            description=""The entry to add to the list. Can be of any type (string, int, dict, etc.)."",
            advanced=False,
            default=None,
        )
        entries: List[Any] = SchemaField(
            default=[],
            description=""The entries to add to the list. This is the batch version of the `entry` field."",
            advanced=True,
        )
        position: int | None = SchemaField(
            default=None,
            description=""The position to insert the new entry. If not provided, the entry will be appended to the end of the list."",
        )

    class Output(BlockSchema):
        updated_list: List[Any] = SchemaField(
            description=""The list with the new entry added.""
        )
        error: str = SchemaField(description=""Error message if the operation failed."")

    def __init__(self):
        super().__init__(
            id=""aeb08fc1-2fc1-4141-bc8e-f758f183a822"",
            description=""Adds a new entry to a list. The entry can be of any type. If no list is provided, a new one is created."",
            categories={BlockCategory.BASIC},
            input_schema=AddToListBlock.Input,
            output_schema=AddToListBlock.Output,
            test_input=[
                {
                    ""list"": [1, ""string"", {""existing_key"": ""existing_value""}],
                    ""entry"": {""new_key"": ""new_value""},
                    ""position"": 1,
                },
                {""entry"": ""first_entry""},
                {""list"": [""a"", ""b"", ""c""], ""entry"": ""d""},
                {
                    ""entry"": ""e"",
                    ""entries"": [""f"", ""g""],
                    ""list"": [""a"", ""b""],
                    ""position"": 1,
                },
            ],
            test_output=[
                (
                    ""updated_list"",
                    [
                        1,
                        {""new_key"": ""new_value""},
                        ""string"",
                        {""existing_key"": ""existing_value""},
                    ],
                ),
                (""updated_list"", [""first_entry""]),
                (""updated_list"", [""a"", ""b"", ""c"", ""d""]),
                (""updated_list"", [""a"", ""f"", ""g"", ""e"", ""b""]),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        entries_added = input_data.entries.copy()
        if input_data.entry:
            entries_added.append(input_data.entry)

        updated_list = input_data.list.copy()
        if (pos := input_data.position) is not None:
            updated_list = updated_list[:pos] + entries_added + updated_list[pos:]
        else:
            updated_list += entries_added

        yield ""updated_list"", updated_list


class NoteBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(description=""The text to display in the sticky note."")

    class Output(BlockSchema):
        output: str = SchemaField(description=""The text to display in the sticky note."")

    def __init__(self):
        super().__init__(
            id=""cc10ff7b-7753-4ff2-9af6-9399b1a7eddc"",
            description=""This block is used to display a sticky note with the given text."",
            categories={BlockCategory.BASIC},
            input_schema=NoteBlock.Input,
            output_schema=NoteBlock.Output,
            test_input={""text"": ""Hello, World!""},
            test_output=[
                (""output"", ""Hello, World!""),
            ],
            block_type=BlockType.NOTE,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""output"", input_data.text


class CreateDictionaryBlock(Block):
    class Input(BlockSchema):
        values: dict[str, Any] = SchemaField(
            description=""Key-value pairs to create the dictionary with"",
            placeholder=""e.g., {'name': 'Alice', 'age': 25}"",
        )

    class Output(BlockSchema):
        dictionary: dict[str, Any] = SchemaField(
            description=""The created dictionary containing the specified key-value pairs""
        )
        error: str = SchemaField(
            description=""Error message if dictionary creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""b924ddf4-de4f-4b56-9a85-358930dcbc91"",
            description=""Creates a dictionary with the specified key-value pairs. Use this when you know all the values you want to add upfront."",
            categories={BlockCategory.DATA},
            input_schema=CreateDictionaryBlock.Input,
            output_schema=CreateDictionaryBlock.Output,
            test_input=[
                {
                    ""values"": {""name"": ""Alice"", ""age"": 25, ""city"": ""New York""},
                },
                {
                    ""values"": {""numbers"": [1, 2, 3], ""active"": True, ""score"": 95.5},
                },
            ],
            test_output=[
                (
                    ""dictionary"",
                    {""name"": ""Alice"", ""age"": 25, ""city"": ""New York""},
                ),
                (
                    ""dictionary"",
                    {""numbers"": [1, 2, 3], ""active"": True, ""score"": 95.5},
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        try:
            # The values are already validated by Pydantic schema
            yield ""dictionary"", input_data.values
        except Exception as e:
            yield ""error"", f""Failed to create dictionary: {str(e)}""


class CreateListBlock(Block):
    class Input(BlockSchema):
        values: List[Any] = SchemaField(
            description=""A list of values to be combined into a new list."",
            placeholder=""e.g., ['Alice', 25, True]"",
        )

    class Output(BlockSchema):
        list: List[Any] = SchemaField(
            description=""The created list containing the specified values.""
        )
        error: str = SchemaField(description=""Error message if list creation failed."")

    def __init__(self):
        super().__init__(
            id=""a912d5c7-6e00-4542-b2a9-8034136930e4"",
            description=""Creates a list with the specified values. Use this when you know all the values you want to add upfront."",
            categories={BlockCategory.DATA},
            input_schema=CreateListBlock.Input,
            output_schema=CreateListBlock.Output,
            test_input=[
                {
                    ""values"": [""Alice"", 25, True],
                },
                {
                    ""values"": [1, 2, 3, ""four"", {""key"": ""value""}],
                },
            ],
            test_output=[
                (
                    ""list"",
                    [""Alice"", 25, True],
                ),
                (
                    ""list"",
                    [1, 2, 3, ""four"", {""key"": ""value""}],
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        try:
            # The values are already validated by Pydantic schema
            yield ""list"", input_data.values
        except Exception as e:
            yield ""error"", f""Failed to create list: {str(e)}""
","from typing import Any, List

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema, BlockType
from backend.data.model import SchemaField
from backend.util.mock import MockObject
from backend.util.text import TextFormatter

formatter = TextFormatter()


class StoreValueBlock(Block):
    """"""
    This block allows you to provide a constant value as a block, in a stateless manner.
    The common use-case is simply pass the `input` data, it will `output` the same data.
    The block output will be static, the output can be consumed multiple times.
    """"""

    class Input(BlockSchema):
        input: Any = SchemaField(
            description=""Trigger the block to produce the output. ""
            ""The value is only used when `data` is None.""
        )
        data: Any = SchemaField(
            description=""The constant data to be retained in the block. ""
            ""This value is passed as `output`."",
            default=None,
        )

    class Output(BlockSchema):
        output: Any = SchemaField(description=""The stored data retained in the block."")

    def __init__(self):
        super().__init__(
            id=""1ff065e9-88e8-4358-9d82-8dc91f622ba9"",
            description=""This block forwards an input value as output, allowing reuse without change."",
            categories={BlockCategory.BASIC},
            input_schema=StoreValueBlock.Input,
            output_schema=StoreValueBlock.Output,
            test_input=[
                {""input"": ""Hello, World!""},
                {""input"": ""Hello, World!"", ""data"": ""Existing Data""},
            ],
            test_output=[
                (""output"", ""Hello, World!""),  # No data provided, so trigger is returned
                (""output"", ""Existing Data""),  # Data is provided, so data is returned.
            ],
            static_output=True,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""output"", input_data.data or input_data.input


class PrintToConsoleBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(description=""The text to print to the console."")

    class Output(BlockSchema):
        status: str = SchemaField(description=""The status of the print operation."")

    def __init__(self):
        super().__init__(
            id=""f3b1c1b2-4c4f-4f0d-8d2f-4c4f0d8d2f4c"",
            description=""Print the given text to the console, this is used for a debugging purpose."",
            categories={BlockCategory.BASIC},
            input_schema=PrintToConsoleBlock.Input,
            output_schema=PrintToConsoleBlock.Output,
            test_input={""text"": ""Hello, World!""},
            test_output=(""status"", ""printed""),
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        print("">>>>> Print: "", input_data.text)
        yield ""status"", ""printed""


class FindInDictionaryBlock(Block):
    class Input(BlockSchema):
        input: Any = SchemaField(description=""Dictionary to lookup from"")
        key: str | int = SchemaField(description=""Key to lookup in the dictionary"")

    class Output(BlockSchema):
        output: Any = SchemaField(description=""Value found for the given key"")
        missing: Any = SchemaField(
            description=""Value of the input that missing the key""
        )

    def __init__(self):
        super().__init__(
            id=""0e50422c-6dee-4145-83d6-3a5a392f65de"",
            description=""Lookup the given key in the input dictionary/object/list and return the value."",
            input_schema=FindInDictionaryBlock.Input,
            output_schema=FindInDictionaryBlock.Output,
            test_input=[
                {""input"": {""apple"": 1, ""banana"": 2, ""cherry"": 3}, ""key"": ""banana""},
                {""input"": {""x"": 10, ""y"": 20, ""z"": 30}, ""key"": ""w""},
                {""input"": [1, 2, 3], ""key"": 1},
                {""input"": [1, 2, 3], ""key"": 3},
                {""input"": MockObject(value=""!!"", key=""key""), ""key"": ""key""},
                {""input"": [{""k1"": ""v1""}, {""k2"": ""v2""}, {""k1"": ""v3""}], ""key"": ""k1""},
            ],
            test_output=[
                (""output"", 2),
                (""missing"", {""x"": 10, ""y"": 20, ""z"": 30}),
                (""output"", 2),
                (""missing"", [1, 2, 3]),
                (""output"", ""key""),
                (""output"", [""v1"", ""v3""]),
            ],
            categories={BlockCategory.BASIC},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        obj = input_data.input
        key = input_data.key

        if isinstance(obj, dict) and key in obj:
            yield ""output"", obj[key]
        elif isinstance(obj, list) and isinstance(key, int) and 0 <= key < len(obj):
            yield ""output"", obj[key]
        elif isinstance(obj, list) and isinstance(key, str):
            if len(obj) == 0:
                yield ""output"", []
            elif isinstance(obj[0], dict) and key in obj[0]:
                yield ""output"", [item[key] for item in obj if key in item]
            else:
                yield ""output"", [getattr(val, key) for val in obj if hasattr(val, key)]
        elif isinstance(obj, object) and isinstance(key, str) and hasattr(obj, key):
            yield ""output"", getattr(obj, key)
        else:
            yield ""missing"", input_data.input


class AgentInputBlock(Block):
    """"""
    This block is used to provide input to the graph.

    It takes in a value, name, description, default values list and bool to limit selection to default values.

    It Outputs the value passed as input.
    """"""

    class Input(BlockSchema):
        name: str = SchemaField(description=""The name of the input."")
        value: Any = SchemaField(
            description=""The value to be passed as input."",
            default=None,
        )
        title: str | None = SchemaField(
            description=""The title of the input."", default=None, advanced=True
        )
        description: str | None = SchemaField(
            description=""The description of the input."",
            default=None,
            advanced=True,
        )
        placeholder_values: List[Any] = SchemaField(
            description=""The placeholder values to be passed as input."",
            default=[],
            advanced=True,
        )
        limit_to_placeholder_values: bool = SchemaField(
            description=""Whether to limit the selection to placeholder values."",
            default=False,
            advanced=True,
        )
        advanced: bool = SchemaField(
            description=""Whether to show the input in the advanced section, if the field is not required."",
            default=False,
            advanced=True,
        )
        secret: bool = SchemaField(
            description=""Whether the input should be treated as a secret."",
            default=False,
            advanced=True,
        )

    class Output(BlockSchema):
        result: Any = SchemaField(description=""The value passed as input."")

    def __init__(self):
        super().__init__(
            id=""c0a8e994-ebf1-4a9c-a4d8-89d09c86741b"",
            description=""This block is used to provide input to the graph."",
            input_schema=AgentInputBlock.Input,
            output_schema=AgentInputBlock.Output,
            test_input=[
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""input_1"",
                    ""description"": ""This is a test input."",
                    ""placeholder_values"": [],
                    ""limit_to_placeholder_values"": False,
                },
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""input_2"",
                    ""description"": ""This is a test input."",
                    ""placeholder_values"": [""Hello, World!""],
                    ""limit_to_placeholder_values"": True,
                },
            ],
            test_output=[
                (""result"", ""Hello, World!""),
                (""result"", ""Hello, World!""),
            ],
            categories={BlockCategory.INPUT, BlockCategory.BASIC},
            block_type=BlockType.INPUT,
            static_output=True,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""result"", input_data.value


class AgentOutputBlock(Block):
    """"""
    Records the output of the graph for users to see.

    Behavior:
        If `format` is provided and the `value` is of a type that can be formatted,
        the block attempts to format the recorded_value using the `format`.
        If formatting fails or no `format` is provided, the raw `value` is output.
    """"""

    class Input(BlockSchema):
        value: Any = SchemaField(
            description=""The value to be recorded as output."",
            default=None,
            advanced=False,
        )
        name: str = SchemaField(description=""The name of the output."")
        title: str | None = SchemaField(
            description=""The title of the output."",
            default=None,
            advanced=True,
        )
        description: str | None = SchemaField(
            description=""The description of the output."",
            default=None,
            advanced=True,
        )
        format: str = SchemaField(
            description=""The format string to be used to format the recorded_value."",
            default="""",
            advanced=True,
        )
        advanced: bool = SchemaField(
            description=""Whether to treat the output as advanced."",
            default=False,
            advanced=True,
        )
        secret: bool = SchemaField(
            description=""Whether the output should be treated as a secret."",
            default=False,
            advanced=True,
        )

    class Output(BlockSchema):
        output: Any = SchemaField(description=""The value recorded as output."")

    def __init__(self):
        super().__init__(
            id=""363ae599-353e-4804-937e-b2ee3cef3da4"",
            description=""Stores the output of the graph for users to see."",
            input_schema=AgentOutputBlock.Input,
            output_schema=AgentOutputBlock.Output,
            test_input=[
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""output_1"",
                    ""description"": ""This is a test output."",
                    ""format"": ""{{ output_1 }}!!"",
                },
                {
                    ""value"": ""42"",
                    ""name"": ""output_2"",
                    ""description"": ""This is another test output."",
                    ""format"": ""{{ output_2 }}"",
                },
                {
                    ""value"": MockObject(value=""!!"", key=""key""),
                    ""name"": ""output_3"",
                    ""description"": ""This is a test output with a mock object."",
                    ""format"": ""{{ output_3 }}"",
                },
            ],
            test_output=[
                (""output"", ""Hello, World!!!""),
                (""output"", ""42""),
                (""output"", MockObject(value=""!!"", key=""key"")),
            ],
            categories={BlockCategory.OUTPUT, BlockCategory.BASIC},
            block_type=BlockType.OUTPUT,
            static_output=True,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        """"""
        Attempts to format the recorded_value using the fmt_string if provided.
        If formatting fails or no fmt_string is given, returns the original recorded_value.
        """"""
        if input_data.format:
            try:
                yield ""output"", formatter.format_string(
                    input_data.format, {input_data.name: input_data.value}
                )
            except Exception as e:
                yield ""output"", f""Error: {e}, {input_data.value}""
        else:
            yield ""output"", input_data.value


class AddToDictionaryBlock(Block):
    class Input(BlockSchema):
        dictionary: dict[Any, Any] = SchemaField(
            default={},
            description=""The dictionary to add the entry to. If not provided, a new dictionary will be created."",
        )
        key: str = SchemaField(
            default="""",
            description=""The key for the new entry."",
            placeholder=""new_key"",
            advanced=False,
        )
        value: Any = SchemaField(
            default=None,
            description=""The value for the new entry."",
            placeholder=""new_value"",
            advanced=False,
        )
        entries: dict[Any, Any] = SchemaField(
            default={},
            description=""The entries to add to the dictionary. This is the batch version of the `key` and `value` fields."",
            advanced=True,
        )

    class Output(BlockSchema):
        updated_dictionary: dict = SchemaField(
            description=""The dictionary with the new entry added.""
        )
        error: str = SchemaField(description=""Error message if the operation failed."")

    def __init__(self):
        super().__init__(
            id=""31d1064e-7446-4693-a7d4-65e5ca1180d1"",
            description=""Adds a new key-value pair to a dictionary. If no dictionary is provided, a new one is created."",
            categories={BlockCategory.BASIC},
            input_schema=AddToDictionaryBlock.Input,
            output_schema=AddToDictionaryBlock.Output,
            test_input=[
                {
                    ""dictionary"": {""existing_key"": ""existing_value""},
                    ""key"": ""new_key"",
                    ""value"": ""new_value"",
                },
                {""key"": ""first_key"", ""value"": ""first_value""},
                {
                    ""dictionary"": {""existing_key"": ""existing_value""},
                    ""entries"": {""new_key"": ""new_value"", ""first_key"": ""first_value""},
                },
            ],
            test_output=[
                (
                    ""updated_dictionary"",
                    {""existing_key"": ""existing_value"", ""new_key"": ""new_value""},
                ),
                (""updated_dictionary"", {""first_key"": ""first_value""}),
                (
                    ""updated_dictionary"",
                    {
                        ""existing_key"": ""existing_value"",
                        ""new_key"": ""new_value"",
                        ""first_key"": ""first_value"",
                    },
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        updated_dict = input_data.dictionary.copy()
        if input_data.value is not None and input_data.key:
            updated_dict[input_data.key] = input_data.value
        for key, value in input_data.entries.items():
            updated_dict[key] = value
        yield ""updated_dictionary"", updated_dict


class AddToListBlock(Block):
    class Input(BlockSchema):
        list: List[Any] = SchemaField(
            default=[],
            advanced=False,
            description=""The list to add the entry to. If not provided, a new list will be created."",
        )
        entry: Any = SchemaField(
            description=""The entry to add to the list. Can be of any type (string, int, dict, etc.)."",
            advanced=False,
            default=None,
        )
        entries: List[Any] = SchemaField(
            default=[],
            description=""The entries to add to the list. This is the batch version of the `entry` field."",
            advanced=True,
        )
        position: int | None = SchemaField(
            default=None,
            description=""The position to insert the new entry. If not provided, the entry will be appended to the end of the list."",
        )

    class Output(BlockSchema):
        updated_list: List[Any] = SchemaField(
            description=""The list with the new entry added.""
        )
        error: str = SchemaField(description=""Error message if the operation failed."")

    def __init__(self):
        super().__init__(
            id=""aeb08fc1-2fc1-4141-bc8e-f758f183a822"",
            description=""Adds a new entry to a list. The entry can be of any type. If no list is provided, a new one is created."",
            categories={BlockCategory.BASIC},
            input_schema=AddToListBlock.Input,
            output_schema=AddToListBlock.Output,
            test_input=[
                {
                    ""list"": [1, ""string"", {""existing_key"": ""existing_value""}],
                    ""entry"": {""new_key"": ""new_value""},
                    ""position"": 1,
                },
                {""entry"": ""first_entry""},
                {""list"": [""a"", ""b"", ""c""], ""entry"": ""d""},
                {
                    ""entry"": ""e"",
                    ""entries"": [""f"", ""g""],
                    ""list"": [""a"", ""b""],
                    ""position"": 1,
                },
            ],
            test_output=[
                (
                    ""updated_list"",
                    [
                        1,
                        {""new_key"": ""new_value""},
                        ""string"",
                        {""existing_key"": ""existing_value""},
                    ],
                ),
                (""updated_list"", [""first_entry""]),
                (""updated_list"", [""a"", ""b"", ""c"", ""d""]),
                (""updated_list"", [""a"", ""f"", ""g"", ""e"", ""b""]),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        entries_added = input_data.entries.copy()
        if input_data.entry:
            entries_added.append(input_data.entry)

        updated_list = input_data.list.copy()
        if (pos := input_data.position) is not None:
            updated_list = updated_list[:pos] + entries_added + updated_list[pos:]
        else:
            updated_list += entries_added

        yield ""updated_list"", updated_list


class NoteBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(description=""The text to display in the sticky note."")

    class Output(BlockSchema):
        output: str = SchemaField(description=""The text to display in the sticky note."")

    def __init__(self):
        super().__init__(
            id=""cc10ff7b-7753-4ff2-9af6-9399b1a7eddc"",
            description=""This block is used to display a sticky note with the given text."",
            categories={BlockCategory.BASIC},
            input_schema=NoteBlock.Input,
            output_schema=NoteBlock.Output,
            test_input={""text"": ""Hello, World!""},
            test_output=[
                (""output"", ""Hello, World!""),
            ],
            block_type=BlockType.NOTE,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""output"", input_data.text


class CreateDictionaryBlock(Block):
    class Input(BlockSchema):
        values: dict[str, Any] = SchemaField(
            description=""Key-value pairs to create the dictionary with"",
            placeholder=""e.g., {'name': 'Alice', 'age': 25}"",
        )

    class Output(BlockSchema):
        dictionary: dict[str, Any] = SchemaField(
            description=""The created dictionary containing the specified key-value pairs""
        )
        error: str = SchemaField(
            description=""Error message if dictionary creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""b924ddf4-de4f-4b56-9a85-358930dcbc91"",
            description=""Creates a dictionary with the specified key-value pairs. Use this when you know all the values you want to add upfront."",
            categories={BlockCategory.DATA},
            input_schema=CreateDictionaryBlock.Input,
            output_schema=CreateDictionaryBlock.Output,
            test_input=[
                {
                    ""values"": {""name"": ""Alice"", ""age"": 25, ""city"": ""New York""},
                },
                {
                    ""values"": {""numbers"": [1, 2, 3], ""active"": True, ""score"": 95.5},
                },
            ],
            test_output=[
                (
                    ""dictionary"",
                    {""name"": ""Alice"", ""age"": 25, ""city"": ""New York""},
                ),
                (
                    ""dictionary"",
                    {""numbers"": [1, 2, 3], ""active"": True, ""score"": 95.5},
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        try:
            # The values are already validated by Pydantic schema
            yield ""dictionary"", input_data.values
        except Exception as e:
            yield ""error"", f""Failed to create dictionary: {str(e)}""


class CreateListBlock(Block):
    class Input(BlockSchema):
        values: List[Any] = SchemaField(
            description=""A list of values to be combined into a new list."",
            placeholder=""e.g., ['Alice', 25, True]"",
        )

    class Output(BlockSchema):
        list: List[Any] = SchemaField(
            description=""The created list containing the specified values.""
        )
        error: str = SchemaField(description=""Error message if list creation failed."")

    def __init__(self):
        super().__init__(
            id=""a912d5c7-6e00-4542-b2a9-8034136930e4"",
            description=""Creates a list with the specified values. Use this when you know all the values you want to add upfront."",
            categories={BlockCategory.DATA},
            input_schema=CreateListBlock.Input,
            output_schema=CreateListBlock.Output,
            test_input=[
                {
                    ""values"": [""Alice"", 25, True],
                },
                {
                    ""values"": [1, 2, 3, ""four"", {""key"": ""value""}],
                },
            ],
            test_output=[
                (
                    ""list"",
                    [""Alice"", 25, True],
                ),
                (
                    ""list"",
                    [1, 2, 3, ""four"", {""key"": ""value""}],
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        try:
            # The values are already validated by Pydantic schema
            yield ""list"", input_data.values
        except Exception as e:
            yield ""error"", f""Failed to create list: {str(e)}""
","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
152,"import subprocess
import sys
import time


def wait_for_postgres(max_retries=5, delay=5):
    for _ in range(max_retries):
        try:
            result = subprocess.run(
                [
                    ""docker"",
                    ""compose"",
                    ""-f"",
                    ""docker-compose.test.yaml"",
                    ""exec"",
                    ""postgres-test"",
                    ""pg_isready"",
                    ""-U"",
                    ""postgres"",
                    ""-d"",
                    ""postgres"",
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            if ""accepting connections"" in result.stdout:
                print(""PostgreSQL is ready."")
                return True
        except subprocess.CalledProcessError:
            print(f""PostgreSQL is not ready yet. Retrying in {delay} seconds..."")
            time.sleep(delay)
    print(""Failed to connect to PostgreSQL."")
    return False


def run_command(command, check=True):
    try:
        subprocess.run(command, check=check)
    except subprocess.CalledProcessError as e:
        print(f""Command failed: {e}"")
        sys.exit(1)


def test():
    # Start PostgreSQL with Docker Compose
    run_command(
        [
            ""docker"",
            ""compose"",
            ""-f"",
            ""docker-compose.test.yaml"",
            ""up"",
            ""-d"",
            ""postgres-test"",
        ]
    )

    if not wait_for_postgres():
        run_command([""docker"", ""compose"", ""-f"", ""docker-compose.test.yaml"", ""down""])
        sys.exit(1)

    # Run Prisma migrations
    run_command([""prisma"", ""migrate"", ""dev""])

    # Run the tests
    result = subprocess.run([""pytest""] + sys.argv[1:], check=False)

    run_command([""docker"", ""compose"", ""-f"", ""docker-compose.test.yaml"", ""down""])

    sys.exit(result.returncode)
","import subprocess
import sys
import time

def wait_for_postgres(max_retries=5, delay=5):
    for _ in range(max_retries):
        try:
            result = subprocess.run(
                [
                    ""docker"",
                    ""compose"",
                    ""-f"",
                    ""docker-compose.test.yaml"",
                    ""exec"",
                    ""postgres-test"",
                    ""pg_isready"",
                    ""-U"",
                    ""postgres"",
                    ""-d"",
                    ""postgres"",
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            if ""accepting connections"" in result.stdout:
                print(""PostgreSQL is ready."")
                return True
        except subprocess.CalledProcessError:
            print(f""PostgreSQL is not ready yet. Retrying in {delay} seconds..."")
            time.sleep(delay)
    print(""Failed to connect to PostgreSQL."")
    return False

def run_command(command, check=True):
    try:
        subprocess.run(command, check=check)
    except subprocess.CalledProcessError as e:
        print(f""Command failed: {e}"")
        sys.exit(1)

def test():
    run_command(
        [
            ""docker"",
            ""compose"",
            ""-f"",
            ""docker-compose.test.yaml"",
            ""up"",
            ""-d"",
            ""postgres-test"",
        ]
    )
    if not wait_for_postgres():
        run_command([""docker"", ""compose"", ""-f"", ""docker-compose.test.yaml"", ""down""])
        sys.exit(1)

    run_command([""prisma"", ""migrate"", ""dev""])
    result = subprocess.run([""pytest""] + sys.argv[1:], check=False)
    run_command([""docker"", ""compose"", ""-f"", ""docker-compose.test.yaml"", ""down""])
    sys.exit(result.returncode)","{'final_score': 94.94, 'length_similarity': 95.19, 'token_similarity': 88.89, 'structure_similarity': 95.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
153,"from datetime import datetime, timezone
from typing import Iterator

import praw
from pydantic import BaseModel, ConfigDict

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import BlockSecret, SchemaField, SecretField
from backend.util.mock import MockObject


class RedditCredentials(BaseModel):
    client_id: BlockSecret = SecretField(key=""reddit_client_id"")
    client_secret: BlockSecret = SecretField(key=""reddit_client_secret"")
    username: BlockSecret = SecretField(key=""reddit_username"")
    password: BlockSecret = SecretField(key=""reddit_password"")
    user_agent: str = ""AutoGPT:1.0 (by /u/autogpt)""

    model_config = ConfigDict(title=""Reddit Credentials"")


class RedditPost(BaseModel):
    id: str
    subreddit: str
    title: str
    body: str


class RedditComment(BaseModel):
    post_id: str
    comment: str


def get_praw(creds: RedditCredentials) -> praw.Reddit:
    client = praw.Reddit(
        client_id=creds.client_id.get_secret_value(),
        client_secret=creds.client_secret.get_secret_value(),
        username=creds.username.get_secret_value(),
        password=creds.password.get_secret_value(),
        user_agent=creds.user_agent,
    )
    me = client.user.me()
    if not me:
        raise ValueError(""Invalid Reddit credentials."")
    print(f""Logged in as Reddit user: {me.name}"")
    return client


class GetRedditPostsBlock(Block):
    class Input(BlockSchema):
        subreddit: str = SchemaField(description=""Subreddit name"")
        creds: RedditCredentials = SchemaField(
            description=""Reddit credentials"",
            default=RedditCredentials(),
        )
        last_minutes: int | None = SchemaField(
            description=""Post time to stop minutes ago while fetching posts"",
            default=None,
        )
        last_post: str | None = SchemaField(
            description=""Post ID to stop when reached while fetching posts"",
            default=None,
        )
        post_limit: int | None = SchemaField(
            description=""Number of posts to fetch"", default=10
        )

    class Output(BlockSchema):
        post: RedditPost = SchemaField(description=""Reddit post"")

    def __init__(self):
        super().__init__(
            disabled=True,
            id=""c6731acb-4285-4ee1-bc9b-03d0766c370f"",
            description=""This block fetches Reddit posts from a defined subreddit name."",
            categories={BlockCategory.SOCIAL},
            input_schema=GetRedditPostsBlock.Input,
            output_schema=GetRedditPostsBlock.Output,
            test_input={
                ""creds"": {
                    ""client_id"": ""client_id"",
                    ""client_secret"": ""client_secret"",
                    ""username"": ""username"",
                    ""password"": ""password"",
                    ""user_agent"": ""user_agent"",
                },
                ""subreddit"": ""subreddit"",
                ""last_post"": ""id3"",
                ""post_limit"": 2,
            },
            test_output=[
                (
                    ""post"",
                    RedditPost(
                        id=""id1"", subreddit=""subreddit"", title=""title1"", body=""body1""
                    ),
                ),
                (
                    ""post"",
                    RedditPost(
                        id=""id2"", subreddit=""subreddit"", title=""title2"", body=""body2""
                    ),
                ),
            ],
            test_mock={
                ""get_posts"": lambda _: [
                    MockObject(id=""id1"", title=""title1"", selftext=""body1""),
                    MockObject(id=""id2"", title=""title2"", selftext=""body2""),
                    MockObject(id=""id3"", title=""title2"", selftext=""body2""),
                ]
            },
        )

    @staticmethod
    def get_posts(input_data: Input) -> Iterator[praw.reddit.Submission]:
        client = get_praw(input_data.creds)
        subreddit = client.subreddit(input_data.subreddit)
        return subreddit.new(limit=input_data.post_limit or 10)

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_time = datetime.now(tz=timezone.utc)
        for post in self.get_posts(input_data):
            if input_data.last_minutes:
                post_datetime = datetime.fromtimestamp(
                    post.created_utc, tz=timezone.utc
                )
                time_difference = current_time - post_datetime
                if time_difference.total_seconds() / 60 > input_data.last_minutes:
                    continue

            if input_data.last_post and post.id == input_data.last_post:
                break

            yield ""post"", RedditPost(
                id=post.id,
                subreddit=input_data.subreddit,
                title=post.title,
                body=post.selftext,
            )


class PostRedditCommentBlock(Block):
    class Input(BlockSchema):
        creds: RedditCredentials = SchemaField(
            description=""Reddit credentials"", default=RedditCredentials()
        )
        data: RedditComment = SchemaField(description=""Reddit comment"")

    class Output(BlockSchema):
        comment_id: str = SchemaField(description=""Posted comment ID"")

    def __init__(self):
        super().__init__(
            id=""4a92261b-701e-4ffb-8970-675fd28e261f"",
            description=""This block posts a Reddit comment on a specified Reddit post."",
            categories={BlockCategory.SOCIAL},
            input_schema=PostRedditCommentBlock.Input,
            output_schema=PostRedditCommentBlock.Output,
            test_input={""data"": {""post_id"": ""id"", ""comment"": ""comment""}},
            test_output=[(""comment_id"", ""dummy_comment_id"")],
            test_mock={""reply_post"": lambda creds, comment: ""dummy_comment_id""},
        )

    @staticmethod
    def reply_post(creds: RedditCredentials, comment: RedditComment) -> str:
        client = get_praw(creds)
        submission = client.submission(id=comment.post_id)
        new_comment = submission.reply(comment.comment)
        if not new_comment:
            raise ValueError(""Failed to post comment."")
        return new_comment.id

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""comment_id"", self.reply_post(input_data.creds, input_data.data)
","from datetime import datetime, timezone
from typing import Iterator
import praw
from pydantic import BaseModel, ConfigDict
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import BlockSecret, SchemaField, SecretField
from backend.util.mock import MockObject

class RedditCredentials(BaseModel):
    client_id: BlockSecret = SecretField(key=""reddit_client_id"")
    client_secret: BlockSecret = SecretField(key=""reddit_client_secret"")
    username: BlockSecret = SecretField(key=""reddit_username"")
    password: BlockSecret = SecretField(key=""reddit_password"")
    user_agent: str = ""AutoGPT:1.0 (by /u/autogpt)""

    model_config = ConfigDict(title=""Reddit Credentials"")

class RedditPost(BaseModel):
    id: str
    subreddit: str
    title: str
    body: str

class RedditComment(BaseModel):
    post_id: str
    comment: str

def get_praw(creds: RedditCredentials) -> praw.Reddit:
    client = praw.Reddit(
        client_id=creds.client_id.get_secret_value(),
        client_secret=creds.client_secret.get_secret_value(),
        username=creds.username.get_secret_value(),
        password=creds.password.get_secret_value(),
        user_agent=creds.user_agent,
    )
    me = client.user.me()
    if not me:
        raise ValueError(""Invalid Reddit credentials."")
    print(f""Logged in as Reddit user: {me.name}"")
    return client

class GetRedditPostsBlock(Block):
    class Input(BlockSchema):
        subreddit: str = SchemaField(description=""Subreddit name"")
        creds: RedditCredentials = SchemaField(
            description=""Reddit credentials"",
            default=RedditCredentials(),
        )
        last_minutes: int | None = SchemaField(
            description=""Post time to stop minutes ago while fetching posts"",
            default=None,
        )
        last_post: str | None = SchemaField(
            description=""Post ID to stop when reached while fetching posts"",
            default=None,
        )
        post_limit: int | None = SchemaField(
            description=""Number of posts to fetch"", default=10
        )

    class Output(BlockSchema):
        post: RedditPost = SchemaField(description=""Reddit post"")

    def __init__(self):
        super().__init__(
            disabled=True,
            id=""c6731acb-4285-4ee1-bc9b-03d0766c370f"",
            description=""This block fetches Reddit posts from a defined subreddit name."",
            categories={BlockCategory.SOCIAL},
            input_schema=GetRedditPostsBlock.Input,
            output_schema=GetRedditPostsBlock.Output,
            test_input={
                ""creds"": {
                    ""client_id"": ""client_id"",
                    ""client_secret"": ""client_secret"",
                    ""username"": ""username"",
                    ""password"": ""password"",
                    ""user_agent"": ""user_agent"",
                },
                ""subreddit"": ""subreddit"",
                ""last_post"": ""id3"",
                ""post_limit"": 2,
            },
            test_output=[
                (
                    ""post"",
                    RedditPost(
                        id=""id1"", subreddit=""subreddit"", title=""title1"", body=""body1""
                    ),
                ),
                (
                    ""post"",
                    RedditPost(
                        id=""id2"", subreddit=""subreddit"", title=""title2"", body=""body2""
                    ),
                ),
            ],
            test_mock={
                ""get_posts"": lambda _: [
                    MockObject(id=""id1"", title=""title1"", selftext=""body1""),
                    MockObject(id=""id2"", title=""title2"", selftext=""body2""),
                    MockObject(id=""id3"", title=""title2"", selftext=""body2""),
                ]
            },
        )

    @staticmethod
    def get_posts(input_data: Input) -> Iterator[praw.reddit.Submission]:
        client = get_praw(input_data.creds)
        subreddit = client.subreddit(input_data.subreddit)
        return subreddit.new(limit=input_data.post_limit or 10)

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_time = datetime.now(tz=timezone.utc)
        for post in self.get_posts(input_data):
            if input_data.last_minutes:
                post_datetime = datetime.fromtimestamp(
                    post.created_utc, tz=timezone.utc
                )
                time_difference = current_time - post_datetime
                if time_difference.total_seconds() / 60 > input_data.last_minutes:
                    continue

            if input_data.last_post and post.id == input_data.last_post:
                break

            yield ""post"", RedditPost(
                id=post.id,
                subreddit=input_data.subreddit,
                title=post.title,
                body=post.selftext,
            )

class PostRedditCommentBlock(Block):
    class Input(BlockSchema):
        creds: RedditCredentials = SchemaField(
            description=""Reddit credentials"", default=RedditCredentials()
        )
        data: RedditComment = SchemaField(description=""Reddit comment"")

    class Output(BlockSchema):
        comment_id: str = SchemaField(description=""Posted comment ID"")

    def __init__(self):
        super().__init__(
            id=""4a92261b-701e-4ffb-8970-675fd28e261f"",
            description=""This block posts a Reddit comment on a specified Reddit post."",
            categories={BlockCategory.SOCIAL},
            input_schema=PostRedditCommentBlock.Input,
            output_schema=PostRedditCommentBlock.Output,
            test_input={""data"": {""post_id"": ""id"", ""comment"": ""comment""}},
            test_output=[(""comment_id"", ""dummy_comment_id"")],
            test_mock={""reply_post"": lambda creds, comment: ""dummy_comment_id""},
        )

    @staticmethod
    def reply_post(creds: RedditCredentials, comment: RedditComment) -> str:
        client = get_praw(creds)
        submission = client.submission(id=comment.post_id)
        new_comment = submission.reply(comment.comment)
        if not new_comment:
            raise ValueError(""Failed to post comment."")
        return new_comment.id

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""comment_id"", self.reply_post(input_data.creds, input_data.data)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
154,"# -*- coding: utf-8 -*-

import unittest

from validate.links import find_links_in_text
from validate.links import check_duplicate_links
from validate.links import fake_user_agent
from validate.links import get_host_from_link
from validate.links import has_cloudflare_protection


class FakeResponse():
    def __init__(self, code: int, headers: dict, text: str) -> None:
        self.status_code = code
        self.headers = headers
        self.text = text


class TestValidateLinks(unittest.TestCase):

    def setUp(self):
        self.duplicate_links = [
            'https://www.example.com',
            'https://www.example.com',
            'https://www.example.com',
            'https://www.anotherexample.com',
        ]
        self.no_duplicate_links = [
            'https://www.firstexample.com',
            'https://www.secondexample.com',
            'https://www.anotherexample.com',
        ]

        self.code_200 = 200
        self.code_403 = 403
        self.code_503 = 503

        self.cloudflare_headers = {'Server': 'cloudflare'}
        self.no_cloudflare_headers = {'Server': 'google'}

        self.text_with_cloudflare_flags = '403 Forbidden Cloudflare We are checking your browser...'
        self.text_without_cloudflare_flags = 'Lorem Ipsum'

    def test_find_link_in_text(self):
        text = """"""
            # this is valid

            http://example.com?param1=1&param2=2#anchor
            https://www.example.com?param1=1&param2=2#anchor
            https://www.example.com.br
            https://www.example.com.gov.br
            [Example](https://www.example.com?param1=1&param2=2#anchor)
            lorem ipsum https://www.example.com?param1=1&param2=2#anchor
            https://www.example.com?param1=1&param2=2#anchor lorem ipsum

            # this not is valid

            example.com
            https:example.com
            https:/example.com
            https//example.com
            https//.com
        """"""

        links = find_links_in_text(text)

        self.assertIsInstance(links, list)
        self.assertEqual(len(links), 7)

        for link in links:
            with self.subTest():
                self.assertIsInstance(link, str)

    def test_find_link_in_text_with_invalid_argument(self):
        with self.assertRaises(TypeError):
            find_links_in_text()
            find_links_in_text(1)
            find_links_in_text(True)

    def test_if_check_duplicate_links_has_the_correct_return(self):
        result_1 = check_duplicate_links(self.duplicate_links)
        result_2 = check_duplicate_links(self.no_duplicate_links)

        self.assertIsInstance(result_1, tuple)
        self.assertIsInstance(result_2, tuple)

        has_duplicate_links, links = result_1
        no_duplicate_links, no_links = result_2

        self.assertTrue(has_duplicate_links)
        self.assertFalse(no_duplicate_links)

        self.assertIsInstance(links, list)
        self.assertIsInstance(no_links, list)

        self.assertEqual(len(links), 2)
        self.assertEqual(len(no_links), 0)

    def test_if_fake_user_agent_has_a_str_as_return(self):
        user_agent = fake_user_agent()
        self.assertIsInstance(user_agent, str)

    def test_get_host_from_link(self):
        links = [
            'example.com',
            'https://example.com',
            'https://www.example.com',
            'https://www.example.com.br',
            'https://www.example.com/route',
            'https://www.example.com?p=1&q=2',
            'https://www.example.com#anchor'
        ]

        for link in links:
            host = get_host_from_link(link)

            with self.subTest():
                self.assertIsInstance(host, str)

                self.assertNotIn('://', host)
                self.assertNotIn('/', host)
                self.assertNotIn('?', host)
                self.assertNotIn('#', host)

        with self.assertRaises(TypeError):
            get_host_from_link()

    def test_has_cloudflare_protection_with_code_403_and_503_in_response(self):
        resp_with_cloudflare_protection_code_403 = FakeResponse(
            code=self.code_403,
            headers=self.cloudflare_headers,
            text=self.text_with_cloudflare_flags
        )

        resp_with_cloudflare_protection_code_503 = FakeResponse(
            code=self.code_503,
            headers=self.cloudflare_headers,
            text=self.text_with_cloudflare_flags
        )

        result1 = has_cloudflare_protection(resp_with_cloudflare_protection_code_403)
        result2 = has_cloudflare_protection(resp_with_cloudflare_protection_code_503)

        self.assertTrue(result1)
        self.assertTrue(result2)

    def test_has_cloudflare_protection_when_there_is_no_protection(self):
        resp_without_cloudflare_protection1 = FakeResponse(
            code=self.code_200,
            headers=self.no_cloudflare_headers,
            text=self.text_without_cloudflare_flags
        )

        resp_without_cloudflare_protection2 = FakeResponse(
            code=self.code_403,
            headers=self.no_cloudflare_headers,
            text=self.text_without_cloudflare_flags
        )

        resp_without_cloudflare_protection3 = FakeResponse(
            code=self.code_503,
            headers=self.no_cloudflare_headers,
            text=self.text_without_cloudflare_flags
        )

        result1 = has_cloudflare_protection(resp_without_cloudflare_protection1)
        result2 = has_cloudflare_protection(resp_without_cloudflare_protection2)
        result3 = has_cloudflare_protection(resp_without_cloudflare_protection3)

        self.assertFalse(result1)
        self.assertFalse(result2)
        self.assertFalse(result3)
","# -*- coding: utf-8 -*-

import unittest


from validate.links import find_links_in_text
from validate.links import check_duplicate_links
from validate.links import fake_user_agent
from validate.links import get_host_from_link
from validate.links import has_cloudflare_protection


class FakeResponse():
    def __init__(self, code: int, headers: dict, text: str) -> None:
        self.status_code = code
        self.headers = headers
        self.text = text


class TestValidateLinks(unittest.TestCase):

    def setUp(self):
        self.duplicate_links = [
            'https://www.example.com',
            'https://www.example.com',
            'https://www.example.com',
            'https://www.anotherexample.com',
        ]
        self.no_duplicate_links = [
            'https://www.firstexample.com',
            'https://www.secondexample.com',
            'https://www.anotherexample.com',
        ]

        self.code_200 = 200
        self.code_403 = 403
        self.code_503 = 503

        self.cloudflare_headers = {'Server': 'cloudflare'}
        self.no_cloudflare_headers = {'Server': 'google'}

        self.text_with_cloudflare_flags = '403 Forbidden Cloudflare We are checking your browser...'
        self.text_without_cloudflare_flags = 'Lorem Ipsum'

    def test_find_link_in_text(self):
        text = """"""
            # this is valid

            http://example.com?param1=1&param2=2#anchor
            https://www.example.com?param1=1&param2=2#anchor
            https://www.example.com.br
            https://www.example.com.gov.br
            [Example](https://www.example.com?param1=1&param2=2#anchor)
            lorem ipsum https://www.example.com?param1=1&param2=2#anchor
            https://www.example.com?param1=1&param2=2#anchor lorem ipsum

            # this not is valid

            example.com
            https:example.com
            https:/example.com
            https//example.com
            https//.com
        """"""
        links = find_links_in_text(text)

        self.assertIsInstance(links, list)
        self.assertEqual(len(links), 7)

        for link in links:
            with self.subTest():
                self.assertIsInstance(link, str)

    def test_find_link_in_text_with_invalid_argument(self):
        with self.assertRaises(TypeError):
            find_links_in_text()
            find_links_in_text(1)
            find_links_in_text(True)

    def test_if_check_duplicate_links_has_the_correct_return(self):
        result_1 = check_duplicate_links(self.duplicate_links)
        result_2 = check_duplicate_links(self.no_duplicate_links)

        self.assertIsInstance(result_1, tuple)
        self.assertIsInstance(result_2, tuple)

        has_duplicate_links, links = result_1
        no_duplicate_links, no_links = result_2

        self.assertTrue(has_duplicate_links)
        self.assertFalse(no_duplicate_links)

        self.assertIsInstance(links, list)
        self.assertIsInstance(no_links, list)

        self.assertEqual(len(links), 2)
        self.assertEqual(len(no_links), 0)

    def test_if_fake_user_agent_has_a_str_as_return(self):
        user_agent = fake_user_agent()
        self.assertIsInstance(user_agent, str)

    def test_get_host_from_link(self):
        links = [
            'example.com',
            'https://example.com',
            'https://www.example.com',
            'https://www.example.com.br',
            'https://www.example.com/route',
            'https://www.example.com?p=1&q=2',
            'https://www.example.com#anchor'
        ]

        for link in links:
            host = get_host_from_link(link)

            with self.subTest():
                self.assertIsInstance(host, str)

                self.assertNotIn('://', host)
                self.assertNotIn('/', host)
                self.assertNotIn('?', host)
                self.assertNotIn('#', host)

        with self.assertRaises(TypeError):
            get_host_from_link()

    def test_has_cloudflare_protection_with_code_403_and_503_in_response(self):
        resp_with_cloudflare_protection_code_403 = FakeResponse(
            code=self.code_403,
            headers=self.cloudflare_headers,
            text=self.text_with_cloudflare_flags
        )

        resp_with_cloudflare_protection_code_503 = FakeResponse(
            code=self.code_503,
            headers=self.cloudflare_headers,
            text=self.text_with_cloudflare_flags
        )

        result1 = has_cloudflare_protection(resp_with_cloudflare_protection_code_403)
        result2 = has_cloudflare_protection(resp_with_cloudflare_protection_code_503)

        self.assertTrue(result1)
        self.assertTrue(result2)

    def test_has_cloudflare_protection_when_there_is_no_protection(self):
        resp_without_cloudflare_protection1 = FakeResponse(
            code=self.code_200,
            headers=self.no_cloudflare_headers,
            text=self.text_without_cloudflare_flags
        )

        resp_without_cloudflare_protection2 = FakeResponse(
            code=self.code_403,
            headers=self.no_cloudflare_headers,
            text=self.text_without_cloudflare_flags
        )

        resp_without_cloudflare_protection3 = FakeResponse(
            code=self.code_503,
            headers=self.no_cloudflare_headers,
            text=self.text_without_cloudflare_flags
        )

        result1 = has_cloudflare_protection(resp_without_cloudflare_protection1)
        result2 = has_cloudflare_protection(resp_without_cloudflare_protection2)
        result3 = has_cloudflare_protection(resp_without_cloudflare_protection3)

        self.assertFalse(result1)
        self.assertFalse(result2)
        self.assertFalse(result3)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
155,"from abc import ABCMeta, abstractmethod
from enum import Enum


class VehicleSize(Enum):

    MOTORCYCLE = 0
    COMPACT = 1
    LARGE = 2


class Vehicle(metaclass=ABCMeta):

    def __init__(self, vehicle_size, license_plate, spot_size):
        self.vehicle_size = vehicle_size
        self.license_plate = license_plate
        self.spot_size
        self.spots_taken = []

    def clear_spots(self):
        for spot in self.spots_taken:
            spot.remove_vehicle(self)
        self.spots_taken = []

    def take_spot(self, spot):
        self.spots_taken.append(spot)

    @abstractmethod
    def can_fit_in_spot(self, spot):
        pass


class Motorcycle(Vehicle):

    def __init__(self, license_plate):
        super(Motorcycle, self).__init__(VehicleSize.MOTORCYCLE, license_plate, spot_size=1)

    def can_fit_in_spot(self, spot):
        return True


class Car(Vehicle):

    def __init__(self, license_plate):
        super(Car, self).__init__(VehicleSize.COMPACT, license_plate, spot_size=1)

    def can_fit_in_spot(self, spot):
        return spot.size in (VehicleSize.LARGE, VehicleSize.COMPACT)


class Bus(Vehicle):

    def __init__(self, license_plate):
        super(Bus, self).__init__(VehicleSize.LARGE, license_plate, spot_size=5)

    def can_fit_in_spot(self, spot):
        return spot.size == VehicleSize.LARGE


class ParkingLot(object):

    def __init__(self, num_levels):
        self.num_levels = num_levels
        self.levels = []  # List of Levels

    def park_vehicle(self, vehicle):
        for level in self.levels:
            if level.park_vehicle(vehicle):
                return True
        return False


class Level(object):

    SPOTS_PER_ROW = 10

    def __init__(self, floor, total_spots):
        self.floor = floor
        self.num_spots = total_spots
        self.available_spots = 0
        self.spots = []  # List of ParkingSpots

    def spot_freed(self):
        self.available_spots += 1

    def park_vehicle(self, vehicle):
        spot = self._find_available_spot(vehicle)
        if spot is None:
            return None
        else:
            spot.park_vehicle(vehicle)
            return spot

    def _find_available_spot(self, vehicle):
        """"""Find an available spot where vehicle can fit, or return None""""""
        pass

    def _park_starting_at_spot(self, spot, vehicle):
        """"""Occupy starting at spot.spot_number to vehicle.spot_size.""""""
        pass


class ParkingSpot(object):

    def __init__(self, level, row, spot_number, spot_size, vehicle_size):
        self.level = level
        self.row = row
        self.spot_number = spot_number
        self.spot_size = spot_size
        self.vehicle_size = vehicle_size
        self.vehicle = None

    def is_available(self):
        return True if self.vehicle is None else False

    def can_fit_vehicle(self, vehicle):
        if self.vehicle is not None:
            return False
        return vehicle.can_fit_in_spot(self)

    def park_vehicle(self, vehicle):
        pass

    def remove_vehicle(self):
        pass
","from abc import ABCMeta, abstractmethod
from enum import Enum

class VehicleSize(Enum):
    MOTORCYCLE = 0
    COMPACT = 1
    LARGE = 2

class Vehicle(metaclass=ABCMeta):
    def __init__(self, vehicle_size, license_plate, spot_size):
        self.vehicle_size = vehicle_size
        self.license_plate = license_plate
        self.spot_size = spot_size
        self.spots_taken = []

    def clear_spots(self):
        for spot in self.spots_taken:
            spot.remove_vehicle(self)
        self.spots_taken = []

    def take_spot(self, spot):
        self.spots_taken.append(spot)

    @abstractmethod
    def can_fit_in_spot(self, spot):
        pass

class Motorcycle(Vehicle):
    def __init__(self, license_plate):
        super(Motorcycle, self).__init__(VehicleSize.MOTORCYCLE, license_plate, spot_size=1)

    def can_fit_in_spot(self, spot):
        return True

class Car(Vehicle):
    def __init__(self, license_plate):
        super(Car, self).__init__(VehicleSize.COMPACT, license_plate, spot_size=1)

    def can_fit_in_spot(self, spot):
        return spot.size in (VehicleSize.LARGE, VehicleSize.COMPACT)

class Bus(Vehicle):
    def __init__(self, license_plate):
        super(Bus, self).__init__(VehicleSize.LARGE, license_plate, spot_size=5)

    def can_fit_in_spot(self, spot):
        return spot.size == VehicleSize.LARGE

class ParkingLot(object):
    def __init__(self, num_levels):
        self.num_levels = num_levels
        self.levels = []  # List of Levels

    def park_vehicle(self, vehicle):
        for level in self.levels:
            if level.park_vehicle(vehicle):
                return True
        return False

class Level(object):
    SPOTS_PER_ROW = 10

    def __init__(self, floor, total_spots):
        self.floor = floor
        self.num_spots = total_spots
        self.available_spots = 0
        self.spots = []  # List of ParkingSpots

    def spot_freed(self):
        self.available_spots += 1

    def park_vehicle(self, vehicle):
        spot = self._find_available_spot(vehicle)
        if spot is None:
            return None
        else:
            spot.park_vehicle(vehicle)
            return spot

    def _find_available_spot(self, vehicle):
        """"""Find an available spot where vehicle can fit, or return None""""""
        pass

    def _park_starting_at_spot(self, spot, vehicle):
        """"""Occupy starting at spot.spot_number to vehicle.spot_size.""""""
        pass

class ParkingSpot(object):
    def __init__(self, level, row, spot_number, spot_size, vehicle_size):
        self.level = level
        self.row = row
        self.spot_number = spot_number
        self.spot_size = spot_size
        self.vehicle_size = vehicle_size
        self.vehicle = None

    def is_available(self):
        return True if self.vehicle is None else False

    def can_fit_vehicle(self, vehicle):
        if self.vehicle is not None:
            return False
        return vehicle.can_fit_in_spot(self)

    def park_vehicle(self, vehicle):
        pass

    def remove_vehicle(self):
        pass","{'final_score': 99.94, 'length_similarity': 99.61, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
156,"from fastapi import HTTPException, Request
from starlette.middleware.base import RequestResponseEndpoint

from .limiter import RateLimiter


async def rate_limit_middleware(request: Request, call_next: RequestResponseEndpoint):
    """"""FastAPI middleware for rate limiting API requests.""""""
    limiter = RateLimiter()

    if not request.url.path.startswith(""/api""):
        return await call_next(request)

    api_key = request.headers.get(""Authorization"")
    if not api_key:
        return await call_next(request)

    api_key = api_key.replace(""Bearer "", """")

    is_allowed, remaining, reset_time = await limiter.check_rate_limit(api_key)

    if not is_allowed:
        raise HTTPException(
            status_code=429, detail=""Rate limit exceeded. Please try again later.""
        )

    response = await call_next(request)
    response.headers[""X-RateLimit-Limit""] = str(limiter.max_requests)
    response.headers[""X-RateLimit-Remaining""] = str(remaining)
    response.headers[""X-RateLimit-Reset""] = str(reset_time)

    return response
","from fastapi import HTTPException, Request
from starlette.middleware.base import RequestResponseEndpoint
from .limiter import RateLimiter

async def rate_limit_middleware(request: Request, call_next: RequestResponseEndpoint):
    """"""FastAPI middleware for rate limiting API requests.""""""
    limiter = RateLimiter()

    if not request.url.path.startswith(""/api""):
        return await call_next(request)

    api_key = request.headers.get(""Authorization"")
    if not api_key:
        return await call_next(request)

    api_key = api_key.replace(""Bearer "", """")
    is_allowed, remaining, reset_time = await limiter.check_rate_limit(api_key)

    if not is_allowed:
        raise HTTPException(
            status_code=429, detail=""Rate limit exceeded. Please try again later.""
        )

    response = await call_next(request)
    response.headers[""X-RateLimit-Limit""] = str(limiter.max_requests)
    response.headers[""X-RateLimit-Remaining""] = str(remaining)
    response.headers[""X-RateLimit-Reset""] = str(reset_time)

    return response","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
157,"import logging
import os

from dotenv import load_dotenv
from redis import Redis
from redis.asyncio import Redis as AsyncRedis

from backend.util.retry import conn_retry

load_dotenv()

HOST = os.getenv(""REDIS_HOST"", ""localhost"")
PORT = int(os.getenv(""REDIS_PORT"", ""6379""))
PASSWORD = os.getenv(""REDIS_PASSWORD"", ""password"")

logger = logging.getLogger(__name__)
connection: Redis | None = None
connection_async: AsyncRedis | None = None


@conn_retry(""Redis"", ""Acquiring connection"")
def connect() -> Redis:
    global connection
    if connection:
        return connection

    c = Redis(
        host=HOST,
        port=PORT,
        password=PASSWORD,
        decode_responses=True,
    )
    c.ping()
    connection = c
    return connection


@conn_retry(""Redis"", ""Releasing connection"")
def disconnect():
    global connection
    if connection:
        connection.close()
    connection = None


def get_redis(auto_connect: bool = True) -> Redis:
    if connection:
        return connection
    if auto_connect:
        return connect()
    raise RuntimeError(""Redis connection is not established"")


@conn_retry(""AsyncRedis"", ""Acquiring connection"")
async def connect_async() -> AsyncRedis:
    global connection_async
    if connection_async:
        return connection_async

    c = AsyncRedis(
        host=HOST,
        port=PORT,
        password=PASSWORD,
        decode_responses=True,
    )
    await c.ping()
    connection_async = c
    return connection_async


@conn_retry(""AsyncRedis"", ""Releasing connection"")
async def disconnect_async():
    global connection_async
    if connection_async:
        await connection_async.close()
    connection_async = None


async def get_redis_async(auto_connect: bool = True) -> AsyncRedis:
    if connection_async:
        return connection_async
    if auto_connect:
        return await connect_async()
    raise RuntimeError(""AsyncRedis connection is not established"")
","import logging
import os

from dotenv import load_dotenv
from redis import Redis
from redis.asyncio import Redis as AsyncRedis

from backend.util.retry import conn_retry

load_dotenv()

HOST = os.getenv(""REDIS_HOST"", ""localhost"")
PORT = int(os.getenv(""REDIS_PORT"", ""6379""))
PASSWORD = os.getenv(""REDIS_PASSWORD"", ""password"")

logger = logging.getLogger(__name__)
connection: Redis | None = None
connection_async: AsyncRedis | None = None

@conn_retry(""Redis"", ""Acquiring connection"")
def connect() -> Redis:
    global connection
    if connection:
        return connection

    c = Redis(
        host=HOST,
        port=PORT,
        password=PASSWORD,
        decode_responses=True,
    )
    c.ping()
    connection = c
    return connection

@conn_retry(""Redis"", ""Releasing connection"")
def disconnect():
    global connection
    if connection:
        connection.close()
    connection = None

def get_redis(auto_connect: bool = True) -> Redis:
    if connection:
        return connection
    if auto_connect:
        return connect()
    raise RuntimeError(""Redis connection is not established"")

@conn_retry(""AsyncRedis"", ""Acquiring connection"")
async def connect_async() -> AsyncRedis:
    global connection_async
    if connection_async:
        return connection_async

    c = AsyncRedis(
        host=HOST,
        port=PORT,
        password=PASSWORD,
        decode_responses=True,
    )
    await c.ping()
    connection_async = c
    return connection_async

@conn_retry(""AsyncRedis"", ""Releasing connection"")
async def disconnect_async():
    global connection_async
    if connection_async:
        await connection_async.close()
    connection_async = None

async def get_redis_async(auto_connect: bool = True) -> AsyncRedis:
    if connection_async:
        return connection_async
    if auto_connect:
        return await connect_async()
    raise RuntimeError(""AsyncRedis connection is not established"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
158,"import io
import unittest.mock

import fastapi
import pytest
import starlette.datastructures

import backend.server.v2.store.exceptions
import backend.server.v2.store.media
from backend.util.settings import Settings


@pytest.fixture
def mock_settings(monkeypatch):
    settings = Settings()
    settings.config.media_gcs_bucket_name = ""test-bucket""
    settings.config.google_application_credentials = ""test-credentials""
    monkeypatch.setattr(""backend.server.v2.store.media.Settings"", lambda: settings)
    return settings


@pytest.fixture
def mock_storage_client(mocker):
    mock_client = unittest.mock.MagicMock()
    mock_bucket = unittest.mock.MagicMock()
    mock_blob = unittest.mock.MagicMock()

    mock_client.bucket.return_value = mock_bucket
    mock_bucket.blob.return_value = mock_blob
    mock_blob.public_url = ""http://test-url/media/laptop.jpeg""

    mocker.patch(""google.cloud.storage.Client"", return_value=mock_client)

    return mock_client


async def test_upload_media_success(mock_settings, mock_storage_client):
    # Create test JPEG data with valid signature
    test_data = b""\xFF\xD8\xFF"" + b""test data""

    test_file = fastapi.UploadFile(
        filename=""laptop.jpeg"",
        file=io.BytesIO(test_data),
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )

    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)

    assert result == ""http://test-url/media/laptop.jpeg""
    mock_bucket = mock_storage_client.bucket.return_value
    mock_blob = mock_bucket.blob.return_value
    mock_blob.upload_from_string.assert_called_once()


async def test_upload_media_invalid_type(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.txt"",
        file=io.BytesIO(b""test data""),
        headers=starlette.datastructures.Headers({""content-type"": ""text/plain""}),
    )

    with pytest.raises(backend.server.v2.store.exceptions.InvalidFileTypeError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)

    mock_bucket = mock_storage_client.bucket.return_value
    mock_blob = mock_bucket.blob.return_value
    mock_blob.upload_from_string.assert_not_called()


async def test_upload_media_missing_credentials(monkeypatch):
    settings = Settings()
    settings.config.media_gcs_bucket_name = """"
    settings.config.google_application_credentials = """"
    monkeypatch.setattr(""backend.server.v2.store.media.Settings"", lambda: settings)

    test_file = fastapi.UploadFile(
        filename=""laptop.jpeg"",
        file=io.BytesIO(b""\xFF\xD8\xFF"" + b""test data""),  # Valid JPEG signature
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )

    with pytest.raises(backend.server.v2.store.exceptions.StorageConfigError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)


async def test_upload_media_video_type(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.mp4"",
        file=io.BytesIO(b""\x00\x00\x00\x18ftypmp42""),  # Valid MP4 signature
        headers=starlette.datastructures.Headers({""content-type"": ""video/mp4""}),
    )

    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)

    assert result == ""http://test-url/media/laptop.jpeg""
    mock_bucket = mock_storage_client.bucket.return_value
    mock_blob = mock_bucket.blob.return_value
    mock_blob.upload_from_string.assert_called_once()


async def test_upload_media_file_too_large(mock_settings, mock_storage_client):
    large_data = b""\xFF\xD8\xFF"" + b""x"" * (
        50 * 1024 * 1024 + 1
    )  # 50MB + 1 byte with valid JPEG signature
    test_file = fastapi.UploadFile(
        filename=""laptop.jpeg"",
        file=io.BytesIO(large_data),
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )

    with pytest.raises(backend.server.v2.store.exceptions.FileSizeTooLargeError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)


async def test_upload_media_file_read_error(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""laptop.jpeg"",
        file=io.BytesIO(b""""),  # Empty file that will raise error on read
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )
    test_file.read = unittest.mock.AsyncMock(side_effect=Exception(""Read error""))

    with pytest.raises(backend.server.v2.store.exceptions.FileReadError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)


async def test_upload_media_png_success(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.png"",
        file=io.BytesIO(b""\x89PNG\r\n\x1a\n""),  # Valid PNG signature
        headers=starlette.datastructures.Headers({""content-type"": ""image/png""}),
    )

    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    assert result == ""http://test-url/media/laptop.jpeg""


async def test_upload_media_gif_success(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.gif"",
        file=io.BytesIO(b""GIF89a""),  # Valid GIF signature
        headers=starlette.datastructures.Headers({""content-type"": ""image/gif""}),
    )

    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    assert result == ""http://test-url/media/laptop.jpeg""


async def test_upload_media_webp_success(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.webp"",
        file=io.BytesIO(b""RIFF\x00\x00\x00\x00WEBP""),  # Valid WebP signature
        headers=starlette.datastructures.Headers({""content-type"": ""image/webp""}),
    )

    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    assert result == ""http://test-url/media/laptop.jpeg""


async def test_upload_media_webm_success(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.webm"",
        file=io.BytesIO(b""\x1a\x45\xdf\xa3""),  # Valid WebM signature
        headers=starlette.datastructures.Headers({""content-type"": ""video/webm""}),
    )

    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    assert result == ""http://test-url/media/laptop.jpeg""


async def test_upload_media_mismatched_signature(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.jpeg"",
        file=io.BytesIO(b""\x89PNG\r\n\x1a\n""),  # PNG signature with JPEG content type
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )

    with pytest.raises(backend.server.v2.store.exceptions.InvalidFileTypeError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)


async def test_upload_media_invalid_signature(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.jpeg"",
        file=io.BytesIO(b""invalid signature""),
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )

    with pytest.raises(backend.server.v2.store.exceptions.InvalidFileTypeError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)
","import io
import unittest.mock
import fastapi
import pytest
import starlette.datastructures
import backend.server.v2.store.exceptions
import backend.server.v2.store.media
from backend.util.settings import Settings

@pytest.fixture
def mock_settings(monkeypatch):
    settings = Settings()
    settings.config.media_gcs_bucket_name = ""test-bucket""
    settings.config.google_application_credentials = ""test-credentials""
    monkeypatch.setattr(""backend.server.v2.store.media.Settings"", lambda: settings)
    return settings

@pytest.fixture
def mock_storage_client(mocker):
    mock_client = unittest.mock.MagicMock()
    mock_bucket = unittest.mock.MagicMock()
    mock_blob = unittest.mock.MagicMock()
    mock_client.bucket.return_value = mock_bucket
    mock_bucket.blob.return_value = mock_blob
    mock_blob.public_url = ""http://test-url/media/laptop.jpeg""
    mocker.patch(""google.cloud.storage.Client"", return_value=mock_client)
    return mock_client

async def test_upload_media_success(mock_settings, mock_storage_client):
    # Create test JPEG data with valid signature
    test_data = b""\xFF\xD8\xFF"" + b""test data""
    test_file = fastapi.UploadFile(
        filename=""laptop.jpeg"",
        file=io.BytesIO(test_data),
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )
    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    assert result == ""http://test-url/media/laptop.jpeg""

async def test_upload_media_invalid_type(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.txt"",
        file=io.BytesIO(b""test data""),
        headers=starlette.datastructures.Headers({""content-type"": ""text/plain""}),
    )
    with pytest.raises(backend.server.v2.store.exceptions.InvalidFileTypeError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    mock_bucket = mock_storage_client.bucket.return_value
    mock_blob = mock_bucket.blob.return_value
    mock_blob.upload_from_string.assert_not_called()

async def test_upload_media_missing_credentials(monkeypatch):
    settings = Settings()
    settings.config.media_gcs_bucket_name = """"
    settings.config.google_application_credentials = """"
    monkeypatch.setattr(""backend.server.v2.store.media.Settings"", lambda: settings)
    test_file = fastapi.UploadFile(
        filename=""laptop.jpeg"",
        file=io.BytesIO(b""\xFF\xD8\xFF"" + b""test data""),  # Valid JPEG signature
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )
    with pytest.raises(backend.server.v2.store.exceptions.StorageConfigError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)

async def test_upload_media_video_type(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.mp4"",
        file=io.BytesIO(b""\x00\x00\x00\x18ftypmp42""),  # Valid MP4 signature
        headers=starlette.datastructures.Headers({""content-type"": ""video/mp4""}),
    )
    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    assert result == ""http://test-url/media/laptop.jpeg""
    mock_bucket = mock_storage_client.bucket.return_value
    mock_blob = mock_bucket.blob.return_value
    mock_blob.upload_from_string.assert_called_once()

async def test_upload_media_file_too_large(mock_settings, mock_storage_client):
    large_data = b""\xFF\xD8\xFF"" + b""x"" * (
        50 * 1024 * 1024 + 1
    )  # 50MB + 1 byte with valid JPEG signature
    test_file = fastapi.UploadFile(
        filename=""laptop.jpeg"",
        file=io.BytesIO(large_data),
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )
    with pytest.raises(backend.server.v2.store.exceptions.FileSizeTooLargeError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)

async def test_upload_media_file_read_error(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""laptop.jpeg"",
        file=io.BytesIO(b""""),  # Empty file that will raise error on read
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )
    test_file.read = unittest.mock.AsyncMock(side_effect=Exception(""Read error""))
    with pytest.raises(backend.server.v2.store.exceptions.FileReadError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)

async def test_upload_media_png_success(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.png"",
        file=io.BytesIO(b""\x89PNG\r\n\x1a\n""),  # Valid PNG signature
        headers=starlette.datastructures.Headers({""content-type"": ""image/png""}),
    )
    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    assert result == ""http://test-url/media/laptop.jpeg""

async def test_upload_media_gif_success(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.gif"",
        file=io.BytesIO(b""GIF89a""),  # Valid GIF signature
        headers=starlette.datastructures.Headers({""content-type"": ""image/gif""}),
    )
    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    assert result == ""http://test-url/media/laptop.jpeg""

async def test_upload_media_webp_success(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.webp"",
        file=io.BytesIO(b""RIFF\x00\x00\x00\x00WEBP""),  # Valid WebP signature
        headers=starlette.datastructures.Headers({""content-type"": ""image/webp""}),
    )
    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    assert result == ""http://test-url/media/laptop.jpeg""

async def test_upload_media_webm_success(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.webm"",
        file=io.BytesIO(b""\x1a\x45\xdf\xa3""),  # Valid WebM signature
        headers=starlette.datastructures.Headers({""content-type"": ""video/webm""}),
    )
    result = await backend.server.v2.store.media.upload_media(""test-user"", test_file)
    assert result == ""http://test-url/media/laptop.jpeg""

async def test_upload_media_mismatched_signature(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.jpeg"",
        file=io.BytesIO(b""\x89PNG\r\n\x1a\n""),  # PNG signature with JPEG content type
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )
    with pytest.raises(backend.server.v2.store.exceptions.InvalidFileTypeError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)

async def test_upload_media_invalid_signature(mock_settings, mock_storage_client):
    test_file = fastapi.UploadFile(
        filename=""test.jpeg"",
        file=io.BytesIO(b""invalid signature""),
        headers=starlette.datastructures.Headers({""content-type"": ""image/jpeg""}),
    )
    with pytest.raises(backend.server.v2.store.exceptions.InvalidFileTypeError):
        await backend.server.v2.store.media.upload_media(""test-user"", test_file)","{'final_score': 99.24, 'length_similarity': 97.77, 'token_similarity': 100.0, 'structure_similarity': 97.86, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
159,"""""""
A Hamiltonian cycle (Hamiltonian circuit) is a graph cycle
through a graph that visits each node exactly once.
Determining whether such paths and cycles exist in graphs
is the 'Hamiltonian path problem', which is NP-complete.

Wikipedia: https://en.wikipedia.org/wiki/Hamiltonian_path
""""""


def valid_connection(
    graph: list[list[int]], next_ver: int, curr_ind: int, path: list[int]
) -> bool:
    """"""
    Checks whether it is possible to add next into path by validating 2 statements
    1. There should be path between current and next vertex
    2. Next vertex should not be in path
    If both validations succeed we return True, saying that it is possible to connect
    this vertices, otherwise we return False

    Case 1:Use exact graph as in main function, with initialized values
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 1],
    ...          [0, 1, 1, 1, 0]]
    >>> path = [0, -1, -1, -1, -1, 0]
    >>> curr_ind = 1
    >>> next_ver = 1
    >>> valid_connection(graph, next_ver, curr_ind, path)
    True

    Case 2: Same graph, but trying to connect to node that is already in path
    >>> path = [0, 1, 2, 4, -1, 0]
    >>> curr_ind = 4
    >>> next_ver = 1
    >>> valid_connection(graph, next_ver, curr_ind, path)
    False
    """"""

    # 1. Validate that path exists between current and next vertices
    if graph[path[curr_ind - 1]][next_ver] == 0:
        return False

    # 2. Validate that next vertex is not already in path
    return not any(vertex == next_ver for vertex in path)


def util_hamilton_cycle(graph: list[list[int]], path: list[int], curr_ind: int) -> bool:
    """"""
    Pseudo-Code
    Base Case:
    1. Check if we visited all of vertices
        1.1 If last visited vertex has path to starting vertex return True either
            return False
    Recursive Step:
    2. Iterate over each vertex
        Check if next vertex is valid for transiting from current vertex
            2.1 Remember next vertex as next transition
            2.2 Do recursive call and check if going to this vertex solves problem
            2.3 If next vertex leads to solution return True
            2.4 Else backtrack, delete remembered vertex

    Case 1: Use exact graph as in main function, with initialized values
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 1],
    ...          [0, 1, 1, 1, 0]]
    >>> path = [0, -1, -1, -1, -1, 0]
    >>> curr_ind = 1
    >>> util_hamilton_cycle(graph, path, curr_ind)
    True
    >>> path
    [0, 1, 2, 4, 3, 0]

    Case 2: Use exact graph as in previous case, but in the properties taken from
        middle of calculation
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 1],
    ...          [0, 1, 1, 1, 0]]
    >>> path = [0, 1, 2, -1, -1, 0]
    >>> curr_ind = 3
    >>> util_hamilton_cycle(graph, path, curr_ind)
    True
    >>> path
    [0, 1, 2, 4, 3, 0]
    """"""

    # Base Case
    if curr_ind == len(graph):
        # return whether path exists between current and starting vertices
        return graph[path[curr_ind - 1]][path[0]] == 1

    # Recursive Step
    for next_ver in range(len(graph)):
        if valid_connection(graph, next_ver, curr_ind, path):
            # Insert current vertex  into path as next transition
            path[curr_ind] = next_ver
            # Validate created path
            if util_hamilton_cycle(graph, path, curr_ind + 1):
                return True
            # Backtrack
            path[curr_ind] = -1
    return False


def hamilton_cycle(graph: list[list[int]], start_index: int = 0) -> list[int]:
    r""""""
    Wrapper function to call subroutine called util_hamilton_cycle,
    which will either return array of vertices indicating hamiltonian cycle
    or an empty list indicating that hamiltonian cycle was not found.
    Case 1:
    Following graph consists of 5 edges.
    If we look closely, we can see that there are multiple Hamiltonian cycles.
    For example one result is when we iterate like:
    (0)->(1)->(2)->(4)->(3)->(0)

    (0)---(1)---(2)
     |   /   \   |
     |  /     \  |
     | /       \ |
     |/         \|
    (3)---------(4)
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 1],
    ...          [0, 1, 1, 1, 0]]
    >>> hamilton_cycle(graph)
    [0, 1, 2, 4, 3, 0]

    Case 2:
    Same Graph as it was in Case 1, changed starting index from default to 3

    (0)---(1)---(2)
     |   /   \   |
     |  /     \  |
     | /       \ |
     |/         \|
    (3)---------(4)
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 1],
    ...          [0, 1, 1, 1, 0]]
    >>> hamilton_cycle(graph, 3)
    [3, 0, 1, 2, 4, 3]

    Case 3:
    Following Graph is exactly what it was before, but edge 3-4 is removed.
    Result is that there is no Hamiltonian Cycle anymore.

    (0)---(1)---(2)
     |   /   \   |
     |  /     \  |
     | /       \ |
     |/         \|
    (3)         (4)
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 0],
    ...          [0, 1, 1, 0, 0]]
    >>> hamilton_cycle(graph,4)
    []
    """"""

    # Initialize path with -1, indicating that we have not visited them yet
    path = [-1] * (len(graph) + 1)
    # initialize start and end of path with starting index
    path[0] = path[-1] = start_index
    # evaluate and if we find answer return path either return empty array
    return path if util_hamilton_cycle(graph, path, 1) else []
","""""""
A Hamiltonian cycle (Hamiltonian circuit) is a graph cycle
through a graph that visits each node exactly once.
Determining whether such paths and cycles exist in graphs
is the 'Hamiltonian path problem', which is NP-complete.

Wikipedia: https://en.wikipedia.org/wiki/Hamiltonian_path
""""""

def valid_connection(
    graph: list[list[int]], next_ver: int, curr_ind: int, path: list[int]
) -> bool:
    """"""
    Checks whether it is possible to add next into path by validating 2 statements
    1. There should be path between current and next vertex
    2. Next vertex should not be in path
    If both validations succeed we return True, saying that it is possible to connect
    this vertices, otherwise we return False

    Case 1:Use exact graph as in main function, with initialized values
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 1],
    ...          [0, 1, 1, 1, 0]]
    >>> path = [0, -1, -1, -1, -1, 0]
    >>> curr_ind = 1
    >>> next_ver = 1
    >>> valid_connection(graph, next_ver, curr_ind, path)
    True

    Case 2: Same graph, but trying to connect to node that is already in path
    >>> path = [0, 1, 2, 4, -1, 0]
    >>> curr_ind = 4
    >>> next_ver = 1
    >>> valid_connection(graph, next_ver, curr_ind, path)
    False
    """"""

    # 1. Validate that path exists between current and next vertices
    if graph[path[curr_ind - 1]][next_ver] == 0:
        return False

    # 2. Validate that next vertex is not already in path
    return not any(vertex == next_ver for vertex in path)


def util_hamilton_cycle(graph: list[list[int]], path: list[int], curr_ind: int) -> bool:
    """"""
    Pseudo-Code
    Base Case:
    1. Check if we visited all of vertices
        1.1 If last visited vertex has path to starting vertex return True either
            return False
    Recursive Step:
    2. Iterate over each vertex
        Check if next vertex is valid for transiting from current vertex
            2.1 Remember next vertex as next transition
            2.2 Do recursive call and check if going to this vertex solves problem
            2.3 If next vertex leads to solution return True
            2.4 Else backtrack, delete remembered vertex

    Case 1: Use exact graph as in main function, with initialized values
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 1],
    ...          [0, 1, 1, 1, 0]]
    >>> path = [0, -1, -1, -1, -1, 0]
    >>> curr_ind = 1
    >>> util_hamilton_cycle(graph, path, curr_ind)
    True
    >>> path
    [0, 1, 2, 4, 3, 0]

    Case 2: Use exact graph as in previous case, but in the properties taken from
        middle of calculation
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 1],
    ...          [0, 1, 1, 1, 0]]
    >>> path = [0, 1, 2, -1, -1, 0]
    >>> curr_ind = 3
    >>> util_hamilton_cycle(graph, path, curr_ind)
    True
    >>> path
    [0, 1, 2, 4, 3, 0]
    """"""

    # Base Case
    if curr_ind == len(graph):
        # return whether path exists between current and starting vertices
        return graph[path[curr_ind - 1]][path[0]] == 1

    # Recursive Step
    for next_ver in range(len(graph)):
        if valid_connection(graph, next_ver, curr_ind, path):
            # Insert current vertex  into path as next transition
            path[curr_ind] = next_ver
            # Validate created path
            if util_hamilton_cycle(graph, path, curr_ind + 1):
                return True
            # Backtrack
            path[curr_ind] = -1
    return False


def hamilton_cycle(graph: list[list[int]], start_index: int = 0) -> list[int]:
    r""""""
    Wrapper function to call subroutine called util_hamilton_cycle,
    which will either return array of vertices indicating hamiltonian cycle
    or an empty list indicating that hamiltonian cycle was not found.
    Case 1:
    Following graph consists of 5 edges.
    If we look closely, we can see that there are multiple Hamiltonian cycles.
    For example one result is when we iterate like:
    (0)->(1)->(2)->(4)->(3)->(0)

    (0)---(1)---(2)
     |   /   \   |
     |  /     \  |
     | /       \ |
     |/         \|
    (3)---------(4)
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 1],
    ...          [0, 1, 1, 1, 0]]
    >>> hamilton_cycle(graph)
    [0, 1, 2, 4, 3, 0]

    Case 2:
    Same Graph as it was in Case 1, changed starting index from default to 3

    (0)---(1)---(2)
     |   /   \   |
     |  /     \  |
     | /       \ |
     |/         \|
    (3)---------(4)
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 1],
    ...          [0, 1, 1, 1, 0]]
    >>> hamilton_cycle(graph, 3)
    [3, 0, 1, 2, 4, 3]

    Case 3:
    Following Graph is exactly what it was before, but edge 3-4 is removed.
    Result is that there is no Hamiltonian Cycle anymore.

    (0)---(1)---(2)
     |   /   \   |
     |  /     \  |
     | /       \ |
     |/         \|
    (3)         (4)
    >>> graph = [[0, 1, 0, 1, 0],
    ...          [1, 0, 1, 1, 1],
    ...          [0, 1, 0, 0, 1],
    ...          [1, 1, 0, 0, 0],
    ...          [0, 1, 1, 0, 0]]
    >>> hamilton_cycle(graph,4)
    []
    """"""

    # Initialize path with -1, indicating that we have not visited them yet
    path = [-1] * (len(graph) + 1)
    # initialize start and end of path with starting index
    path[0] = path[-1] = start_index
    # evaluate and if we find answer return path either return empty array
    return path if util_hamilton_cycle(graph, path, 1) else []","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
160,"import datetime
import typing
from enum import Enum
from typing import Generic, Literal, TypeVar, Union

import prisma.enums
import pydantic


class InstallationLocation(str, Enum):
    LOCAL = ""local""
    CLOUD = ""cloud""


class AgentInstalledFromMarketplaceEventData(pydantic.BaseModel):
    marketplace_agent_id: str
    installed_agent_id: str
    installation_location: InstallationLocation


class AgentInstalledFromTemplateEventData(pydantic.BaseModel):
    template_id: str
    installed_agent_id: str
    installation_location: InstallationLocation


class AgentInstalledFromMarketplaceEvent(pydantic.BaseModel):
    event_name: Literal[""agent_installed_from_marketplace""]
    event_data: AgentInstalledFromMarketplaceEventData


class AgentInstalledFromTemplateEvent(pydantic.BaseModel):
    event_name: Literal[""agent_installed_from_template""]
    event_data: AgentInstalledFromTemplateEventData


AnalyticsEvent = Union[
    AgentInstalledFromMarketplaceEvent, AgentInstalledFromTemplateEvent
]


class AnalyticsRequest(pydantic.BaseModel):
    event: AnalyticsEvent


class AddAgentRequest(pydantic.BaseModel):
    graph: dict[str, typing.Any]
    author: str
    keywords: list[str]
    categories: list[str]


class SubmissionReviewRequest(pydantic.BaseModel):
    agent_id: str
    version: int
    status: prisma.enums.SubmissionStatus
    comments: str | None


class AgentResponse(pydantic.BaseModel):
    """"""
    Represents a response from an agent.

    Attributes:
        id (str): The ID of the agent.
        name (str, optional): The name of the agent.
        description (str, optional): The description of the agent.
        author (str, optional): The author of the agent.
        keywords (list[str]): The keywords associated with the agent.
        categories (list[str]): The categories the agent belongs to.
        version (int): The version of the agent.
        createdAt (str): The creation date of the agent.
        updatedAt (str): The last update date of the agent.
    """"""

    id: str
    name: typing.Optional[str]
    description: typing.Optional[str]
    author: typing.Optional[str]
    keywords: list[str]
    categories: list[str]
    version: int
    createdAt: datetime.datetime
    updatedAt: datetime.datetime
    submissionStatus: str
    views: int = 0
    downloads: int = 0


class AgentDetailResponse(pydantic.BaseModel):
    """"""
    Represents the response data for an agent detail.

    Attributes:
        id (str): The ID of the agent.
        name (Optional[str]): The name of the agent.
        description (Optional[str]): The description of the agent.
        author (Optional[str]): The author of the agent.
        keywords (List[str]): The keywords associated with the agent.
        categories (List[str]): The categories the agent belongs to.
        version (int): The version of the agent.
        createdAt (str): The creation date of the agent.
        updatedAt (str): The last update date of the agent.
        graph (Dict[str, Any]): The graph data of the agent.
    """"""

    id: str
    name: typing.Optional[str]
    description: typing.Optional[str]
    author: typing.Optional[str]
    keywords: list[str]
    categories: list[str]
    version: int
    createdAt: datetime.datetime
    updatedAt: datetime.datetime
    graph: dict[str, typing.Any]


class FeaturedAgentResponse(pydantic.BaseModel):
    """"""
    Represents the response data for an agent detail.
    """"""

    agentId: str
    featuredCategories: list[str]
    createdAt: datetime.datetime
    updatedAt: datetime.datetime
    isActive: bool


class CategoriesResponse(pydantic.BaseModel):
    """"""
    Represents the response data for a list of categories.

    Attributes:
        unique_categories (list[str]): The list of unique categories.
    """"""

    unique_categories: list[str]


T = TypeVar(""T"")


class ListResponse(pydantic.BaseModel, Generic[T]):
    """"""
    Represents a list response.

    Attributes:
        items (list[T]): The list of items.
        total_count (int): The total count of items.
        page (int): The current page number.
        page_size (int): The number of items per page.
        total_pages (int): The total number of pages.
    """"""

    items: list[T]
    total_count: int
    page: int
    page_size: int
    total_pages: int
","import datetime
import typing
from enum import Enum
from typing import Generic, Literal, TypeVar, Union

import prisma.enums
import pydantic

class InstallationLocation(str, Enum):
    LOCAL = ""local""
    CLOUD = ""cloud""

class AgentInstalledFromMarketplaceEventData(pydantic.BaseModel):
    marketplace_agent_id: str
    installed_agent_id: str
    installation_location: InstallationLocation

class AgentInstalledFromTemplateEventData(pydantic.BaseModel):
    template_id: str
    installed_agent_id: str
    installation_location: InstallationLocation

class AgentInstalledFromMarketplaceEvent(pydantic.BaseModel):
    event_name: Literal[""agent_installed_from_marketplace""]
    event_data: AgentInstalledFromMarketplaceEventData

class AgentInstalledFromTemplateEvent(pydantic.BaseModel):
    event_name: Literal[""agent_installed_from_template""]
    event_data: AgentInstalledFromTemplateEventData

AnalyticsEvent = Union[
    AgentInstalledFromMarketplaceEvent, AgentInstalledFromTemplateEvent
]

class AnalyticsRequest(pydantic.BaseModel):
    event: AnalyticsEvent

class AddAgentRequest(pydantic.BaseModel):
    graph: dict[str, typing.Any]
    author: str
    keywords: list[str]
    categories: list[str]

class SubmissionReviewRequest(pydantic.BaseModel):
    agent_id: str
    version: int
    status: prisma.enums.SubmissionStatus
    comments: str | None

class AgentResponse(pydantic.BaseModel):
    """"""
    Represents a response from an agent.

    Attributes:
        id (str): The ID of the agent.
        name (str, optional): The name of the agent.
        description (str, optional): The description of the agent.
        author (str, optional): The author of the agent.
        keywords (list[str]): The keywords associated with the agent.
        categories (list[str]): The categories the agent belongs to.
        version (int): The version of the agent.
        createdAt (str): The creation date of the agent.
        updatedAt (str): The last update date of the agent.
    """"""
    id: str
    name: typing.Optional[str]
    description: typing.Optional[str]
    author: typing.Optional[str]
    keywords: list[str]
    categories: list[str]
    version: int
    createdAt: datetime.datetime
    updatedAt: datetime.datetime
    submissionStatus: str
    views: int = 0
    downloads: int = 0

class AgentDetailResponse(pydantic.BaseModel):
    """"""
    Represents the response data for an agent detail.

    Attributes:
        id (str): The ID of the agent.
        name (Optional[str]): The name of the agent.
        description (Optional[str]): The description of the agent.
        author (Optional[str]): The author of the agent.
        keywords (List[str]): The keywords associated with the agent.
        categories (List[str]): The categories the agent belongs to.
        version (int): The version of the agent.
        createdAt (str): The creation date of the agent.
        updatedAt (str): The last update date of the agent.
        graph (Dict[str, Any]): The graph data of the agent.
    """"""
    id: str
    name: typing.Optional[str]
    description: typing.Optional[str]
    author: typing.Optional[str]
    keywords: list[str]
    categories: list[str]
    version: int
    createdAt: datetime.datetime
    updatedAt: datetime.datetime
    graph: dict[str, typing.Any]

class FeaturedAgentResponse(pydantic.BaseModel):
    """"""
    Represents the response data for an agent detail.
    """"""
    agentId: str
    featuredCategories: list[str]
    createdAt: datetime.datetime
    updatedAt: datetime.datetime
    isActive: bool

class CategoriesResponse(pydantic.BaseModel):
    """"""
    Represents the response data for a list of categories.

    Attributes:
        unique_categories (list[str]): The list of unique categories.
    """"""

    unique_categories: list[str]

T = TypeVar(""T"")

class ListResponse(pydantic.BaseModel, Generic[T]):
    """"""
    Represents a list response.

    Attributes:
        items (list[T]): The list of items.
        total_count (int): The total count of items.
        page (int): The current page number.
        page_size (int): The number of items per page.
        total_pages (int): The total number of pages.
    """"""
    items: list[T]
    total_count: int
    page: int
    page_size: int
    total_pages: int","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
161,"import random
from collections import defaultdict
from enum import Enum
from typing import Any, Dict, List, Optional, Union

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class SamplingMethod(str, Enum):
    RANDOM = ""random""
    SYSTEMATIC = ""systematic""
    TOP = ""top""
    BOTTOM = ""bottom""
    STRATIFIED = ""stratified""
    WEIGHTED = ""weighted""
    RESERVOIR = ""reservoir""
    CLUSTER = ""cluster""


class DataSamplingBlock(Block):
    class Input(BlockSchema):
        data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(
            description=""The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists."",
            placeholder=""{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]"",
        )
        sample_size: int = SchemaField(
            description=""The number of samples to take from the dataset."",
            placeholder=""10"",
            default=10,
        )
        sampling_method: SamplingMethod = SchemaField(
            description=""The method to use for sampling."",
            default=SamplingMethod.RANDOM,
        )
        accumulate: bool = SchemaField(
            description=""Whether to accumulate data before sampling."",
            default=False,
        )
        random_seed: Optional[int] = SchemaField(
            description=""Seed for random number generator (optional)."",
            default=None,
        )
        stratify_key: Optional[str] = SchemaField(
            description=""Key to use for stratified sampling (required for stratified sampling)."",
            default=None,
        )
        weight_key: Optional[str] = SchemaField(
            description=""Key to use for weighted sampling (required for weighted sampling)."",
            default=None,
        )
        cluster_key: Optional[str] = SchemaField(
            description=""Key to use for cluster sampling (required for cluster sampling)."",
            default=None,
        )

    class Output(BlockSchema):
        sampled_data: List[Union[dict, List[Any]]] = SchemaField(
            description=""The sampled subset of the input data.""
        )
        sample_indices: List[int] = SchemaField(
            description=""The indices of the sampled data in the original dataset.""
        )

    def __init__(self):
        super().__init__(
            id=""4a448883-71fa-49cf-91cf-70d793bd7d87"",
            description=""This block samples data from a given dataset using various sampling methods."",
            categories={BlockCategory.LOGIC},
            input_schema=DataSamplingBlock.Input,
            output_schema=DataSamplingBlock.Output,
            test_input={
                ""data"": [
                    {""id"": i, ""value"": chr(97 + i), ""group"": i % 3} for i in range(10)
                ],
                ""sample_size"": 3,
                ""sampling_method"": SamplingMethod.STRATIFIED,
                ""accumulate"": False,
                ""random_seed"": 42,
                ""stratify_key"": ""group"",
            },
            test_output=[
                (
                    ""sampled_data"",
                    [
                        {""id"": 0, ""value"": ""a"", ""group"": 0},
                        {""id"": 1, ""value"": ""b"", ""group"": 1},
                        {""id"": 8, ""value"": ""i"", ""group"": 2},
                    ],
                ),
                (""sample_indices"", [0, 1, 8]),
            ],
        )
        self.accumulated_data = []

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        if input_data.accumulate:
            if isinstance(input_data.data, dict):
                self.accumulated_data.append(input_data.data)
            elif isinstance(input_data.data, list):
                self.accumulated_data.extend(input_data.data)
            else:
                raise ValueError(f""Unsupported data type: {type(input_data.data)}"")

            # If we don't have enough data yet, return without sampling
            if len(self.accumulated_data) < input_data.sample_size:
                return

            data_to_sample = self.accumulated_data
        else:
            # If not accumulating, use the input data directly
            data_to_sample = (
                input_data.data
                if isinstance(input_data.data, list)
                else [input_data.data]
            )

        if input_data.random_seed is not None:
            random.seed(input_data.random_seed)

        data_size = len(data_to_sample)

        if input_data.sample_size > data_size:
            raise ValueError(
                f""Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).""
            )

        indices = []

        if input_data.sampling_method == SamplingMethod.RANDOM:
            indices = random.sample(range(data_size), input_data.sample_size)
        elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:
            step = data_size // input_data.sample_size
            start = random.randint(0, step - 1)
            indices = list(range(start, data_size, step))[: input_data.sample_size]
        elif input_data.sampling_method == SamplingMethod.TOP:
            indices = list(range(input_data.sample_size))
        elif input_data.sampling_method == SamplingMethod.BOTTOM:
            indices = list(range(data_size - input_data.sample_size, data_size))
        elif input_data.sampling_method == SamplingMethod.STRATIFIED:
            if not input_data.stratify_key:
                raise ValueError(
                    ""Stratify key must be provided for stratified sampling.""
                )
            strata = defaultdict(list)
            for i, item in enumerate(data_to_sample):
                if isinstance(item, dict):
                    strata_value = item.get(input_data.stratify_key)
                elif hasattr(item, input_data.stratify_key):
                    strata_value = getattr(item, input_data.stratify_key)
                else:
                    raise ValueError(
                        f""Stratify key '{input_data.stratify_key}' not found in item {item}""
                    )

                if strata_value is None:
                    raise ValueError(
                        f""Stratify value for key '{input_data.stratify_key}' is None""
                    )

                strata[str(strata_value)].append(i)

            # Calculate the number of samples to take from each stratum
            stratum_sizes = {
                k: max(1, int(len(v) / data_size * input_data.sample_size))
                for k, v in strata.items()
            }

            # Adjust sizes to ensure we get exactly sample_size samples
            while sum(stratum_sizes.values()) != input_data.sample_size:
                if sum(stratum_sizes.values()) < input_data.sample_size:
                    stratum_sizes[
                        max(stratum_sizes, key=lambda k: stratum_sizes[k])
                    ] += 1
                else:
                    stratum_sizes[
                        max(stratum_sizes, key=lambda k: stratum_sizes[k])
                    ] -= 1

            for stratum, size in stratum_sizes.items():
                indices.extend(random.sample(strata[stratum], size))
        elif input_data.sampling_method == SamplingMethod.WEIGHTED:
            if not input_data.weight_key:
                raise ValueError(""Weight key must be provided for weighted sampling."")
            weights = []
            for item in data_to_sample:
                if isinstance(item, dict):
                    weight = item.get(input_data.weight_key)
                elif hasattr(item, input_data.weight_key):
                    weight = getattr(item, input_data.weight_key)
                else:
                    raise ValueError(
                        f""Weight key '{input_data.weight_key}' not found in item {item}""
                    )

                if weight is None:
                    raise ValueError(
                        f""Weight value for key '{input_data.weight_key}' is None""
                    )
                try:
                    weights.append(float(weight))
                except ValueError:
                    raise ValueError(
                        f""Weight value '{weight}' cannot be converted to a number""
                    )

            if not weights:
                raise ValueError(
                    f""No valid weights found using key '{input_data.weight_key}'""
                )

            indices = random.choices(
                range(data_size), weights=weights, k=input_data.sample_size
            )
        elif input_data.sampling_method == SamplingMethod.RESERVOIR:
            indices = list(range(input_data.sample_size))
            for i in range(input_data.sample_size, data_size):
                j = random.randint(0, i)
                if j < input_data.sample_size:
                    indices[j] = i
        elif input_data.sampling_method == SamplingMethod.CLUSTER:
            if not input_data.cluster_key:
                raise ValueError(""Cluster key must be provided for cluster sampling."")
            clusters = defaultdict(list)
            for i, item in enumerate(data_to_sample):
                if isinstance(item, dict):
                    cluster_value = item.get(input_data.cluster_key)
                elif hasattr(item, input_data.cluster_key):
                    cluster_value = getattr(item, input_data.cluster_key)
                else:
                    raise TypeError(
                        f""Item {item} does not have the cluster key '{input_data.cluster_key}'""
                    )

                clusters[str(cluster_value)].append(i)

            # Randomly select clusters until we have enough samples
            selected_clusters = []
            while (
                sum(len(clusters[c]) for c in selected_clusters)
                < input_data.sample_size
            ):
                available_clusters = [c for c in clusters if c not in selected_clusters]
                if not available_clusters:
                    break
                selected_clusters.append(random.choice(available_clusters))

            for cluster in selected_clusters:
                indices.extend(clusters[cluster])

            # If we have more samples than needed, randomly remove some
            if len(indices) > input_data.sample_size:
                indices = random.sample(indices, input_data.sample_size)
        else:
            raise ValueError(f""Unknown sampling method: {input_data.sampling_method}"")

        sampled_data = [data_to_sample[i] for i in indices]

        # Clear accumulated data after sampling if accumulation is enabled
        if input_data.accumulate:
            self.accumulated_data = []

        yield ""sampled_data"", sampled_data
        yield ""sample_indices"", indices
","import random
from collections import defaultdict
from enum import Enum
from typing import Any, Dict, List, Optional, Union

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class SamplingMethod(str, Enum):
    RANDOM = ""random""
    SYSTEMATIC = ""systematic""
    TOP = ""top""
    BOTTOM = ""bottom""
    STRATIFIED = ""stratified""
    WEIGHTED = ""weighted""
    RESERVOIR = ""reservoir""
    CLUSTER = ""cluster""


class DataSamplingBlock(Block):
    class Input(BlockSchema):
        data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(
            description=""The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists."",
            placeholder=""{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]"",
        )
        sample_size: int = SchemaField(
            description=""The number of samples to take from the dataset."",
            placeholder=""10"",
            default=10,
        )
        sampling_method: SamplingMethod = SchemaField(
            description=""The method to use for sampling."",
            default=SamplingMethod.RANDOM,
        )
        accumulate: bool = SchemaField(
            description=""Whether to accumulate data before sampling."",
            default=False,
        )
        random_seed: Optional[int] = SchemaField(
            description=""Seed for random number generator (optional)."",
            default=None,
        )
        stratify_key: Optional[str] = SchemaField(
            description=""Key to use for stratified sampling (required for stratified sampling)."",
            default=None,
        )
        weight_key: Optional[str] = SchemaField(
            description=""Key to use for weighted sampling (required for weighted sampling)."",
            default=None,
        )
        cluster_key: Optional[str] = SchemaField(
            description=""Key to use for cluster sampling (required for cluster sampling)."",
            default=None,
        )

    class Output(BlockSchema):
        sampled_data: List[Union[dict, List[Any]]] = SchemaField(
            description=""The sampled subset of the input data.""
        )
        sample_indices: List[int] = SchemaField(
            description=""The indices of the sampled data in the original dataset.""
        )

    def __init__(self):
        super().__init__(
            id=""4a448883-71fa-49cf-91cf-70d793bd7d87"",
            description=""This block samples data from a given dataset using various sampling methods."",
            categories={BlockCategory.LOGIC},
            input_schema=DataSamplingBlock.Input,
            output_schema=DataSamplingBlock.Output,
            test_input={
                ""data"": [
                    {""id"": i, ""value"": chr(97 + i), ""group"": i % 3} for i in range(10)
                ],
                ""sample_size"": 3,
                ""sampling_method"": SamplingMethod.STRATIFIED,
                ""accumulate"": False,
                ""random_seed"": 42,
                ""stratify_key"": ""group"",
            },
            test_output=[
                (
                    ""sampled_data"",
                    [
                        {""id"": 0, ""value"": ""a"", ""group"": 0},
                        {""id"": 1, ""value"": ""b"", ""group"": 1},
                        {""id"": 8, ""value"": ""i"", ""group"": 2},
                    ],
                ),
                (""sample_indices"", [0, 1, 8]),
            ],
        )
        self.accumulated_data = []

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        if input_data.accumulate:
            if isinstance(input_data.data, dict):
                self.accumulated_data.append(input_data.data)
            elif isinstance(input_data.data, list):
                self.accumulated_data.extend(input_data.data)
            else:
                raise ValueError(f""Unsupported data type: {type(input_data.data)}"")

            # If we don't have enough data yet, return without sampling
            if len(self.accumulated_data) < input_data.sample_size:
                return

            data_to_sample = self.accumulated_data
        else:
            # If not accumulating, use the input data directly
            data_to_sample = (
                input_data.data
                if isinstance(input_data.data, list)
                else [input_data.data]
            )

        if input_data.random_seed is not None:
            random.seed(input_data.random_seed)

        data_size = len(data_to_sample)

        if input_data.sample_size > data_size:
            raise ValueError(
                f""Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).""
            )

        indices = []

        if input_data.sampling_method == SamplingMethod.RANDOM:
            indices = random.sample(range(data_size), input_data.sample_size)
        elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:
            step = data_size // input_data.sample_size
            start = random.randint(0, step - 1)
            indices = list(range(start, data_size, step))[: input_data.sample_size]
        elif input_data.sampling_method == SamplingMethod.TOP:
            indices = list(range(input_data.sample_size))
        elif input_data.sampling_method == SamplingMethod.BOTTOM:
            indices = list(range(data_size - input_data.sample_size, data_size))
        elif input_data.sampling_method == SamplingMethod.STRATIFIED:
            if not input_data.stratify_key:
                raise ValueError(
                    ""Stratify key must be provided for stratified sampling.""
                )
            strata = defaultdict(list)
            for i, item in enumerate(data_to_sample):
                if isinstance(item, dict):
                    strata_value = item.get(input_data.stratify_key)
                elif hasattr(item, input_data.stratify_key):
                    strata_value = getattr(item, input_data.stratify_key)
                else:
                    raise ValueError(
                        f""Stratify key '{input_data.stratify_key}' not found in item {item}""
                    )

                if strata_value is None:
                    raise ValueError(
                        f""Stratify value for key '{input_data.stratify_key}' is None""
                    )

                strata[str(strata_value)].append(i)

            # Calculate the number of samples to take from each stratum
            stratum_sizes = {
                k: max(1, int(len(v) / data_size * input_data.sample_size))
                for k, v in strata.items()
            }

            # Adjust sizes to ensure we get exactly sample_size samples
            while sum(stratum_sizes.values()) != input_data.sample_size:
                if sum(stratum_sizes.values()) < input_data.sample_size:
                    stratum_sizes[
                        max(stratum_sizes, key=lambda k: stratum_sizes[k])
                    ] += 1
                else:
                    stratum_sizes[
                        max(stratum_sizes, key=lambda k: stratum_sizes[k])
                    ] -= 1

            for stratum, size in stratum_sizes.items():
                indices.extend(random.sample(strata[stratum], size))
        elif input_data.sampling_method == SamplingMethod.WEIGHTED:
            if not input_data.weight_key:
                raise ValueError(""Weight key must be provided for weighted sampling."")
            weights = []
            for item in data_to_sample:
                if isinstance(item, dict):
                    weight = item.get(input_data.weight_key)
                elif hasattr(item, input_data.weight_key):
                    weight = getattr(item, input_data.weight_key)
                else:
                    raise ValueError(
                        f""Weight key '{input_data.weight_key}' not found in item {item}""
                    )

                if weight is None:
                    raise ValueError(
                        f""Weight value for key '{input_data.weight_key}' is None""
                    )
                try:
                    weights.append(float(weight))
                except ValueError:
                    raise ValueError(
                        f""Weight value '{weight}' cannot be converted to a number""
                    )

            if not weights:
                raise ValueError(
                    f""No valid weights found using key '{input_data.weight_key}'""
                )

            indices = random.choices(
                range(data_size), weights=weights, k=input_data.sample_size
            )
        elif input_data.sampling_method == SamplingMethod.RESERVOIR:
            indices = list(range(input_data.sample_size))
            for i in range(input_data.sample_size, data_size):
                j = random.randint(0, i)
                if j < input_data.sample_size:
                    indices[j] = i
        elif input_data.sampling_method == SamplingMethod.CLUSTER:
            if not input_data.cluster_key:
                raise ValueError(""Cluster key must be provided for cluster sampling."")
            clusters = defaultdict(list)
            for i, item in enumerate(data_to_sample):
                if isinstance(item, dict):
                    cluster_value = item.get(input_data.cluster_key)
                elif hasattr(item, input_data.cluster_key):
                    cluster_value = getattr(item, input_data.cluster_key)
                else:
                    raise TypeError(
                        f""Item {item} does not have the cluster key '{input_data.cluster_key}'""
                    )

                clusters[str(cluster_value)].append(i)

            # Randomly select clusters until we have enough samples
            selected_clusters = []
            while (
                sum(len(clusters[c]) for c in selected_clusters)
                < input_data.sample_size
            ):
                available_clusters = [c for c in clusters if c not in selected_clusters]
                if not available_clusters:
                    break
                selected_clusters.append(random.choice(available_clusters))

            for cluster in selected_clusters:
                indices.extend(clusters[cluster])

            # If we have more samples than needed, randomly remove some
            if len(indices) > input_data.sample_size:
                indices = random.sample(indices, input_data.sample_size)
        else:
            raise ValueError(f""Unknown sampling method: {input_data.sampling_method}"")

        sampled_data = [data_to_sample[i] for i in indices]

        # Clear accumulated data after sampling if accumulation is enabled
        if input_data.accumulate:
            self.accumulated_data = []

        yield ""sampled_data"", sampled_data
        yield ""sample_indices"", indices","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
162,"from abc import ABCMeta, abstractmethod
from collections import deque
from enum import Enum


class Rank(Enum):

    OPERATOR = 0
    SUPERVISOR = 1
    DIRECTOR = 2


class Employee(metaclass=ABCMeta):

    def __init__(self, employee_id, name, rank, call_center):
        self.employee_id = employee_id
        self.name = name
        self.rank = rank
        self.call = None
        self.call_center = call_center

    def take_call(self, call):
        """"""Assume the employee will always successfully take the call.""""""
        self.call = call
        self.call.employee = self
        self.call.state = CallState.IN_PROGRESS

    def complete_call(self):
        self.call.state = CallState.COMPLETE
        self.call_center.notify_call_completed(self.call)

    @abstractmethod
    def escalate_call(self):
        pass

    def _escalate_call(self):
        self.call.state = CallState.READY
        call = self.call
        self.call = None
        self.call_center.notify_call_escalated(call)


class Operator(Employee):

    def __init__(self, employee_id, name):
        super(Operator, self).__init__(employee_id, name, Rank.OPERATOR)

    def escalate_call(self):
        self.call.level = Rank.SUPERVISOR
        self._escalate_call()


class Supervisor(Employee):

    def __init__(self, employee_id, name):
        super(Operator, self).__init__(employee_id, name, Rank.SUPERVISOR)

    def escalate_call(self):
        self.call.level = Rank.DIRECTOR
        self._escalate_call()


class Director(Employee):

    def __init__(self, employee_id, name):
        super(Operator, self).__init__(employee_id, name, Rank.DIRECTOR)

    def escalate_call(self):
        raise NotImplementedError('Directors must be able to handle any call')


class CallState(Enum):

    READY = 0
    IN_PROGRESS = 1
    COMPLETE = 2


class Call(object):

    def __init__(self, rank):
        self.state = CallState.READY
        self.rank = rank
        self.employee = None


class CallCenter(object):

    def __init__(self, operators, supervisors, directors):
        self.operators = operators
        self.supervisors = supervisors
        self.directors = directors
        self.queued_calls = deque()

    def dispatch_call(self, call):
        if call.rank not in (Rank.OPERATOR, Rank.SUPERVISOR, Rank.DIRECTOR):
            raise ValueError('Invalid call rank: {}'.format(call.rank))
        employee = None
        if call.rank == Rank.OPERATOR:
            employee = self._dispatch_call(call, self.operators)
        if call.rank == Rank.SUPERVISOR or employee is None:
            employee = self._dispatch_call(call, self.supervisors)
        if call.rank == Rank.DIRECTOR or employee is None:
            employee = self._dispatch_call(call, self.directors)
        if employee is None:
            self.queued_calls.append(call)

    def _dispatch_call(self, call, employees):
        for employee in employees:
            if employee.call is None:
                employee.take_call(call)
                return employee
        return None

    def notify_call_escalated(self, call):
        pass

    def notify_call_completed(self, call):
        pass

    def dispatch_queued_call_to_newly_freed_employee(self, call, employee):
        pass
","from abc import ABCMeta, abstractmethod
from collections import deque
from enum import Enum

class Rank(Enum):
    OPERATOR = 0
    SUPERVISOR = 1
    DIRECTOR = 2

class Employee(metaclass=ABCMeta):

    def __init__(self, employee_id, name, rank, call_center):
        self.employee_id = employee_id
        self.name = name
        self.rank = rank
        self.call = None
        self.call_center = call_center

    def take_call(self, call):
        """"""Assume the employee will always successfully take the call.""""""
        self.call = call
        self.call.employee = self
        self.call.state = CallState.IN_PROGRESS

    def complete_call(self):
        self.call.state = CallState.COMPLETE
        self.call_center.notify_call_completed(self.call)

    @abstractmethod
    def escalate_call(self):
        pass

    def _escalate_call(self):
        self.call.state = CallState.READY
        call = self.call
        self.call = None
        self.call_center.notify_call_escalated(call)

class Operator(Employee):

    def __init__(self, employee_id, name, call_center):
        super(Operator, self).__init__(employee_id, name, Rank.OPERATOR, call_center)

    def escalate_call(self):
        self.call.rank = Rank.SUPERVISOR
        self._escalate_call()

class Supervisor(Employee):

    def __init__(self, employee_id, name, call_center):
        super(Supervisor, self).__init__(employee_id, name, Rank.SUPERVISOR, call_center)

    def escalate_call(self):
        self.call.rank = Rank.DIRECTOR
        self._escalate_call()

class Director(Employee):

    def __init__(self, employee_id, name, call_center):
        super(Director, self).__init__(employee_id, name, Rank.DIRECTOR, call_center)

    def escalate_call(self):
        raise NotImplementedError('Directors must be able to handle any call')

class CallState(Enum):
    READY = 0
    IN_PROGRESS = 1
    COMPLETE = 2

class Call(object):

    def __init__(self, rank):
        self.state = CallState.READY
        self.rank = rank
        self.employee = None

class CallCenter(object):

    def __init__(self, operators, supervisors, directors):
        self.operators = operators
        self.supervisors = supervisors
        self.directors = directors
        self.queued_calls = deque()

    def dispatch_call(self, call):
        if call.rank not in (Rank.OPERATOR, Rank.SUPERVISOR, Rank.DIRECTOR):
            raise ValueError('Invalid call rank: {}'.format(call.rank))

        employee = None

        if call.rank == Rank.OPERATOR:
            employee = self._dispatch_call(call, self.operators)

        if call.rank == Rank.SUPERVISOR or employee is None:
            employee = self._dispatch_call(call, self.supervisors)

        if call.rank == Rank.DIRECTOR or employee is None:
            employee = self._dispatch_call(call, self.directors)

        if employee is None:
            self.queued_calls.append(call)

    def _dispatch_call(self, call, employees):
        for employee in employees:
            if employee.call is None:
                employee.take_call(call)
                return employee
        return None

    def notify_call_escalated(self, call):
        pass

    def notify_call_completed(self, call):
        pass

    def dispatch_queued_call_to_newly_freed_employee(self, call, employee):
        pass","{'final_score': 98.32, 'length_similarity': 97.64, 'token_similarity': 95.59, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
163,"# -*- coding: utf-8 -*-


class PagesDataStore(object):

    def __init__(self, db):
        self.db = db
        pass

    def add_link_to_crawl(self, url):
        """"""Add the given link to `links_to_crawl`.""""""
        pass

    def remove_link_to_crawl(self, url):
        """"""Remove the given link from `links_to_crawl`.""""""
        pass

    def reduce_priority_link_to_crawl(self, url):
        """"""Reduce the priority of a link in `links_to_crawl` to avoid cycles.""""""
        pass

    def extract_max_priority_page(self):
        """"""Return the highest priority link in `links_to_crawl`.""""""
        pass

    def insert_crawled_link(self, url, signature):
        """"""Add the given link to `crawled_links`.""""""
        pass

    def crawled_similar(self, signature):
        """"""Determine if we've already crawled a page matching the given signature""""""
        pass


class Page(object):

    def __init__(self, url, contents, child_urls):
        self.url = url
        self.contents = contents
        self.child_urls = child_urls
        self.signature = self.create_signature()

    def create_signature(self):
        # Create signature based on url and contents
        pass


class Crawler(object):

    def __init__(self, pages, data_store, reverse_index_queue, doc_index_queue):
        self.pages = pages
        self.data_store = data_store
        self.reverse_index_queue = reverse_index_queue
        self.doc_index_queue = doc_index_queue

    def crawl_page(self, page):
        for url in page.child_urls:
            self.data_store.add_link_to_crawl(url)
        self.reverse_index_queue.generate(page)
        self.doc_index_queue.generate(page)
        self.data_store.remove_link_to_crawl(page.url)
        self.data_store.insert_crawled_link(page.url, page.signature)

    def crawl(self):
        while True:
            page = self.data_store.extract_max_priority_page()
            if page is None:
                break
            if self.data_store.crawled_similar(page.signature):
                self.data_store.reduce_priority_link_to_crawl(page.url)
            else:
                self.crawl_page(page)
            page = self.data_store.extract_max_priority_page()
","# -*- coding: utf-8 -*-


class PagesDataStore(object):

    def __init__(self, db):
        self.db = db
        pass

    def add_link_to_crawl(self, url):
        """"""Add the given link to `links_to_crawl`.""""""
        pass

    def remove_link_to_crawl(self, url):
        """"""Remove the given link from `links_to_crawl`.""""""
        pass

    def reduce_priority_link_to_crawl(self, url):
        """"""Reduce the priority of a link in `links_to_crawl` to avoid cycles.""""""
        pass

    def extract_max_priority_page(self):
        """"""Return the highest priority link in `links_to_crawl`.""""""
        pass

    def insert_crawled_link(self, url, signature):
        """"""Add the given link to `crawled_links`.""""""
        pass

    def crawled_similar(self, signature):
        """"""Determine if we've already crawled a page matching the given signature""""""
        pass


class Page(object):

    def __init__(self, url, contents, child_urls):
        self.url = url
        self.contents = contents
        self.child_urls = child_urls
        self.signature = self.create_signature()

    def create_signature(self):
        # Create signature based on url and contents
        pass


class Crawler(object):

    def __init__(self, pages, data_store, reverse_index_queue, doc_index_queue):
        self.pages = pages
        self.data_store = data_store
        self.reverse_index_queue = reverse_index_queue
        self.doc_index_queue = doc_index_queue

    def crawl_page(self, page):
        for url in page.child_urls:
            self.data_store.add_link_to_crawl(url)
        self.reverse_index_queue.generate(page)
        self.doc_index_queue.generate(page)
        self.data_store.remove_link_to_crawl(page.url)
        self.data_store.insert_crawled_link(page.url, page.signature)

    def crawl(self):
        while True:
            page = self.data_store.extract_max_priority_page()
            if page is None:
                break
            if self.data_store.crawled_similar(page.signature):
                self.data_store.reduce_priority_link_to_crawl(page.url)
            else:
                self.crawl_page(page)
            page = self.data_store.extract_max_priority_page()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
164,"def get_highest_set_bit_position(number: int) -> int:
    """"""
    Returns position of the highest set bit of a number.
    Ref - https://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious
    >>> get_highest_set_bit_position(25)
    5
    >>> get_highest_set_bit_position(37)
    6
    >>> get_highest_set_bit_position(1)
    1
    >>> get_highest_set_bit_position(4)
    3
    >>> get_highest_set_bit_position(0)
    0
    >>> get_highest_set_bit_position(0.8)
    Traceback (most recent call last):
        ...
    TypeError: Input value must be an 'int' type
    """"""
    if not isinstance(number, int):
        raise TypeError(""Input value must be an 'int' type"")

    position = 0
    while number:
        position += 1
        number >>= 1

    return position


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","def get_highest_set_bit_position(number: int) -> int:
    """"""
    Returns position of the highest set bit of a number.
    Ref - https://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious
    >>> get_highest_set_bit_position(25)
    5
    >>> get_highest_set_bit_position(37)
    6
    >>> get_highest_set_bit_position(1)
    1
    >>> get_highest_set_bit_position(4)
    3
    >>> get_highest_set_bit_position(0)
    0
    >>> get_highest_set_bit_position(0.8)
    Traceback (most recent call last):
        ...
    TypeError: Input value must be an 'int' type
    """"""
    if not isinstance(number, int):
        raise TypeError(""Input value must be an 'int' type"")
    
    position = 0
    while number:
        position += 1
        number >>= 1
    
    return position

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
165,"import pytest
from ldclient import LDClient

from autogpt_libs.feature_flag.client import feature_flag, mock_flag_variation


@pytest.fixture
def ld_client(mocker):
    client = mocker.Mock(spec=LDClient)
    mocker.patch(""ldclient.get"", return_value=client)
    client.is_initialized.return_value = True
    return client


@pytest.mark.asyncio
async def test_feature_flag_enabled(ld_client):
    ld_client.variation.return_value = True

    @feature_flag(""test-flag"")
    async def test_function(user_id: str):
        return ""success""

    result = test_function(user_id=""test-user"")
    assert result == ""success""
    ld_client.variation.assert_called_once()


@pytest.mark.asyncio
async def test_feature_flag_unauthorized_response(ld_client):
    ld_client.variation.return_value = False

    @feature_flag(""test-flag"")
    async def test_function(user_id: str):
        return ""success""

    result = test_function(user_id=""test-user"")
    assert result == {""error"": ""disabled""}


def test_mock_flag_variation(ld_client):
    with mock_flag_variation(""test-flag"", True):
        assert ld_client.variation(""test-flag"", None, False)

    with mock_flag_variation(""test-flag"", False):
        assert ld_client.variation(""test-flag"", None, False)
","import pytest
from ldclient import LDClient
from autogpt_libs.feature_flag.client import feature_flag, mock_flag_variation

@pytest.fixture
def ld_client(mocker):
    client = mocker.Mock(spec=LDClient)
    mocker.patch(""ldclient.get"", return_value=client)
    client.is_initialized.return_value = True
    return client

@pytest.mark.asyncio
async def test_feature_flag_enabled(ld_client):
    ld_client.variation.return_value = True
    
    @feature_flag(""test-flag"")
    async def test_function(user_id: str):
        return ""success""
    
    result = test_function(user_id=""test-user"")
    assert result == ""success""
    ld_client.variation.assert_called_once()

@pytest.mark.asyncio
async def test_feature_flag_unauthorized_response(ld_client):
    ld_client.variation.return_value = False
    
    @feature_flag(""test-flag"")
    async def test_function(user_id: str):
        return ""success""
    
    result = test_function(user_id=""test-user"")
    assert result == {""error"": ""disabled""}

def test_mock_flag_variation(ld_client):
    with mock_flag_variation(""test-flag"", True):
        assert ld_client.variation(""test-flag"", None, False)

    with mock_flag_variation(""test-flag"", False):
        assert ld_client.variation(""test-flag"", None, False)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
166,"import base64
from email.utils import parseaddr
from typing import List

from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from pydantic import BaseModel

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

from ._auth import (
    GOOGLE_OAUTH_IS_CONFIGURED,
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GoogleCredentials,
    GoogleCredentialsField,
    GoogleCredentialsInput,
)


class Attachment(BaseModel):
    filename: str
    content_type: str
    size: int
    attachment_id: str


class Email(BaseModel):
    id: str
    subject: str
    snippet: str
    from_: str
    to: str
    date: str
    body: str = """"  # Default to an empty string
    sizeEstimate: int
    attachments: List[Attachment]


class GmailReadBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.readonly""]
        )
        query: str = SchemaField(
            description=""Search query for reading emails"",
            default=""is:unread"",
        )
        max_results: int = SchemaField(
            description=""Maximum number of emails to retrieve"",
            default=10,
        )

    class Output(BlockSchema):
        email: Email = SchemaField(
            description=""Email data"",
        )
        emails: list[Email] = SchemaField(
            description=""List of email data"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""25310c70-b89b-43ba-b25c-4dfa7e2a481c"",
            description=""This block reads emails from Gmail."",
            categories={BlockCategory.COMMUNICATION},
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            input_schema=GmailReadBlock.Input,
            output_schema=GmailReadBlock.Output,
            test_input={
                ""query"": ""is:unread"",
                ""max_results"": 5,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""email"",
                    {
                        ""id"": ""1"",
                        ""subject"": ""Test Email"",
                        ""snippet"": ""This is a test email"",
                        ""from_"": ""test@example.com"",
                        ""to"": ""recipient@example.com"",
                        ""date"": ""2024-01-01"",
                        ""body"": ""This is a test email"",
                        ""sizeEstimate"": 100,
                        ""attachments"": [],
                    },
                ),
                (
                    ""emails"",
                    [
                        {
                            ""id"": ""1"",
                            ""subject"": ""Test Email"",
                            ""snippet"": ""This is a test email"",
                            ""from_"": ""test@example.com"",
                            ""to"": ""recipient@example.com"",
                            ""date"": ""2024-01-01"",
                            ""body"": ""This is a test email"",
                            ""sizeEstimate"": 100,
                            ""attachments"": [],
                        }
                    ],
                ),
            ],
            test_mock={
                ""_read_emails"": lambda *args, **kwargs: [
                    {
                        ""id"": ""1"",
                        ""subject"": ""Test Email"",
                        ""snippet"": ""This is a test email"",
                        ""from_"": ""test@example.com"",
                        ""to"": ""recipient@example.com"",
                        ""date"": ""2024-01-01"",
                        ""body"": ""This is a test email"",
                        ""sizeEstimate"": 100,
                        ""attachments"": [],
                    }
                ],
                ""_send_email"": lambda *args, **kwargs: {""id"": ""1"", ""status"": ""sent""},
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = self._build_service(credentials, **kwargs)
        messages = self._read_emails(service, input_data.query, input_data.max_results)
        for email in messages:
            yield ""email"", email
        yield ""emails"", messages

    @staticmethod
    def _build_service(credentials: GoogleCredentials, **kwargs):
        creds = Credentials(
            token=(
                credentials.access_token.get_secret_value()
                if credentials.access_token
                else None
            ),
            refresh_token=(
                credentials.refresh_token.get_secret_value()
                if credentials.refresh_token
                else None
            ),
            token_uri=""https://oauth2.googleapis.com/token"",
            client_id=kwargs.get(""client_id""),
            client_secret=kwargs.get(""client_secret""),
            scopes=credentials.scopes,
        )
        return build(""gmail"", ""v1"", credentials=creds)

    def _read_emails(
        self, service, query: str | None, max_results: int | None
    ) -> list[Email]:
        results = (
            service.users()
            .messages()
            .list(userId=""me"", q=query or """", maxResults=max_results or 10)
            .execute()
        )
        messages = results.get(""messages"", [])

        email_data = []
        for message in messages:
            msg = (
                service.users()
                .messages()
                .get(userId=""me"", id=message[""id""], format=""full"")
                .execute()
            )

            headers = {
                header[""name""].lower(): header[""value""]
                for header in msg[""payload""][""headers""]
            }

            attachments = self._get_attachments(service, msg)

            email = Email(
                id=msg[""id""],
                subject=headers.get(""subject"", ""No Subject""),
                snippet=msg[""snippet""],
                from_=parseaddr(headers.get(""from"", """"))[1],
                to=parseaddr(headers.get(""to"", """"))[1],
                date=headers.get(""date"", """"),
                body=self._get_email_body(msg),
                sizeEstimate=msg[""sizeEstimate""],
                attachments=attachments,
            )
            email_data.append(email)

        return email_data

    def _get_email_body(self, msg):
        if ""parts"" in msg[""payload""]:
            for part in msg[""payload""][""parts""]:
                if part[""mimeType""] == ""text/plain"":
                    return base64.urlsafe_b64decode(part[""body""][""data""]).decode(
                        ""utf-8""
                    )
        elif msg[""payload""][""mimeType""] == ""text/plain"":
            return base64.urlsafe_b64decode(msg[""payload""][""body""][""data""]).decode(
                ""utf-8""
            )

        return ""This email does not contain a text body.""

    def _get_attachments(self, service, message):
        attachments = []
        if ""parts"" in message[""payload""]:
            for part in message[""payload""][""parts""]:
                if part[""filename""]:
                    attachment = Attachment(
                        filename=part[""filename""],
                        content_type=part[""mimeType""],
                        size=int(part[""body""].get(""size"", 0)),
                        attachment_id=part[""body""][""attachmentId""],
                    )
                    attachments.append(attachment)
        return attachments

    # Add a new method to download attachment content
    def download_attachment(self, service, message_id: str, attachment_id: str):
        attachment = (
            service.users()
            .messages()
            .attachments()
            .get(userId=""me"", messageId=message_id, id=attachment_id)
            .execute()
        )
        file_data = base64.urlsafe_b64decode(attachment[""data""].encode(""UTF-8""))
        return file_data


class GmailSendBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.send""]
        )
        to: str = SchemaField(
            description=""Recipient email address"",
        )
        subject: str = SchemaField(
            description=""Email subject"",
        )
        body: str = SchemaField(
            description=""Email body"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Send confirmation"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""6c27abc2-e51d-499e-a85f-5a0041ba94f0"",
            description=""This block sends an email using Gmail."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailSendBlock.Input,
            output_schema=GmailSendBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""to"": ""recipient@example.com"",
                ""subject"": ""Test Email"",
                ""body"": ""This is a test email sent from GmailSendBlock."",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""result"", {""id"": ""1"", ""status"": ""sent""}),
            ],
            test_mock={
                ""_send_email"": lambda *args, **kwargs: {""id"": ""1"", ""status"": ""sent""},
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        send_result = self._send_email(
            service, input_data.to, input_data.subject, input_data.body
        )
        yield ""result"", send_result

    def _send_email(self, service, to: str, subject: str, body: str) -> dict:
        if not to or not subject or not body:
            raise ValueError(""To, subject, and body are required for sending an email"")
        message = self._create_message(to, subject, body)
        sent_message = (
            service.users().messages().send(userId=""me"", body=message).execute()
        )
        return {""id"": sent_message[""id""], ""status"": ""sent""}

    def _create_message(self, to: str, subject: str, body: str) -> dict:
        import base64
        from email.mime.text import MIMEText

        message = MIMEText(body)
        message[""to""] = to
        message[""subject""] = subject
        raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode(""utf-8"")
        return {""raw"": raw_message}


class GmailListLabelsBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.labels""]
        )

    class Output(BlockSchema):
        result: list[dict] = SchemaField(
            description=""List of labels"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""3e1c2c1c-c689-4520-b956-1f3bf4e02bb7"",
            description=""This block lists all labels in Gmail."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailListLabelsBlock.Input,
            output_schema=GmailListLabelsBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    [
                        {""id"": ""Label_1"", ""name"": ""Important""},
                        {""id"": ""Label_2"", ""name"": ""Work""},
                    ],
                ),
            ],
            test_mock={
                ""_list_labels"": lambda *args, **kwargs: [
                    {""id"": ""Label_1"", ""name"": ""Important""},
                    {""id"": ""Label_2"", ""name"": ""Work""},
                ],
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        labels = self._list_labels(service)
        yield ""result"", labels

    def _list_labels(self, service) -> list[dict]:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        return [{""id"": label[""id""], ""name"": label[""name""]} for label in labels]


class GmailAddLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.modify""]
        )
        message_id: str = SchemaField(
            description=""Message ID to add label to"",
        )
        label_name: str = SchemaField(
            description=""Label name to add"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Label addition result"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""f884b2fb-04f4-4265-9658-14f433926ac9"",
            description=""This block adds a label to a Gmail message."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailAddLabelBlock.Input,
            output_schema=GmailAddLabelBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""message_id"": ""12345"",
                ""label_name"": ""Important"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""status"": ""Label added successfully"", ""label_id"": ""Label_1""},
                ),
            ],
            test_mock={
                ""_add_label"": lambda *args, **kwargs: {
                    ""status"": ""Label added successfully"",
                    ""label_id"": ""Label_1"",
                },
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        result = self._add_label(service, input_data.message_id, input_data.label_name)
        yield ""result"", result

    def _add_label(self, service, message_id: str, label_name: str) -> dict:
        label_id = self._get_or_create_label(service, label_name)
        service.users().messages().modify(
            userId=""me"", id=message_id, body={""addLabelIds"": [label_id]}
        ).execute()
        return {""status"": ""Label added successfully"", ""label_id"": label_id}

    def _get_or_create_label(self, service, label_name: str) -> str:
        label_id = self._get_label_id(service, label_name)
        if not label_id:
            label = (
                service.users()
                .labels()
                .create(userId=""me"", body={""name"": label_name})
                .execute()
            )
            label_id = label[""id""]
        return label_id

    def _get_label_id(self, service, label_name: str) -> str | None:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        for label in labels:
            if label[""name""] == label_name:
                return label[""id""]
        return None


class GmailRemoveLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.modify""]
        )
        message_id: str = SchemaField(
            description=""Message ID to remove label from"",
        )
        label_name: str = SchemaField(
            description=""Label name to remove"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Label removal result"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""0afc0526-aba1-4b2b-888e-a22b7c3f359d"",
            description=""This block removes a label from a Gmail message."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailRemoveLabelBlock.Input,
            output_schema=GmailRemoveLabelBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""message_id"": ""12345"",
                ""label_name"": ""Important"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""status"": ""Label removed successfully"", ""label_id"": ""Label_1""},
                ),
            ],
            test_mock={
                ""_remove_label"": lambda *args, **kwargs: {
                    ""status"": ""Label removed successfully"",
                    ""label_id"": ""Label_1"",
                },
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        result = self._remove_label(
            service, input_data.message_id, input_data.label_name
        )
        yield ""result"", result

    def _remove_label(self, service, message_id: str, label_name: str) -> dict:
        label_id = self._get_label_id(service, label_name)
        if label_id:
            service.users().messages().modify(
                userId=""me"", id=message_id, body={""removeLabelIds"": [label_id]}
            ).execute()
            return {""status"": ""Label removed successfully"", ""label_id"": label_id}
        else:
            return {""status"": ""Label not found"", ""label_name"": label_name}

    def _get_label_id(self, service, label_name: str) -> str | None:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        for label in labels:
            if label[""name""] == label_name:
                return label[""id""]
        return None
","import base64
from email.utils import parseaddr
from typing import List
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from pydantic import BaseModel
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from ._auth import (
    GOOGLE_OAUTH_IS_CONFIGURED,
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    GoogleCredentials,
    GoogleCredentialsField,
    GoogleCredentialsInput,
)

class Attachment(BaseModel):
    filename: str
    content_type: str
    size: int
    attachment_id: str

class Email(BaseModel):
    id: str
    subject: str
    snippet: str
    from_: str
    to: str
    date: str
    body: str = """"  # Default to an empty string
    sizeEstimate: int
    attachments: List[Attachment]

class GmailReadBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.readonly""]
        )
        query: str = SchemaField(
            description=""Search query for reading emails"",
            default=""is:unread"",
        )
        max_results: int = SchemaField(
            description=""Maximum number of emails to retrieve"",
            default=10,
        )

    class Output(BlockSchema):
        email: Email = SchemaField(
            description=""Email data"",
        )
        emails: list[Email] = SchemaField(
            description=""List of email data"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""25310c70-b89b-43ba-b25c-4dfa7e2a481c"",
            description=""This block reads emails from Gmail."",
            categories={BlockCategory.COMMUNICATION},
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            input_schema=GmailReadBlock.Input,
            output_schema=GmailReadBlock.Output,
            test_input={
                ""query"": ""is:unread"",
                ""max_results"": 5,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""email"",
                    {
                        ""id"": ""1"",
                        ""subject"": ""Test Email"",
                        ""snippet"": ""This is a test email"",
                        ""from_"": ""test@example.com"",
                        ""to"": ""recipient@example.com"",
                        ""date"": ""2024-01-01"",
                        ""body"": ""This is a test email"",
                        ""sizeEstimate"": 100,
                        ""attachments"": [],
                    },
                ),
                (
                    ""emails"",
                    [
                        {
                            ""id"": ""1"",
                            ""subject"": ""Test Email"",
                            ""snippet"": ""This is a test email"",
                            ""from_"": ""test@example.com"",
                            ""to"": ""recipient@example.com"",
                            ""date"": ""2024-01-01"",
                            ""body"": ""This is a test email"",
                            ""sizeEstimate"": 100,
                            ""attachments"": [],
                        }
                    ],
                ),
            ],
            test_mock={
                ""_read_emails"": lambda *args, **kwargs: [
                    {
                        ""id"": ""1"",
                        ""subject"": ""Test Email"",
                        ""snippet"": ""This is a test email"",
                        ""from_"": ""test@example.com"",
                        ""to"": ""recipient@example.com"",
                        ""date"": ""2024-01-01"",
                        ""body"": ""This is a test email"",
                        ""sizeEstimate"": 100,
                        ""attachments"": [],
                    }
                ],
                ""_send_email"": lambda *args, **kwargs: {""id"": ""1"", ""status"": ""sent""},
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = self._build_service(credentials, **kwargs)
        messages = self._read_emails(service, input_data.query, input_data.max_results)
        for email in messages:
            yield ""email"", email
        yield ""emails"", messages

    @staticmethod
    def _build_service(credentials: GoogleCredentials, **kwargs):
        creds = Credentials(
            token=(
                credentials.access_token.get_secret_value()
                if credentials.access_token
                else None
            ),
            refresh_token=(
                credentials.refresh_token.get_secret_value()
                if credentials.refresh_token
                else None
            ),
            token_uri=""https://oauth2.googleapis.com/token"",
            client_id=kwargs.get(""client_id""),
            client_secret=kwargs.get(""client_secret""),
            scopes=credentials.scopes,
        )
        return build(""gmail"", ""v1"", credentials=creds)

    def _read_emails(
        self, service, query: str | None, max_results: int | None
    ) -> list[Email]:
        results = (
            service.users()
            .messages()
            .list(userId=""me"", q=query or """", maxResults=max_results or 10)
            .execute()
        )
        messages = results.get(""messages"", [])

        email_data = []
        for message in messages:
            msg = (
                service.users()
                .messages()
                .get(userId=""me"", id=message[""id""], format=""full"")
                .execute()
            )

            headers = {
                header[""name""].lower(): header[""value""]
                for header in msg[""payload""][""headers""]
            }

            attachments = self._get_attachments(service, msg)

            email = Email(
                id=msg[""id""],
                subject=headers.get(""subject"", ""No Subject""),
                snippet=msg[""snippet""],
                from_=parseaddr(headers.get(""from"", """"))[1],
                to=parseaddr(headers.get(""to"", """"))[1],
                date=headers.get(""date"", """"),
                body=self._get_email_body(msg),
                sizeEstimate=msg[""sizeEstimate""],
                attachments=attachments,
            )
            email_data.append(email)

        return email_data

    def _get_email_body(self, msg):
        if ""parts"" in msg[""payload""]:
            for part in msg[""payload""][""parts""]:
                if part[""mimeType""] == ""text/plain"":
                    return base64.urlsafe_b64decode(part[""body""][""data""]).decode(
                        ""utf-8""
                    )
        elif msg[""payload""][""mimeType""] == ""text/plain"":
            return base64.urlsafe_b64decode(msg[""payload""][""body""][""data""]).decode(
                ""utf-8""
            )

        return ""This email does not contain a text body.""

    def _get_attachments(self, service, message):
        attachments = []
        if ""parts"" in message[""payload""]:
            for part in message[""payload""][""parts""]:
                if part[""filename""]:
                    attachment = Attachment(
                        filename=part[""filename""],
                        content_type=part[""mimeType""],
                        size=int(part[""body""].get(""size"", 0)),
                        attachment_id=part[""body""][""attachmentId""],
                    )
                    attachments.append(attachment)
        return attachments

    def download_attachment(self, service, message_id: str, attachment_id: str):
        attachment = (
            service.users()
            .messages()
            .attachments()
            .get(userId=""me"", messageId=message_id, id=attachment_id)
            .execute()
        )
        file_data = base64.urlsafe_b64decode(attachment[""data""].encode(""UTF-8""))
        return file_data

class GmailSendBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.send""]
        )
        to: str = SchemaField(
            description=""Recipient email address"",
        )
        subject: str = SchemaField(
            description=""Email subject"",
        )
        body: str = SchemaField(
            description=""Email body"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Send confirmation"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""6c27abc2-e51d-499e-a85f-5a0041ba94f0"",
            description=""This block sends an email using Gmail."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailSendBlock.Input,
            output_schema=GmailSendBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""to"": ""recipient@example.com"",
                ""subject"": ""Test Email"",
                ""body"": ""This is a test email sent from GmailSendBlock."",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""result"", {""id"": ""1"", ""status"": ""sent""}),
            ],
            test_mock={
                ""_send_email"": lambda *args, **kwargs: {""id"": ""1"", ""status"": ""sent""},
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        send_result = self._send_email(
            service, input_data.to, input_data.subject, input_data.body
        )
        yield ""result"", send_result

    def _send_email(self, service, to: str, subject: str, body: str) -> dict:
        if not to or not subject or not body:
            raise ValueError(""To, subject, and body are required for sending an email"")
        message = self._create_message(to, subject, body)
        sent_message = (
            service.users().messages().send(userId=""me"", body=message).execute()
        )
        return {""id"": sent_message[""id""], ""status"": ""sent""}

    def _create_message(self, to: str, subject: str, body: str) -> dict:
        import base64
        from email.mime.text import MIMEText

        message = MIMEText(body)
        message[""to""] = to
        message[""subject""] = subject
        raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode(""utf-8"")
        return {""raw"": raw_message}

class GmailListLabelsBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.labels""]
        )

    class Output(BlockSchema):
        result: list[dict] = SchemaField(
            description=""List of labels"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""3e1c2c1c-c689-4520-b956-1f3bf4e02bb7"",
            description=""This block lists all labels in Gmail."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailListLabelsBlock.Input,
            output_schema=GmailListLabelsBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    [
                        {""id"": ""Label_1"", ""name"": ""Important""},
                        {""id"": ""Label_2"", ""name"": ""Work""},
                    ],
                ),
            ],
            test_mock={
                ""_list_labels"": lambda *args, **kwargs: [
                    {""id"": ""Label_1"", ""name"": ""Important""},
                    {""id"": ""Label_2"", ""name"": ""Work""},
                ],
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        labels = self._list_labels(service)
        yield ""result"", labels

    def _list_labels(self, service) -> list[dict]:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        return [{""id"": label[""id""], ""name"": label[""name""]} for label in labels]

class GmailAddLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.modify""]
        )
        message_id: str = SchemaField(
            description=""Message ID to add label to"",
        )
        label_name: str = SchemaField(
            description=""Label name to add"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Label addition result"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""f884b2fb-04f4-4265-9658-14f433926ac9"",
            description=""This block adds a label to a Gmail message."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailAddLabelBlock.Input,
            output_schema=GmailAddLabelBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""message_id"": ""12345"",
                ""label_name"": ""Important"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""status"": ""Label added successfully"", ""label_id"": ""Label_1""},
                ),
            ],
            test_mock={
                ""_add_label"": lambda *args, **kwargs: {
                    ""status"": ""Label added successfully"",
                    ""label_id"": ""Label_1"",
                },
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        result = self._add_label(service, input_data.message_id, input_data.label_name)
        yield ""result"", result

    def _add_label(self, service, message_id: str, label_name: str) -> dict:
        label_id = self._get_or_create_label(service, label_name)
        service.users().messages().modify(
            userId=""me"", id=message_id, body={""addLabelIds"": [label_id]}
        ).execute()
        return {""status"": ""Label added successfully"", ""label_id"": label_id}

    def _get_or_create_label(self, service, label_name: str) -> str:
        label_id = self._get_label_id(service, label_name)
        if not label_id:
            label = (
                service.users()
                .labels()
                .create(userId=""me"", body={""name"": label_name})
                .execute()
            )
            label_id = label[""id""]
        return label_id

    def _get_label_id(self, service, label_name: str) -> str | None:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        for label in labels:
            if label[""name""] == label_name:
                return label[""id""]
        return None

class GmailRemoveLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.modify""]
        )
        message_id: str = SchemaField(
            description=""Message ID to remove label from"",
        )
        label_name: str = SchemaField(
            description=""Label name to remove"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Label removal result"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""0afc0526-aba1-4b2b-888e-a22b7c3f359d"",
            description=""This block removes a label from a Gmail message."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailRemoveLabelBlock.Input,
            output_schema=GmailRemoveLabelBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""message_id"": ""12345"",
                ""label_name"": ""Important"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""status"": ""Label removed successfully"", ""label_id"": ""Label_1""},
                ),
            ],
            test_mock={
                ""_remove_label"": lambda *args, **kwargs: {
                    ""status"": ""Label removed successfully"",
                    ""label_id"": ""Label_1"",
                },
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        result = self._remove_label(
            service, input_data.message_id, input_data.label_name
        )
        yield ""result"", result

    def _remove_label(self, service, message_id: str, label_name: str) -> dict:
        label_id = self._get_label_id(service, label_name)
        if label_id:
            service.users().messages().modify(
                userId=""me"", id=message_id, body={""removeLabelIds"": [label_id]}
            ).execute()
            return {""status"": ""Label removed successfully"", ""label_id"": label_id}
        else:
            return {""status"": ""Label not found"", ""label_name"": label_name}

    def _get_label_id(self, service, label_name: str) -> str | None:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        for label in labels:
            if label[""name""] == label_name:
                return label[""id""]
        return None","{'final_score': 99.56, 'length_similarity': 99.71, 'token_similarity': 98.83, 'structure_similarity': 99.79, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
167,"import json
from typing import Optional

from cryptography.fernet import Fernet

from backend.util.settings import Settings

ENCRYPTION_KEY = Settings().secrets.encryption_key


class JSONCryptor:
    def __init__(self, key: Optional[str] = None):
        # Use provided key or get from environment
        self.key = key or ENCRYPTION_KEY
        if not self.key:
            raise ValueError(
                ""Encryption key must be provided or set in ENCRYPTION_KEY environment variable""
            )
        self.fernet = Fernet(
            self.key.encode() if isinstance(self.key, str) else self.key
        )

    def encrypt(self, data: dict) -> str:
        """"""Encrypt dictionary data to string""""""
        json_str = json.dumps(data)
        encrypted = self.fernet.encrypt(json_str.encode())
        return encrypted.decode()

    def decrypt(self, encrypted_str: str) -> dict:
        """"""Decrypt string to dictionary""""""
        if not encrypted_str:
            return {}
        decrypted = self.fernet.decrypt(encrypted_str.encode())
        return json.loads(decrypted.decode())
","import json
from typing import Optional
from cryptography.fernet import Fernet
from backend.util.settings import Settings

ENCRYPTION_KEY = Settings().secrets.encryption_key

class JSONCryptor:
    def __init__(self, key: Optional[str] = None):
        # Use provided key or get from environment
        self.key = key or ENCRYPTION_KEY
        if not self.key:
            raise ValueError(
                ""Encryption key must be provided or set in ENCRYPTION_KEY environment variable""
            )
        self.fernet = Fernet(
            self.key.encode() if isinstance(self.key, str) else self.key
        )

    def encrypt(self, data: dict) -> str:
        """"""Encrypt dictionary data to string""""""
        json_str = json.dumps(data)
        encrypted = self.fernet.encrypt(json_str.encode())
        return encrypted.decode()

    def decrypt(self, encrypted_str: str) -> dict:
        """"""Decrypt string to dictionary""""""
        if not encrypted_str:
            return {}
        decrypted = self.fernet.decrypt(encrypted_str.encode())
        return json.loads(decrypted.decode())","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
168,"import time
from typing import Tuple

from redis import Redis

from .config import RATE_LIMIT_SETTINGS


class RateLimiter:
    def __init__(
        self,
        redis_host: str = RATE_LIMIT_SETTINGS.redis_host,
        redis_port: str = RATE_LIMIT_SETTINGS.redis_port,
        redis_password: str = RATE_LIMIT_SETTINGS.redis_password,
        requests_per_minute: int = RATE_LIMIT_SETTINGS.requests_per_minute,
    ):
        self.redis = Redis(
            host=redis_host,
            port=int(redis_port),
            password=redis_password,
            decode_responses=True,
        )
        self.window = 60
        self.max_requests = requests_per_minute

    async def check_rate_limit(self, api_key_id: str) -> Tuple[bool, int, int]:
        """"""
        Check if request is within rate limits.

        Args:
            api_key_id: The API key identifier to check

        Returns:
            Tuple of (is_allowed, remaining_requests, reset_time)
        """"""
        now = time.time()
        window_start = now - self.window
        key = f""ratelimit:{api_key_id}:1min""

        pipe = self.redis.pipeline()
        pipe.zremrangebyscore(key, 0, window_start)
        pipe.zadd(key, {str(now): now})
        pipe.zcount(key, window_start, now)
        pipe.expire(key, self.window)

        _, _, request_count, _ = pipe.execute()

        remaining = max(0, self.max_requests - request_count)
        reset_time = int(now + self.window)

        return request_count <= self.max_requests, remaining, reset_time
","import time
from typing import Tuple
from redis import Redis
from .config import RATE_LIMIT_SETTINGS

class RateLimiter:
    def __init__(
        self,
        redis_host: str = RATE_LIMIT_SETTINGS.redis_host,
        redis_port: str = RATE_LIMIT_SETTINGS.redis_port,
        redis_password: str = RATE_LIMIT_SETTINGS.redis_password,
        requests_per_minute: int = RATE_LIMIT_SETTINGS.requests_per_minute,
    ):
        self.redis = Redis(
            host=redis_host,
            port=int(redis_port),
            password=redis_password,
            decode_responses=True,
        )
        self.window = 60
        self.max_requests = requests_per_minute

    async def check_rate_limit(self, api_key_id: str) -> Tuple[bool, int, int]:
        """"""
        Check if request is within rate limits.

        Args:
            api_key_id: The API key identifier to check

        Returns:
            Tuple of (is_allowed, remaining_requests, reset_time)
        """"""
        now = time.time()
        window_start = now - self.window
        key = f""ratelimit:{api_key_id}:1min""

        pipe = self.redis.pipeline()
        pipe.zremrangebyscore(key, 0, window_start)
        pipe.zadd(key, {str(now): now})
        pipe.zcount(key, window_start, now)
        pipe.expire(key, self.window)

        _, _, request_count, _ = pipe.execute()
        
        remaining = max(0, self.max_requests - request_count)
        reset_time = int(now + self.window)

        return request_count <= self.max_requests, remaining, reset_time","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
169,"import re

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class CodeExtractionBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""Text containing code blocks to extract (e.g., AI response)"",
            placeholder=""Enter text containing code blocks"",
        )

    class Output(BlockSchema):
        html: str = SchemaField(description=""Extracted HTML code"")
        css: str = SchemaField(description=""Extracted CSS code"")
        javascript: str = SchemaField(description=""Extracted JavaScript code"")
        python: str = SchemaField(description=""Extracted Python code"")
        sql: str = SchemaField(description=""Extracted SQL code"")
        java: str = SchemaField(description=""Extracted Java code"")
        cpp: str = SchemaField(description=""Extracted C++ code"")
        csharp: str = SchemaField(description=""Extracted C# code"")
        json_code: str = SchemaField(description=""Extracted JSON code"")
        bash: str = SchemaField(description=""Extracted Bash code"")
        php: str = SchemaField(description=""Extracted PHP code"")
        ruby: str = SchemaField(description=""Extracted Ruby code"")
        yaml: str = SchemaField(description=""Extracted YAML code"")
        markdown: str = SchemaField(description=""Extracted Markdown code"")
        typescript: str = SchemaField(description=""Extracted TypeScript code"")
        xml: str = SchemaField(description=""Extracted XML code"")
        remaining_text: str = SchemaField(
            description=""Remaining text after code extraction""
        )

    def __init__(self):
        super().__init__(
            id=""d3a7d896-3b78-4f44-8b4b-48fbf4f0bcd8"",
            description=""Extracts code blocks from text and identifies their programming languages"",
            categories={BlockCategory.TEXT},
            input_schema=CodeExtractionBlock.Input,
            output_schema=CodeExtractionBlock.Output,
            test_input={
                ""text"": ""Here's a Python example:\n```python\nprint('Hello World')\n```\nAnd some HTML:\n```html\n<h1>Title</h1>\n```""
            },
            test_output=[
                (""html"", ""<h1>Title</h1>""),
                (""python"", ""print('Hello World')""),
                (""remaining_text"", ""Here's a Python example:\nAnd some HTML:""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        # List of supported programming languages with mapped aliases
        language_aliases = {
            ""html"": [""html"", ""htm""],
            ""css"": [""css""],
            ""javascript"": [""javascript"", ""js""],
            ""python"": [""python"", ""py""],
            ""sql"": [""sql""],
            ""java"": [""java""],
            ""cpp"": [""cpp"", ""c++""],
            ""csharp"": [""csharp"", ""c#"", ""cs""],
            ""json_code"": [""json""],
            ""bash"": [""bash"", ""shell"", ""sh""],
            ""php"": [""php""],
            ""ruby"": [""ruby"", ""rb""],
            ""yaml"": [""yaml"", ""yml""],
            ""markdown"": [""markdown"", ""md""],
            ""typescript"": [""typescript"", ""ts""],
            ""xml"": [""xml""],
        }

        # Extract code for each language
        for canonical_name, aliases in language_aliases.items():
            code = """"
            # Try each alias for the language
            for alias in aliases:
                code_for_alias = self.extract_code(input_data.text, alias)
                if code_for_alias:
                    code = code + ""\n\n"" + code_for_alias if code else code_for_alias

            if code:  # Only yield if there's actual code content
                yield canonical_name, code

        # Remove all code blocks from the text to get remaining text
        pattern = (
            r""```(?:""
            + ""|"".join(
                re.escape(alias)
                for aliases in language_aliases.values()
                for alias in aliases
            )
            + r"")\s+[\s\S]*?```""
        )

        remaining_text = re.sub(pattern, """", input_data.text).strip()
        remaining_text = re.sub(r""\n\s*\n"", ""\n"", remaining_text)

        if remaining_text:  # Only yield if there's remaining text
            yield ""remaining_text"", remaining_text

    def extract_code(self, text: str, language: str) -> str:
        # Escape special regex characters in the language string
        language = re.escape(language)
        # Extract all code blocks enclosed in ```language``` blocks
        pattern = re.compile(rf""```{language}\s+(.*?)```"", re.DOTALL | re.IGNORECASE)
        matches = pattern.finditer(text)
        # Combine all code blocks for this language with newlines between them
        code_blocks = [match.group(1).strip() for match in matches]
        return ""\n\n"".join(code_blocks) if code_blocks else """"
","import re
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField

class CodeExtractionBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""Text containing code blocks to extract (e.g., AI response)"",
            placeholder=""Enter text containing code blocks"",
        )

    class Output(BlockSchema):
        html: str = SchemaField(description=""Extracted HTML code"")
        css: str = SchemaField(description=""Extracted CSS code"")
        javascript: str = SchemaField(description=""Extracted JavaScript code"")
        python: str = SchemaField(description=""Extracted Python code"")
        sql: str = SchemaField(description=""Extracted SQL code"")
        java: str = SchemaField(description=""Extracted Java code"")
        cpp: str = SchemaField(description=""Extracted C++ code"")
        csharp: str = SchemaField(description=""Extracted C# code"")
        json_code: str = SchemaField(description=""Extracted JSON code"")
        bash: str = SchemaField(description=""Extracted Bash code"")
        php: str = SchemaField(description=""Extracted PHP code"")
        ruby: str = SchemaField(description=""Extracted Ruby code"")
        yaml: str = SchemaField(description=""Extracted YAML code"")
        markdown: str = SchemaField(description=""Extracted Markdown code"")
        typescript: str = SchemaField(description=""Extracted TypeScript code"")
        xml: str = SchemaField(description=""Extracted XML code"")
        remaining_text: str = SchemaField(
            description=""Remaining text after code extraction""
        )

    def __init__(self):
        super().__init__(
            id=""d3a7d896-3b78-4f44-8b4b-48fbf4f0bcd8"",
            description=""Extracts code blocks from text and identifies their programming languages"",
            categories={BlockCategory.TEXT},
            input_schema=CodeExtractionBlock.Input,
            output_schema=CodeExtractionBlock.Output,
            test_input={
                ""text"": ""Here's a Python example:\n```python\nprint('Hello World')\n```\nAnd some HTML:\n```html\n<h1>Title</h1>\n```""
            },
            test_output=[
                (""html"", ""<h1>Title</h1>""),
                (""python"", ""print('Hello World')""),
                (""remaining_text"", ""Here's a Python example:\nAnd some HTML:""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        # List of supported programming languages with mapped aliases
        language_aliases = {
            ""html"": [""html"", ""htm""],
            ""css"": [""css""],
            ""javascript"": [""javascript"", ""js""],
            ""python"": [""python"", ""py""],
            ""sql"": [""sql""],
            ""java"": [""java""],
            ""cpp"": [""cpp"", ""c++""],
            ""csharp"": [""csharp"", ""c#"", ""cs""],
            ""json_code"": [""json""],
            ""bash"": [""bash"", ""shell"", ""sh""],
            ""php"": [""php""],
            ""ruby"": [""ruby"", ""rb""],
            ""yaml"": [""yaml"", ""yml""],
            ""markdown"": [""markdown"", ""md""],
            ""typescript"": [""typescript"", ""ts""],
            ""xml"": [""xml""],
        }
        
        # Extract code for each language
        for canonical_name, aliases in language_aliases.items():
            code = """"
            # Try each alias for the language
            for alias in aliases:
                code_for_alias = self.extract_code(input_data.text, alias)
                if code_for_alias:
                    code = code + ""\n\n"" + code_for_alias if code else code_for_alias
            if code:  # Only yield if there's actual code content
                yield canonical_name, code

        # Remove all code blocks from the text to get remaining text
        pattern = (
            r""```(?:""
            + ""|"".join(
                re.escape(alias)
                for aliases in language_aliases.values()
                for alias in aliases
            )
            + r"")\s+[\s\S]*?```""
        )
        
        remaining_text = re.sub(pattern, """", input_data.text).strip()
        remaining_text = re.sub(r""\n\s*\n"", ""\n"", remaining_text)

        if remaining_text:  # Only yield if there's remaining text
            yield ""remaining_text"", remaining_text

    def extract_code(self, text: str, language: str) -> str:
        # Escape special regex characters in the language string
        language = re.escape(language)
        # Extract all code blocks enclosed in ```language``` blocks
        pattern = re.compile(rf""```{language}\s+(.*?)```"", re.DOTALL | re.IGNORECASE)
        matches = pattern.finditer(text)
        # Combine all code blocks for this language with newlines between them
        code_blocks = [match.group(1).strip() for match in matches]
        return ""\n\n"".join(code_blocks) if code_blocks else """"","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
170,"# -*- coding: utf-8 -*-

from mrjob.job import MRJob


class SpendingByCategory(MRJob):

    def __init__(self, categorizer):
        self.categorizer = categorizer
        ...

    def current_year_month(self):
        """"""Return the current year and month.""""""
        ...

    def extract_year_month(self, timestamp):
        """"""Return the year and month portions of the timestamp.""""""
        ...

    def handle_budget_notifications(self, key, total):
        """"""Call notification API if nearing or exceeded budget.""""""
        ...

    def mapper(self, _, line):
        """"""Parse each log line, extract and transform relevant lines.

        Emit key value pairs of the form:

        (2016-01, shopping), 25
        (2016-01, shopping), 100
        (2016-01, gas), 50
        """"""
        timestamp, category, amount = line.split('\t')
        period = self. extract_year_month(timestamp)
        if period == self.current_year_month():
            yield (period, category), amount

    def reducer(self, key, values):
        """"""Sum values for each key.

        (2016-01, shopping), 125
        (2016-01, gas), 50
        """"""
        total = sum(values)
        self.handle_budget_notifications(key, total)
        yield key, sum(values)

    def steps(self):
        """"""Run the map and reduce steps.""""""
        return [
            self.mr(mapper=self.mapper,
                    reducer=self.reducer)
        ]


if __name__ == '__main__':
    SpendingByCategory.run()
","# -*- coding: utf-8 -*-
from mrjob.job import MRJob

class SpendingByCategory(MRJob):

    def __init__(self, categorizer):
        self.categorizer = categorizer
        ...

    def current_year_month(self):
        """"""Return the current year and month.""""""
        ...

    def extract_year_month(self, timestamp):
        """"""Return the year and month portions of the timestamp.""""""
        ...

    def handle_budget_notifications(self, key, total):
        """"""Call notification API if nearing or exceeded budget.""""""
        ...

    def mapper(self, _, line):
        """"""Parse each log line, extract and transform relevant lines.

        Emit key value pairs of the form:

        (2016-01, shopping), 25
        (2016-01, shopping), 100
        (2016-01, gas), 50
        """"""
        timestamp, category, amount = line.split('\t')
        period = self.extract_year_month(timestamp)
        if period == self.current_year_month():
            yield (period, category), amount

    def reducer(self, key, values):
        """"""Sum values for each key.

        (2016-01, shopping), 125
        (2016-01, gas), 50
        """"""
        total = sum(values)
        self.handle_budget_notifications(key, total)
        yield key, sum(values)

    def steps(self):
        """"""Run the map and reduce steps.""""""
        return [
            self.mr(mapper=self.mapper,
                    reducer=self.reducer)
        ]

if __name__ == '__main__':
    SpendingByCategory.run()","{'final_score': 99.41, 'length_similarity': 99.93, 'token_similarity': 98.08, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
171,"import asyncio
import builtins
import logging
import os
import threading
import time
import typing
from abc import ABC, abstractmethod
from enum import Enum
from types import NoneType, UnionType
from typing import (
    Annotated,
    Any,
    Awaitable,
    Callable,
    Coroutine,
    Dict,
    FrozenSet,
    Iterator,
    List,
    Set,
    Tuple,
    Type,
    TypeVar,
    Union,
    cast,
    get_args,
    get_origin,
)

import Pyro5.api
from pydantic import BaseModel
from Pyro5 import api as pyro
from Pyro5 import config as pyro_config

from backend.data import db, redis
from backend.util.process import AppProcess
from backend.util.retry import conn_retry
from backend.util.settings import Config, Secrets

logger = logging.getLogger(__name__)
T = TypeVar(""T"")
C = TypeVar(""C"", bound=Callable)

config = Config()
pyro_host = config.pyro_host
pyro_config.MAX_RETRIES = config.pyro_client_comm_retry  # type: ignore
pyro_config.COMMTIMEOUT = config.pyro_client_comm_timeout  # type: ignore


def expose(func: C) -> C:
    """"""
    Decorator to mark a method or class to be exposed for remote calls.

    ## ⚠️ Gotcha
    Aside from ""simple"" types, only Pydantic models are passed unscathed *if annotated*.
    Any other passed or returned class objects are converted to dictionaries by Pyro.
    """"""

    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            msg = f""Error in {func.__name__}: {e.__str__()}""
            logger.exception(msg)
            raise

    register_pydantic_serializers(func)

    return pyro.expose(wrapper)  # type: ignore


def register_pydantic_serializers(func: Callable):
    """"""Register custom serializers and deserializers for annotated Pydantic models""""""
    for name, annotation in func.__annotations__.items():
        try:
            pydantic_types = _pydantic_models_from_type_annotation(annotation)
        except Exception as e:
            raise TypeError(f""Error while exposing {func.__name__}: {e.__str__()}"")

        for model in pydantic_types:
            logger.debug(
                f""Registering Pyro (de)serializers for {func.__name__} annotation ""
                f""'{name}': {model.__qualname__}""
            )
            pyro.register_class_to_dict(model, _make_custom_serializer(model))
            pyro.register_dict_to_class(
                model.__qualname__, _make_custom_deserializer(model)
            )


def _make_custom_serializer(model: Type[BaseModel]):
    def custom_class_to_dict(obj):
        data = {
            ""__class__"": obj.__class__.__qualname__,
            **obj.model_dump(),
        }
        logger.debug(f""Serializing {obj.__class__.__qualname__} with data: {data}"")
        return data

    return custom_class_to_dict


def _make_custom_deserializer(model: Type[BaseModel]):
    def custom_dict_to_class(qualname, data: dict):
        logger.debug(f""Deserializing {model.__qualname__} from data: {data}"")
        return model(**data)

    return custom_dict_to_class


class AppService(AppProcess, ABC):
    shared_event_loop: asyncio.AbstractEventLoop
    use_db: bool = False
    use_redis: bool = False
    use_supabase: bool = False

    def __init__(self):
        self.uri = None

    @classmethod
    @abstractmethod
    def get_port(cls) -> int:
        pass

    @classmethod
    def get_host(cls) -> str:
        return os.environ.get(f""{cls.service_name.upper()}_HOST"", config.pyro_host)

    def run_service(self) -> None:
        while True:
            time.sleep(10)

    def __run_async(self, coro: Coroutine[Any, Any, T]):
        return asyncio.run_coroutine_threadsafe(coro, self.shared_event_loop)

    def run_and_wait(self, coro: Coroutine[Any, Any, T]) -> T:
        future = self.__run_async(coro)
        return future.result()

    def run(self):
        self.shared_event_loop = asyncio.get_event_loop()
        if self.use_db:
            self.shared_event_loop.run_until_complete(db.connect())
        if self.use_redis:
            redis.connect()
        if self.use_supabase:
            from supabase import create_client

            secrets = Secrets()
            self.supabase = create_client(
                secrets.supabase_url, secrets.supabase_service_role_key
            )

        # Initialize the async loop.
        async_thread = threading.Thread(target=self.__start_async_loop)
        async_thread.daemon = True
        async_thread.start()

        # Initialize pyro service
        daemon_thread = threading.Thread(target=self.__start_pyro)
        daemon_thread.daemon = True
        daemon_thread.start()

        # Run the main service (if it's not implemented, just sleep).
        self.run_service()

    def cleanup(self):
        if self.use_db:
            logger.info(f""[{self.__class__.__name__}] ⏳ Disconnecting DB..."")
            self.run_and_wait(db.disconnect())
        if self.use_redis:
            logger.info(f""[{self.__class__.__name__}] ⏳ Disconnecting Redis..."")
            redis.disconnect()

    @conn_retry(""Pyro"", ""Starting Pyro Service"")
    def __start_pyro(self):
        maximum_connection_thread_count = max(
            Pyro5.config.THREADPOOL_SIZE,
            config.num_node_workers * config.num_graph_workers,
        )

        Pyro5.config.THREADPOOL_SIZE = maximum_connection_thread_count  # type: ignore
        daemon = Pyro5.api.Daemon(host=config.pyro_host, port=self.get_port())
        self.uri = daemon.register(self, objectId=self.service_name)
        logger.info(f""[{self.service_name}] Connected to Pyro; URI = {self.uri}"")
        daemon.requestLoop()

    def __start_async_loop(self):
        self.shared_event_loop.run_forever()


# --------- UTILITIES --------- #


AS = TypeVar(""AS"", bound=AppService)


class PyroClient:
    proxy: Pyro5.api.Proxy


def close_service_client(client: AppService) -> None:
    if isinstance(client, PyroClient):
        client.proxy._pyroRelease()
    else:
        raise RuntimeError(f""Client {client.__class__} is not a Pyro client."")


def get_service_client(service_type: Type[AS]) -> AS:
    service_name = service_type.service_name

    class DynamicClient(PyroClient):
        @conn_retry(""Pyro"", f""Connecting to [{service_name}]"")
        def __init__(self):
            host = os.environ.get(f""{service_name.upper()}_HOST"", pyro_host)
            uri = f""PYRO:{service_type.service_name}@{host}:{service_type.get_port()}""
            logger.debug(f""Connecting to service [{service_name}]. URI = {uri}"")
            self.proxy = Pyro5.api.Proxy(uri)
            # Attempt to bind to ensure the connection is established
            self.proxy._pyroBind()
            logger.debug(f""Successfully connected to service [{service_name}]"")

        def __getattr__(self, name: str) -> Callable[..., Any]:
            res = getattr(self.proxy, name)
            return res

    return cast(AS, DynamicClient())


builtin_types = [*vars(builtins).values(), NoneType, Enum]


def _pydantic_models_from_type_annotation(annotation) -> Iterator[type[BaseModel]]:
    # Peel Annotated parameters
    if (origin := get_origin(annotation)) and origin is Annotated:
        annotation = get_args(annotation)[0]

    origin = get_origin(annotation)
    args = get_args(annotation)

    if origin in (
        Union,
        UnionType,
        list,
        List,
        tuple,
        Tuple,
        set,
        Set,
        frozenset,
        FrozenSet,
    ):
        for arg in args:
            yield from _pydantic_models_from_type_annotation(arg)
    elif origin in (dict, Dict):
        key_type, value_type = args
        yield from _pydantic_models_from_type_annotation(key_type)
        yield from _pydantic_models_from_type_annotation(value_type)
    elif origin in (Awaitable, Coroutine):
        # For coroutines and awaitables, check the return type
        return_type = args[-1]
        yield from _pydantic_models_from_type_annotation(return_type)
    else:
        annotype = annotation if origin is None else origin

        # Exclude generic types and aliases
        if (
            annotype is not None
            and not hasattr(typing, getattr(annotype, ""__name__"", """"))
            and isinstance(annotype, type)
        ):
            if issubclass(annotype, BaseModel):
                yield annotype
            elif annotype not in builtin_types and not issubclass(annotype, Enum):
                raise TypeError(f""Unsupported type encountered: {annotype}"")
","import asyncio
import builtins
import logging
import os
import threading
import time
import typing
from abc import ABC, abstractmethod
from enum import Enum
from types import NoneType, UnionType
from typing import (
    Annotated,
    Any,
    Awaitable,
    Callable,
    Coroutine,
    Dict,
    FrozenSet,
    Iterator,
    List,
    Set,
    Tuple,
    Type,
    TypeVar,
    Union,
    cast,
    get_args,
    get_origin,
)
import Pyro5.api
from pydantic import BaseModel
from Pyro5 import api as pyro
from Pyro5 import config as pyro_config
from backend.data import db, redis
from backend.util.process import AppProcess
from backend.util.retry import conn_retry
from backend.util.settings import Config, Secrets

logger = logging.getLogger(__name__)

T = TypeVar(""T"")
C = TypeVar(""C"", bound=Callable)

config = Config()
pyro_host = config.pyro_host
pyro_config.MAX_RETRIES = config.pyro_client_comm_retry  # type: ignore
pyro_config.COMMTIMEOUT = config.pyro_client_comm_timeout  # type: ignore

def expose(func: C) -> C:
    """"""
    Decorator to mark a method or class to be exposed for remote calls.

    ## ⚠️ Gotcha
    Aside from ""simple"" types, only Pydantic models are passed unscathed *if annotated*.
    Any other passed or returned class objects are converted to dictionaries by Pyro.
    """"""

    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            msg = f""Error in {func.__name__}: {e.__str__()}""
            logger.exception(msg)
            raise

    register_pydantic_serializers(func)

    return pyro.expose(wrapper)  # type: ignore


def register_pydantic_serializers(func: Callable):
    """"""Register custom serializers and deserializers for annotated Pydantic models""""""
    for name, annotation in func.__annotations__.items():
        try:
            pydantic_types = _pydantic_models_from_type_annotation(annotation)
        except Exception as e:
            raise TypeError(f""Error while exposing {func.__name__}: {e.__str__()}"")
        for model in pydantic_types:
            logger.debug(
                f""Registering Pyro (de)serializers for {func.__name__} annotation ""
                f""'{name}': {model.__qualname__}""
            )
            pyro.register_class_to_dict(model, _make_custom_serializer(model))
            pyro.register_dict_to_class(
                model.__qualname__, _make_custom_deserializer(model)
            )


def _make_custom_serializer(model: Type[BaseModel]):
    def custom_class_to_dict(obj):
        data = {
            ""__class__"": obj.__class__.__qualname__,
            **obj.model_dump(),
        }
        logger.debug(f""Serializing {obj.__class__.__qualname__} with data: {data}"")
        return data
    return custom_class_to_dict


def _make_custom_deserializer(model: Type[BaseModel]):
    def custom_dict_to_class(qualname, data: dict):
        logger.debug(f""Deserializing {model.__qualname__} from data: {data}"")
        return model(**data)
    return custom_dict_to_class


class AppService(AppProcess, ABC):
    shared_event_loop: asyncio.AbstractEventLoop
    use_db: bool = False
    use_redis: bool = False
    use_supabase: bool = False

    def __init__(self):
        self.uri = None

    @classmethod
    @abstractmethod
    def get_port(cls) -> int:
        pass

    @classmethod
    def get_host(cls) -> str:
        return os.environ.get(f""{cls.service_name.upper()}_HOST"", config.pyro_host)

    def run_service(self) -> None:
        while True:
            time.sleep(10)

    def __run_async(self, coro: Coroutine[Any, Any, T]):
        return asyncio.run_coroutine_threadsafe(coro, self.shared_event_loop)

    def run_and_wait(self, coro: Coroutine[Any, Any, T]) -> T:
        future = self.__run_async(coro)
        return future.result()

    def run(self):
        self.shared_event_loop = asyncio.get_event_loop()

        if self.use_db:
            self.shared_event_loop.run_until_complete(db.connect())

        if self.use_redis:
            redis.connect()

        if self.use_supabase:
            from supabase import create_client

            secrets = Secrets()
            self.supabase = create_client(
                secrets.supabase_url, secrets.supabase_service_role_key
            )

        # Initialize the async loop.
        async_thread = threading.Thread(target=self.__start_async_loop)
        async_thread.daemon = True
        async_thread.start()

        # Initialize pyro service
        daemon_thread = threading.Thread(target=self.__start_pyro)
        daemon_thread.daemon = True
        daemon_thread.start()

        # Run the main service (if it's not implemented, just sleep).
        self.run_service()

    def cleanup(self):
        if self.use_db:
            logger.info(f""[{self.__class__.__name__}] ⏳ Disconnecting DB..."")
            self.run_and_wait(db.disconnect())

        if self.use_redis:
            logger.info(f""[{self.__class__.__name__}] ⏳ Disconnecting Redis..."")
            redis.disconnect()

    @conn_retry(""Pyro"", ""Starting Pyro Service"")
    def __start_pyro(self):
        maximum_connection_thread_count = max(
            Pyro5.config.THREADPOOL_SIZE,
            config.num_node_workers * config.num_graph_workers,
        )

        Pyro5.config.THREADPOOL_SIZE = maximum_connection_thread_count  # type: ignore
        daemon = Pyro5.api.Daemon(host=config.pyro_host, port=self.get_port())
        self.uri = daemon.register(self, objectId=self.service_name)
        logger.info(f""[{self.service_name}] Connected to Pyro; URI = {self.uri}"")
        daemon.requestLoop()

    def __start_async_loop(self):
        self.shared_event_loop.run_forever()


# --------- UTILITIES --------- #

AS = TypeVar(""AS"", bound=AppService)


class PyroClient:
    proxy: Pyro5.api.Proxy


def close_service_client(client: AppService) -> None:
    if isinstance(client, PyroClient):
        client.proxy._pyroRelease()
    else:
        raise RuntimeError(f""Client {client.__class__} is not a Pyro client."")


def get_service_client(service_type: Type[AS]) -> AS:
    service_name = service_type.service_name

    class DynamicClient(PyroClient):
        @conn_retry(""Pyro"", f""Connecting to [{service_name}]"")
        def __init__(self):
            host = os.environ.get(f""{service_name.upper()}_HOST"", pyro_host)
            uri = f""PYRO:{service_type.service_name}@{host}:{service_type.get_port()}""
            logger.debug(f""Connecting to service [{service_name}]. URI = {uri}"")
            self.proxy = Pyro5.api.Proxy(uri)
            # Attempt to bind to ensure the connection is established
            self.proxy._pyroBind()
            logger.debug(f""Successfully connected to service [{service_name}]"")

        def __getattr__(self, name: str) -> Callable[..., Any]:
            res = getattr(self.proxy, name)
            return res

    return cast(AS, DynamicClient())

builtin_types = [*vars(builtins).values(), NoneType, Enum]

def _pydantic_models_from_type_annotation(annotation) -> Iterator[type[BaseModel]]:
    # Peel Annotated parameters
    if (origin := get_origin(annotation)) and origin is Annotated:
        annotation = get_args(annotation)[0]

    origin = get_origin(annotation)
    args = get_args(annotation)

    if origin in (
        Union,
        UnionType,
        list,
        List,
        tuple,
        Tuple,
        set,
        Set,
        frozenset,
        FrozenSet,
    ):
        for arg in args:
            yield from _pydantic_models_from_type_annotation(arg)
    elif origin in (dict, Dict):
        key_type, value_type = args
        yield from _pydantic_models_from_type_annotation(key_type)
        yield from _pydantic_models_from_type_annotation(value_type)
    elif origin in (Awaitable, Coroutine):
        # For coroutines and awaitables, check the return type
        return_type = args[-1]
        yield from _pydantic_models_from_type_annotation(return_type)
    else:
        annotype = annotation if origin is None else origin
        # Exclude generic types and aliases
        if (
            annotype is not None
            and not hasattr(typing, getattr(annotype, ""__name__"", """"))
            and isinstance(annotype, type)
        ):
            if issubclass(annotype, BaseModel):
                yield annotype
            elif annotype not in builtin_types and not issubclass(annotype, Enum):
                raise TypeError(f""Unsupported type encountered: {annotype}"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
172,"""""""
The command line interface for the agent server
""""""

import os
import pathlib

import click
import psutil

from backend import app
from backend.util.process import AppProcess


def get_pid_path() -> pathlib.Path:
    home_dir = pathlib.Path.home()
    new_dir = home_dir / "".config"" / ""agpt""
    file_path = new_dir / ""running.tmp""
    return file_path


def get_pid() -> int | None:
    file_path = get_pid_path()
    if not file_path.exists():
        return None

    os.makedirs(file_path.parent, exist_ok=True)
    with open(file_path, ""r"", encoding=""utf-8"") as file:
        pid = file.read()
    try:
        return int(pid)
    except ValueError:
        return None


def write_pid(pid: int):
    file_path = get_pid_path()
    os.makedirs(file_path.parent, exist_ok=True)
    with open(file_path, ""w"", encoding=""utf-8"") as file:
        file.write(str(pid))


class MainApp(AppProcess):
    def run(self):
        app.main(silent=True)


@click.group()
def main():
    """"""AutoGPT Server CLI Tool""""""
    pass


@main.command()
def start():
    """"""
    Starts the server in the background and saves the PID
    """"""
    # Define the path for the new directory and file
    pid = get_pid()
    if pid and psutil.pid_exists(pid):
        print(""Server is already running"")
        exit(1)
    elif pid:
        print(""PID does not exist deleting file"")
        os.remove(get_pid_path())

    print(""Starting server"")
    pid = MainApp().start(background=True, silent=True)
    print(f""Server running in process: {pid}"")

    write_pid(pid)
    print(""done"")
    os._exit(status=0)


@main.command()
def stop():
    """"""
    Stops the server
    """"""
    pid = get_pid()
    if not pid:
        print(""Server is not running"")
        return

    os.remove(get_pid_path())
    process = psutil.Process(int(pid))
    for child in process.children(recursive=True):
        child.terminate()
    process.terminate()

    print(""Server Stopped"")


@main.command()
def gen_encrypt_key():
    """"""
    Generate a new encryption key
    """"""
    from cryptography.fernet import Fernet

    print(Fernet.generate_key().decode())


@click.group()
def test():
    """"""
    Group for test commands
    """"""
    pass


@test.command()
@click.argument(""server_address"")
def reddit(server_address: str):
    """"""
    Create an event graph
    """"""
    import requests

    from backend.usecases.reddit_marketing import create_test_graph

    test_graph = create_test_graph()
    url = f""{server_address}/graphs""
    headers = {""Content-Type"": ""application/json""}
    data = test_graph.model_dump_json()

    response = requests.post(url, headers=headers, data=data)

    graph_id = response.json()[""id""]
    print(f""Graph created with ID: {graph_id}"")


@test.command()
@click.argument(""server_address"")
def populate_db(server_address: str):
    """"""
    Create an event graph
    """"""
    import requests

    from backend.usecases.sample import create_test_graph

    test_graph = create_test_graph()
    url = f""{server_address}/graphs""
    headers = {""Content-Type"": ""application/json""}
    data = test_graph.model_dump_json()

    response = requests.post(url, headers=headers, data=data)

    graph_id = response.json()[""id""]

    if response.status_code == 200:
        execute_url = f""{server_address}/graphs/{response.json()['id']}/execute""
        text = ""Hello, World!""
        input_data = {""input"": text}
        response = requests.post(execute_url, headers=headers, json=input_data)

        schedule_url = f""{server_address}/graphs/{graph_id}/schedules""
        data = {
            ""graph_id"": graph_id,
            ""cron"": ""*/5 * * * *"",
            ""input_data"": {""input"": ""Hello, World!""},
        }
        response = requests.post(schedule_url, headers=headers, json=data)

    print(""Database populated with: \n- graph\n- execution\n- schedule"")


@test.command()
@click.argument(""server_address"")
def graph(server_address: str):
    """"""
    Create an event graph
    """"""
    import requests

    from backend.usecases.sample import create_test_graph

    url = f""{server_address}/graphs""
    headers = {""Content-Type"": ""application/json""}
    data = create_test_graph().model_dump_json()
    response = requests.post(url, headers=headers, data=data)

    if response.status_code == 200:
        print(response.json()[""id""])
        execute_url = f""{server_address}/graphs/{response.json()['id']}/execute""
        text = ""Hello, World!""
        input_data = {""input"": text}
        response = requests.post(execute_url, headers=headers, json=input_data)

    else:
        print(""Failed to send graph"")
        print(f""Response: {response.text}"")


@test.command()
@click.argument(""graph_id"")
@click.argument(""content"")
def execute(graph_id: str, content: dict):
    """"""
    Create an event graph
    """"""
    import requests

    headers = {""Content-Type"": ""application/json""}

    execute_url = f""http://0.0.0.0:8000/graphs/{graph_id}/execute""
    requests.post(execute_url, headers=headers, json=content)


@test.command()
def event():
    """"""
    Send an event to the running server
    """"""
    print(""Event sent"")


@test.command()
@click.argument(""server_address"")
@click.argument(""graph_id"")
def websocket(server_address: str, graph_id: str):
    """"""
    Tests the websocket connection.
    """"""
    import asyncio

    import websockets.asyncio.client

    from backend.server.ws_api import ExecutionSubscription, Methods, WsMessage

    async def send_message(server_address: str):
        uri = f""ws://{server_address}""
        async with websockets.asyncio.client.connect(uri) as websocket:
            try:
                msg = WsMessage(
                    method=Methods.SUBSCRIBE,
                    data=ExecutionSubscription(graph_id=graph_id).model_dump(),
                ).model_dump_json()
                await websocket.send(msg)
                print(f""Sending: {msg}"")
                while True:
                    response = await websocket.recv()
                    print(f""Response from server: {response}"")
            except InterruptedError:
                exit(0)

    asyncio.run(send_message(server_address))
    print(""Testing WS"")


main.add_command(test)

if __name__ == ""__main__"":
    main()
","""""""
The command line interface for the agent server
""""""

import os
import pathlib
import click
import psutil
from backend import app
from backend.util.process import AppProcess


def get_pid_path() -> pathlib.Path:
    home_dir = pathlib.Path.home()
    new_dir = home_dir / "".config"" / ""agpt""
    file_path = new_dir / ""running.tmp""
    return file_path


def get_pid() -> int | None:
    file_path = get_pid_path()
    if not file_path.exists():
        return None
    os.makedirs(file_path.parent, exist_ok=True)
    with open(file_path, ""r"", encoding=""utf-8"") as file:
        pid = file.read()
    try:
        return int(pid)
    except ValueError:
        return None


def write_pid(pid: int):
    file_path = get_pid_path()
    os.makedirs(file_path.parent, exist_ok=True)
    with open(file_path, ""w"", encoding=""utf-8"") as file:
        file.write(str(pid))


class MainApp(AppProcess):
    def run(self):
        app.main(silent=True)


@click.group()
def main():
    """"""AutoGPT Server CLI Tool""""""
    pass


@main.command()
def start():
    """"""
    Starts the server in the background and saves the PID
    """"""
    # Define the path for the new directory and file
    pid = get_pid()
    if pid and psutil.pid_exists(pid):
        print(""Server is already running"")
        exit(1)
    elif pid:
        print(""PID does not exist deleting file"")
        os.remove(get_pid_path())
    print(""Starting server"")
    pid = MainApp().start(background=True, silent=True)
    print(f""Server running in process: {pid}"")
    write_pid(pid)
    print(""done"")
    os._exit(status=0)


@main.command()
def stop():
    """"""
    Stops the server
    """"""
    pid = get_pid()
    if not pid:
        print(""Server is not running"")
        return
    os.remove(get_pid_path())
    process = psutil.Process(int(pid))
    for child in process.children(recursive=True):
        child.terminate()
    process.terminate()
    print(""Server Stopped"")


@main.command()
def gen_encrypt_key():
    """"""
    Generate a new encryption key
    """"""
    from cryptography.fernet import Fernet
    print(Fernet.generate_key().decode())


@click.group()
def test():
    """"""
    Group for test commands
    """"""
    pass


@test.command()
@click.argument(""server_address"")
def reddit(server_address: str):
    """"""
    Create an event graph
    """"""
    import requests
    from backend.usecases.reddit_marketing import create_test_graph
    test_graph = create_test_graph()
    url = f""{server_address}/graphs""
    headers = {""Content-Type"": ""application/json""}
    data = test_graph.model_dump_json()
    response = requests.post(url, headers=headers, data=data)
    graph_id = response.json()[""id""]
    print(f""Graph created with ID: {graph_id}"")


@test.command()
@click.argument(""server_address"")
def populate_db(server_address: str):
    """"""
    Create an event graph
    """"""
    import requests
    from backend.usecases.sample import create_test_graph
    test_graph = create_test_graph()
    url = f""{server_address}/graphs""
    headers = {""Content-Type"": ""application/json""}
    data = test_graph.model_dump_json()
    response = requests.post(url, headers=headers, data=data)
    graph_id = response.json()[""id""]
    if response.status_code == 200:
        execute_url = f""{server_address}/graphs/{response.json()['id']}/execute""
        text = ""Hello, World!""
        input_data = {""input"": text}
        response = requests.post(execute_url, headers=headers, json=input_data)
        schedule_url = f""{server_address}/graphs/{graph_id}/schedules""
        data = {
            ""graph_id"": graph_id,
            ""cron"": ""*/5 * * * *"",
            ""input_data"": {""input"": ""Hello, World!""},
        }
        response = requests.post(schedule_url, headers=headers, json=data)
    print(""Database populated with: \n- graph\n- execution\n- schedule"")


@test.command()
@click.argument(""server_address"")
def graph(server_address: str):
    """"""
    Create an event graph
    """"""
    import requests
    from backend.usecases.sample import create_test_graph
    url = f""{server_address}/graphs""
    headers = {""Content-Type"": ""application/json""}
    data = create_test_graph().model_dump_json()
    response = requests.post(url, headers=headers, data=data)
    if response.status_code == 200:
        print(response.json()[""id""])
        execute_url = f""{server_address}/graphs/{response.json()['id']}/execute""
        text = ""Hello, World!""
        input_data = {""input"": text}
        response = requests.post(execute_url, headers=headers, json=input_data)
    else:
        print(""Failed to send graph"")
        print(f""Response: {response.text}"")


@test.command()
@click.argument(""graph_id"")
@click.argument(""content"")
def execute(graph_id: str, content: dict):
    """"""
    Create an event graph
    """"""
    import requests
    headers = {""Content-Type"": ""application/json""}
    execute_url = f""http://0.0.0.0:8000/graphs/{graph_id}/execute""
    requests.post(execute_url, headers=headers, json=content)


@test.command()
def event():
    """"""
    Send an event to the running server
    """"""
    print(""Event sent"")


@test.command()
@click.argument(""server_address"")
@click.argument(""graph_id"")
def websocket(server_address: str, graph_id: str):
    """"""
    Tests the websocket connection.
    """"""
    import asyncio
    import websockets.asyncio.client
    from backend.server.ws_api import ExecutionSubscription, Methods, WsMessage
    async def send_message(server_address: str):
        uri = f""ws://{server_address}""
        async with websockets.asyncio.client.connect(uri) as websocket:
            try:
                msg = WsMessage(
                    method=Methods.SUBSCRIBE,
                    data=ExecutionSubscription(graph_id=graph_id).model_dump(),
                ).model_dump_json()
                await websocket.send(msg)
                print(f""Sending: {msg}"")
                while True:
                    response = await websocket.recv()
                    print(f""Response from server: {response}"")
            except InterruptedError:
                exit(0)
    asyncio.run(send_message(server_address))
    print(""Testing WS"")


main.add_command(test)

if __name__ == ""__main__"":
    main()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
173,"# -*- coding: utf-8 -*-

from mrjob.job import MRJob


class SalesRanker(MRJob):

    def within_past_week(self, timestamp):
        """"""Return True if timestamp is within past week, False otherwise.""""""
        ...

    def mapper(self, _, line):
        """"""Parse each log line, extract and transform relevant lines.

        Emit key value pairs of the form:

        (foo, p1), 2
        (bar, p1), 2
        (bar, p1), 1
        (foo, p2), 3
        (bar, p3), 10
        (foo, p4), 1
        """"""
        timestamp, product_id, category, quantity = line.split('\t')
        if self.within_past_week(timestamp):
            yield (category, product_id), quantity

    def reducer(self, key, values):
        """"""Sum values for each key.

        (foo, p1), 2
        (bar, p1), 3
        (foo, p2), 3
        (bar, p3), 10
        (foo, p4), 1
        """"""
        yield key, sum(values)

    def mapper_sort(self, key, value):
        """"""Construct key to ensure proper sorting.

        Transform key and value to the form:

        (foo, 2), p1
        (bar, 3), p1
        (foo, 3), p2
        (bar, 10), p3
        (foo, 1), p4

        The shuffle/sort step of MapReduce will then do a
        distributed sort on the keys, resulting in:

        (category1, 1), product4
        (category1, 2), product1
        (category1, 3), product2
        (category2, 3), product1
        (category2, 7), product3
        """"""
        category, product_id = key
        quantity = value
        yield (category, quantity), product_id

    def reducer_identity(self, key, value):
        yield key, value

    def steps(self):
        """"""Run the map and reduce steps.""""""
        return [
            self.mr(mapper=self.mapper,
                    reducer=self.reducer),
            self.mr(mapper=self.mapper_sort,
                    reducer=self.reducer_identity),
        ]


if __name__ == '__main__':
    SalesRanker.run()
","# -*- coding: utf-8 -*-
from mrjob.job import MRJob

class SalesRanker(MRJob):

    def within_past_week(self, timestamp):
        """"""Return True if timestamp is within past week, False otherwise.""""""
        ...

    def mapper(self, _, line):
        """"""Parse each log line, extract and transform relevant lines.

        Emit key value pairs of the form:

        (foo, p1), 2
        (bar, p1), 2
        (bar, p1), 1
        (foo, p2), 3
        (bar, p3), 10
        (foo, p4), 1
        """"""
        timestamp, product_id, category, quantity = line.split('\t')
        if self.within_past_week(timestamp):
            yield (category, product_id), quantity

    def reducer(self, key, values):
        """"""Sum values for each key.

        (foo, p1), 2
        (bar, p1), 3
        (foo, p2), 3
        (bar, p3), 10
        (foo, p4), 1
        """"""
        yield key, sum(values)

    def mapper_sort(self, key, value):
        """"""Construct key to ensure proper sorting.

        Transform key and value to the form:

        (foo, 2), p1
        (bar, 3), p1
        (foo, 3), p2
        (bar, 10), p3
        (foo, 1), p4

        The shuffle/sort step of MapReduce will then do a
        distributed sort on the keys, resulting in:

        (category1, 1), product4
        (category1, 2), product1
        (category1, 3), product2
        (category2, 3), product1
        (category2, 7), product3
        """"""
        category, product_id = key
        quantity = value
        yield (category, quantity), product_id

    def reducer_identity(self, key, value):
        yield key, value

    def steps(self):
        """"""Run the map and reduce steps.""""""
        return [
            self.mr(mapper=self.mapper,
                    reducer=self.reducer),
            self.mr(mapper=self.mapper_sort,
                    reducer=self.reducer_identity),
        ]

if __name__ == '__main__':
    SalesRanker.run()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
174,"# -*- coding: utf-8 -*-
from collections import deque
from enum import Enum


class State(Enum):
    unvisited = 0
    visited = 1


class Graph(object):

    def bfs(self, source, dest):
        if source is None:
            return False
        queue = deque()
        queue.append(source)
        source.visit_state = State.visited
        while queue:
            node = queue.popleft()
            print(node)
            if dest is node:
                return True
            for adjacent_node in node.adj_nodes.values():
                if adjacent_node.visit_state == State.unvisited:
                    queue.append(adjacent_node)
                    adjacent_node.visit_state = State.visited
        return False


class Person(object):

    def __init__(self, id, name):
        self.id = id
        self.name = name
        self.friend_ids = []


class LookupService(object):

    def __init__(self):
        self.lookup = {}  # key: person_id, value: person_server

    def get_person(self, person_id):
        person_server = self.lookup[person_id]
        return person_server.people[person_id]


class PersonServer(object):

    def __init__(self):
        self.people = {}  # key: person_id, value: person

    def get_people(self, ids):
        results = []
        for id in ids:
            if id in self.people:
                results.append(self.people[id])
        return results


class UserGraphService(object):

    def __init__(self, person_ids, lookup):
        self.lookup = lookup
        self.person_ids = person_ids
        self.visited_ids = set()

    def bfs(self, source, dest):
        # Use self.visited_ids to track visited nodes
        # Use self.lookup to translate a person_id to a Person
        pass
","# -*- coding: utf-8 -*-
from collections import deque
from enum import Enum

class State(Enum):
    unvisited = 0
    visited = 1

class Graph(object):

    def bfs(self, source, dest):
        if source is None:
            return False
        queue = deque()
        queue.append(source)
        source.visit_state = State.visited
        while queue:
            node = queue.popleft()
            print(node)
            if dest is node:
                return True
            for adjacent_node in node.adj_nodes.values():
                if adjacent_node.visit_state == State.unvisited:
                    queue.append(adjacent_node)
                    adjacent_node.visit_state = State.visited
        return False

class Person(object):

    def __init__(self, id, name):
        self.id = id
        self.name = name
        self.friend_ids = []

class LookupService(object):

    def __init__(self):
        self.lookup = {}  # key: person_id, value: person_server

    def get_person(self, person_id):
        person_server = self.lookup[person_id]
        return person_server.people[person_id]

class PersonServer(object):

    def __init__(self):
        self.people = {}  # key: person_id, value: person

    def get_people(self, ids):
        results = []
        for id in ids:
            if id in self.people:
                results.append(self.people[id])
        return results

class UserGraphService(object):

    def __init__(self, person_ids, lookup):
        self.lookup = lookup
        self.person_ids = person_ids
        self.visited_ids = set()

    def bfs(self, source, dest):
        # Use self.visited_ids to track visited nodes
        # Use self.lookup to translate a person_id to a Person
        pass","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
175,"import logging
import os
from urllib.parse import parse_qs, urlencode, urlparse, urlunparse

from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED
from apscheduler.job import Job as JobObj
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
from autogpt_libs.utils.cache import thread_cached
from dotenv import load_dotenv
from pydantic import BaseModel
from sqlalchemy import MetaData, create_engine

from backend.data.block import BlockInput
from backend.executor.manager import ExecutionManager
from backend.util.service import AppService, expose, get_service_client
from backend.util.settings import Config


def _extract_schema_from_url(database_url) -> tuple[str, str]:
    """"""
    Extracts the schema from the DATABASE_URL and returns the schema and cleaned URL.
    """"""
    parsed_url = urlparse(database_url)
    query_params = parse_qs(parsed_url.query)

    # Extract the 'schema' parameter
    schema_list = query_params.pop(""schema"", None)
    schema = schema_list[0] if schema_list else ""public""

    # Reconstruct the query string without the 'schema' parameter
    new_query = urlencode(query_params, doseq=True)
    new_parsed_url = parsed_url._replace(query=new_query)
    database_url_clean = str(urlunparse(new_parsed_url))

    return schema, database_url_clean


logger = logging.getLogger(__name__)
config = Config()


def log(msg, **kwargs):
    logger.info(""[ExecutionScheduler] "" + msg, **kwargs)


def job_listener(event):
    """"""Logs job execution outcomes for better monitoring.""""""
    if event.exception:
        log(f""Job {event.job_id} failed."")
    else:
        log(f""Job {event.job_id} completed successfully."")


@thread_cached
def get_execution_client() -> ExecutionManager:
    return get_service_client(ExecutionManager)


def execute_graph(**kwargs):
    args = JobArgs(**kwargs)
    try:
        log(f""Executing recurring job for graph #{args.graph_id}"")
        get_execution_client().add_execution(
            args.graph_id, args.input_data, args.user_id
        )
    except Exception as e:
        logger.exception(f""Error executing graph {args.graph_id}: {e}"")


class JobArgs(BaseModel):
    graph_id: str
    input_data: BlockInput
    user_id: str
    graph_version: int
    cron: str


class JobInfo(JobArgs):
    id: str
    name: str
    next_run_time: str

    @staticmethod
    def from_db(job_args: JobArgs, job_obj: JobObj) -> ""JobInfo"":
        return JobInfo(
            id=job_obj.id,
            name=job_obj.name,
            next_run_time=job_obj.next_run_time.isoformat(),
            **job_args.model_dump(),
        )


class ExecutionScheduler(AppService):
    scheduler: BlockingScheduler

    @classmethod
    def get_port(cls) -> int:
        return config.execution_scheduler_port

    @property
    @thread_cached
    def execution_client(self) -> ExecutionManager:
        return get_service_client(ExecutionManager)

    def run_service(self):
        load_dotenv()
        db_schema, db_url = _extract_schema_from_url(os.getenv(""DATABASE_URL""))
        self.scheduler = BlockingScheduler(
            jobstores={
                ""default"": SQLAlchemyJobStore(
                    engine=create_engine(db_url),
                    metadata=MetaData(schema=db_schema),
                )
            }
        )
        self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)
        self.scheduler.start()

    @expose
    def add_execution_schedule(
        self,
        graph_id: str,
        graph_version: int,
        cron: str,
        input_data: BlockInput,
        user_id: str,
    ) -> JobInfo:
        job_args = JobArgs(
            graph_id=graph_id,
            input_data=input_data,
            user_id=user_id,
            graph_version=graph_version,
            cron=cron,
        )
        job = self.scheduler.add_job(
            execute_graph,
            CronTrigger.from_crontab(cron),
            kwargs=job_args.model_dump(),
            replace_existing=True,
        )
        log(f""Added job {job.id} with cron schedule '{cron}' input data: {input_data}"")
        return JobInfo.from_db(job_args, job)

    @expose
    def delete_schedule(self, schedule_id: str, user_id: str) -> JobInfo:
        job = self.scheduler.get_job(schedule_id)
        if not job:
            log(f""Job {schedule_id} not found."")
            raise ValueError(f""Job #{schedule_id} not found."")

        job_args = JobArgs(**job.kwargs)
        if job_args.user_id != user_id:
            raise ValueError(""User ID does not match the job's user ID."")

        log(f""Deleting job {schedule_id}"")
        job.remove()

        return JobInfo.from_db(job_args, job)

    @expose
    def get_execution_schedules(
        self, graph_id: str | None = None, user_id: str | None = None
    ) -> list[JobInfo]:
        schedules = []
        for job in self.scheduler.get_jobs():
            job_args = JobArgs(**job.kwargs)
            if (
                job.next_run_time is not None
                and (graph_id is None or job_args.graph_id == graph_id)
                and (user_id is None or job_args.user_id == user_id)
            ):
                schedules.append(JobInfo.from_db(job_args, job))
        return schedules
","import logging
import os
from urllib.parse import parse_qs, urlencode, urlparse, urlunparse

from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED
from apscheduler.job import Job as JobObj
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
from autogpt_libs.utils.cache import thread_cached
from dotenv import load_dotenv
from pydantic import BaseModel
from sqlalchemy import MetaData, create_engine

from backend.data.block import BlockInput
from backend.executor.manager import ExecutionManager
from backend.util.service import AppService, expose, get_service_client
from backend.util.settings import Config


def _extract_schema_from_url(database_url) -> tuple[str, str]:
    """"""
    Extracts the schema from the DATABASE_URL and returns the schema and cleaned URL.
    """"""

    parsed_url = urlparse(database_url)
    query_params = parse_qs(parsed_url.query)

    # Extract the 'schema' parameter
    schema_list = query_params.pop(""schema"", None)
    schema = schema_list[0] if schema_list else ""public""

    # Reconstruct the query string without the 'schema' parameter
    new_query = urlencode(query_params, doseq=True)
    new_parsed_url = parsed_url._replace(query=new_query)
    database_url_clean = str(urlunparse(new_parsed_url))

    return schema, database_url_clean


logger = logging.getLogger(__name__)
config = Config()


def log(msg, **kwargs):
    logger.info(""[ExecutionScheduler] "" + msg, **kwargs)


def job_listener(event):
    """"""Logs job execution outcomes for better monitoring.""""""

    if event.exception:
        log(f""Job {event.job_id} failed."")
    else:
        log(f""Job {event.job_id} completed successfully."")


@thread_cached
def get_execution_client() -> ExecutionManager:
    return get_service_client(ExecutionManager)


def execute_graph(**kwargs):
    args = JobArgs(**kwargs)

    try:
        log(f""Executing recurring job for graph #{args.graph_id}"")
        get_execution_client().add_execution(
            args.graph_id, args.input_data, args.user_id
        )
    except Exception as e:
        logger.exception(f""Error executing graph {args.graph_id}: {e}"")


class JobArgs(BaseModel):
    graph_id: str
    input_data: BlockInput
    user_id: str
    graph_version: int
    cron: str


class JobInfo(JobArgs):
    id: str
    name: str
    next_run_time: str

    @staticmethod
    def from_db(job_args: JobArgs, job_obj: JobObj) -> ""JobInfo"":
        return JobInfo(
            id=job_obj.id,
            name=job_obj.name,
            next_run_time=job_obj.next_run_time.isoformat(),
            **job_args.model_dump(),
        )


class ExecutionScheduler(AppService):
    scheduler: BlockingScheduler

    @classmethod
    def get_port(cls) -> int:
        return config.execution_scheduler_port

    @property
    @thread_cached
    def execution_client(self) -> ExecutionManager:
        return get_service_client(ExecutionManager)

    def run_service(self):
        load_dotenv()
        db_schema, db_url = _extract_schema_from_url(os.getenv(""DATABASE_URL""))
        self.scheduler = BlockingScheduler(
            jobstores={
                ""default"": SQLAlchemyJobStore(
                    engine=create_engine(db_url),
                    metadata=MetaData(schema=db_schema),
                )
            }
        )
        self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)
        self.scheduler.start()

    @expose
    def add_execution_schedule(
        self,
        graph_id: str,
        graph_version: int,
        cron: str,
        input_data: BlockInput,
        user_id: str,
    ) -> JobInfo:
        job_args = JobArgs(
            graph_id=graph_id,
            input_data=input_data,
            user_id=user_id,
            graph_version=graph_version,
            cron=cron,
        )
        job = self.scheduler.add_job(
            execute_graph,
            CronTrigger.from_crontab(cron),
            kwargs=job_args.model_dump(),
            replace_existing=True,
        )
        log(f""Added job {job.id} with cron schedule '{cron}' input data: {input_data}"")
        return JobInfo.from_db(job_args, job)

    @expose
    def delete_schedule(self, schedule_id: str, user_id: str) -> JobInfo:
        job = self.scheduler.get_job(schedule_id)
        if not job:
            log(f""Job {schedule_id} not found."")
            raise ValueError(f""Job #{schedule_id} not found."")

        job_args = JobArgs(**job.kwargs)
        if job_args.user_id != user_id:
            raise ValueError(""User ID does not match the job's user ID."")

        log(f""Deleting job {schedule_id}"")
        job.remove()

        return JobInfo.from_db(job_args, job)

    @expose
    def get_execution_schedules(
        self, graph_id: str | None = None, user_id: str | None = None
    ) -> list[JobInfo]:
        schedules = []
        for job in self.scheduler.get_jobs():
            job_args = JobArgs(**job.kwargs)
            if (
                job.next_run_time is not None
                and (graph_id is None or job_args.graph_id == graph_id)
                and (user_id is None or job_args.user_id == user_id)
            ):
                schedules.append(JobInfo.from_db(job_args, job))
        return schedules","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
176,"import asyncio
import logging
from contextlib import asynccontextmanager

import uvicorn
from autogpt_libs.auth import parse_jwt_token
from fastapi import Depends, FastAPI, WebSocket, WebSocketDisconnect
from starlette.middleware.cors import CORSMiddleware

from backend.data import redis
from backend.data.execution import AsyncRedisExecutionEventBus
from backend.data.user import DEFAULT_USER_ID
from backend.server.conn_manager import ConnectionManager
from backend.server.model import ExecutionSubscription, Methods, WsMessage
from backend.util.service import AppProcess
from backend.util.settings import AppEnvironment, Config, Settings

logger = logging.getLogger(__name__)
settings = Settings()


@asynccontextmanager
async def lifespan(app: FastAPI):
    manager = get_connection_manager()
    fut = asyncio.create_task(event_broadcaster(manager))
    fut.add_done_callback(lambda _: logger.info(""Event broadcaster stopped""))
    yield


docs_url = ""/docs"" if settings.config.app_env == AppEnvironment.LOCAL else None
app = FastAPI(lifespan=lifespan, docs_url=docs_url)
_connection_manager = None


def get_connection_manager():
    global _connection_manager
    if _connection_manager is None:
        _connection_manager = ConnectionManager()
    return _connection_manager


async def event_broadcaster(manager: ConnectionManager):
    try:
        redis.connect()
        event_queue = AsyncRedisExecutionEventBus()
        async for event in event_queue.listen():
            await manager.send_execution_result(event)
    except Exception as e:
        logger.exception(f""Event broadcaster error: {e}"")
        raise
    finally:
        redis.disconnect()


async def authenticate_websocket(websocket: WebSocket) -> str:
    if not settings.config.enable_auth:
        return DEFAULT_USER_ID

    token = websocket.query_params.get(""token"")
    if not token:
        await websocket.close(code=4001, reason=""Missing authentication token"")
        return """"

    try:
        payload = parse_jwt_token(token)
        user_id = payload.get(""sub"")
        if not user_id:
            await websocket.close(code=4002, reason=""Invalid token"")
            return """"
        return user_id
    except ValueError:
        await websocket.close(code=4003, reason=""Invalid token"")
        return """"


async def handle_subscribe(
    websocket: WebSocket, manager: ConnectionManager, message: WsMessage
):
    if not message.data:
        await websocket.send_text(
            WsMessage(
                method=Methods.ERROR,
                success=False,
                error=""Subscription data missing"",
            ).model_dump_json()
        )
    else:
        ex_sub = ExecutionSubscription.model_validate(message.data)
        await manager.subscribe(ex_sub.graph_id, websocket)
        logger.debug(f""New execution subscription for graph {ex_sub.graph_id}"")
        await websocket.send_text(
            WsMessage(
                method=Methods.SUBSCRIBE,
                success=True,
                channel=ex_sub.graph_id,
            ).model_dump_json()
        )


async def handle_unsubscribe(
    websocket: WebSocket, manager: ConnectionManager, message: WsMessage
):
    if not message.data:
        await websocket.send_text(
            WsMessage(
                method=Methods.ERROR,
                success=False,
                error=""Subscription data missing"",
            ).model_dump_json()
        )
    else:
        ex_sub = ExecutionSubscription.model_validate(message.data)
        await manager.unsubscribe(ex_sub.graph_id, websocket)
        logger.debug(f""Removed execution subscription for graph {ex_sub.graph_id}"")
        await websocket.send_text(
            WsMessage(
                method=Methods.UNSUBSCRIBE,
                success=True,
                channel=ex_sub.graph_id,
            ).model_dump_json()
        )


@app.get(""/"")
async def health():
    return {""status"": ""healthy""}


@app.websocket(""/ws"")
async def websocket_router(
    websocket: WebSocket, manager: ConnectionManager = Depends(get_connection_manager)
):
    user_id = await authenticate_websocket(websocket)
    if not user_id:
        return
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            message = WsMessage.model_validate_json(data)

            if message.method == Methods.HEARTBEAT:
                await websocket.send_json(
                    {""method"": Methods.HEARTBEAT.value, ""data"": ""pong"", ""success"": True}
                )
                continue

            if message.method == Methods.SUBSCRIBE:
                await handle_subscribe(websocket, manager, message)

            elif message.method == Methods.UNSUBSCRIBE:
                await handle_unsubscribe(websocket, manager, message)

            elif message.method == Methods.ERROR:
                logger.error(f""WebSocket Error message received: {message.data}"")

            else:
                logger.warning(
                    f""Unknown WebSocket message type {message.method} received: ""
                    f""{message.data}""
                )
                await websocket.send_text(
                    WsMessage(
                        method=Methods.ERROR,
                        success=False,
                        error=""Message type is not processed by the server"",
                    ).model_dump_json()
                )

    except WebSocketDisconnect:
        manager.disconnect(websocket)
        logger.debug(""WebSocket client disconnected"")


class WebsocketServer(AppProcess):
    def run(self):
        logger.info(f""CORS allow origins: {settings.config.backend_cors_allow_origins}"")
        server_app = CORSMiddleware(
            app=app,
            allow_origins=settings.config.backend_cors_allow_origins,
            allow_credentials=True,
            allow_methods=[""*""],
            allow_headers=[""*""],
        )
        uvicorn.run(
            server_app,
            host=Config().websocket_server_host,
            port=Config().websocket_server_port,
        )
","import asyncio
import logging
from contextlib import asynccontextmanager
import uvicorn
from autogpt_libs.auth import parse_jwt_token
from fastapi import Depends, FastAPI, WebSocket, WebSocketDisconnect
from starlette.middleware.cors import CORSMiddleware
from backend.data import redis
from backend.data.execution import AsyncRedisExecutionEventBus
from backend.data.user import DEFAULT_USER_ID
from backend.server.conn_manager import ConnectionManager
from backend.server.model import ExecutionSubscription, Methods, WsMessage
from backend.util.service import AppProcess
from backend.util.settings import AppEnvironment, Config, Settings

logger = logging.getLogger(__name__)
settings = Settings()

@asynccontextmanager
async def lifespan(app: FastAPI):
    manager = get_connection_manager()
    fut = asyncio.create_task(event_broadcaster(manager))
    fut.add_done_callback(lambda _: logger.info(""Event broadcaster stopped""))
    yield

docs_url = ""/docs"" if settings.config.app_env == AppEnvironment.LOCAL else None
app = FastAPI(lifespan=lifespan, docs_url=docs_url)
_connection_manager = None

def get_connection_manager():
    global _connection_manager
    if _connection_manager is None:
        _connection_manager = ConnectionManager()
    return _connection_manager

async def event_broadcaster(manager: ConnectionManager):
    try:
        redis.connect()
        event_queue = AsyncRedisExecutionEventBus()
        async for event in event_queue.listen():
            await manager.send_execution_result(event)
    except Exception as e:
        logger.exception(f""Event broadcaster error: {e}"")
        raise
    finally:
        redis.disconnect()

async def authenticate_websocket(websocket: WebSocket) -> str:
    if not settings.config.enable_auth:
        return DEFAULT_USER_ID
    token = websocket.query_params.get(""token"")
    if not token:
        await websocket.close(code=4001, reason=""Missing authentication token"")
        return """"
    try:
        payload = parse_jwt_token(token)
        user_id = payload.get(""sub"")
        if not user_id:
            await websocket.close(code=4002, reason=""Invalid token"")
            return """"
        return user_id
    except ValueError:
        await websocket.close(code=4003, reason=""Invalid token"")
        return """"

async def handle_subscribe(
    websocket: WebSocket, manager: ConnectionManager, message: WsMessage
):
    if not message.data:
        await websocket.send_text(
            WsMessage(
                method=Methods.ERROR,
                success=False,
                error=""Subscription data missing"",
            ).model_dump_json()
        )
    else:
        ex_sub = ExecutionSubscription.model_validate(message.data)
        await manager.subscribe(ex_sub.graph_id, websocket)
        logger.debug(f""New execution subscription for graph {ex_sub.graph_id}"")
        await websocket.send_text(
            WsMessage(
                method=Methods.SUBSCRIBE,
                success=True,
                channel=ex_sub.graph_id,
            ).model_dump_json()
        )

async def handle_unsubscribe(
    websocket: WebSocket, manager: ConnectionManager, message: WsMessage
):
    if not message.data:
        await websocket.send_text(
            WsMessage(
                method=Methods.ERROR,
                success=False,
                error=""Subscription data missing"",
            ).model_dump_json()
        )
    else:
        ex_sub = ExecutionSubscription.model_validate(message.data)
        await manager.unsubscribe(ex_sub.graph_id, websocket)
        logger.debug(f""Removed execution subscription for graph {ex_sub.graph_id}"")
        await websocket.send_text(
            WsMessage(
                method=Methods.UNSUBSCRIBE,
                success=True,
                channel=ex_sub.graph_id,
            ).model_dump_json()
        )

@app.get(""/"")
async def health():
    return {""status"": ""healthy""}

@app.websocket(""/ws"")
async def websocket_router(
    websocket: WebSocket, manager: ConnectionManager = Depends(get_connection_manager)
):
    user_id = await authenticate_websocket(websocket)
    if not user_id:
        return
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            message = WsMessage.model_validate_json(data)
            if message.method == Methods.HEARTBEAT:
                await websocket.send_json(
                    {""method"": Methods.HEARTBEAT.value, ""data"": ""pong"", ""success"": True}
                )
                continue
            if message.method == Methods.SUBSCRIBE:
                await handle_subscribe(websocket, manager, message)
            elif message.method == Methods.UNSUBSCRIBE:
                await handle_unsubscribe(websocket, manager, message)
            elif message.method == Methods.ERROR:
                logger.error(f""WebSocket Error message received: {message.data}"")
            else:
                logger.warning(
                    f""Unknown WebSocket message type {message.method} received: ""
                    f""{message.data}""
                )
                await websocket.send_text(
                    WsMessage(
                        method=Methods.ERROR,
                        success=False,
                        error=""Message type is not processed by the server"",
                    ).model_dump_json()
                )
    except WebSocketDisconnect:
        manager.disconnect(websocket)
        logger.debug(""WebSocket client disconnected"")

class WebsocketServer(AppProcess):
    def run(self):
        logger.info(f""CORS allow origins: {settings.config.backend_cors_allow_origins}"")
        server_app = CORSMiddleware(
            app=app,
            allow_origins=settings.config.backend_cors_allow_origins,
            allow_credentials=True,
            allow_methods=[""*""],
            allow_headers=[""*""],
        )
        uvicorn.run(
            server_app,
            host=Config().websocket_server_host,
            port=Config().websocket_server_port,
        )
","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
177,"from contextlib import contextmanager
from threading import Lock
from typing import TYPE_CHECKING, Any

from expiringdict import ExpiringDict

if TYPE_CHECKING:
    from redis import Redis
    from redis.lock import Lock as RedisLock


class RedisKeyedMutex:
    """"""
    This class provides a mutex that can be locked and unlocked by a specific key,
    using Redis as a distributed locking provider.
    It uses an ExpiringDict to automatically clear the mutex after a specified timeout,
    in case the key is not unlocked for a specified duration, to prevent memory leaks.
    """"""

    def __init__(self, redis: ""Redis"", timeout: int | None = 60):
        self.redis = redis
        self.timeout = timeout
        self.locks: dict[Any, ""RedisLock""] = ExpiringDict(
            max_len=6000, max_age_seconds=self.timeout
        )
        self.locks_lock = Lock()

    @contextmanager
    def locked(self, key: Any):
        lock = self.acquire(key)
        try:
            yield
        finally:
            if lock.locked():
                lock.release()

    def acquire(self, key: Any) -> ""RedisLock"":
        """"""Acquires and returns a lock with the given key""""""
        with self.locks_lock:
            if key not in self.locks:
                self.locks[key] = self.redis.lock(
                    str(key), self.timeout, thread_local=False
                )
            lock = self.locks[key]
        lock.acquire()
        return lock

    def release(self, key: Any):
        if (lock := self.locks.get(key)) and lock.locked() and lock.owned():
            lock.release()

    def release_all_locks(self):
        """"""Call this on process termination to ensure all locks are released""""""
        self.locks_lock.acquire(blocking=False)
        for lock in self.locks.values():
            if lock.locked() and lock.owned():
                lock.release()
","from threading import Lock
from typing import TYPE_CHECKING, Any
from contextlib import contextmanager
from expiringdict import ExpiringDict

if TYPE_CHECKING:
    from redis import Redis
    from redis.lock import Lock as RedisLock

class RedisKeyedMutex:
    """"""
    This class provides a mutex that can be locked and unlocked by a specific key,
    using Redis as a distributed locking provider.
    It uses an ExpiringDict to automatically clear the mutex after a specified timeout,
    in case the key is not unlocked for a specified duration, to prevent memory leaks.
    """"""

    def __init__(self, redis: ""Redis"", timeout: int | None = 60):
        self.redis = redis
        self.timeout = timeout
        self.locks: dict[Any, ""RedisLock""] = ExpiringDict(
            max_len=6000, max_age_seconds=self.timeout
        )
        self.locks_lock = Lock()

    @contextmanager
    def locked(self, key: Any):
        lock = self.acquire(key)
        try:
            yield
        finally:
            if lock.locked():
                lock.release()

    def acquire(self, key: Any) -> ""RedisLock"":
        """"""Acquires and returns a lock with the given key""""""
        with self.locks_lock:
            if key not in self.locks:
                self.locks[key] = self.redis.lock(
                    str(key), self.timeout, thread_local=False
                )
            lock = self.locks[key]
        lock.acquire()
        return lock

    def release(self, key: Any):
        if (lock := self.locks.get(key)) and lock.locked() and lock.owned():
            lock.release()

    def release_all_locks(self):
        """"""Call this on process termination to ensure all locks are released""""""
        self.locks_lock.acquire(blocking=False)
        for lock in self.locks.values():
            if lock.locked() and lock.owned():
                lock.release()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
178,"import os
from enum import Enum
from typing import Literal

import replicate
from pydantic import SecretStr
from replicate.helpers import FileOutput

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""replicate"",
    api_key=SecretStr(""mock-replicate-api-key""),
    title=""Mock Replicate API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}


# Model name enum
class ReplicateFluxModelName(str, Enum):
    FLUX_SCHNELL = (""Flux Schnell"",)
    FLUX_PRO = (""Flux Pro"",)
    FLUX_PRO1_1 = (""Flux Pro 1.1"",)

    @property
    def api_name(self):
        api_names = {
            ReplicateFluxModelName.FLUX_SCHNELL: ""black-forest-labs/flux-schnell"",
            ReplicateFluxModelName.FLUX_PRO: ""black-forest-labs/flux-pro"",
            ReplicateFluxModelName.FLUX_PRO1_1: ""black-forest-labs/flux-1.1-pro"",
        }
        return api_names[self]


# Image type Enum
class ImageType(str, Enum):
    WEBP = ""webp""
    JPG = ""jpg""
    PNG = ""png""


class ReplicateFluxAdvancedModelBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.REPLICATE], Literal[""api_key""]
        ] = CredentialsField(
            description=""The Replicate integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        prompt: str = SchemaField(
            description=""Text prompt for image generation"",
            placeholder=""e.g., 'A futuristic cityscape at sunset'"",
            title=""Prompt"",
        )
        replicate_model_name: ReplicateFluxModelName = SchemaField(
            description=""The name of the Image Generation Model, i.e Flux Schnell"",
            default=ReplicateFluxModelName.FLUX_SCHNELL,
            title=""Image Generation Model"",
            advanced=False,
        )
        seed: int | None = SchemaField(
            description=""Random seed. Set for reproducible generation"",
            default=None,
            title=""Seed"",
        )
        steps: int = SchemaField(
            description=""Number of diffusion steps"",
            default=25,
            title=""Steps"",
        )
        guidance: float = SchemaField(
            description=(
                ""Controls the balance between adherence to the text prompt and image quality/diversity. ""
                ""Higher values make the output more closely match the prompt but may reduce overall image quality.""
            ),
            default=3,
            title=""Guidance"",
        )
        interval: float = SchemaField(
            description=(
                ""Interval is a setting that increases the variance in possible outputs. ""
                ""Setting this value low will ensure strong prompt following with more consistent outputs.""
            ),
            default=2,
            title=""Interval"",
        )
        aspect_ratio: str = SchemaField(
            description=""Aspect ratio for the generated image"",
            default=""1:1"",
            title=""Aspect Ratio"",
            placeholder=""Choose from: 1:1, 16:9, 2:3, 3:2, 4:5, 5:4, 9:16"",
        )
        output_format: ImageType = SchemaField(
            description=""File format of the output image"",
            default=ImageType.WEBP,
            title=""Output Format"",
        )
        output_quality: int = SchemaField(
            description=(
                ""Quality when saving the output images, from 0 to 100. ""
                ""Not relevant for .png outputs""
            ),
            default=80,
            title=""Output Quality"",
        )
        safety_tolerance: int = SchemaField(
            description=""Safety tolerance, 1 is most strict and 5 is most permissive"",
            default=2,
            title=""Safety Tolerance"",
        )

    class Output(BlockSchema):
        result: str = SchemaField(description=""Generated output"")
        error: str = SchemaField(description=""Error message if the model run failed"")

    def __init__(self):
        super().__init__(
            id=""90f8c45e-e983-4644-aa0b-b4ebe2f531bc"",
            description=""This block runs Flux models on Replicate with advanced settings."",
            categories={BlockCategory.AI},
            input_schema=ReplicateFluxAdvancedModelBlock.Input,
            output_schema=ReplicateFluxAdvancedModelBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""replicate_model_name"": ReplicateFluxModelName.FLUX_SCHNELL,
                ""prompt"": ""A beautiful landscape painting of a serene lake at sunrise"",
                ""seed"": None,
                ""steps"": 25,
                ""guidance"": 3.0,
                ""interval"": 2.0,
                ""aspect_ratio"": ""1:1"",
                ""output_format"": ImageType.PNG,
                ""output_quality"": 80,
                ""safety_tolerance"": 2,
            },
            test_output=[
                (
                    ""result"",
                    ""https://replicate.com/output/generated-image-url.jpg"",
                ),
            ],
            test_mock={
                ""run_model"": lambda api_key, model_name, prompt, seed, steps, guidance, interval, aspect_ratio, output_format, output_quality, safety_tolerance: ""https://replicate.com/output/generated-image-url.jpg"",
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # If the seed is not provided, generate a random seed
        seed = input_data.seed
        if seed is None:
            seed = int.from_bytes(os.urandom(4), ""big"")

        # Run the model using the provided inputs
        result = self.run_model(
            api_key=credentials.api_key,
            model_name=input_data.replicate_model_name.api_name,
            prompt=input_data.prompt,
            seed=seed,
            steps=input_data.steps,
            guidance=input_data.guidance,
            interval=input_data.interval,
            aspect_ratio=input_data.aspect_ratio,
            output_format=input_data.output_format,
            output_quality=input_data.output_quality,
            safety_tolerance=input_data.safety_tolerance,
        )
        yield ""result"", result

    def run_model(
        self,
        api_key: SecretStr,
        model_name,
        prompt,
        seed,
        steps,
        guidance,
        interval,
        aspect_ratio,
        output_format,
        output_quality,
        safety_tolerance,
    ):
        # Initialize Replicate client with the API key
        client = replicate.Client(api_token=api_key.get_secret_value())

        # Run the model with additional parameters
        output: FileOutput | list[FileOutput] = client.run(  # type: ignore This is because they changed the return type, and didn't update the type hint! It should be overloaded depending on the value of `use_file_output` to `FileOutput | list[FileOutput]` but it's `Any | Iterator[Any]`
            f""{model_name}"",
            input={
                ""prompt"": prompt,
                ""seed"": seed,
                ""steps"": steps,
                ""guidance"": guidance,
                ""interval"": interval,
                ""aspect_ratio"": aspect_ratio,
                ""output_format"": output_format,
                ""output_quality"": output_quality,
                ""safety_tolerance"": safety_tolerance,
            },
            wait=False,  # don't arbitrarily return data:octect/stream or sometimes url depending on the model???? what is this api
        )

        # Check if output is a list or a string and extract accordingly; otherwise, assign a default message
        if isinstance(output, list) and len(output) > 0:
            if isinstance(output[0], FileOutput):
                result_url = output[0].url  # If output is a list, get the first element
            else:
                result_url = output[
                    0
                ]  # If output is a list and not a FileOutput, get the first element. Should never happen, but just in case.
        elif isinstance(output, FileOutput):
            result_url = output.url  # If output is a FileOutput, use the url
        elif isinstance(output, str):
            result_url = output  # If output is a string (for some reason due to their janky type hinting), use it directly
        else:
            result_url = (
                ""No output received""  # Fallback message if output is not as expected
            )

        return result_url
","import os
from enum import Enum
from typing import Literal
import replicate
from pydantic import SecretStr
from replicate.helpers import FileOutput

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

TEST_CREDENTIALS = APIKeyCredentials(
    id=""01234567-89ab-cdef-0123-456789abcdef"",
    provider=""replicate"",
    api_key=SecretStr(""mock-replicate-api-key""),
    title=""Mock Replicate API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.type,
}

class ReplicateFluxModelName(str, Enum):
    FLUX_SCHNELL = (""Flux Schnell"",)
    FLUX_PRO = (""Flux Pro"",)
    FLUX_PRO1_1 = (""Flux Pro 1.1"",)

    @property
    def api_name(self):
        api_names = {
            ReplicateFluxModelName.FLUX_SCHNELL: ""black-forest-labs/flux-schnell"",
            ReplicateFluxModelName.FLUX_PRO: ""black-forest-labs/flux-pro"",
            ReplicateFluxModelName.FLUX_PRO1_1: ""black-forest-labs/flux-1.1-pro"",
        }
        return api_names[self]

class ImageType(str, Enum):
    WEBP = ""webp""
    JPG = ""jpg""
    PNG = ""png""

class ReplicateFluxAdvancedModelBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[ProviderName.REPLICATE], Literal[""api_key""]
        ] = CredentialsField(
            description=""The Replicate integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        prompt: str = SchemaField(
            description=""Text prompt for image generation"",
            placeholder=""e.g., 'A futuristic cityscape at sunset'"",
            title=""Prompt"",
        )
        replicate_model_name: ReplicateFluxModelName = SchemaField(
            description=""The name of the Image Generation Model, i.e Flux Schnell"",
            default=ReplicateFluxModelName.FLUX_SCHNELL,
            title=""Image Generation Model"",
            advanced=False,
        )
        seed: int | None = SchemaField(
            description=""Random seed. Set for reproducible generation"",
            default=None,
            title=""Seed"",
        )
        steps: int = SchemaField(
            description=""Number of diffusion steps"",
            default=25,
            title=""Steps"",
        )
        guidance: float = SchemaField(
            description=(
                ""Controls the balance between adherence to the text prompt and image quality/diversity. ""
                ""Higher values make the output more closely match the prompt but may reduce overall image quality.""
            ),
            default=3,
            title=""Guidance"",
        )
        interval: float = SchemaField(
            description=(
                ""Interval is a setting that increases the variance in possible outputs. ""
                ""Setting this value low will ensure strong prompt following with more consistent outputs.""
            ),
            default=2,
            title=""Interval"",
        )
        aspect_ratio: str = SchemaField(
            description=""Aspect ratio for the generated image"",
            default=""1:1"",
            title=""Aspect Ratio"",
            placeholder=""Choose from: 1:1, 16:9, 2:3, 3:2, 4:5, 5:4, 9:16"",
        )
        output_format: ImageType = SchemaField(
            description=""File format of the output image"",
            default=ImageType.WEBP,
            title=""Output Format"",
        )
        output_quality: int = SchemaField(
            description=(
                ""Quality when saving the output images, from 0 to 100. ""
                ""Not relevant for .png outputs""
            ),
            default=80,
            title=""Output Quality"",
        )
        safety_tolerance: int = SchemaField(
            description=""Safety tolerance, 1 is most strict and 5 is most permissive"",
            default=2,
            title=""Safety Tolerance"",
        )

    class Output(BlockSchema):
        result: str = SchemaField(description=""Generated output"")
        error: str = SchemaField(description=""Error message if the model run failed"")

    def __init__(self):
        super().__init__(
            id=""90f8c45e-e983-4644-aa0b-b4ebe2f531bc"",
            description=""This block runs Flux models on Replicate with advanced settings."",
            categories={BlockCategory.AI},
            input_schema=ReplicateFluxAdvancedModelBlock.Input,
            output_schema=ReplicateFluxAdvancedModelBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""replicate_model_name"": ReplicateFluxModelName.FLUX_SCHNELL,
                ""prompt"": ""A beautiful landscape painting of a serene lake at sunrise"",
                ""seed"": None,
                ""steps"": 25,
                ""guidance"": 3.0,
                ""interval"": 2.0,
                ""aspect_ratio"": ""1:1"",
                ""output_format"": ImageType.PNG,
                ""output_quality"": 80,
                ""safety_tolerance"": 2,
            },
            test_output=[
                (
                    ""result"",
                    ""https://replicate.com/output/generated-image-url.jpg"",
                ),
            ],
            test_mock={
                ""run_model"": lambda api_key, model_name, prompt, seed, steps, guidance, interval, aspect_ratio, output_format, output_quality, safety_tolerance: ""https://replicate.com/output/generated-image-url.jpg"",
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # If the seed is not provided, generate a random seed
        seed = input_data.seed
        if seed is None:
            seed = int.from_bytes(os.urandom(4), ""big"")

        # Run the model using the provided inputs
        result = self.run_model(
            api_key=credentials.api_key,
            model_name=input_data.replicate_model_name.api_name,
            prompt=input_data.prompt,
            seed=seed,
            steps=input_data.steps,
            guidance=input_data.guidance,
            interval=input_data.interval,
            aspect_ratio=input_data.aspect_ratio,
            output_format=input_data.output_format,
            output_quality=input_data.output_quality,
            safety_tolerance=input_data.safety_tolerance,
        )
        yield ""result"", result

    def run_model(
        self,
        api_key: SecretStr,
        model_name,
        prompt,
        seed,
        steps,
        guidance,
        interval,
        aspect_ratio,
        output_format,
        output_quality,
        safety_tolerance,
    ):
        # Initialize Replicate client with the API key
        client = replicate.Client(api_token=api_key.get_secret_value())

        # Run the model with additional parameters
        output: FileOutput | list[FileOutput] = client.run(  # type: ignore This is because they changed the return type, and didn't update the type hint! It should be overloaded depending on the value of `use_file_output` to `FileOutput | list[FileOutput]` but it's `Any | Iterator[Any]`
            f""{model_name}"",
            input={
                ""prompt"": prompt,
                ""seed"": seed,
                ""steps"": steps,
                ""guidance"": guidance,
                ""interval"": interval,
                ""aspect_ratio"": aspect_ratio,
                ""output_format"": output_format,
                ""output_quality"": output_quality,
                ""safety_tolerance"": safety_tolerance,
            },
            wait=False,  # don't arbitrarily return data:octect/stream or sometimes url depending on the model???? what is this api
        )

        # Check if output is a list or a string and extract accordingly; otherwise, assign a default message
        if isinstance(output, list) and len(output) > 0:
            if isinstance(output[0], FileOutput):
                result_url = output[0].url  # If output is a list, get the first element
            else:
                result_url = output[
                    0
                ]  # If output is a list and not a FileOutput, get the first element. Should never happen, but just in case.
        elif isinstance(output, FileOutput):
            result_url = output.url  # If output is a FileOutput, use the url
        elif isinstance(output, str):
            result_url = output  # If output is a string (for some reason due to their janky type hinting), use it directly
        else:
            result_url = (
                ""No output received""  # Fallback message if output is not as expected
            )

        return result_url","{'final_score': 99.69, 'length_similarity': 99.6, 'token_similarity': 99.78, 'structure_similarity': 99.09, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
179,"from typing import List

from backend.data.block import BlockOutput, BlockSchema
from backend.data.model import APIKeyCredentials, SchemaField

from ._api import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    Filament,
    Slant3DCredentialsField,
    Slant3DCredentialsInput,
)
from .base import Slant3DBlockBase


class Slant3DFilamentBlock(Slant3DBlockBase):
    """"""Block for retrieving available filaments""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()

    class Output(BlockSchema):
        filaments: List[Filament] = SchemaField(
            description=""List of available filaments""
        )
        error: str = SchemaField(description=""Error message if request failed"")

    def __init__(self):
        super().__init__(
            id=""7cc416f4-f305-4606-9b3b-452b8a81031c"",
            description=""Get list of available filaments"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={""credentials"": TEST_CREDENTIALS_INPUT},
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""filaments"",
                    [
                        {
                            ""filament"": ""PLA BLACK"",
                            ""hexColor"": ""000000"",
                            ""colorTag"": ""black"",
                            ""profile"": ""PLA"",
                        },
                        {
                            ""filament"": ""PLA WHITE"",
                            ""hexColor"": ""ffffff"",
                            ""colorTag"": ""white"",
                            ""profile"": ""PLA"",
                        },
                    ],
                )
            ],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""filaments"": [
                        {
                            ""filament"": ""PLA BLACK"",
                            ""hexColor"": ""000000"",
                            ""colorTag"": ""black"",
                            ""profile"": ""PLA"",
                        },
                        {
                            ""filament"": ""PLA WHITE"",
                            ""hexColor"": ""ffffff"",
                            ""colorTag"": ""white"",
                            ""profile"": ""PLA"",
                        },
                    ]
                }
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            result = self._make_request(
                ""GET"", ""filament"", credentials.api_key.get_secret_value()
            )
            yield ""filaments"", result[""filaments""]
        except Exception as e:
            yield ""error"", str(e)
            raise
","from typing import List
from backend.data.block import BlockOutput, BlockSchema
from backend.data.model import APIKeyCredentials, SchemaField
from ._api import (
    TEST_CREDENTIALS,
    TEST_CREDENTIALS_INPUT,
    Filament,
    Slant3DCredentialsField,
    Slant3DCredentialsInput,
)
from .base import Slant3DBlockBase

class Slant3DFilamentBlock(Slant3DBlockBase):
    """"""Block for retrieving available filaments""""""

    class Input(BlockSchema):
        credentials: Slant3DCredentialsInput = Slant3DCredentialsField()

    class Output(BlockSchema):
        filaments: List[Filament] = SchemaField(
            description=""List of available filaments""
        )
        error: str = SchemaField(description=""Error message if request failed"")

    def __init__(self):
        super().__init__(
            id=""7cc416f4-f305-4606-9b3b-452b8a81031c"",
            description=""Get list of available filaments"",
            input_schema=self.Input,
            output_schema=self.Output,
            test_input={""credentials"": TEST_CREDENTIALS_INPUT},
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""filaments"",
                    [
                        {
                            ""filament"": ""PLA BLACK"",
                            ""hexColor"": ""000000"",
                            ""colorTag"": ""black"",
                            ""profile"": ""PLA"",
                        },
                        {
                            ""filament"": ""PLA WHITE"",
                            ""hexColor"": ""ffffff"",
                            ""colorTag"": ""white"",
                            ""profile"": ""PLA"",
                        },
                    ],
                )
            ],
            test_mock={
                ""_make_request"": lambda *args, **kwargs: {
                    ""filaments"": [
                        {
                            ""filament"": ""PLA BLACK"",
                            ""hexColor"": ""000000"",
                            ""colorTag"": ""black"",
                            ""profile"": ""PLA"",
                        },
                        {
                            ""filament"": ""PLA WHITE"",
                            ""hexColor"": ""ffffff"",
                            ""colorTag"": ""white"",
                            ""profile"": ""PLA"",
                        },
                    ]
                }
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            result = self._make_request(
                ""GET"", ""filament"", credentials.api_key.get_secret_value()
            )
            yield ""filaments"", result[""filaments""]
        except Exception as e:
            yield ""error"", str(e)
            raise","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
180,"import logging
from typing import Optional, cast

from autogpt_libs.auth.models import DEFAULT_USER_ID
from fastapi import HTTPException
from prisma import Json
from prisma.models import User

from backend.data.db import prisma
from backend.data.model import UserIntegrations, UserMetadata, UserMetadataRaw
from backend.util.encryption import JSONCryptor

logger = logging.getLogger(__name__)


async def get_or_create_user(user_data: dict) -> User:
    user_id = user_data.get(""sub"")
    if not user_id:
        raise HTTPException(status_code=401, detail=""User ID not found in token"")

    user_email = user_data.get(""email"")
    if not user_email:
        raise HTTPException(status_code=401, detail=""Email not found in token"")

    user = await prisma.user.find_unique(where={""id"": user_id})
    if not user:
        user = await prisma.user.create(
            data={
                ""id"": user_id,
                ""email"": user_email,
                ""name"": user_data.get(""user_metadata"", {}).get(""name""),
            }
        )
    return User.model_validate(user)


async def get_user_by_id(user_id: str) -> Optional[User]:
    user = await prisma.user.find_unique(where={""id"": user_id})
    return User.model_validate(user) if user else None


async def create_default_user() -> Optional[User]:
    user = await prisma.user.find_unique(where={""id"": DEFAULT_USER_ID})
    if not user:
        user = await prisma.user.create(
            data={
                ""id"": DEFAULT_USER_ID,
                ""email"": ""default@example.com"",
                ""name"": ""Default User"",
            }
        )
    return User.model_validate(user)


async def get_user_metadata(user_id: str) -> UserMetadata:
    user = await User.prisma().find_unique_or_raise(
        where={""id"": user_id},
    )

    metadata = cast(UserMetadataRaw, user.metadata)
    return UserMetadata.model_validate(metadata)


async def update_user_metadata(user_id: str, metadata: UserMetadata):
    await User.prisma().update(
        where={""id"": user_id},
        data={""metadata"": Json(metadata.model_dump())},
    )


async def get_user_integrations(user_id: str) -> UserIntegrations:
    user = await User.prisma().find_unique_or_raise(
        where={""id"": user_id},
    )

    encrypted_integrations = user.integrations
    if not encrypted_integrations:
        return UserIntegrations()
    else:
        return UserIntegrations.model_validate(
            JSONCryptor().decrypt(encrypted_integrations)
        )


async def update_user_integrations(user_id: str, data: UserIntegrations):
    encrypted_data = JSONCryptor().encrypt(data.model_dump())
    await User.prisma().update(
        where={""id"": user_id},
        data={""integrations"": encrypted_data},
    )


async def migrate_and_encrypt_user_integrations():
    """"""Migrate integration credentials and OAuth states from metadata to integrations column.""""""
    users = await User.prisma().find_many(
        where={
            ""metadata"": {
                ""path"": [""integration_credentials""],
                ""not"": Json({""a"": ""yolo""}),  # bogus value works to check if key exists
            }  # type: ignore
        }
    )
    logger.info(f""Migrating integration credentials for {len(users)} users"")

    for user in users:
        raw_metadata = cast(UserMetadataRaw, user.metadata)
        metadata = UserMetadata.model_validate(raw_metadata)

        # Get existing integrations data
        integrations = await get_user_integrations(user_id=user.id)

        # Copy credentials and oauth states from metadata if they exist
        if metadata.integration_credentials and not integrations.credentials:
            integrations.credentials = metadata.integration_credentials
        if metadata.integration_oauth_states:
            integrations.oauth_states = metadata.integration_oauth_states

        # Save to integrations column
        await update_user_integrations(user_id=user.id, data=integrations)

        # Remove from metadata
        raw_metadata = dict(raw_metadata)
        raw_metadata.pop(""integration_credentials"", None)
        raw_metadata.pop(""integration_oauth_states"", None)

        # Update metadata without integration data
        await User.prisma().update(
            where={""id"": user.id},
            data={""metadata"": Json(raw_metadata)},
        )
","import logging
from typing import Optional, cast
from autogpt_libs.auth.models import DEFAULT_USER_ID
from fastapi import HTTPException
from prisma import Json
from prisma.models import User
from backend.data.db import prisma
from backend.data.model import UserIntegrations, UserMetadata, UserMetadataRaw
from backend.util.encryption import JSONCryptor

logger = logging.getLogger(__name__)

async def get_or_create_user(user_data: dict) -> User:
    user_id = user_data.get(""sub"")
    if not user_id:
        raise HTTPException(status_code=401, detail=""User ID not found in token"")

    user_email = user_data.get(""email"")
    if not user_email:
        raise HTTPException(status_code=401, detail=""Email not found in token"")

    user = await prisma.user.find_unique(where={""id"": user_id})
    if not user:
        user = await prisma.user.create(
            data={
                ""id"": user_id,
                ""email"": user_email,
                ""name"": user_data.get(""user_metadata"", {}).get(""name""),
            }
        )
    return User.model_validate(user)

async def get_user_by_id(user_id: str) -> Optional[User]:
    user = await prisma.user.find_unique(where={""id"": user_id})
    return User.model_validate(user) if user else None

async def create_default_user() -> Optional[User]:
    user = await prisma.user.find_unique(where={""id"": DEFAULT_USER_ID})
    if not user:
        user = await prisma.user.create(
            data={
                ""id"": DEFAULT_USER_ID,
                ""email"": ""default@example.com"",
                ""name"": ""Default User"",
            }
        )
    return User.model_validate(user)

async def get_user_metadata(user_id: str) -> UserMetadata:
    user = await User.prisma().find_unique_or_raise(
        where={""id"": user_id},
    )
    metadata = cast(UserMetadataRaw, user.metadata)
    return UserMetadata.model_validate(metadata)

async def update_user_metadata(user_id: str, metadata: UserMetadata):
    await User.prisma().update(
        where={""id"": user_id},
        data={""metadata"": Json(metadata.model_dump())},
    )

async def get_user_integrations(user_id: str) -> UserIntegrations:
    user = await User.prisma().find_unique_or_raise(
        where={""id"": user_id},
    )
    encrypted_integrations = user.integrations
    if not encrypted_integrations:
        return UserIntegrations()
    else:
        return UserIntegrations.model_validate(
            JSONCryptor().decrypt(encrypted_integrations)
        )

async def update_user_integrations(user_id: str, data: UserIntegrations):
    encrypted_data = JSONCryptor().encrypt(data.model_dump())
    await User.prisma().update(
        where={""id"": user_id},
        data={""integrations"": encrypted_data},
    )

async def migrate_and_encrypt_user_integrations():
    """"""Migrate integration credentials and OAuth states from metadata to integrations column.""""""
    users = await User.prisma().find_many(
        where={
            ""metadata"": {
                ""path"": [""integration_credentials""],
                ""not"": Json({""a"": ""yolo""}),  # bogus value works to check if key exists
            }  # type: ignore
        }
    )
    logger.info(f""Migrating integration credentials for {len(users)} users"")
    for user in users:
        raw_metadata = cast(UserMetadataRaw, user.metadata)
        metadata = UserMetadata.model_validate(raw_metadata)

        # Get existing integrations data
        integrations = await get_user_integrations(user_id=user.id)

        # Copy credentials and oauth states from metadata if they exist
        if metadata.integration_credentials and not integrations.credentials:
            integrations.credentials = metadata.integration_credentials
        if metadata.integration_oauth_states:
            integrations.oauth_states = metadata.integration_oauth_states

        # Save to integrations column
        await update_user_integrations(user_id=user.id, data=integrations)

        # Remove from metadata
        raw_metadata = dict(raw_metadata)
        raw_metadata.pop(""integration_credentials"", None)
        raw_metadata.pop(""integration_oauth_states"", None)

        # Update metadata without integration data
        await User.prisma().update(
            where={""id"": user.id},
            data={""metadata"": Json(raw_metadata)},
        )","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
181,"import logging
import secrets
from abc import ABC, abstractmethod
from typing import ClassVar, Generic, Optional, TypeVar
from uuid import uuid4

from fastapi import Request
from strenum import StrEnum

from backend.data import integrations
from backend.data.model import Credentials
from backend.integrations.providers import ProviderName
from backend.integrations.webhooks.utils import webhook_ingress_url
from backend.util.exceptions import MissingConfigError
from backend.util.settings import Config

logger = logging.getLogger(__name__)
app_config = Config()

WT = TypeVar(""WT"", bound=StrEnum)


class BaseWebhooksManager(ABC, Generic[WT]):
    # --8<-- [start:BaseWebhooksManager1]
    PROVIDER_NAME: ClassVar[ProviderName]
    # --8<-- [end:BaseWebhooksManager1]

    WebhookType: WT

    async def get_suitable_auto_webhook(
        self,
        user_id: str,
        credentials: Credentials,
        webhook_type: WT,
        resource: str,
        events: list[str],
    ) -> integrations.Webhook:
        if not app_config.platform_base_url:
            raise MissingConfigError(
                ""PLATFORM_BASE_URL must be set to use Webhook functionality""
            )

        if webhook := await integrations.find_webhook_by_credentials_and_props(
            credentials.id, webhook_type, resource, events
        ):
            return webhook
        return await self._create_webhook(
            user_id, webhook_type, events, resource, credentials
        )

    async def get_manual_webhook(
        self,
        user_id: str,
        graph_id: str,
        webhook_type: WT,
        events: list[str],
    ):
        if current_webhook := await integrations.find_webhook_by_graph_and_props(
            graph_id, self.PROVIDER_NAME, webhook_type, events
        ):
            return current_webhook
        return await self._create_webhook(
            user_id,
            webhook_type,
            events,
            register=False,
        )

    async def prune_webhook_if_dangling(
        self, webhook_id: str, credentials: Optional[Credentials]
    ) -> bool:
        webhook = await integrations.get_webhook(webhook_id)
        if webhook.attached_nodes is None:
            raise ValueError(""Error retrieving webhook including attached nodes"")
        if webhook.attached_nodes:
            # Don't prune webhook if in use
            return False

        if credentials:
            await self._deregister_webhook(webhook, credentials)
        await integrations.delete_webhook(webhook.id)
        return True

    # --8<-- [start:BaseWebhooksManager3]
    @classmethod
    @abstractmethod
    async def validate_payload(
        cls, webhook: integrations.Webhook, request: Request
    ) -> tuple[dict, str]:
        """"""
        Validates an incoming webhook request and returns its payload and type.

        Params:
            webhook: Object representing the configured webhook and its properties in our system.
            request: Incoming FastAPI `Request`

        Returns:
            dict: The validated payload
            str: The event type associated with the payload
        """"""

    # --8<-- [end:BaseWebhooksManager3]

    # --8<-- [start:BaseWebhooksManager5]
    async def trigger_ping(
        self, webhook: integrations.Webhook, credentials: Credentials | None
    ) -> None:
        """"""
        Triggers a ping to the given webhook.

        Raises:
            NotImplementedError: if the provider doesn't support pinging
        """"""
        # --8<-- [end:BaseWebhooksManager5]
        raise NotImplementedError(f""{self.__class__.__name__} doesn't support pinging"")

    # --8<-- [start:BaseWebhooksManager2]
    @abstractmethod
    async def _register_webhook(
        self,
        credentials: Credentials,
        webhook_type: WT,
        resource: str,
        events: list[str],
        ingress_url: str,
        secret: str,
    ) -> tuple[str, dict]:
        """"""
        Registers a new webhook with the provider.

        Params:
            credentials: The credentials with which to create the webhook
            webhook_type: The provider-specific webhook type to create
            resource: The resource to receive events for
            events: The events to subscribe to
            ingress_url: The ingress URL for webhook payloads
            secret: Secret used to verify webhook payloads

        Returns:
            str: Webhook ID assigned by the provider
            config: Provider-specific configuration for the webhook
        """"""
        ...

    # --8<-- [end:BaseWebhooksManager2]

    # --8<-- [start:BaseWebhooksManager4]
    @abstractmethod
    async def _deregister_webhook(
        self, webhook: integrations.Webhook, credentials: Credentials
    ) -> None: ...

    # --8<-- [end:BaseWebhooksManager4]

    async def _create_webhook(
        self,
        user_id: str,
        webhook_type: WT,
        events: list[str],
        resource: str = """",
        credentials: Optional[Credentials] = None,
        register: bool = True,
    ) -> integrations.Webhook:
        if not app_config.platform_base_url:
            raise MissingConfigError(
                ""PLATFORM_BASE_URL must be set to use Webhook functionality""
            )

        id = str(uuid4())
        secret = secrets.token_hex(32)
        provider_name = self.PROVIDER_NAME
        ingress_url = webhook_ingress_url(provider_name=provider_name, webhook_id=id)
        if register:
            if not credentials:
                raise TypeError(""credentials are required if register = True"")
            provider_webhook_id, config = await self._register_webhook(
                credentials, webhook_type, resource, events, ingress_url, secret
            )
        else:
            provider_webhook_id, config = """", {}

        return await integrations.create_webhook(
            integrations.Webhook(
                id=id,
                user_id=user_id,
                provider=provider_name,
                credentials_id=credentials.id if credentials else """",
                webhook_type=webhook_type,
                resource=resource,
                events=events,
                provider_webhook_id=provider_webhook_id,
                config=config,
                secret=secret,
            )
        )
","import logging
import secrets
from abc import ABC, abstractmethod
from typing import ClassVar, Generic, Optional, TypeVar
from uuid import uuid4

from fastapi import Request
from strenum import StrEnum

from backend.data import integrations
from backend.data.model import Credentials
from backend.integrations.providers import ProviderName
from backend.integrations.webhooks.utils import webhook_ingress_url
from backend.util.exceptions import MissingConfigError
from backend.util.settings import Config

logger = logging.getLogger(__name__)
app_config = Config()

WT = TypeVar(""WT"", bound=StrEnum)

class BaseWebhooksManager(ABC, Generic[WT]):
    # --8<-- [start:BaseWebhooksManager1]
    PROVIDER_NAME: ClassVar[ProviderName]
    # --8<-- [end:BaseWebhooksManager1]

    WebhookType: WT

    async def get_suitable_auto_webhook(
        self,
        user_id: str,
        credentials: Credentials,
        webhook_type: WT,
        resource: str,
        events: list[str],
    ) -> integrations.Webhook:
        if not app_config.platform_base_url:
            raise MissingConfigError(
                ""PLATFORM_BASE_URL must be set to use Webhook functionality""
            )
        if webhook := await integrations.find_webhook_by_credentials_and_props(
            credentials.id, webhook_type, resource, events
        ):
            return webhook
        return await self._create_webhook(
            user_id, webhook_type, events, resource, credentials
        )

    async def get_manual_webhook(
        self,
        user_id: str,
        graph_id: str,
        webhook_type: WT,
        events: list[str],
    ):
        if current_webhook := await integrations.find_webhook_by_graph_and_props(
            graph_id, self.PROVIDER_NAME, webhook_type, events
        ):
            return current_webhook
        return await self._create_webhook(
            user_id,
            webhook_type,
            events,
            register=False,
        )

    async def prune_webhook_if_dangling(
        self, webhook_id: str, credentials: Optional[Credentials]
    ) -> bool:
        webhook = await integrations.get_webhook(webhook_id)
        if webhook.attached_nodes is None:
            raise ValueError(""Error retrieving webhook including attached nodes"")
        if webhook.attached_nodes:
            # Don't prune webhook if in use
            return False
        if credentials:
            await self._deregister_webhook(webhook, credentials)
        await integrations.delete_webhook(webhook.id)
        return True

    # --8<-- [start:BaseWebhooksManager3]
    @classmethod
    @abstractmethod
    async def validate_payload(
        cls, webhook: integrations.Webhook, request: Request
    ) -> tuple[dict, str]:
        """"""
        Validates an incoming webhook request and returns its payload and type.

        Params:
            webhook: Object representing the configured webhook and its properties in our system.
            request: Incoming FastAPI `Request`

        Returns:
            dict: The validated payload
            str: The event type associated with the payload
        """"""

    # --8<-- [end:BaseWebhooksManager3]

    # --8<-- [start:BaseWebhooksManager5]
    async def trigger_ping(
        self, webhook: integrations.Webhook, credentials: Credentials | None
    ) -> None:
        """"""
        Triggers a ping to the given webhook.

        Raises:
            NotImplementedError: if the provider doesn't support pinging
        """"""
    # --8<-- [end:BaseWebhooksManager5]
        raise NotImplementedError(f""{self.__class__.__name__} doesn't support pinging"")

    # --8<-- [start:BaseWebhooksManager2]
    @abstractmethod
    async def _register_webhook(
        self,
        credentials: Credentials,
        webhook_type: WT,
        resource: str,
        events: list[str],
        ingress_url: str,
        secret: str,
    ) -> tuple[str, dict]:
        """"""
        Registers a new webhook with the provider.

        Params:
            credentials: The credentials with which to create the webhook
            webhook_type: The provider-specific webhook type to create
            resource: The resource to receive events for
            events: The events to subscribe to
            ingress_url: The ingress URL for webhook payloads
            secret: Secret used to verify webhook payloads

        Returns:
            str: Webhook ID assigned by the provider
            config: Provider-specific configuration for the webhook
        """"""
        ...

    # --8<-- [end:BaseWebhooksManager2]

    # --8<-- [start:BaseWebhooksManager4]
    @abstractmethod
    async def _deregister_webhook(
        self, webhook: integrations.Webhook, credentials: Credentials
    ) -> None: ...

    # --8<-- [end:BaseWebhooksManager4]

    async def _create_webhook(
        self,
        user_id: str,
        webhook_type: WT,
        events: list[str],
        resource: str = """",
        credentials: Optional[Credentials] = None,
        register: bool = True,
    ) -> integrations.Webhook:
        if not app_config.platform_base_url:
            raise MissingConfigError(
                ""PLATFORM_BASE_URL must be set to use Webhook functionality""
            )
        id = str(uuid4())
        secret = secrets.token_hex(32)
        provider_name = self.PROVIDER_NAME
        ingress_url = webhook_ingress_url(provider_name=provider_name, webhook_id=id)

        if register:
            if not credentials:
                raise TypeError(""credentials are required if register = True"")
            provider_webhook_id, config = await self._register_webhook(
                credentials, webhook_type, resource, events, ingress_url, secret
            )
        else:
            provider_webhook_id, config = """", {}

        return await integrations.create_webhook(
            integrations.Webhook(
                id=id,
                user_id=user_id,
                provider=provider_name,
                credentials_id=credentials.id if credentials else """",
                webhook_type=webhook_type,
                resource=resource,
                events=events,
                provider_webhook_id=provider_webhook_id,
                config=config,
                secret=secret,
            )
        )","{'final_score': 99.87, 'length_similarity': 99.94, 'token_similarity': 100.0, 'structure_similarity': 99.4, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
182,"# -*- coding: utf-8 -*-

from enum import Enum


class DefaultCategories(Enum):

    HOUSING = 0
    FOOD = 1
    GAS = 2
    SHOPPING = 3
    # ...


seller_category_map = {}
seller_category_map['Exxon'] = DefaultCategories.GAS
seller_category_map['Target'] = DefaultCategories.SHOPPING


class Categorizer(object):

    def __init__(self, seller_category_map, seller_category_overrides_map):
        self.seller_category_map = seller_category_map
        self.seller_category_overrides_map = seller_category_overrides_map

    def categorize(self, transaction):
        if transaction.seller in self.seller_category_map:
            return self.seller_category_map[transaction.seller]
        if transaction.seller in self.seller_category_overrides_map:
            seller_category_map[transaction.seller] = \
                self.manual_overrides[transaction.seller].peek_min()
            return self.seller_category_map[transaction.seller]
        return None


class Transaction(object):

    def __init__(self, timestamp, seller, amount):
        self.timestamp = timestamp
        self.seller = seller
        self.amount = amount


class Budget(object):

    def __init__(self, template_categories_to_budget_map):
        self.categories_to_budget_map = template_categories_to_budget_map

    def override_category_budget(self, category, amount):
        self.categories_to_budget_map[category] = amount
","# -*- coding: utf-8 -*-
from enum import Enum

class DefaultCategories(Enum):
    HOUSING = 0
    FOOD = 1
    GAS = 2
    SHOPPING = 3
    # ...

seller_category_map = {}
seller_category_map['Exxon'] = DefaultCategories.GAS
seller_category_map['Target'] = DefaultCategories.SHOPPING

class Categorizer(object):

    def __init__(self, seller_category_map, seller_category_overrides_map):
        self.seller_category_map = seller_category_map
        self.seller_category_overrides_map = seller_category_overrides_map

    def categorize(self, transaction):
        if transaction.seller in self.seller_category_map:
            return self.seller_category_map[transaction.seller]
        if transaction.seller in self.seller_category_overrides_map:
            seller_category_map[transaction.seller] = \
                self.seller_category_overrides_map[transaction.seller].peek_min()
            return self.seller_category_map[transaction.seller]
        return None

class Transaction(object):

    def __init__(self, timestamp, seller, amount):
        self.timestamp = timestamp
        self.seller = seller
        self.amount = amount

class Budget(object):

    def __init__(self, template_categories_to_budget_map):
        self.categories_to_budget_map = template_categories_to_budget_map

    def override_category_budget(self, category, amount):
        self.categories_to_budget_map[category] = amount","{'final_score': 98.98, 'length_similarity': 99.08, 'token_similarity': 97.06, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
183,"class Item(object):

    def __init__(self, key, value):
        self.key = key
        self.value = value


class HashTable(object):

    def __init__(self, size):
        self.size = size
        self.table = [[] for _ in range(self.size)]

    def _hash_function(self, key):
        return key % self.size

    def set(self, key, value):
        hash_index = self._hash_function(key)
        for item in self.table[hash_index]:
            if item.key == key:
                item.value = value
                return
        self.table[hash_index].append(Item(key, value))

    def get(self, key):
        hash_index = self._hash_function(key)
        for item in self.table[hash_index]:
            if item.key == key:
                return item.value
        raise KeyError('Key not found')

    def remove(self, key):
        hash_index = self._hash_function(key)
        for index, item in enumerate(self.table[hash_index]):
            if item.key == key:
                del self.table[hash_index][index]
                return
        raise KeyError('Key not found')
","class Item(object):

    def __init__(self, key, value):
        self.key = key
        self.value = value


class HashTable(object):

    def __init__(self, size):
        self.size = size
        self.table = [[] for _ in range(self.size)]

    def _hash_function(self, key):
        return key % self.size

    def set(self, key, value):
        hash_index = self._hash_function(key)
        for item in self.table[hash_index]:
            if item.key == key:
                item.value = value
                return
        self.table[hash_index].append(Item(key, value))

    def get(self, key):
        hash_index = self._hash_function(key)
        for item in self.table[hash_index]:
            if item.key == key:
                return item.value
        raise KeyError('Key not found')

    def remove(self, key):
        hash_index = self._hash_function(key)
        for index, item in enumerate(self.table[hash_index]):
            if item.key == key:
                del self.table[hash_index][index]
                return
        raise KeyError('Key not found')","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
184,"from typing import Dict, Set

from fastapi import WebSocket

from backend.data import execution
from backend.server.model import Methods, WsMessage


class ConnectionManager:
    def __init__(self):
        self.active_connections: Set[WebSocket] = set()
        self.subscriptions: Dict[str, Set[WebSocket]] = {}

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.add(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        for subscribers in self.subscriptions.values():
            subscribers.discard(websocket)

    async def subscribe(self, graph_id: str, websocket: WebSocket):
        if graph_id not in self.subscriptions:
            self.subscriptions[graph_id] = set()
        self.subscriptions[graph_id].add(websocket)

    async def unsubscribe(self, graph_id: str, websocket: WebSocket):
        if graph_id in self.subscriptions:
            self.subscriptions[graph_id].discard(websocket)
            if not self.subscriptions[graph_id]:
                del self.subscriptions[graph_id]

    async def send_execution_result(self, result: execution.ExecutionResult):
        graph_id = result.graph_id
        if graph_id in self.subscriptions:
            message = WsMessage(
                method=Methods.EXECUTION_EVENT,
                channel=graph_id,
                data=result.model_dump(),
            ).model_dump_json()
            for connection in self.subscriptions[graph_id]:
                await connection.send_text(message)
","from typing import Dict, Set
from fastapi import WebSocket
from backend.data import execution
from backend.server.model import Methods, WsMessage


class ConnectionManager:
    def __init__(self):
        self.active_connections: Set[WebSocket] = set()
        self.subscriptions: Dict[str, Set[WebSocket]] = {}

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.add(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        for subscribers in self.subscriptions.values():
            subscribers.discard(websocket)

    async def subscribe(self, graph_id: str, websocket: WebSocket):
        if graph_id not in self.subscriptions:
            self.subscriptions[graph_id] = set()
        self.subscriptions[graph_id].add(websocket)

    async def unsubscribe(self, graph_id: str, websocket: WebSocket):
        if graph_id in self.subscriptions:
            self.subscriptions[graph_id].discard(websocket)
            if not self.subscriptions[graph_id]:
                del self.subscriptions[graph_id]

    async def send_execution_result(self, result: execution.ExecutionResult):
        graph_id = result.graph_id
        if graph_id in self.subscriptions:
            message = WsMessage(
                method=Methods.EXECUTION_EVENT,
                channel=graph_id,
                data=result.model_dump(),
            ).model_dump_json()
            for connection in self.subscriptions[graph_id]:
                await connection.send_text(message)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
185,"""""""
In this problem, we want to determine all possible combinations of k
numbers out of 1 ... n. We use backtracking to solve this problem.

Time complexity: O(C(n,k)) which is O(n choose k) = O((n!/(k! * (n - k)!))),
""""""

from __future__ import annotations

from itertools import combinations


def combination_lists(n: int, k: int) -> list[list[int]]:
    """"""
    >>> combination_lists(n=4, k=2)
    [[1, 2], [1, 3], [1, 4], [2, 3], [2, 4], [3, 4]]
    """"""
    return [list(x) for x in combinations(range(1, n + 1), k)]


def generate_all_combinations(n: int, k: int) -> list[list[int]]:
    """"""
    >>> generate_all_combinations(n=4, k=2)
    [[1, 2], [1, 3], [1, 4], [2, 3], [2, 4], [3, 4]]
    >>> generate_all_combinations(n=0, k=0)
    [[]]
    >>> generate_all_combinations(n=10, k=-1)
    Traceback (most recent call last):
        ...
    ValueError: k must not be negative
    >>> generate_all_combinations(n=-1, k=10)
    Traceback (most recent call last):
        ...
    ValueError: n must not be negative
    >>> generate_all_combinations(n=5, k=4)
    [[1, 2, 3, 4], [1, 2, 3, 5], [1, 2, 4, 5], [1, 3, 4, 5], [2, 3, 4, 5]]
    >>> from itertools import combinations
    >>> all(generate_all_combinations(n, k) == combination_lists(n, k)
    ...     for n in range(1, 6) for k in range(1, 6))
    True
    """"""
    if k < 0:
        raise ValueError(""k must not be negative"")
    if n < 0:
        raise ValueError(""n must not be negative"")

    result: list[list[int]] = []
    create_all_state(1, n, k, [], result)
    return result


def create_all_state(
    increment: int,
    total_number: int,
    level: int,
    current_list: list[int],
    total_list: list[list[int]],
) -> None:
    if level == 0:
        total_list.append(current_list[:])
        return

    for i in range(increment, total_number - level + 2):
        current_list.append(i)
        create_all_state(i + 1, total_number, level - 1, current_list, total_list)
        current_list.pop()


if __name__ == ""__main__"":
    from doctest import testmod

    testmod()
    print(generate_all_combinations(n=4, k=2))
    tests = ((n, k) for n in range(1, 5) for k in range(1, 5))
    for n, k in tests:
        print(n, k, generate_all_combinations(n, k) == combination_lists(n, k))

    print(""Benchmark:"")
    from timeit import timeit

    for func in (""combination_lists"", ""generate_all_combinations""):
        print(f""{func:>25}(): {timeit(f'{func}(n=4, k = 2)', globals=globals())}"")
","""""""
In this problem, we want to determine all possible combinations of k
numbers out of 1 ... n. We use backtracking to solve this problem.

Time complexity: O(C(n,k)) which is O(n choose k) = O((n!/(k! * (n - k)!))),
""""""

from __future__ import annotations
from itertools import combinations

def combination_lists(n: int, k: int) -> list[list[int]]:
    """"""
    >>> combination_lists(n=4, k=2)
    [[1, 2], [1, 3], [1, 4], [2, 3], [2, 4], [3, 4]]
    """"""
    return [list(x) for x in combinations(range(1, n + 1), k)]

def generate_all_combinations(n: int, k: int) -> list[list[int]]:
    """"""
    >>> generate_all_combinations(n=4, k=2)
    [[1, 2], [1, 3], [1, 4], [2, 3], [2, 4], [3, 4]]
    >>> generate_all_combinations(n=0, k=0)
    [[]]
    >>> generate_all_combinations(n=10, k=-1)
    Traceback (most recent call last):
        ...
    ValueError: k must not be negative
    >>> generate_all_combinations(n=-1, k=10)
    Traceback (most recent call last):
        ...
    ValueError: n must not be negative
    >>> generate_all_combinations(n=5, k=4)
    [[1, 2, 3, 4], [1, 2, 3, 5], [1, 2, 4, 5], [1, 3, 4, 5], [2, 3, 4, 5]]
    >>> from itertools import combinations
    >>> all(generate_all_combinations(n, k) == combination_lists(n, k)
    ...     for n in range(1, 6) for k in range(1, 6))
    True
    """"""
    if k < 0:
        raise ValueError(""k must not be negative"")
    if n < 0:
        raise ValueError(""n must not be negative"")
    result: list[list[int]] = []
    create_all_state(1, n, k, [], result)
    return result

def create_all_state(
    increment: int,
    total_number: int,
    level: int,
    current_list: list[int],
    total_list: list[list[int]],
) -> None:
    if level == 0:
        total_list.append(current_list[:])
        return
    for i in range(increment, total_number - level + 2):
        current_list.append(i)
        create_all_state(i + 1, total_number, level - 1, current_list, total_list)
        current_list.pop()

if __name__ == ""__main__"":
    from doctest import testmod

    testmod()
    print(generate_all_combinations(n=4, k=2))
    tests = ((n, k) for n in range(1, 5) for k in range(1, 5))
    for n, k in tests:
        print(n, k, generate_all_combinations(n, k) == combination_lists(n, k))
    print(""Benchmark:"")
    from timeit import timeit
    for func in (""combination_lists"", ""generate_all_combinations""):
        print(f""{func:>25}(): {timeit(f'{func}(n=4, k = 2)', globals=globals())}"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
186,"import ast
import logging
from enum import Enum, EnumMeta
from json import JSONDecodeError
from types import MappingProxyType
from typing import TYPE_CHECKING, Any, List, Literal, NamedTuple

from pydantic import SecretStr

from backend.integrations.providers import ProviderName

if TYPE_CHECKING:
    from enum import _EnumMemberT

import anthropic
import ollama
import openai
from groq import Groq

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.util import json
from backend.util.settings import BehaveAs, Settings

logger = logging.getLogger(__name__)

LLMProviderName = Literal[
    ProviderName.ANTHROPIC,
    ProviderName.GROQ,
    ProviderName.OLLAMA,
    ProviderName.OPENAI,
    ProviderName.OPEN_ROUTER,
]
AICredentials = CredentialsMetaInput[LLMProviderName, Literal[""api_key""]]

TEST_CREDENTIALS = APIKeyCredentials(
    id=""ed55ac19-356e-4243-a6cb-bc599e9b716f"",
    provider=""openai"",
    api_key=SecretStr(""mock-openai-api-key""),
    title=""Mock OpenAI API key"",
    expires_at=None,
)
TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.title,
}


def AICredentialsField() -> AICredentials:
    return CredentialsField(
        description=""API key for the LLM provider."",
        discriminator=""model"",
        discriminator_mapping={
            model.value: model.metadata.provider for model in LlmModel
        },
    )


class ModelMetadata(NamedTuple):
    provider: str
    context_window: int


class LlmModelMeta(EnumMeta):
    @property
    def __members__(
        self: type[""_EnumMemberT""],
    ) -> MappingProxyType[str, ""_EnumMemberT""]:
        if Settings().config.behave_as == BehaveAs.LOCAL:
            members = super().__members__
            return members
        else:
            removed_providers = [""ollama""]
            existing_members = super().__members__
            members = {
                name: member
                for name, member in existing_members.items()
                if LlmModel[name].provider not in removed_providers
            }
            return MappingProxyType(members)


class LlmModel(str, Enum, metaclass=LlmModelMeta):
    # OpenAI models
    O1_PREVIEW = ""o1-preview""
    O1_MINI = ""o1-mini""
    GPT4O_MINI = ""gpt-4o-mini""
    GPT4O = ""gpt-4o""
    GPT4_TURBO = ""gpt-4-turbo""
    GPT3_5_TURBO = ""gpt-3.5-turbo""
    # Anthropic models
    CLAUDE_3_5_SONNET = ""claude-3-5-sonnet-latest""
    CLAUDE_3_HAIKU = ""claude-3-haiku-20240307""
    # Groq models
    LLAMA3_8B = ""llama3-8b-8192""
    LLAMA3_70B = ""llama3-70b-8192""
    MIXTRAL_8X7B = ""mixtral-8x7b-32768""
    GEMMA_7B = ""gemma-7b-it""
    GEMMA2_9B = ""gemma2-9b-it""
    # New Groq models (Preview)
    LLAMA3_1_405B = ""llama-3.1-405b-reasoning""
    LLAMA3_1_70B = ""llama-3.1-70b-versatile""
    LLAMA3_1_8B = ""llama-3.1-8b-instant""
    # Ollama models
    OLLAMA_LLAMA3_8B = ""llama3""
    OLLAMA_LLAMA3_405B = ""llama3.1:405b""
    OLLAMA_DOLPHIN = ""dolphin-mistral:latest""
    # OpenRouter models
    GEMINI_FLASH_1_5_8B = ""google/gemini-flash-1.5""
    GROK_BETA = ""x-ai/grok-beta""
    MISTRAL_NEMO = ""mistralai/mistral-nemo""
    COHERE_COMMAND_R_08_2024 = ""cohere/command-r-08-2024""
    COHERE_COMMAND_R_PLUS_08_2024 = ""cohere/command-r-plus-08-2024""
    EVA_QWEN_2_5_32B = ""eva-unit-01/eva-qwen-2.5-32b""
    DEEPSEEK_CHAT = ""deepseek/deepseek-chat""
    PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE = (
        ""perplexity/llama-3.1-sonar-large-128k-online""
    )
    QWEN_QWQ_32B_PREVIEW = ""qwen/qwq-32b-preview""
    NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B = ""nousresearch/hermes-3-llama-3.1-405b""
    NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B = ""nousresearch/hermes-3-llama-3.1-70b""
    AMAZON_NOVA_LITE_V1 = ""amazon/nova-lite-v1""
    AMAZON_NOVA_MICRO_V1 = ""amazon/nova-micro-v1""
    AMAZON_NOVA_PRO_V1 = ""amazon/nova-pro-v1""
    MICROSOFT_WIZARDLM_2_8X22B = ""microsoft/wizardlm-2-8x22b""
    GRYPHE_MYTHOMAX_L2_13B = ""gryphe/mythomax-l2-13b""

    @property
    def metadata(self) -> ModelMetadata:
        return MODEL_METADATA[self]

    @property
    def provider(self) -> str:
        return self.metadata.provider

    @property
    def context_window(self) -> int:
        return self.metadata.context_window


MODEL_METADATA = {
    LlmModel.O1_PREVIEW: ModelMetadata(""openai"", 32000),
    LlmModel.O1_MINI: ModelMetadata(""openai"", 62000),
    LlmModel.GPT4O_MINI: ModelMetadata(""openai"", 128000),
    LlmModel.GPT4O: ModelMetadata(""openai"", 128000),
    LlmModel.GPT4_TURBO: ModelMetadata(""openai"", 128000),
    LlmModel.GPT3_5_TURBO: ModelMetadata(""openai"", 16385),
    LlmModel.CLAUDE_3_5_SONNET: ModelMetadata(""anthropic"", 200000),
    LlmModel.CLAUDE_3_HAIKU: ModelMetadata(""anthropic"", 200000),
    LlmModel.LLAMA3_8B: ModelMetadata(""groq"", 8192),
    LlmModel.LLAMA3_70B: ModelMetadata(""groq"", 8192),
    LlmModel.MIXTRAL_8X7B: ModelMetadata(""groq"", 32768),
    LlmModel.GEMMA_7B: ModelMetadata(""groq"", 8192),
    LlmModel.GEMMA2_9B: ModelMetadata(""groq"", 8192),
    LlmModel.LLAMA3_1_405B: ModelMetadata(""groq"", 8192),
    # Limited to 16k during preview
    LlmModel.LLAMA3_1_70B: ModelMetadata(""groq"", 131072),
    LlmModel.LLAMA3_1_8B: ModelMetadata(""groq"", 131072),
    LlmModel.OLLAMA_LLAMA3_8B: ModelMetadata(""ollama"", 8192),
    LlmModel.OLLAMA_LLAMA3_405B: ModelMetadata(""ollama"", 8192),
    LlmModel.OLLAMA_DOLPHIN: ModelMetadata(""ollama"", 32768),
    LlmModel.GEMINI_FLASH_1_5_8B: ModelMetadata(""open_router"", 8192),
    LlmModel.GROK_BETA: ModelMetadata(""open_router"", 8192),
    LlmModel.MISTRAL_NEMO: ModelMetadata(""open_router"", 4000),
    LlmModel.COHERE_COMMAND_R_08_2024: ModelMetadata(""open_router"", 4000),
    LlmModel.COHERE_COMMAND_R_PLUS_08_2024: ModelMetadata(""open_router"", 4000),
    LlmModel.EVA_QWEN_2_5_32B: ModelMetadata(""open_router"", 4000),
    LlmModel.DEEPSEEK_CHAT: ModelMetadata(""open_router"", 8192),
    LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: ModelMetadata(
        ""open_router"", 8192
    ),
    LlmModel.QWEN_QWQ_32B_PREVIEW: ModelMetadata(""open_router"", 4000),
    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: ModelMetadata(""open_router"", 4000),
    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: ModelMetadata(""open_router"", 4000),
    LlmModel.AMAZON_NOVA_LITE_V1: ModelMetadata(""open_router"", 4000),
    LlmModel.AMAZON_NOVA_MICRO_V1: ModelMetadata(""open_router"", 4000),
    LlmModel.AMAZON_NOVA_PRO_V1: ModelMetadata(""open_router"", 4000),
    LlmModel.MICROSOFT_WIZARDLM_2_8X22B: ModelMetadata(""open_router"", 4000),
    LlmModel.GRYPHE_MYTHOMAX_L2_13B: ModelMetadata(""open_router"", 4000),
}

for model in LlmModel:
    if model not in MODEL_METADATA:
        raise ValueError(f""Missing MODEL_METADATA metadata for model: {model}"")


class MessageRole(str, Enum):
    SYSTEM = ""system""
    USER = ""user""
    ASSISTANT = ""assistant""


class Message(BlockSchema):
    role: MessageRole
    content: str


class AIStructuredResponseGeneratorBlock(Block):
    class Input(BlockSchema):
        prompt: str = SchemaField(
            description=""The prompt to send to the language model."",
            placeholder=""Enter your prompt here..."",
        )
        expected_format: dict[str, str] = SchemaField(
            description=""Expected format of the response. If provided, the response will be validated against this format. ""
            ""The keys should be the expected fields in the response, and the values should be the description of the field."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for answering the prompt."",
            advanced=False,
        )
        credentials: AICredentials = AICredentialsField()
        sys_prompt: str = SchemaField(
            title=""System Prompt"",
            default="""",
            description=""The system prompt to provide additional context to the model."",
        )
        conversation_history: list[Message] = SchemaField(
            default=[],
            description=""The conversation history to provide context for the prompt."",
        )
        retry: int = SchemaField(
            title=""Retry Count"",
            default=3,
            description=""Number of times to retry the LLM call if the response does not match the expected format."",
        )
        prompt_values: dict[str, str] = SchemaField(
            advanced=False, default={}, description=""Values used to fill in the prompt.""
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )

        ollama_host: str = SchemaField(
            advanced=True,
            default=""localhost:11434"",
            description=""Ollama host for local  models"",
        )

    class Output(BlockSchema):
        response: dict[str, Any] = SchemaField(
            description=""The response object generated by the language model.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""ed55ac19-356e-4243-a6cb-bc599e9b716f"",
            description=""Call a Large Language Model (LLM) to generate formatted object based on the given prompt."",
            categories={BlockCategory.AI},
            input_schema=AIStructuredResponseGeneratorBlock.Input,
            output_schema=AIStructuredResponseGeneratorBlock.Output,
            test_input={
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""expected_format"": {
                    ""key1"": ""value1"",
                    ""key2"": ""value2"",
                },
                ""prompt"": ""User prompt"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""response"", {""key1"": ""key1Value"", ""key2"": ""key2Value""}),
            test_mock={
                ""llm_call"": lambda *args, **kwargs: (
                    json.dumps(
                        {
                            ""key1"": ""key1Value"",
                            ""key2"": ""key2Value"",
                        }
                    ),
                    0,
                    0,
                )
            },
        )

    @staticmethod
    def llm_call(
        credentials: APIKeyCredentials,
        llm_model: LlmModel,
        prompt: list[dict],
        json_format: bool,
        max_tokens: int | None = None,
        ollama_host: str = ""localhost:11434"",
    ) -> tuple[str, int, int]:
        """"""
        Args:
            api_key: API key for the LLM provider.
            llm_model: The LLM model to use.
            prompt: The prompt to send to the LLM.
            json_format: Whether the response should be in JSON format.
            max_tokens: The maximum number of tokens to generate in the chat completion.
            ollama_host: The host for ollama to use

        Returns:
            The response from the LLM.
            The number of tokens used in the prompt.
            The number of tokens used in the completion.
        """"""
        provider = llm_model.metadata.provider

        if provider == ""openai"":
            oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())
            response_format = None

            if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:
                sys_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
                usr_messages = [p[""content""] for p in prompt if p[""role""] != ""system""]
                prompt = [
                    {""role"": ""user"", ""content"": ""\n"".join(sys_messages)},
                    {""role"": ""user"", ""content"": ""\n"".join(usr_messages)},
                ]
            elif json_format:
                response_format = {""type"": ""json_object""}

            response = oai_client.chat.completions.create(
                model=llm_model.value,
                messages=prompt,  # type: ignore
                response_format=response_format,  # type: ignore
                max_completion_tokens=max_tokens,
            )

            return (
                response.choices[0].message.content or """",
                response.usage.prompt_tokens if response.usage else 0,
                response.usage.completion_tokens if response.usage else 0,
            )
        elif provider == ""anthropic"":
            system_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
            sysprompt = "" "".join(system_messages)

            messages = []
            last_role = None
            for p in prompt:
                if p[""role""] in [""user"", ""assistant""]:
                    if p[""role""] != last_role:
                        messages.append({""role"": p[""role""], ""content"": p[""content""]})
                        last_role = p[""role""]
                    else:
                        # If the role is the same as the last one, combine the content
                        messages[-1][""content""] += ""\n"" + p[""content""]

            client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())
            try:
                resp = client.messages.create(
                    model=llm_model.value,
                    system=sysprompt,
                    messages=messages,
                    max_tokens=max_tokens or 8192,
                )

                if not resp.content:
                    raise ValueError(""No content returned from Anthropic."")

                return (
                    (
                        resp.content[0].name
                        if isinstance(resp.content[0], anthropic.types.ToolUseBlock)
                        else resp.content[0].text
                    ),
                    resp.usage.input_tokens,
                    resp.usage.output_tokens,
                )
            except anthropic.APIError as e:
                error_message = f""Anthropic API error: {str(e)}""
                logger.error(error_message)
                raise ValueError(error_message)
        elif provider == ""groq"":
            client = Groq(api_key=credentials.api_key.get_secret_value())
            response_format = {""type"": ""json_object""} if json_format else None
            response = client.chat.completions.create(
                model=llm_model.value,
                messages=prompt,  # type: ignore
                response_format=response_format,  # type: ignore
                max_tokens=max_tokens,
            )
            return (
                response.choices[0].message.content or """",
                response.usage.prompt_tokens if response.usage else 0,
                response.usage.completion_tokens if response.usage else 0,
            )
        elif provider == ""ollama"":
            client = ollama.Client(host=ollama_host)
            sys_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
            usr_messages = [p[""content""] for p in prompt if p[""role""] != ""system""]
            response = client.generate(
                model=llm_model.value,
                prompt=f""{sys_messages}\n\n{usr_messages}"",
                stream=False,
            )
            return (
                response.get(""response"") or """",
                response.get(""prompt_eval_count"") or 0,
                response.get(""eval_count"") or 0,
            )
        elif provider == ""open_router"":
            client = openai.OpenAI(
                base_url=""https://openrouter.ai/api/v1"",
                api_key=credentials.api_key.get_secret_value(),
            )

            response = client.chat.completions.create(
                extra_headers={
                    ""HTTP-Referer"": ""https://agpt.co"",
                    ""X-Title"": ""AutoGPT"",
                },
                model=llm_model.value,
                messages=prompt,  # type: ignore
                max_tokens=max_tokens,
            )

            # If there's no response, raise an error
            if not response.choices:
                if response:
                    raise ValueError(f""OpenRouter error: {response}"")
                else:
                    raise ValueError(""No response from OpenRouter."")

            return (
                response.choices[0].message.content or """",
                response.usage.prompt_tokens if response.usage else 0,
                response.usage.completion_tokens if response.usage else 0,
            )
        else:
            raise ValueError(f""Unsupported LLM provider: {provider}"")

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        logger.debug(f""Calling LLM with input data: {input_data}"")
        prompt = [p.model_dump() for p in input_data.conversation_history]

        def trim_prompt(s: str) -> str:
            lines = s.strip().split(""\n"")
            return ""\n"".join([line.strip().lstrip(""|"") for line in lines])

        values = input_data.prompt_values
        if values:
            input_data.prompt = input_data.prompt.format(**values)
            input_data.sys_prompt = input_data.sys_prompt.format(**values)

        if input_data.sys_prompt:
            prompt.append({""role"": ""system"", ""content"": input_data.sys_prompt})

        if input_data.expected_format:
            expected_format = [
                f'""{k}"": ""{v}""' for k, v in input_data.expected_format.items()
            ]
            format_prompt = "",\n  "".join(expected_format)
            sys_prompt = trim_prompt(
                f""""""
                  |Reply strictly only in the following JSON format:
                  |{{
                  |  {format_prompt}
                  |}}
                """"""
            )
            prompt.append({""role"": ""system"", ""content"": sys_prompt})

        if input_data.prompt:
            prompt.append({""role"": ""user"", ""content"": input_data.prompt})

        def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:
            try:
                parsed = json.loads(resp)
                if not isinstance(parsed, dict):
                    return {}, f""Expected a dictionary, but got {type(parsed)}""
                miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())
                if miss_keys:
                    return parsed, f""Missing keys: {miss_keys}""
                return parsed, None
            except JSONDecodeError as e:
                return {}, f""JSON decode error: {e}""

        logger.info(f""LLM request: {prompt}"")
        retry_prompt = """"
        llm_model = input_data.model

        for retry_count in range(input_data.retry):
            try:
                response_text, input_token, output_token = self.llm_call(
                    credentials=credentials,
                    llm_model=llm_model,
                    prompt=prompt,
                    json_format=bool(input_data.expected_format),
                    ollama_host=input_data.ollama_host,
                    max_tokens=input_data.max_tokens,
                )
                self.merge_stats(
                    {
                        ""input_token_count"": input_token,
                        ""output_token_count"": output_token,
                    }
                )
                logger.info(f""LLM attempt-{retry_count} response: {response_text}"")

                if input_data.expected_format:
                    parsed_dict, parsed_error = parse_response(response_text)
                    if not parsed_error:
                        yield ""response"", {
                            k: (
                                json.loads(v)
                                if isinstance(v, str)
                                and v.startswith(""["")
                                and v.endswith(""]"")
                                else ("", "".join(v) if isinstance(v, list) else v)
                            )
                            for k, v in parsed_dict.items()
                        }
                        return
                else:
                    yield ""response"", {""response"": response_text}
                    return

                retry_prompt = trim_prompt(
                    f""""""
                  |This is your previous error response:
                  |--
                  |{response_text}
                  |--
                  |
                  |And this is the error:
                  |--
                  |{parsed_error}
                  |--
                """"""
                )
                prompt.append({""role"": ""user"", ""content"": retry_prompt})
            except Exception as e:
                logger.exception(f""Error calling LLM: {e}"")
                retry_prompt = f""Error calling LLM: {e}""
            finally:
                self.merge_stats(
                    {
                        ""llm_call_count"": retry_count + 1,
                        ""llm_retry_count"": retry_count,
                    }
                )

        raise RuntimeError(retry_prompt)


class AITextGeneratorBlock(Block):
    class Input(BlockSchema):
        prompt: str = SchemaField(
            description=""The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces."",
            placeholder=""Enter your prompt here..."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for answering the prompt."",
            advanced=False,
        )
        credentials: AICredentials = AICredentialsField()
        sys_prompt: str = SchemaField(
            title=""System Prompt"",
            default="""",
            description=""The system prompt to provide additional context to the model."",
        )
        retry: int = SchemaField(
            title=""Retry Count"",
            default=3,
            description=""Number of times to retry the LLM call if the response does not match the expected format."",
        )
        prompt_values: dict[str, str] = SchemaField(
            advanced=False, default={}, description=""Values used to fill in the prompt.""
        )
        ollama_host: str = SchemaField(
            advanced=True,
            default=""localhost:11434"",
            description=""Ollama host for local  models"",
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )

    class Output(BlockSchema):
        response: str = SchemaField(
            description=""The response generated by the language model.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""1f292d4a-41a4-4977-9684-7c8d560b9f91"",
            description=""Call a Large Language Model (LLM) to generate a string based on the given prompt."",
            categories={BlockCategory.AI},
            input_schema=AITextGeneratorBlock.Input,
            output_schema=AITextGeneratorBlock.Output,
            test_input={
                ""prompt"": ""User prompt"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""response"", ""Response text""),
            test_mock={""llm_call"": lambda *args, **kwargs: ""Response text""},
        )

    def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> str:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response[""response""]

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        object_input_data = AIStructuredResponseGeneratorBlock.Input(
            **{attr: getattr(input_data, attr) for attr in input_data.model_fields},
            expected_format={},
        )
        yield ""response"", self.llm_call(object_input_data, credentials)


class SummaryStyle(Enum):
    CONCISE = ""concise""
    DETAILED = ""detailed""
    BULLET_POINTS = ""bullet points""
    NUMBERED_LIST = ""numbered list""


class AITextSummarizerBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""The text to summarize."",
            placeholder=""Enter the text to summarize here..."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for summarizing the text."",
        )
        focus: str = SchemaField(
            title=""Focus"",
            default=""general information"",
            description=""The topic to focus on in the summary"",
        )
        style: SummaryStyle = SchemaField(
            title=""Summary Style"",
            default=SummaryStyle.CONCISE,
            description=""The style of the summary to generate."",
        )
        credentials: AICredentials = AICredentialsField()
        # TODO: Make this dynamic
        max_tokens: int = SchemaField(
            title=""Max Tokens"",
            default=4096,
            description=""The maximum number of tokens to generate in the chat completion."",
            ge=1,
        )
        chunk_overlap: int = SchemaField(
            title=""Chunk Overlap"",
            default=100,
            description=""The number of overlapping tokens between chunks to maintain context."",
            ge=0,
        )
        ollama_host: str = SchemaField(
            advanced=True,
            default=""localhost:11434"",
            description=""Ollama host for local  models"",
        )

    class Output(BlockSchema):
        summary: str = SchemaField(description=""The final summary of the text."")
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""a0a69be1-4528-491c-a85a-a4ab6873e3f0"",
            description=""Utilize a Large Language Model (LLM) to summarize a long text."",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=AITextSummarizerBlock.Input,
            output_schema=AITextSummarizerBlock.Output,
            test_input={
                ""text"": ""Lorem ipsum..."" * 100,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""summary"", ""Final summary of a long text""),
            test_mock={
                ""llm_call"": lambda input_data, credentials: (
                    {""final_summary"": ""Final summary of a long text""}
                    if ""final_summary"" in input_data.expected_format
                    else {""summary"": ""Summary of a chunk of text""}
                )
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        for output in self._run(input_data, credentials):
            yield output

    def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:
        chunks = self._split_text(
            input_data.text, input_data.max_tokens, input_data.chunk_overlap
        )
        summaries = []

        for chunk in chunks:
            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)
            summaries.append(chunk_summary)

        final_summary = self._combine_summaries(summaries, input_data, credentials)
        yield ""summary"", final_summary

    @staticmethod
    def _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:
        words = text.split()
        chunks = []
        chunk_size = max_tokens - overlap

        for i in range(0, len(words), chunk_size):
            chunk = "" "".join(words[i : i + max_tokens])
            chunks.append(chunk)

        return chunks

    def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> dict:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response

    def _summarize_chunk(
        self, chunk: str, input_data: Input, credentials: APIKeyCredentials
    ) -> str:
        prompt = f""Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\n\n```{chunk}```""

        llm_response = self.llm_call(
            AIStructuredResponseGeneratorBlock.Input(
                prompt=prompt,
                credentials=input_data.credentials,
                model=input_data.model,
                expected_format={""summary"": ""The summary of the given text.""},
            ),
            credentials=credentials,
        )

        return llm_response[""summary""]

    def _combine_summaries(
        self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials
    ) -> str:
        combined_text = ""\n\n"".join(summaries)

        if len(combined_text.split()) <= input_data.max_tokens:
            prompt = f""Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\n\n ```{combined_text}```\n\n Just respond with the final_summary in the format specified.""

            llm_response = self.llm_call(
                AIStructuredResponseGeneratorBlock.Input(
                    prompt=prompt,
                    credentials=input_data.credentials,
                    model=input_data.model,
                    expected_format={
                        ""final_summary"": ""The final summary of all provided summaries.""
                    },
                ),
                credentials=credentials,
            )

            return llm_response[""final_summary""]
        else:
            # If combined summaries are still too long, recursively summarize
            return self._run(
                AITextSummarizerBlock.Input(
                    text=combined_text,
                    credentials=input_data.credentials,
                    model=input_data.model,
                    max_tokens=input_data.max_tokens,
                    chunk_overlap=input_data.chunk_overlap,
                ),
                credentials=credentials,
            ).send(None)[
                1
            ]  # Get the first yielded value


class AIConversationBlock(Block):
    class Input(BlockSchema):
        messages: List[Message] = SchemaField(
            description=""List of messages in the conversation."", min_length=1
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for the conversation."",
        )
        credentials: AICredentials = AICredentialsField()
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )
        ollama_host: str = SchemaField(
            advanced=True,
            default=""localhost:11434"",
            description=""Ollama host for local  models"",
        )

    class Output(BlockSchema):
        response: str = SchemaField(
            description=""The model's response to the conversation.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""32a87eab-381e-4dd4-bdb8-4c47151be35a"",
            description=""Advanced LLM call that takes a list of messages and sends them to the language model."",
            categories={BlockCategory.AI},
            input_schema=AIConversationBlock.Input,
            output_schema=AIConversationBlock.Output,
            test_input={
                ""messages"": [
                    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
                    {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
                    {
                        ""role"": ""assistant"",
                        ""content"": ""The Los Angeles Dodgers won the World Series in 2020."",
                    },
                    {""role"": ""user"", ""content"": ""Where was it played?""},
                ],
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(
                ""response"",
                ""The 2020 World Series was played at Globe Life Field in Arlington, Texas."",
            ),
            test_mock={
                ""llm_call"": lambda *args, **kwargs: ""The 2020 World Series was played at Globe Life Field in Arlington, Texas.""
            },
        )

    def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> str:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response[""response""]

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        response = self.llm_call(
            AIStructuredResponseGeneratorBlock.Input(
                prompt="""",
                credentials=input_data.credentials,
                model=input_data.model,
                conversation_history=input_data.messages,
                max_tokens=input_data.max_tokens,
                expected_format={},
            ),
            credentials=credentials,
        )

        yield ""response"", response


class AIListGeneratorBlock(Block):
    class Input(BlockSchema):
        focus: str | None = SchemaField(
            description=""The focus of the list to generate."",
            placeholder=""The top 5 most interesting news stories in the data."",
            default=None,
            advanced=False,
        )
        source_data: str | None = SchemaField(
            description=""The data to generate the list from."",
            placeholder=""News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released."",
            default=None,
            advanced=False,
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for generating the list."",
            advanced=True,
        )
        credentials: AICredentials = AICredentialsField()
        max_retries: int = SchemaField(
            default=3,
            description=""Maximum number of retries for generating a valid list."",
            ge=1,
            le=5,
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )
        ollama_host: str = SchemaField(
            advanced=True,
            default=""localhost:11434"",
            description=""Ollama host for local  models"",
        )

    class Output(BlockSchema):
        generated_list: List[str] = SchemaField(description=""The generated list."")
        list_item: str = SchemaField(
            description=""Each individual item in the list."",
        )
        error: str = SchemaField(
            description=""Error message if the list generation failed.""
        )

    def __init__(self):
        super().__init__(
            id=""9c0b0450-d199-458b-a731-072189dd6593"",
            description=""Generate a Python list based on the given prompt using a Large Language Model (LLM)."",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=AIListGeneratorBlock.Input,
            output_schema=AIListGeneratorBlock.Output,
            test_input={
                ""focus"": ""planets"",
                ""source_data"": (
                    ""Zylora Prime is a glowing jungle world with bioluminescent plants, ""
                    ""while Kharon-9 is a harsh desert planet with underground cities. ""
                    ""Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to ""
                    ""intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, ""
                    ""drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of ""
                    ""fictional worlds.""
                ),
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""max_retries"": 3,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""generated_list"",
                    [""Zylora Prime"", ""Kharon-9"", ""Vortexia"", ""Oceara"", ""Draknos""],
                ),
                (""list_item"", ""Zylora Prime""),
                (""list_item"", ""Kharon-9""),
                (""list_item"", ""Vortexia""),
                (""list_item"", ""Oceara""),
                (""list_item"", ""Draknos""),
            ],
            test_mock={
                ""llm_call"": lambda input_data, credentials: {
                    ""response"": ""['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']""
                },
            },
        )

    @staticmethod
    def llm_call(
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> dict[str, str]:
        llm_block = AIStructuredResponseGeneratorBlock()
        response = llm_block.run_once(input_data, ""response"", credentials=credentials)
        return response

    @staticmethod
    def string_to_list(string):
        """"""
        Converts a string representation of a list into an actual Python list object.
        """"""
        logger.debug(f""Converting string to list. Input string: {string}"")
        try:
            # Use ast.literal_eval to safely evaluate the string
            python_list = ast.literal_eval(string)
            if isinstance(python_list, list):
                logger.debug(f""Successfully converted string to list: {python_list}"")
                return python_list
            else:
                logger.error(f""The provided string '{string}' is not a valid list"")
                raise ValueError(f""The provided string '{string}' is not a valid list."")
        except (SyntaxError, ValueError) as e:
            logger.error(f""Failed to convert string to list: {e}"")
            raise ValueError(""Invalid list format. Could not convert to list."")

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        logger.debug(f""Starting AIListGeneratorBlock.run with input data: {input_data}"")

        # Check for API key
        api_key_check = credentials.api_key.get_secret_value()
        if not api_key_check:
            raise ValueError(""No LLM API key provided."")

        # Prepare the system prompt
        sys_prompt = """"""You are a Python list generator. Your task is to generate a Python list based on the user's prompt. 
            |Respond ONLY with a valid python list. 
            |The list can contain strings, numbers, or nested lists as appropriate. 
            |Do not include any explanations or additional text.

            |Valid Example string formats:

            |Example 1:
            |```
            |['1', '2', '3', '4']
            |```

            |Example 2:
            |```
            |[['1', '2'], ['3', '4'], ['5', '6']]
            |```

            |Example 3:
            |```
            |['1', ['2', '3'], ['4', ['5', '6']]]
            |```

            |Example 4:
            |```
            |['a', 'b', 'c']
            |```

            |Example 5:
            |```
            |['1', '2.5', 'string', 'True', ['False', 'None']]
            |```

            |Do not include any explanations or additional text, just respond with the list in the format specified above.
            """"""
        # If a focus is provided, add it to the prompt
        if input_data.focus:
            prompt = f""Generate a list with the following focus:\n<focus>\n\n{input_data.focus}</focus>""
        else:
            # If there's source data
            if input_data.source_data:
                prompt = ""Extract the main focus of the source data to a list.\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.""
            else:
                # No focus or source data provided, generat a random list
                prompt = ""Generate a random list.""

        # If the source data is provided, add it to the prompt
        if input_data.source_data:
            prompt += f""\n\nUse the following source data to generate the list from:\n\n<source_data>\n\n{input_data.source_data}</source_data>\n\nDo not invent fictional data that is not present in the source data.""
        # Else, tell the LLM to synthesize the data
        else:
            prompt += ""\n\nInvent the data to generate the list from.""

        for attempt in range(input_data.max_retries):
            try:
                logger.debug(""Calling LLM"")
                llm_response = self.llm_call(
                    AIStructuredResponseGeneratorBlock.Input(
                        sys_prompt=sys_prompt,
                        prompt=prompt,
                        credentials=input_data.credentials,
                        model=input_data.model,
                        expected_format={},  # Do not use structured response
                        ollama_host=input_data.ollama_host,
                    ),
                    credentials=credentials,
                )

                logger.debug(f""LLM response: {llm_response}"")

                # Extract Response string
                response_string = llm_response[""response""]
                logger.debug(f""Response string: {response_string}"")

                # Convert the string to a Python list
                logger.debug(""Converting string to Python list"")
                parsed_list = self.string_to_list(response_string)
                logger.debug(f""Parsed list: {parsed_list}"")

                # If we reach here, we have a valid Python list
                logger.debug(""Successfully generated a valid Python list"")
                yield ""generated_list"", parsed_list

                # Yield each item in the list
                for item in parsed_list:
                    yield ""list_item"", item
                return

            except Exception as e:
                logger.error(f""Error in attempt {attempt + 1}: {str(e)}"")
                if attempt == input_data.max_retries - 1:
                    logger.error(
                        f""Failed to generate a valid Python list after {input_data.max_retries} attempts""
                    )
                    raise RuntimeError(
                        f""Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}""
                    )
                else:
                    # Add a retry prompt
                    logger.debug(""Preparing retry prompt"")
                    prompt = f""""""
                    The previous attempt failed due to `{e}`
                    Generate a valid Python list based on the original prompt.
                    Remember to respond ONLY with a valid Python list as per the format specified earlier.
                    Original prompt: 
                    ```{prompt}```
                    
                    Respond only with the list in the format specified with no commentary or apologies.
                    """"""
                    logger.debug(f""Retry prompt: {prompt}"")

        logger.debug(""AIListGeneratorBlock.run completed"")
","```python
import ast
import logging
from enum import Enum, EnumMeta
from json import JSONDecodeError
from types import MappingProxyType
from typing import TYPE_CHECKING, Any, List, Literal, NamedTuple

from pydantic import SecretStr

from backend.integrations.providers import ProviderName

if TYPE_CHECKING:
    from enum import _EnumMemberT

import anthropic
import ollama
import openai
from groq import Groq

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.util import json
from backend.util.settings import BehaveAs, Settings

logger = logging.getLogger(__name__)

LLMProviderName = Literal[
    ProviderName.ANTHROPIC,
    ProviderName.GROQ,
    ProviderName.OLLAMA,
    ProviderName.OPENAI,
    ProviderName.OPEN_ROUTER,
]

AICredentials = CredentialsMetaInput[LLMProviderName, Literal[""api_key""]]

TEST_CREDENTIALS = APIKeyCredentials(
    id=""ed55ac19-356e-4243-a6cb-bc599e9b716f"",
    provider=""openai"",
    api_key=SecretStr(""mock-openai-api-key""),
    title=""Mock OpenAI API key"",
    expires_at=None,
)

TEST_CREDENTIALS_INPUT = {
    ""provider"": TEST_CREDENTIALS.provider,
    ""id"": TEST_CREDENTIALS.id,
    ""type"": TEST_CREDENTIALS.type,
    ""title"": TEST_CREDENTIALS.title,
}


def AICredentialsField() -> AICredentials:
    return CredentialsField(
        description=""API key for the LLM provider."",
        discriminator=""model"",
        discriminator_mapping={
            model.value: model.metadata.provider for model in LlmModel
        },
    )


class ModelMetadata(NamedTuple):
    provider: str
    context_window: int


class LlmModelMeta(EnumMeta):
    @property
    def __members__(
        self: type[""_EnumMemberT""],
    ) -> MappingProxyType[str, ""_EnumMemberT""]:
        if Settings().config.behave_as == BehaveAs.LOCAL:
            members = super().__members__
            return members
        else:
            removed_providers = [""ollama""]
            existing_members = super().__members__
            members = {
                name: member
                for name, member in existing_members.items()
                if LlmModel[name].provider not in removed_providers
            }
            return MappingProxyType(members)


class LlmModel(str, Enum, metaclass=LlmModelMeta):
    # OpenAI models
    O1_PREVIEW = ""o1-preview""
    O1_MINI = ""o1-mini""
    GPT4O_MINI = ""gpt-4o-mini""
    GPT4O = ""gpt-4o""
    GPT4_TURBO = ""gpt-4-turbo""
    GPT3_5_TURBO = ""gpt-3.5-turbo""

    # Anthropic models
    CLAUDE_3_5_SONNET = ""claude-3-5-sonnet-latest""
    CLAUDE_3_HAIKU = ""claude-3-haiku-20240307""

    # Groq models
    LLAMA3_8B = ""llama3-8b-8192""
    LLAMA3_70B = ""llama3-70b-8192""
    MIXTRAL_8X7B = ""mixtral-8x7b-32768""
    GEMMA_7B = ""gemma-7b-it""
    GEMMA2_9B = ""gemma2-9b-it""

    # New Groq models (Preview)
    LLAMA3_1_405B = ""llama-3.1-405b-reasoning""
    LLAMA3_1_70B = ""llama-3.1-70b-versatile""
    LLAMA3_1_8B = ""llama-3.1-8b-instant""

    # Ollama models
    OLLAMA_LLAMA3_8B = ""llama3""
    OLLAMA_LLAMA3_405B = ""llama3.1:405b""
    OLLAMA_DOLPHIN = ""dolphin-mistral:latest""

    # OpenRouter models
    GEMINI_FLASH_1_5_8B = ""google/gemini-flash-1.5""
    GROK_BETA = ""x-ai/grok-beta""
    MISTRAL_NEMO = ""mistralai/mistral-nemo""
    COHERE_COMMAND_R_08_2024 = ""cohere/command-r-08-2024""
    COHERE_COMMAND_R_PLUS_08_2024 = ""cohere/command-r-plus-08-2024""
    EVA_QWEN_2_5_32B = ""eva-unit-01/eva-qwen-2.5-32b""
    DEEPSEEK_CHAT = ""deepseek/deepseek-chat""

    PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE = (
        ""perplexity/llama-3.1-sonar-large-128k-online""
    )

    QWEN_QWQ_32B_PREVIEW = ""qwen/qwq-32b-preview""
    NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B = ""nousresearch/hermes-3-llama-3.1-405b""
    NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B = ""nousresearch/hermes-3-llama-3.1-70b""
    AMAZON_NOVA_LITE_V1 = ""amazon/nova-lite-v1""
    AMAZON_NOVA_MICRO_V1 = ""amazon/nova-micro-v1""
    AMAZON_NOVA_PRO_V1 = ""amazon/nova-pro-v1""
    MICROSOFT_WIZARDLM_2_8X22B = ""microsoft/wizardlm-2-8x22b""
    GRYPHE_MYTHOMAX_L2_13B = ""gryphe/mythomax-l2-13b""

    @property
    def metadata(self) -> ModelMetadata:
        return MODEL_METADATA[self]

    @property
    def provider(self) -> str:
        return self.metadata.provider

    @property
    def context_window(self) -> int:
        return self.metadata.context_window


MODEL_METADATA = {
    LlmModel.O1_PREVIEW: ModelMetadata(""openai"", 32000),
    LlmModel.O1_MINI: ModelMetadata(""openai"", 62000),
    LlmModel.GPT4O_MINI: ModelMetadata(""openai"", 128000),
    LlmModel.GPT4O: ModelMetadata(""openai"", 128000),
    LlmModel.GPT4_TURBO: ModelMetadata(""openai"", 128000),
    LlmModel.GPT3_5_TURBO: ModelMetadata(""openai"", 16385),
    LlmModel.CLAUDE_3_5_SONNET: ModelMetadata(""anthropic"", 200000),
    LlmModel.CLAUDE_3_HAIKU: ModelMetadata(""anthropic"", 200000),
    LlmModel.LLAMA3_8B: ModelMetadata(""groq"", 8192),
    LlmModel.LLAMA3_70B: ModelMetadata(""groq"", 8192),
    LlmModel.MIXTRAL_8X7B: ModelMetadata(""groq"", 32768),
    LlmModel.GEMMA_7B: ModelMetadata(""groq"", 8192),
    LlmModel.GEMMA2_9B: ModelMetadata(""groq"", 8192),
    LlmModel.LLAMA3_1_405B: ModelMetadata(""groq"", 8192),
    # Limited to 16k during preview
    LlmModel.LLAMA3_1_70B: ModelMetadata(""groq"", 131072),
    LlmModel.LLAMA3_1_8B: ModelMetadata(""groq"", 131072),
    LlmModel.OLLAMA_LLAMA3_8B: ModelMetadata(""ollama"", 8192),
    LlmModel.OLLAMA_LLAMA3_405B: ModelMetadata(""ollama"", 8192),
    LlmModel.OLLAMA_DOLPHIN: ModelMetadata(""ollama"", 32768),
    LlmModel.GEMINI_FLASH_1_5_8B: ModelMetadata(""open_router"", 8192),
    LlmModel.GROK_BETA: ModelMetadata(""open_router"", 8192),
    LlmModel.MISTRAL_NEMO: ModelMetadata(""open_router"", 4000),
    LlmModel.COHERE_COMMAND_R_08_2024: ModelMetadata(""open_router"", 4000),
    LlmModel.COHERE_COMMAND_R_PLUS_08_2024: ModelMetadata(""open_router"", 4000),
    LlmModel.EVA_QWEN_2_5_32B: ModelMetadata(""open_router"", 4000),
    LlmModel.DEEPSEEK_CHAT: ModelMetadata(""open_router"", 8192),
    LlmModel.PERPLEXITY_LLAMA_3_1_SONAR_LARGE_128K_ONLINE: ModelMetadata(
        ""open_router"", 8192
    ),
    LlmModel.QWEN_QWQ_32B_PREVIEW: ModelMetadata(""open_router"", 4000),
    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_405B: ModelMetadata(""open_router"", 4000),
    LlmModel.NOUSRESEARCH_HERMES_3_LLAMA_3_1_70B: ModelMetadata(""open_router"", 4000),
    LlmModel.AMAZON_NOVA_LITE_V1: ModelMetadata(""open_router"", 4000),
    LlmModel.AMAZON_NOVA_MICRO_V1: ModelMetadata(""open_router"", 4000),
    LlmModel.AMAZON_NOVA_PRO_V1: ModelMetadata(""open_router"", 4000),
    LlmModel.MICROSOFT_WIZARDLM_2_8X22B: ModelMetadata(""open_router"", 4000),
    LlmModel.GRYPHE_MYTHOMAX_L2_13B: ModelMetadata(""open_router"", 4000),
}

for model in LlmModel:
    if model not in MODEL_METADATA:
        raise ValueError(f""Missing MODEL_METADATA metadata for model: {model}"")


class MessageRole(str, Enum):
    SYSTEM = ""system""
    USER = ""user""
    ASSISTANT = ""assistant""


class Message(BlockSchema):
    role: MessageRole
    content: str


class AIStructuredResponseGeneratorBlock(Block):
    class Input(BlockSchema):
        prompt: str = SchemaField(
            description=""The prompt to send to the language model."",
            placeholder=""Enter your prompt here..."",
        )
        expected_format: dict[str, str] = SchemaField(
            description=""Expected format of the response. If provided, the response will be validated against this format. ""
            ""The keys should be the expected fields in the response, and the values should be the description of the field."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for answering the prompt."",
            advanced=False,
        )
        credentials: AICredentials = AICredentialsField()
        sys_prompt: str = SchemaField(
            title=""System Prompt"",
            default="""",
            description=""The system prompt to provide additional context to the model."",
        )
        conversation_history: list[Message] = SchemaField(
            default=[],
            description=""The conversation history to provide context for the prompt."",
        )
        retry: int = SchemaField(
            title=""Retry Count"",
            default=3,
            description=""Number of times to retry the LLM call if the response does not match the expected format."",
        )
        prompt_values: dict[str, str] = SchemaField(
            advanced=False, default={}, description=""Values used to fill in the prompt.""
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )
        ollama_host: str = SchemaField(
            advanced=True,
            default=""localhost:11434"",
            description=""Ollama host for local  models"",
        )

    class Output(BlockSchema):
        response: dict[str, Any] = SchemaField(
            description=""The response object generated by the language model.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""ed55ac19-356e-4243-a6cb-bc599e9b716f"",
            description=""Call a Large Language Model (LLM) to generate formatted object based on the given prompt."",
            categories={BlockCategory.AI},
            input_schema=AIStructuredResponseGeneratorBlock.Input,
            output_schema=AIStructuredResponseGeneratorBlock.Output,
            test_input={
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""expected_format"": {
                    ""key1"": ""value1"",
                    ""key2"": ""value2"",
                },
                ""prompt"": ""User prompt"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""response"", {""key1"": ""key1Value"", ""key2"": ""key2Value""}),
            test_mock={
                ""llm_call"": lambda *args, **kwargs: (
                    json.dumps(
                        {
                            ""key1"": ""key1Value"",
                            ""key2"": ""key2Value"",
                        }
                    ),
                    0,
                    0,
                )
            },
        )

    @staticmethod
    def llm_call(
        credentials: APIKeyCredentials,
        llm_model: LlmModel,
        prompt: list[dict],
        json_format: bool,
        max_tokens: int | None = None,
        ollama_host: str = ""localhost:11434"",
    ) -> tuple[str, int, int]:
        """"""
        Args:
            api_key: API key for the LLM provider.
            llm_model: The LLM model to use.
            prompt: The prompt to send to the LLM.
            json_format: Whether the response should be in JSON format.
            max_tokens: The maximum number of tokens to generate in the chat completion.
            ollama_host: The host for ollama to use

        Returns:
            The response from the LLM.
            The number of tokens used in the prompt.
            The number of tokens used in the completion.
        """"""
        provider = llm_model.metadata.provider

        if provider == ""openai"":
            oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())
            response_format = None

            if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:
                sys_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
                usr_messages = [p[""content""] for p in prompt if p[""role""] != ""system""]
                prompt = [
                    {""role"": ""user"", ""content"": ""\n"".join(sys_messages)},
                    {""role"": ""user"", ""content"": ""\n"".join(usr_messages)},
                ]

            elif json_format:
                response_format = {""type"": ""json_object""}

            response = oai_client.chat.completions.create(
                model=llm_model.value,
                messages=prompt,  # type: ignore
                response_format=response_format,  # type: ignore
                max_completion_tokens=max_tokens,
            )

            return (
                response.choices[0].message.content or """",
                response.usage.prompt_tokens if response.usage else 0,
                response.usage.completion_tokens if response.usage else 0,
            )

        elif provider == ""anthropic"":
            system_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
            sysprompt = "" "".join(system_messages)

            messages = []
            last_role = None
            for p in prompt:
                if p[""role""] in [""user"", ""assistant""]:
                    if p[""role""] != last_role:
                        messages.append({""role"": p[""role""], ""content"": p[""content""]})
                        last_role = p[""role""]
                    else:
                        # If the role is the same as the last one, combine the content
                        messages[-1][""content""] += ""\n"" + p[""content""]

            client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())
            try:
                resp = client.messages.create(
                    model=llm_model.value,
                    system=sysprompt,
                    messages=messages,
                    max_tokens=max_tokens or 8192,
                )

                if not resp.content:
                    raise ValueError(""No content returned from Anthropic."")

                return (
                    (
                        resp.content[0].name
                        if isinstance(resp.content[0], anthropic.types.ToolUseBlock)
                        else resp.content[0].text
                    ),
                    resp.usage.input_tokens,
                    resp.usage.output_tokens,
                )
            except anthropic.APIError as e:
                error_message = f""Anthropic API error: {str(e)}""
                logger.error(error_message)
                raise ValueError(error_message)

        elif provider == ""groq"":
            client = Groq(api_key=credentials.api_key.get_secret_value())
            response_format = {""type"": ""json_object""} if json_format else None

            response = client.chat.completions.create(
                model=llm_model.value,
                messages=prompt,  # type: ignore
                response_format=response_format,  # type: ignore
                max_tokens=max_tokens,
            )
            return (
                response.choices[0].message.content or """",
                response.usage.prompt_tokens if response.usage else 0,
                response.usage.completion_tokens if response.usage else 0,
            )

        elif provider == ""ollama"":
            client = ollama.Client(host=ollama_host)
            sys_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
            usr_messages = [p[""content""] for p in prompt if p[""role""] != ""system""]

            response = client.generate(
                model=llm_model.value,
                prompt=f""{sys_messages}\n\n{usr_messages}"",
                stream=False,
            )
            return (
                response.get(""response"") or """",
                response.get(""prompt_eval_count"") or 0,
                response.get(""eval_count"") or 0,
            )

        elif provider == ""open_router"":
            client = openai.OpenAI(
                base_url=""https://openrouter.ai/api/v1"",
                api_key=credentials.api_key.get_secret_value(),
            )

            response = client.chat.completions.create(
                extra_headers={
                    ""HTTP-Referer"": ""https://agpt.co"",
                    ""X-Title"": ""AutoGPT"",
                },
                model=llm_model.value,
                messages=prompt,  # type: ignore
                max_tokens=max_tokens,
            )

            # If there's no response, raise an error
            if not response.choices:
                if response:
                    raise ValueError(f""OpenRouter error: {response}"")
                else:
                    raise ValueError(""No response from OpenRouter."")

            return (
                response.choices[0].message.content or """",
                response.usage.prompt_tokens if response.usage else 0,
                response.usage.completion_tokens if response.usage else 0,
            )

        else:
            raise ValueError(f""Unsupported LLM provider: {provider}"")

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        logger.debug(f""Calling LLM with input data: {input_data}"")
        prompt = [p.model_dump() for p in input_data.conversation_history]

        def trim_prompt(s: str) -> str:
            lines = s.strip().split(""\n"")
            return ""\n"".join([line.strip().lstrip(""|"") for line in lines])

        values = input_data.prompt_values

        if values:
            input_data.prompt = input_data.prompt.format(**values)
            input_data.sys_prompt = input_data.sys_prompt.format(**values)

        if input_data.sys_prompt:
            prompt.append({""role"": ""system"", ""content"": input_data.sys_prompt})

        if input_data.expected_format:
            expected_format = [
                f'""{k}"": ""{v}""' for k, v in input_data.expected_format.items()
            ]
            format_prompt = "",\n  "".join(expected_format)

            sys_prompt = trim_prompt(
                f""""""
                  |Reply strictly only in the following JSON format:
                  |{{
                  |  {format_prompt}
                  |}}
                """"""
            )
            prompt.append({""role"": ""system"", ""content"": sys_prompt})

        if input_data.prompt:
            prompt.append({""role"": ""user"", ""content"": input_data.prompt})

        def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:
            try:
                parsed = json.loads(resp)
                if not isinstance(parsed, dict):
                    return {}, f""Expected a dictionary, but got {type(parsed)}""
                miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())
                if miss_keys:
                    return parsed, f""Missing keys: {miss_keys}""
                return parsed, None
            except JSONDecodeError as e:
                return {}, f""JSON decode error: {e}""

        logger.info(f""LLM request: {prompt}"")
        retry_prompt = """"
        llm_model = input_data.model

        for retry_count in range(input_data.retry):
            try:
                response_text, input_token, output_token = self.llm_call(
                    credentials=credentials,
                    llm_model=llm_model,
                    prompt=prompt,
                    json_format=bool(input_data.expected_format),
                    ollama_host=input_data.ollama_host,
                    max_tokens=input_data.max_tokens,
                )
                self.merge_stats(
                    {
                        ""input_token_count"": input_token,
                        ""output_token_count"": output_token,
                    }
                )
                logger.info(f""LLM attempt-{retry_count} response: {response_text}"")
                if input_data.expected_format:
                    parsed_dict, parsed_error = parse_response(response_text)

                    if not parsed_error:
                        yield ""response"", {
                            k: (
                                json.loads(v)
                                if isinstance(v, str)
                                and v.startswith(""["")
                                and v.endswith(""]"")
                                else ("", "".join(v) if isinstance(v, list) else v)
                            )
                            for k, v in parsed_dict.items()
                        }
                        return
                else:
                    yield ""response"", {""response"": response_text}
                    return

                retry_prompt = trim_prompt(
                    f""""""
                  |This is your previous error response:
                  |--
                  |{response_text}
                  |--
                  |
                  |And this is the error:
                  |--
                  |{parsed_error}
                  |--
                """"""
                )
                prompt.append({""role"": ""user"", ""content"": retry_prompt})

            except Exception as e:
                logger.exception(f""Error calling LLM: {e}"")
                retry_prompt = f""Error calling LLM: {e}""
                self.merge_stats(
                    {
                        ""llm_call_count"": retry_count + 1,
                        ""llm_retry_count"": retry_count,
                    }
                )

        raise RuntimeError(retry_prompt)


class AITextGeneratorBlock(Block):
    class Input(BlockSchema):
        prompt: str = SchemaField(
            description=""The prompt to send to the language model. You can use any of the {keys} from Prompt Values to fill in the prompt with values from the prompt values dictionary by putting them in curly braces."",
            placeholder=""Enter your prompt here..."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for answering the prompt."",
            advanced=False,
        )
        credentials: AICredentials = AICredentialsField()
        sys_prompt: str = SchemaField(
            title=""System Prompt"",
            default="""",
            description=""The system prompt to provide additional context to the model."",
        )
        retry: int = SchemaField(
            title=""Retry Count"",
            default=3,
            description=""Number of times to retry the LLM call if the response does not match the expected format."",
        )
        prompt_values: dict[str, str] = SchemaField(
            advanced=False, default={}, description=""Values used to fill in the prompt.""
        )
        ollama_host: str = SchemaField(
            advanced=True,
            default=""localhost:11434"",
            description=""Ollama host for local  models"",
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )

    class Output(BlockSchema):
        response: str = SchemaField(
            description=""The response generated by the language model.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""1f292d4a-41a4-4977-9684-7c8d560b9f91"",
            description=""Call a Large Language Model (LLM) to generate a string based on the given prompt."",
            categories={BlockCategory.AI},
            input_schema=AITextGeneratorBlock.Input,
            output_schema=AITextGeneratorBlock.Output,
            test_input={
                ""prompt"": ""User prompt"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""response"", ""Response text""),
            test_mock={""llm_call"": lambda *args, **kwargs: ""Response text""},
        )

    def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> str:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response[""response""]

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        object_input_data = AIStructuredResponseGeneratorBlock.Input(
            **{attr: getattr(input_data, attr) for attr in input_data.model_fields},
            expected_format={},
        )
        yield ""response"", self.llm_call(object_input_data, credentials)


class SummaryStyle(Enum):
    CONCISE = ""concise""
    DETAILED = ""detailed""
    BULLET_POINTS = ""bullet points""
    NUMBERED_LIST = ""numbered list""


class AITextSummarizerBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""The text to summarize."",
            placeholder=""Enter the text to summarize here..."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for summarizing the text."",
        )
        focus: str = SchemaField(
            title=""Focus"",
            default=""general information"",
            description=""The topic to focus on in the summary"",
        )
        style: SummaryStyle = SchemaField(
            title=""Summary Style"",
            default=SummaryStyle.CONCISE,
            description=""The style of the summary to generate."",
        )
        credentials: AICredentials = AICredentialsField()
        # TODO: Make this dynamic
        max_tokens: int = SchemaField(
            title=""Max Tokens"",
            default=4096,
            description=""The maximum number of tokens to generate in the chat completion."",
            ge=1,
        )
        chunk_overlap: int = SchemaField(
            title=""Chunk Overlap"",
            default=100,
            description=""The number of overlapping tokens between chunks to maintain context."",
            ge=0,
        )
        ollama_host: str = SchemaField(
            advanced=True,
            default=""localhost:11434"",
            description=""Ollama host for local  models"",
        )

    class Output(BlockSchema):
        summary: str = SchemaField(description=""The final summary of the text."")
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""a0a69be1-4528-491c-a85a-a4ab6873e3f0"",
            description=""Utilize a Large Language Model (LLM) to summarize a long text."",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=AITextSummarizerBlock.Input,
            output_schema=AITextSummarizerBlock.Output,
            test_input={
                ""text"": ""Lorem ipsum..."" * 100,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""summary"", ""Final summary of a long text""),
            test_mock={
                ""llm_call"": lambda input_data, credentials: (
                    {""final_summary"": ""Final summary of a long text""}
                    if ""final_summary"" in input_data.expected_format
                    else {""summary"": ""Summary of a chunk of text""}
                )
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        for output in self._run(input_data, credentials):
            yield output

    def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:
        chunks = self._split_text(
            input_data.text, input_data.max_tokens, input_data.chunk_overlap
        )
        summaries = []

        for chunk in chunks:
            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)
            summaries.append(chunk_summary)

        final_summary = self._combine_summaries(summaries, input_data, credentials)
        yield ""summary"", final_summary

    @staticmethod
    def _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:
        words = text.split()
        chunks = []
        chunk_size = max_tokens - overlap

        for i in range(0, len(words), chunk_size):
            chunk = "" "".join(words[i : i + max_tokens])
            chunks.append(chunk)

        return chunks

    def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> dict:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response

    def _summarize_chunk(
        self, chunk: str, input_data: Input, credentials: APIKeyCredentials
    ) -> str:
        prompt = f""Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\\n\\n```{chunk}```""

        llm_response = self.llm_call(
            AIStructuredResponseGeneratorBlock.Input(
                prompt=prompt,
                credentials=input_data.credentials,
                model=input_data.model,
                expected_format={""summary"": ""The summary of the given text.""},
            ),
            credentials=credentials,
        )

        return llm_response[""summary""]

    def _combine_summaries(
        self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials
    ) -> str:
        combined_text = ""\n\n"".join(summaries)

        if len(combined_text.split()) <= input_data.max_tokens:
            prompt = f""Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\\n\\n ```{combined_text}```\\n\\n Just respond with the final_summary in the format specified.""

            llm_response = self.llm_call(
                AIStructuredResponseGeneratorBlock.Input(
                    prompt=prompt,
                    credentials=input_data.credentials,
                    model=input_data.model,
                    expected_format={
                        ""final_summary"": ""The final summary of all provided summaries.""
                    },
                ),
                credentials=credentials,
            )

            return llm_response[""final_summary""]

        else:
            # If combined summaries are still too long, recursively summarize
            return self._run(
                AITextSummarizerBlock.Input(
                    text=combined_text,
                    credentials=input_data.credentials,
                    model=input_data.model,
                    max_tokens=input_data.max_tokens,
                    chunk_overlap=input_data.chunk_overlap,
                ),
                credentials=credentials,
            ).send(None)[
                1
            ]  # Get the first yielded value


class AIConversationBlock(Block):
    class Input(BlockSchema):
        messages: List[Message] = SchemaField(
            description=""List of messages in the conversation."", min_length=1
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for the conversation."",
        )
        credentials: AICredentials = AICredentialsField()
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )
        ollama_host: str = SchemaField(
            advanced=True,
            default=""localhost:11434"",
            description=""Ollama host for local  models"",
        )

    class Output(BlockSchema):
        response: str = SchemaField(
            description=""The model's response to the conversation.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""32a87eab-381e-4dd4-bdb8-4c47151be35a"",
            description=""Advanced LLM call that takes a list of messages and sends them to the language model."",
            categories={BlockCategory.AI},
            input_schema=AIConversationBlock.Input,
            output_schema=AIConversationBlock.Output,
            test_input={
                ""messages"": [
                    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
                    {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
                    {
                        ""role"": ""assistant"",
                        ""content"": ""The Los Angeles Dodgers won the World Series in 2020."",
                    },
                    {""role"": ""user"", ""content"": ""Where was it played?""},
                ],
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(
                ""response"",
                ""The 2020 World Series was played at Globe Life Field in Arlington, Texas."",
            ),
            test_mock={
                ""llm_call"": lambda *args, **kwargs: ""The 2020 World Series was played at Globe Life Field in Arlington, Texas.""
            },
        )

    def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> str:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response[""response""]

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        response = self.llm_call(
            AIStructuredResponseGeneratorBlock.Input(
                prompt="""",
                credentials=input_data.credentials,
                model=input_data.model,
                conversation_history=input_data.messages,
                max_tokens=input_data.max_tokens,
                expected_format={},
            ),
            credentials=credentials,
        )

        yield ""response"", response


class AIListGeneratorBlock(Block):
    class Input(BlockSchema):
        focus: str | None = SchemaField(
            description=""The focus of the list to generate."",
            placeholder=""The top 5 most interesting news stories in the data."",
            default=None,
            advanced=False,
        )
        source_data: str | None = SchemaField(
            description=""The data to generate the list from."",
            placeholder=""News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released."",
            default=None,
            advanced=False,
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for generating the list."",
            advanced=True,
        )
        credentials: AICredentials = AICredentialsField()
        max_retries: int = SchemaField(
            default=3,
            description=""Maximum number of retries for generating a valid list."",
            ge=1,
            le=5,
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )
        ollama_host: str = SchemaField(
            advanced=True,
            default=""localhost:11434"",
            description=""Ollama host for local  models"",
        )

    class Output(BlockSchema):
        generated_list: List[str] = SchemaField(description=""The generated list."")
        list_item: str = SchemaField(
            description=""Each individual item in the list."",
        )
        error: str = SchemaField(
            description=""Error message if the list generation failed.""
        )

    def __init__(self):
        super().__init__(
            id=""9c0b0450-d199-458b-a731-072189dd6593"",
            description=""Generate a Python list based on the given prompt using a Large Language Model (LLM)."",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input","{'final_score': 86.85, 'length_similarity': 81.86, 'token_similarity': 79.98, 'structure_similarity': 83.75, 'function_similarity': 100, 'complexity_similarity': 88.3, 'variable_similarity': 100}",
187,"from backend.app import run_processes
from backend.executor import DatabaseManager, ExecutionManager


def main():
    """"""
    Run all the processes required for the AutoGPT-server REST API.
    """"""
    run_processes(
        DatabaseManager(),
        ExecutionManager(),
    )


if __name__ == ""__main__"":
    main()
","from backend.app import run_processes
from backend.executor import DatabaseManager, ExecutionManager

def main():
    """"""
    Run all the processes required for the AutoGPT-server REST API.
    """"""
    run_processes(
        DatabaseManager(),
        ExecutionManager(),
    )

if __name__ == ""__main__"":
    main()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
188,"from datetime import datetime

import prisma.errors
import prisma.models
import pytest
from prisma import Prisma

import backend.server.v2.store.db as db
from backend.server.v2.store.model import Profile


@pytest.fixture(autouse=True)
async def setup_prisma():
    # Don't register client if already registered
    try:
        Prisma()
    except prisma.errors.ClientAlreadyRegisteredError:
        pass
    yield


@pytest.mark.asyncio
async def test_get_store_agents(mocker):
    # Mock data
    mock_agents = [
        prisma.models.StoreAgent(
            listing_id=""test-id"",
            storeListingVersionId=""version123"",
            slug=""test-agent"",
            agent_name=""Test Agent"",
            agent_video=None,
            agent_image=[""image.jpg""],
            featured=False,
            creator_username=""creator"",
            creator_avatar=""avatar.jpg"",
            sub_heading=""Test heading"",
            description=""Test description"",
            categories=[],
            runs=10,
            rating=4.5,
            versions=[""1.0""],
            updated_at=datetime.now(),
        )
    ]

    # Mock prisma calls
    mock_store_agent = mocker.patch(""prisma.models.StoreAgent.prisma"")
    mock_store_agent.return_value.find_many = mocker.AsyncMock(return_value=mock_agents)
    mock_store_agent.return_value.count = mocker.AsyncMock(return_value=1)

    # Call function
    result = await db.get_store_agents()

    # Verify results
    assert len(result.agents) == 1
    assert result.agents[0].slug == ""test-agent""
    assert result.pagination.total_items == 1

    # Verify mocks called correctly
    mock_store_agent.return_value.find_many.assert_called_once()
    mock_store_agent.return_value.count.assert_called_once()


@pytest.mark.asyncio
async def test_get_store_agent_details(mocker):
    # Mock data
    mock_agent = prisma.models.StoreAgent(
        listing_id=""test-id"",
        storeListingVersionId=""version123"",
        slug=""test-agent"",
        agent_name=""Test Agent"",
        agent_video=""video.mp4"",
        agent_image=[""image.jpg""],
        featured=False,
        creator_username=""creator"",
        creator_avatar=""avatar.jpg"",
        sub_heading=""Test heading"",
        description=""Test description"",
        categories=[""test""],
        runs=10,
        rating=4.5,
        versions=[""1.0""],
        updated_at=datetime.now(),
    )

    # Mock prisma call
    mock_store_agent = mocker.patch(""prisma.models.StoreAgent.prisma"")
    mock_store_agent.return_value.find_first = mocker.AsyncMock(return_value=mock_agent)

    # Call function
    result = await db.get_store_agent_details(""creator"", ""test-agent"")

    # Verify results
    assert result.slug == ""test-agent""
    assert result.agent_name == ""Test Agent""

    # Verify mock called correctly
    mock_store_agent.return_value.find_first.assert_called_once_with(
        where={""creator_username"": ""creator"", ""slug"": ""test-agent""}
    )


@pytest.mark.asyncio
async def test_get_store_creator_details(mocker):
    # Mock data
    mock_creator_data = prisma.models.Creator(
        name=""Test Creator"",
        username=""creator"",
        description=""Test description"",
        links=[""link1""],
        avatar_url=""avatar.jpg"",
        num_agents=1,
        agent_rating=4.5,
        agent_runs=10,
        top_categories=[""test""],
        is_featured=False,
    )

    # Mock prisma call
    mock_creator = mocker.patch(""prisma.models.Creator.prisma"")
    mock_creator.return_value.find_unique = mocker.AsyncMock()
    # Configure the mock to return values that will pass validation
    mock_creator.return_value.find_unique.return_value = mock_creator_data

    # Call function
    result = await db.get_store_creator_details(""creator"")

    # Verify results
    assert result.username == ""creator""
    assert result.name == ""Test Creator""
    assert result.description == ""Test description""
    assert result.avatar_url == ""avatar.jpg""

    # Verify mock called correctly
    mock_creator.return_value.find_unique.assert_called_once_with(
        where={""username"": ""creator""}
    )


@pytest.mark.asyncio
async def test_create_store_submission(mocker):
    # Mock data
    mock_agent = prisma.models.AgentGraph(
        id=""agent-id"",
        version=1,
        userId=""user-id"",
        createdAt=datetime.now(),
        isActive=True,
        isTemplate=False,
    )

    mock_listing = prisma.models.StoreListing(
        id=""listing-id"",
        createdAt=datetime.now(),
        updatedAt=datetime.now(),
        isDeleted=False,
        isApproved=False,
        agentId=""agent-id"",
        agentVersion=1,
        owningUserId=""user-id"",
    )

    # Mock prisma calls
    mock_agent_graph = mocker.patch(""prisma.models.AgentGraph.prisma"")
    mock_agent_graph.return_value.find_first = mocker.AsyncMock(return_value=mock_agent)

    mock_store_listing = mocker.patch(""prisma.models.StoreListing.prisma"")
    mock_store_listing.return_value.find_first = mocker.AsyncMock(return_value=None)
    mock_store_listing.return_value.create = mocker.AsyncMock(return_value=mock_listing)

    # Call function
    result = await db.create_store_submission(
        user_id=""user-id"",
        agent_id=""agent-id"",
        agent_version=1,
        slug=""test-agent"",
        name=""Test Agent"",
        description=""Test description"",
    )

    # Verify results
    assert result.name == ""Test Agent""
    assert result.description == ""Test description""

    # Verify mocks called correctly
    mock_agent_graph.return_value.find_first.assert_called_once()
    mock_store_listing.return_value.find_first.assert_called_once()
    mock_store_listing.return_value.create.assert_called_once()


@pytest.mark.asyncio
async def test_update_profile(mocker):
    # Mock data
    mock_profile = prisma.models.Profile(
        id=""profile-id"",
        name=""Test Creator"",
        username=""creator"",
        description=""Test description"",
        links=[""link1""],
        avatarUrl=""avatar.jpg"",
        isFeatured=False,
        createdAt=datetime.now(),
        updatedAt=datetime.now(),
    )

    # Mock prisma calls
    mock_profile_db = mocker.patch(""prisma.models.Profile.prisma"")
    mock_profile_db.return_value.find_first = mocker.AsyncMock(
        return_value=mock_profile
    )
    mock_profile_db.return_value.update = mocker.AsyncMock(return_value=mock_profile)

    # Test data
    profile = Profile(
        name=""Test Creator"",
        username=""creator"",
        description=""Test description"",
        links=[""link1""],
        avatar_url=""avatar.jpg"",
        is_featured=False,
    )

    # Call function
    result = await db.update_or_create_profile(""user-id"", profile)

    # Verify results
    assert result.username == ""creator""
    assert result.name == ""Test Creator""

    # Verify mocks called correctly
    mock_profile_db.return_value.find_first.assert_called_once()
    mock_profile_db.return_value.update.assert_called_once()


@pytest.mark.asyncio
async def test_get_user_profile(mocker):
    # Mock data
    mock_profile = prisma.models.Profile(
        id=""profile-id"",
        name=""No Profile Data"",
        username=""testuser"",
        description=""Test description"",
        links=[""link1"", ""link2""],
        avatarUrl=""avatar.jpg"",
        isFeatured=False,
        createdAt=datetime.now(),
        updatedAt=datetime.now(),
    )

    # Mock prisma calls
    mock_profile_db = mocker.patch(""prisma.models.Profile.prisma"")
    mock_profile_db.return_value.find_unique = mocker.AsyncMock(
        return_value=mock_profile
    )

    # Call function
    result = await db.get_user_profile(""user-id"")

    # Verify results
    assert result.name == ""No Profile Data""
    assert result.username == ""No Profile Data""
    assert result.description == ""No Profile Data""
    assert result.links == []
    assert result.avatar_url == """"
","from datetime import datetime
import prisma.errors
import prisma.models
import pytest
from prisma import Prisma
import backend.server.v2.store.db as db
from backend.server.v2.store.model import Profile

@pytest.fixture(autouse=True)
async def setup_prisma():
    # Don't register client if already registered
    try:
        Prisma()
    except prisma.errors.ClientAlreadyRegisteredError:
        pass
    yield

@pytest.mark.asyncio
async def test_get_store_agents(mocker):
    # Mock data
    mock_agents = [
        prisma.models.StoreAgent(
            listing_id=""test-id"",
            storeListingVersionId=""version123"",
            slug=""test-agent"",
            agent_name=""Test Agent"",
            agent_video=None,
            agent_image=[""image.jpg""],
            featured=False,
            creator_username=""creator"",
            creator_avatar=""avatar.jpg"",
            sub_heading=""Test heading"",
            description=""Test description"",
            categories=[],
            runs=10,
            rating=4.5,
            versions=[""1.0""],
            updated_at=datetime.now(),
        )
    ]

    # Mock prisma calls
    mock_store_agent = mocker.patch(""prisma.models.StoreAgent.prisma"")
    mock_store_agent.return_value.find_many = mocker.AsyncMock(return_value=mock_agents)
    mock_store_agent.return_value.count = mocker.AsyncMock(return_value=1)

    # Call function
    result = await db.get_store_agents()

    # Verify results
    assert len(result.agents) == 1
    assert result.agents[0].slug == ""test-agent""
    assert result.pagination.total_items == 1

    # Verify mocks called correctly
    mock_store_agent.return_value.find_many.assert_called_once()
    mock_store_agent.return_value.count.assert_called_once()

@pytest.mark.asyncio
async def test_get_store_agent_details(mocker):
    # Mock data
    mock_agent = prisma.models.StoreAgent(
        listing_id=""test-id"",
        storeListingVersionId=""version123"",
        slug=""test-agent"",
        agent_name=""Test Agent"",
        agent_video=""video.mp4"",
        agent_image=[""image.jpg""],
        featured=False,
        creator_username=""creator"",
        creator_avatar=""avatar.jpg"",
        sub_heading=""Test heading"",
        description=""Test description"",
        categories=[""test""],
        runs=10,
        rating=4.5,
        versions=[""1.0""],
        updated_at=datetime.now(),
    )

    # Mock prisma call
    mock_store_agent = mocker.patch(""prisma.models.StoreAgent.prisma"")
    mock_store_agent.return_value.find_first = mocker.AsyncMock(return_value=mock_agent)

    # Call function
    result = await db.get_store_agent_details(""creator"", ""test-agent"")

    # Verify results
    assert result.slug == ""test-agent""
    assert result.agent_name == ""Test Agent""

    # Verify mock called correctly
    mock_store_agent.return_value.find_first.assert_called_once_with(
        where={""creator_username"": ""creator"", ""slug"": ""test-agent""}
    )

@pytest.mark.asyncio
async def test_get_store_creator_details(mocker):
    # Mock data
    mock_creator_data = prisma.models.Creator(
        name=""Test Creator"",
        username=""creator"",
        description=""Test description"",
        links=[""link1""],
        avatar_url=""avatar.jpg"",
        num_agents=1,
        agent_rating=4.5,
        agent_runs=10,
        top_categories=[""test""],
        is_featured=False,
    )

    # Mock prisma call
    mock_creator = mocker.patch(""prisma.models.Creator.prisma"")
    mock_creator.return_value.find_unique = mocker.AsyncMock()
    # Configure the mock to return values that will pass validation
    mock_creator.return_value.find_unique.return_value = mock_creator_data

    # Call function
    result = await db.get_store_creator_details(""creator"")

    # Verify results
    assert result.username == ""creator""
    assert result.name == ""Test Creator""
    assert result.description == ""Test description""
    assert result.avatar_url == ""avatar.jpg""

    # Verify mock called correctly
    mock_creator.return_value.find_unique.assert_called_once_with(
        where={""username"": ""creator""}
    )

@pytest.mark.asyncio
async def test_create_store_submission(mocker):
    # Mock data
    mock_agent = prisma.models.AgentGraph(
        id=""agent-id"",
        version=1,
        userId=""user-id"",
        createdAt=datetime.now(),
        isActive=True,
        isTemplate=False,
    )

    mock_listing = prisma.models.StoreListing(
        id=""listing-id"",
        createdAt=datetime.now(),
        updatedAt=datetime.now(),
        isDeleted=False,
        isApproved=False,
        agentId=""agent-id"",
        agentVersion=1,
        owningUserId=""user-id"",
    )

    # Mock prisma calls
    mock_agent_graph = mocker.patch(""prisma.models.AgentGraph.prisma"")
    mock_agent_graph.return_value.find_first = mocker.AsyncMock(return_value=mock_agent)

    mock_store_listing = mocker.patch(""prisma.models.StoreListing.prisma"")
    mock_store_listing.return_value.find_first = mocker.AsyncMock(return_value=None)
    mock_store_listing.return_value.create = mocker.AsyncMock(return_value=mock_listing)

    # Call function
    result = await db.create_store_submission(
        user_id=""user-id"",
        agent_id=""agent-id"",
        agent_version=1,
        slug=""test-agent"",
        name=""Test Agent"",
        description=""Test description"",
    )

    # Verify results
    assert result.name == ""Test Agent""
    assert result.description == ""Test description""

    # Verify mocks called correctly
    mock_agent_graph.return_value.find_first.assert_called_once()
    mock_store_listing.return_value.find_first.assert_called_once()
    mock_store_listing.return_value.create.assert_called_once()

@pytest.mark.asyncio
async def test_update_profile(mocker):
    # Mock data
    mock_profile = prisma.models.Profile(
        id=""profile-id"",
        name=""Test Creator"",
        username=""creator"",
        description=""Test description"",
        links=[""link1""],
        avatarUrl=""avatar.jpg"",
        isFeatured=False,
        createdAt=datetime.now(),
        updatedAt=datetime.now(),
    )

    # Mock prisma calls
    mock_profile_db = mocker.patch(""prisma.models.Profile.prisma"")
    mock_profile_db.return_value.find_first = mocker.AsyncMock(
        return_value=mock_profile
    )
    mock_profile_db.return_value.update = mocker.AsyncMock(return_value=mock_profile)

    # Test data
    profile = Profile(
        name=""Test Creator"",
        username=""creator"",
        description=""Test description"",
        links=[""link1""],
        avatar_url=""avatar.jpg"",
        is_featured=False,
    )

    # Call function
    result = await db.update_or_create_profile(""user-id"", profile)

    # Verify results
    assert result.username == ""creator""
    assert result.name == ""Test Creator""

    # Verify mocks called correctly
    mock_profile_db.return_value.find_first.assert_called_once()
    mock_profile_db.return_value.update.assert_called_once()

@pytest.mark.asyncio
async def test_get_user_profile(mocker):
    # Mock data
    mock_profile = prisma.models.Profile(
        id=""profile-id"",
        name=""No Profile Data"",
        username=""testuser"",
        description=""Test description"",
        links=[""link1"", ""link2""],
        avatarUrl=""avatar.jpg"",
        isFeatured=False,
        createdAt=datetime.now(),
        updatedAt=datetime.now(),
    )

    # Mock prisma calls
    mock_profile_db = mocker.patch(""prisma.models.Profile.prisma"")
    mock_profile_db.return_value.find_unique = mocker.AsyncMock(
        return_value=mock_profile
    )

    # Call function
    result = await db.get_user_profile(""user-id"")

    # Verify results
    assert result.name == ""No Profile Data""
    assert result.username == ""No Profile Data""
    assert result.description == ""No Profile Data""
    assert result.links == []
    assert result.avatar_url == """"","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
189,"from pydantic import BaseModel


class LogConfig(BaseModel):
    """"""Logging configuration to be set for the server""""""

    LOGGER_NAME: str = ""marketplace""
    LOG_FORMAT: str = ""%(levelprefix)s | %(asctime)s | %(message)s""
    LOG_LEVEL: str = ""DEBUG""

    # Logging config
    version: int = 1
    disable_existing_loggers: bool = False
    formatters: dict = {
        ""default"": {
            ""()"": ""uvicorn.logging.DefaultFormatter"",
            ""fmt"": LOG_FORMAT,
            ""datefmt"": ""%Y-%m-%d %H:%M:%S"",
        },
    }
    handlers: dict = {
        ""default"": {
            ""formatter"": ""default"",
            ""class"": ""logging.StreamHandler"",
            ""stream"": ""ext://sys.stderr"",
        },
    }
    loggers: dict = {
        LOGGER_NAME: {""handlers"": [""default""], ""level"": LOG_LEVEL},
    }
","from pydantic import BaseModel

class LogConfig(BaseModel):
    """"""Logging configuration to be set for the server""""""

    LOGGER_NAME: str = ""marketplace""
    LOG_FORMAT: str = ""%(levelprefix)s | %(asctime)s | %(message)s""
    LOG_LEVEL: str = ""DEBUG""

    # Logging config
    version: int = 1
    disable_existing_loggers: bool = False

    formatters: dict = {
        ""default"": {
            ""()"": ""uvicorn.logging.DefaultFormatter"",
            ""fmt"": LOG_FORMAT,
            ""datefmt"": ""%Y-%m-%d %H:%M:%S"",
        },
    }

    handlers: dict = {
        ""default"": {
            ""formatter"": ""default"",
            ""class"": ""logging.StreamHandler"",
            ""stream"": ""ext://sys.stderr"",
        },
    }

    loggers: dict = {
        LOGGER_NAME: {""handlers"": [""default""], ""level"": LOG_LEVEL},
    }","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
190,"class MissingConfigError(Exception):
    """"""The attempted operation requires configuration which is not available""""""


class NeedConfirmation(Exception):
    """"""The user must explicitly confirm that they want to proceed""""""
","class MissingConfigError(Exception):
    """"""The attempted operation requires configuration which is not available""""""
    pass

class NeedConfirmation(Exception):
    """"""The user must explicitly confirm that they want to proceed""""""
    pass","{'final_score': 91.05, 'length_similarity': 92.44, 'token_similarity': 96.15, 'structure_similarity': 66.67, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
191,"# Knight Tour Intro: https://www.youtube.com/watch?v=ab_dY3dZFHM

from __future__ import annotations


def get_valid_pos(position: tuple[int, int], n: int) -> list[tuple[int, int]]:
    """"""
    Find all the valid positions a knight can move to from the current position.

    >>> get_valid_pos((1, 3), 4)
    [(2, 1), (0, 1), (3, 2)]
    """"""

    y, x = position
    positions = [
        (y + 1, x + 2),
        (y - 1, x + 2),
        (y + 1, x - 2),
        (y - 1, x - 2),
        (y + 2, x + 1),
        (y + 2, x - 1),
        (y - 2, x + 1),
        (y - 2, x - 1),
    ]
    permissible_positions = []

    for inner_position in positions:
        y_test, x_test = inner_position
        if 0 <= y_test < n and 0 <= x_test < n:
            permissible_positions.append(inner_position)

    return permissible_positions


def is_complete(board: list[list[int]]) -> bool:
    """"""
    Check if the board (matrix) has been completely filled with non-zero values.

    >>> is_complete([[1]])
    True

    >>> is_complete([[1, 2], [3, 0]])
    False
    """"""

    return not any(elem == 0 for row in board for elem in row)


def open_knight_tour_helper(
    board: list[list[int]], pos: tuple[int, int], curr: int
) -> bool:
    """"""
    Helper function to solve knight tour problem.
    """"""

    if is_complete(board):
        return True

    for position in get_valid_pos(pos, len(board)):
        y, x = position

        if board[y][x] == 0:
            board[y][x] = curr + 1
            if open_knight_tour_helper(board, position, curr + 1):
                return True
            board[y][x] = 0

    return False


def open_knight_tour(n: int) -> list[list[int]]:
    """"""
    Find the solution for the knight tour problem for a board of size n. Raises
    ValueError if the tour cannot be performed for the given size.

    >>> open_knight_tour(1)
    [[1]]

    >>> open_knight_tour(2)
    Traceback (most recent call last):
        ...
    ValueError: Open Knight Tour cannot be performed on a board of size 2
    """"""

    board = [[0 for i in range(n)] for j in range(n)]

    for i in range(n):
        for j in range(n):
            board[i][j] = 1
            if open_knight_tour_helper(board, (i, j), 1):
                return board
            board[i][j] = 0

    msg = f""Open Knight Tour cannot be performed on a board of size {n}""
    raise ValueError(msg)


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","from __future__ import annotations

def get_valid_pos(position: tuple[int, int], n: int) -> list[tuple[int, int]]:
    """"""
    Find all the valid positions a knight can move to from the current position.

    >>> get_valid_pos((1, 3), 4)
    [(2, 1), (0, 1), (3, 2)]
    """"""
    y, x = position
    positions = [
        (y + 1, x + 2),
        (y - 1, x + 2),
        (y + 1, x - 2),
        (y - 1, x - 2),
        (y + 2, x + 1),
        (y + 2, x - 1),
        (y - 2, x + 1),
        (y - 2, x - 1),
    ]
    permissible_positions = []
    for inner_position in positions:
        y_test, x_test = inner_position
        if 0 <= y_test < n and 0 <= x_test < n:
            permissible_positions.append(inner_position)
    return permissible_positions

def is_complete(board: list[list[int]]) -> bool:
    """"""
    Check if the board (matrix) has been completely filled with non-zero values.

    >>> is_complete([[1]])
    True

    >>> is_complete([[1, 2], [3, 0]])
    False
    """"""
    return not any(elem == 0 for row in board for elem in row)

def open_knight_tour_helper(
    board: list[list[int]], pos: tuple[int, int], curr: int
) -> bool:
    """"""
    Helper function to solve knight tour problem.
    """"""
    if is_complete(board):
        return True

    for position in get_valid_pos(pos, len(board)):
        y, x = position
        if board[y][x] == 0:
            board[y][x] = curr + 1
            if open_knight_tour_helper(board, position, curr + 1):
                return True
            board[y][x] = 0

    return False

def open_knight_tour(n: int) -> list[list[int]]:
    """"""
    Find the solution for the knight tour problem for a board of size n. Raises
    ValueError if the tour cannot be performed for the given size.

    >>> open_knight_tour(1)
    [[1]]

    >>> open_knight_tour(2)
    Traceback (most recent call last):
        ...
    ValueError: Open Knight Tour cannot be performed on a board of size 2
    """"""
    board = [[0 for i in range(n)] for j in range(n)]
    for i in range(n):
        for j in range(n):
            board[i][j] = 1
            if open_knight_tour_helper(board, (i, j), 1):
                return board
            board[i][j] = 0

    msg = f""Open Knight Tour cannot be performed on a board of size {n}""
    raise ValueError(msg)

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 98.96, 'length_similarity': 98.83, 'token_similarity': 98.03, 'structure_similarity': 98.63, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
192,"import operator
from enum import Enum
from typing import Any

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class Operation(Enum):
    ADD = ""Add""
    SUBTRACT = ""Subtract""
    MULTIPLY = ""Multiply""
    DIVIDE = ""Divide""
    POWER = ""Power""


class CalculatorBlock(Block):
    class Input(BlockSchema):
        operation: Operation = SchemaField(
            description=""Choose the math operation you want to perform"",
            placeholder=""Select an operation"",
        )
        a: float = SchemaField(
            description=""Enter the first number (A)"", placeholder=""For example: 10""
        )
        b: float = SchemaField(
            description=""Enter the second number (B)"", placeholder=""For example: 5""
        )
        round_result: bool = SchemaField(
            description=""Do you want to round the result to a whole number?"",
            default=False,
        )

    class Output(BlockSchema):
        result: float = SchemaField(description=""The result of your calculation"")

    def __init__(self):
        super().__init__(
            id=""b1ab9b19-67a6-406d-abf5-2dba76d00c79"",
            input_schema=CalculatorBlock.Input,
            output_schema=CalculatorBlock.Output,
            description=""Performs a mathematical operation on two numbers."",
            categories={BlockCategory.LOGIC},
            test_input={
                ""operation"": Operation.ADD.value,
                ""a"": 10.0,
                ""b"": 5.0,
                ""round_result"": False,
            },
            test_output=[
                (""result"", 15.0),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        operation = input_data.operation
        a = input_data.a
        b = input_data.b

        operations = {
            Operation.ADD: operator.add,
            Operation.SUBTRACT: operator.sub,
            Operation.MULTIPLY: operator.mul,
            Operation.DIVIDE: operator.truediv,
            Operation.POWER: operator.pow,
        }

        op_func = operations[operation]

        try:
            if operation == Operation.DIVIDE and b == 0:
                raise ZeroDivisionError(""Cannot divide by zero"")

            result = op_func(a, b)

            if input_data.round_result:
                result = round(result)

            yield ""result"", result

        except ZeroDivisionError:
            yield ""result"", float(""inf"")  # Return infinity for division by zero
        except Exception:
            yield ""result"", float(""nan"")  # Return NaN for other errors


class CountItemsBlock(Block):
    class Input(BlockSchema):
        collection: Any = SchemaField(
            description=""Enter the collection you want to count. This can be a list, dictionary, string, or any other iterable."",
            placeholder=""For example: [1, 2, 3] or {'a': 1, 'b': 2} or 'hello'"",
        )

    class Output(BlockSchema):
        count: int = SchemaField(description=""The number of items in the collection"")

    def __init__(self):
        super().__init__(
            id=""3c9c2f42-b0c3-435f-ba35-05f7a25c772a"",
            input_schema=CountItemsBlock.Input,
            output_schema=CountItemsBlock.Output,
            description=""Counts the number of items in a collection."",
            categories={BlockCategory.LOGIC},
            test_input={""collection"": [1, 2, 3, 4, 5]},
            test_output=[
                (""count"", 5),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        collection = input_data.collection

        try:
            if isinstance(collection, (str, list, tuple, set, dict)):
                count = len(collection)
            elif hasattr(collection, ""__iter__""):
                count = sum(1 for _ in collection)
            else:
                raise ValueError(""Input is not a countable collection"")

            yield ""count"", count

        except Exception:
            yield ""count"", -1  # Return -1 to indicate an error
","import operator
from enum import Enum
from typing import Any

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField


class Operation(Enum):
    ADD = ""Add""
    SUBTRACT = ""Subtract""
    MULTIPLY = ""Multiply""
    DIVIDE = ""Divide""
    POWER = ""Power""


class CalculatorBlock(Block):
    class Input(BlockSchema):
        operation: Operation = SchemaField(
            description=""Choose the math operation you want to perform"",
            placeholder=""Select an operation"",
        )
        a: float = SchemaField(
            description=""Enter the first number (A)"", placeholder=""For example: 10""
        )
        b: float = SchemaField(
            description=""Enter the second number (B)"", placeholder=""For example: 5""
        )
        round_result: bool = SchemaField(
            description=""Do you want to round the result to a whole number?"",
            default=False,
        )

    class Output(BlockSchema):
        result: float = SchemaField(description=""The result of your calculation"")

    def __init__(self):
        super().__init__(
            id=""b1ab9b19-67a6-406d-abf5-2dba76d00c79"",
            input_schema=CalculatorBlock.Input,
            output_schema=CalculatorBlock.Output,
            description=""Performs a mathematical operation on two numbers."",
            categories={BlockCategory.LOGIC},
            test_input={
                ""operation"": Operation.ADD.value,
                ""a"": 10.0,
                ""b"": 5.0,
                ""round_result"": False,
            },
            test_output=[
                (""result"", 15.0),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        operation = input_data.operation
        a = input_data.a
        b = input_data.b

        operations = {
            Operation.ADD: operator.add,
            Operation.SUBTRACT: operator.sub,
            Operation.MULTIPLY: operator.mul,
            Operation.DIVIDE: operator.truediv,
            Operation.POWER: operator.pow,
        }

        op_func = operations[operation]

        try:
            if operation == Operation.DIVIDE and b == 0:
                raise ZeroDivisionError(""Cannot divide by zero"")

            result = op_func(a, b)

            if input_data.round_result:
                result = round(result)

            yield ""result"", result

        except ZeroDivisionError:
            yield ""result"", float(""inf"")  # Return infinity for division by zero
        except Exception:
            yield ""result"", float(""nan"")  # Return NaN for other errors


class CountItemsBlock(Block):
    class Input(BlockSchema):
        collection: Any = SchemaField(
            description=""Enter the collection you want to count. This can be a list, dictionary, string, or any other iterable."",
            placeholder=""For example: [1, 2, 3] or {'a': 1, 'b': 2} or 'hello'"",
        )

    class Output(BlockSchema):
        count: int = SchemaField(description=""The number of items in the collection"")

    def __init__(self):
        super().__init__(
            id=""3c9c2f42-b0c3-435f-ba35-05f7a25c772a"",
            input_schema=CountItemsBlock.Input,
            output_schema=CountItemsBlock.Output,
            description=""Counts the number of items in a collection."",
            categories={BlockCategory.LOGIC},
            test_input={""collection"": [1, 2, 3, 4, 5]},
            test_output=[
                (""count"", 5),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        collection = input_data.collection

        try:
            if isinstance(collection, (str, list, tuple, set, dict)):
                count = len(collection)
            elif hasattr(collection, ""__iter__""):
                count = sum(1 for _ in collection)
            else:
                raise ValueError(""Input is not a countable collection"")

            yield ""count"", count
        except Exception:
            yield ""count"", -1  # Return -1 to indicate an error","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
193,"import pytest

from .depends import requires_admin_user, requires_user, verify_user


def test_verify_user_no_payload():
    user = verify_user(None, admin_only=False)
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""admin""


def test_verify_user_no_user_id():
    with pytest.raises(Exception):
        verify_user({""role"": ""admin""}, admin_only=False)


def test_verify_user_not_admin():
    with pytest.raises(Exception):
        verify_user(
            {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""},
            admin_only=True,
        )


def test_verify_user_with_admin_role():
    user = verify_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""admin""},
        admin_only=True,
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""admin""


def test_verify_user_with_user_role():
    user = verify_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""},
        admin_only=False,
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""user""


def test_requires_user():
    user = requires_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""}
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""user""


def test_requires_user_no_user_id():
    with pytest.raises(Exception):
        requires_user({""role"": ""user""})


def test_requires_admin_user():
    user = requires_admin_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""admin""}
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""admin""


def test_requires_admin_user_not_admin():
    with pytest.raises(Exception):
        requires_admin_user(
            {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""}
        )
","import pytest
from .depends import requires_admin_user, requires_user, verify_user

def test_verify_user_no_payload():
    user = verify_user(None, admin_only=False)
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""admin""

def test_verify_user_no_user_id():
    with pytest.raises(Exception):
        verify_user({""role"": ""admin""}, admin_only=False)

def test_verify_user_not_admin():
    with pytest.raises(Exception):
        verify_user(
            {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""},
            admin_only=True,
        )

def test_verify_user_with_admin_role():
    user = verify_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""admin""},
        admin_only=True,
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""admin""

def test_verify_user_with_user_role():
    user = verify_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""},
        admin_only=False,
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""user""

def test_requires_user():
    user = requires_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""}
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""user""

def test_requires_user_no_user_id():
    with pytest.raises(Exception):
        requires_user({""role"": ""user""})

def test_requires_admin_user():
    user = requires_admin_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""admin""}
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""admin""

def test_requires_admin_user_not_admin():
    with pytest.raises(Exception):
        requires_admin_user(
            {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""}
        )","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
194,"""""""
In this problem, we want to determine all possible permutations
of the given sequence. We use backtracking to solve this problem.

Time complexity: O(n! * n),
where n denotes the length of the given sequence.
""""""

from __future__ import annotations


def generate_all_permutations(sequence: list[int | str]) -> None:
    create_state_space_tree(sequence, [], 0, [0 for i in range(len(sequence))])


def create_state_space_tree(
    sequence: list[int | str],
    current_sequence: list[int | str],
    index: int,
    index_used: list[int],
) -> None:
    """"""
    Creates a state space tree to iterate through each branch using DFS.
    We know that each state has exactly len(sequence) - index children.
    It terminates when it reaches the end of the given sequence.

    :param sequence: The input sequence for which permutations are generated.
    :param current_sequence: The current permutation being built.
    :param index: The current index in the sequence.
    :param index_used: list to track which elements are used in permutation.

    Example 1:
    >>> sequence = [1, 2, 3]
    >>> current_sequence = []
    >>> index_used = [False, False, False]
    >>> create_state_space_tree(sequence, current_sequence, 0, index_used)
    [1, 2, 3]
    [1, 3, 2]
    [2, 1, 3]
    [2, 3, 1]
    [3, 1, 2]
    [3, 2, 1]

    Example 2:
    >>> sequence = [""A"", ""B"", ""C""]
    >>> current_sequence = []
    >>> index_used = [False, False, False]
    >>> create_state_space_tree(sequence, current_sequence, 0, index_used)
    ['A', 'B', 'C']
    ['A', 'C', 'B']
    ['B', 'A', 'C']
    ['B', 'C', 'A']
    ['C', 'A', 'B']
    ['C', 'B', 'A']

    Example 3:
    >>> sequence = [1]
    >>> current_sequence = []
    >>> index_used = [False]
    >>> create_state_space_tree(sequence, current_sequence, 0, index_used)
    [1]
    """"""

    if index == len(sequence):
        print(current_sequence)
        return

    for i in range(len(sequence)):
        if not index_used[i]:
            current_sequence.append(sequence[i])
            index_used[i] = True
            create_state_space_tree(sequence, current_sequence, index + 1, index_used)
            current_sequence.pop()
            index_used[i] = False


""""""
remove the comment to take an input from the user

print(""Enter the elements"")
sequence = list(map(int, input().split()))
""""""

sequence: list[int | str] = [3, 1, 2, 4]
generate_all_permutations(sequence)

sequence_2: list[int | str] = [""A"", ""B"", ""C""]
generate_all_permutations(sequence_2)
","""""""
In this problem, we want to determine all possible permutations
of the given sequence. We use backtracking to solve this problem.

Time complexity: O(n! * n),
where n denotes the length of the given sequence.
""""""

from __future__ import annotations


def generate_all_permutations(sequence: list[int | str]) -> None:
    create_state_space_tree(sequence, [], 0, [0 for i in range(len(sequence))])


def create_state_space_tree(
    sequence: list[int | str],
    current_sequence: list[int | str],
    index: int,
    index_used: list[int],
) -> None:
    """"""
    Creates a state space tree to iterate through each branch using DFS.
    We know that each state has exactly len(sequence) - index children.
    It terminates when it reaches the end of the given sequence.

    :param sequence: The input sequence for which permutations are generated.
    :param current_sequence: The current permutation being built.
    :param index: The current index in the sequence.
    :param index_used: list to track which elements are used in permutation.

    Example 1:
    >>> sequence = [1, 2, 3]
    >>> current_sequence = []
    >>> index_used = [False, False, False]
    >>> create_state_space_tree(sequence, current_sequence, 0, index_used)
    [1, 2, 3]
    [1, 3, 2]
    [2, 1, 3]
    [2, 3, 1]
    [3, 1, 2]
    [3, 2, 1]

    Example 2:
    >>> sequence = [""A"", ""B"", ""C""]
    >>> current_sequence = []
    >>> index_used = [False, False, False]
    >>> create_state_space_tree(sequence, current_sequence, 0, index_used)
    ['A', 'B', 'C']
    ['A', 'C', 'B']
    ['B', 'A', 'C']
    ['B', 'C', 'A']
    ['C', 'A', 'B']
    ['C', 'B', 'A']

    Example 3:
    >>> sequence = [1]
    >>> current_sequence = []
    >>> index_used = [False]
    >>> create_state_space_tree(sequence, current_sequence, 0, index_used)
    [1]
    """"""
    if index == len(sequence):
        print(current_sequence)
        return

    for i in range(len(sequence)):
        if not index_used[i]:
            current_sequence.append(sequence[i])
            index_used[i] = True
            create_state_space_tree(sequence, current_sequence, index + 1, index_used)
            current_sequence.pop()
            index_used[i] = False


""""""
remove the comment to take an input from the user

print(""Enter the elements"")
sequence = list(map(int, input().split()))
""""""

sequence: list[int | str] = [3, 1, 2, 4]
generate_all_permutations(sequence)

sequence_2: list[int | str] = [""A"", ""B"", ""C""]
generate_all_permutations(sequence_2)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
195,"import uuid
from typing import Any, Literal

from pinecone import Pinecone, ServerlessSpec

from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

PineconeCredentials = APIKeyCredentials
PineconeCredentialsInput = CredentialsMetaInput[
    Literal[ProviderName.PINECONE],
    Literal[""api_key""],
]


def PineconeCredentialsField() -> PineconeCredentialsInput:
    """"""Creates a Pinecone credentials input on a block.""""""
    return CredentialsField(
        description=""The Pinecone integration can be used with an API Key."",
    )


class PineconeInitBlock(Block):
    class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        index_name: str = SchemaField(description=""Name of the Pinecone index"")
        dimension: int = SchemaField(
            description=""Dimension of the vectors"", default=768
        )
        metric: str = SchemaField(
            description=""Distance metric for the index"", default=""cosine""
        )
        cloud: str = SchemaField(
            description=""Cloud provider for serverless"", default=""aws""
        )
        region: str = SchemaField(
            description=""Region for serverless"", default=""us-east-1""
        )

    class Output(BlockSchema):
        index: str = SchemaField(description=""Name of the initialized Pinecone index"")
        message: str = SchemaField(description=""Status message"")

    def __init__(self):
        super().__init__(
            id=""48d8fdab-8f03-41f3-8407-8107ba11ec9b"",
            description=""Initializes a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeInitBlock.Input,
            output_schema=PineconeInitBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        pc = Pinecone(api_key=credentials.api_key.get_secret_value())

        try:
            existing_indexes = pc.list_indexes()
            if input_data.index_name not in [index.name for index in existing_indexes]:
                pc.create_index(
                    name=input_data.index_name,
                    dimension=input_data.dimension,
                    metric=input_data.metric,
                    spec=ServerlessSpec(
                        cloud=input_data.cloud, region=input_data.region
                    ),
                )
                message = f""Created new index: {input_data.index_name}""
            else:
                message = f""Using existing index: {input_data.index_name}""

            yield ""index"", input_data.index_name
            yield ""message"", message
        except Exception as e:
            yield ""message"", f""Error initializing Pinecone index: {str(e)}""


class PineconeQueryBlock(Block):
    class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        query_vector: list = SchemaField(description=""Query vector"")
        namespace: str = SchemaField(
            description=""Namespace to query in Pinecone"", default=""""
        )
        top_k: int = SchemaField(
            description=""Number of top results to return"", default=3
        )
        include_values: bool = SchemaField(
            description=""Whether to include vector values in the response"",
            default=False,
        )
        include_metadata: bool = SchemaField(
            description=""Whether to include metadata in the response"", default=True
        )
        host: str = SchemaField(description=""Host for pinecone"", default="""")
        idx_name: str = SchemaField(description=""Index name for pinecone"")

    class Output(BlockSchema):
        results: Any = SchemaField(description=""Query results from Pinecone"")
        combined_results: Any = SchemaField(
            description=""Combined results from Pinecone""
        )

    def __init__(self):
        super().__init__(
            id=""9ad93d0f-91b4-4c9c-8eb1-82e26b4a01c5"",
            description=""Queries a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeQueryBlock.Input,
            output_schema=PineconeQueryBlock.Output,
        )

    def run(
        self,
        input_data: Input,
        *,
        credentials: APIKeyCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            # Create a new client instance
            pc = Pinecone(api_key=credentials.api_key.get_secret_value())

            # Get the index
            idx = pc.Index(input_data.idx_name)

            # Ensure query_vector is in correct format
            query_vector = input_data.query_vector
            if isinstance(query_vector, list) and len(query_vector) > 0:
                if isinstance(query_vector[0], list):
                    query_vector = query_vector[0]

            results = idx.query(
                namespace=input_data.namespace,
                vector=query_vector,
                top_k=input_data.top_k,
                include_values=input_data.include_values,
                include_metadata=input_data.include_metadata,
            ).to_dict()  # type: ignore
            combined_text = """"
            if results[""matches""]:
                texts = [
                    match[""metadata""][""text""]
                    for match in results[""matches""]
                    if match.get(""metadata"", {}).get(""text"")
                ]
                combined_text = ""\n\n"".join(texts)

            # Return both the raw matches and combined text
            yield ""results"", {
                ""matches"": results[""matches""],
                ""combined_text"": combined_text,
            }
            yield ""combined_results"", combined_text

        except Exception as e:
            error_msg = f""Error querying Pinecone: {str(e)}""
            raise RuntimeError(error_msg) from e


class PineconeInsertBlock(Block):
    class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        index: str = SchemaField(description=""Initialized Pinecone index"")
        chunks: list = SchemaField(description=""List of text chunks to ingest"")
        embeddings: list = SchemaField(
            description=""List of embeddings corresponding to the chunks""
        )
        namespace: str = SchemaField(
            description=""Namespace to use in Pinecone"", default=""""
        )
        metadata: dict = SchemaField(
            description=""Additional metadata to store with each vector"", default={}
        )

    class Output(BlockSchema):
        upsert_response: str = SchemaField(
            description=""Response from Pinecone upsert operation""
        )

    def __init__(self):
        super().__init__(
            id=""477f2168-cd91-475a-8146-9499a5982434"",
            description=""Upload data to a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeInsertBlock.Input,
            output_schema=PineconeInsertBlock.Output,
        )

    def run(
        self,
        input_data: Input,
        *,
        credentials: APIKeyCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            # Create a new client instance
            pc = Pinecone(api_key=credentials.api_key.get_secret_value())

            # Get the index
            idx = pc.Index(input_data.index)

            vectors = []
            for chunk, embedding in zip(input_data.chunks, input_data.embeddings):
                vector_metadata = input_data.metadata.copy()
                vector_metadata[""text""] = chunk
                vectors.append(
                    {
                        ""id"": str(uuid.uuid4()),
                        ""values"": embedding,
                        ""metadata"": vector_metadata,
                    }
                )
            idx.upsert(vectors=vectors, namespace=input_data.namespace)

            yield ""upsert_response"", ""successfully upserted""

        except Exception as e:
            error_msg = f""Error uploading to Pinecone: {str(e)}""
            raise RuntimeError(error_msg) from e
","import uuid
from typing import Any, Literal
from pinecone import Pinecone, ServerlessSpec
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import (
    APIKeyCredentials,
    CredentialsField,
    CredentialsMetaInput,
    SchemaField,
)
from backend.integrations.providers import ProviderName

PineconeCredentials = APIKeyCredentials

PineconeCredentialsInput = CredentialsMetaInput[
    Literal[ProviderName.PINECONE],
    Literal[""api_key""],
]

def PineconeCredentialsField() -> PineconeCredentialsInput:
    """"""Creates a Pinecone credentials input on a block.""""""
    return CredentialsField(
        description=""The Pinecone integration can be used with an API Key."",
    )

class PineconeInitBlock(Block):
    class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        index_name: str = SchemaField(description=""Name of the Pinecone index"")
        dimension: int = SchemaField(
            description=""Dimension of the vectors"", default=768
        )
        metric: str = SchemaField(
            description=""Distance metric for the index"", default=""cosine""
        )
        cloud: str = SchemaField(
            description=""Cloud provider for serverless"", default=""aws""
        )
        region: str = SchemaField(
            description=""Region for serverless"", default=""us-east-1""
        )

    class Output(BlockSchema):
        index: str = SchemaField(description=""Name of the initialized Pinecone index"")
        message: str = SchemaField(description=""Status message"")

    def __init__(self):
        super().__init__(
            id=""48d8fdab-8f03-41f3-8407-8107ba11ec9b"",
            description=""Initializes a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeInitBlock.Input,
            output_schema=PineconeInitBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        pc = Pinecone(api_key=credentials.api_key.get_secret_value())

        try:
            existing_indexes = pc.list_indexes()
            if input_data.index_name not in [index.name for index in existing_indexes]:
                pc.create_index(
                    name=input_data.index_name,
                    dimension=input_data.dimension,
                    metric=input_data.metric,
                    spec=ServerlessSpec(
                        cloud=input_data.cloud, region=input_data.region
                    ),
                )
                message = f""Created new index: {input_data.index_name}""
            else:
                message = f""Using existing index: {input_data.index_name}""

            yield ""index"", input_data.index_name
            yield ""message"", message
        except Exception as e:
            yield ""message"", f""Error initializing Pinecone index: {str(e)}""

class PineconeQueryBlock(Block):
    class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        query_vector: list = SchemaField(description=""Query vector"")
        namespace: str = SchemaField(
            description=""Namespace to query in Pinecone"", default=""""
        )
        top_k: int = SchemaField(
            description=""Number of top results to return"", default=3
        )
        include_values: bool = SchemaField(
            description=""Whether to include vector values in the response"",
            default=False,
        )
        include_metadata: bool = SchemaField(
            description=""Whether to include metadata in the response"", default=True
        )
        host: str = SchemaField(description=""Host for pinecone"", default="""")
        idx_name: str = SchemaField(description=""Index name for pinecone"")

    class Output(BlockSchema):
        results: Any = SchemaField(description=""Query results from Pinecone"")
        combined_results: Any = SchemaField(
            description=""Combined results from Pinecone""
        )

    def __init__(self):
        super().__init__(
            id=""9ad93d0f-91b4-4c9c-8eb1-82e26b4a01c5"",
            description=""Queries a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeQueryBlock.Input,
            output_schema=PineconeQueryBlock.Output,
        )

    def run(
        self,
        input_data: Input,
        *,
        credentials: APIKeyCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            # Create a new client instance
            pc = Pinecone(api_key=credentials.api_key.get_secret_value())
            # Get the index
            idx = pc.Index(input_data.idx_name)
            # Ensure query_vector is in correct format
            query_vector = input_data.query_vector
            if isinstance(query_vector, list) and len(query_vector) > 0:
                if isinstance(query_vector[0], list):
                    query_vector = query_vector[0]

            results = idx.query(
                namespace=input_data.namespace,
                vector=query_vector,
                top_k=input_data.top_k,
                include_values=input_data.include_values,
                include_metadata=input_data.include_metadata,
            ).to_dict()  # type: ignore
            
            combined_text = """"
            if results[""matches""]:
                texts = [
                    match[""metadata""][""text""]
                    for match in results[""matches""]
                    if match.get(""metadata"", {}).get(""text"")
                ]
                combined_text = ""\n\n"".join(texts)

            # Return both the raw matches and combined text
            yield ""results"", {
                ""matches"": results[""matches""],
                ""combined_text"": combined_text,
            }
            yield ""combined_results"", combined_text
        except Exception as e:
            error_msg = f""Error querying Pinecone: {str(e)}""
            raise RuntimeError(error_msg) from e

class PineconeInsertBlock(Block):
    class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        index: str = SchemaField(description=""Initialized Pinecone index"")
        chunks: list = SchemaField(description=""List of text chunks to ingest"")
        embeddings: list = SchemaField(
            description=""List of embeddings corresponding to the chunks""
        )
        namespace: str = SchemaField(
            description=""Namespace to use in Pinecone"", default=""""
        )
        metadata: dict = SchemaField(
            description=""Additional metadata to store with each vector"", default={}
        )

    class Output(BlockSchema):
        upsert_response: str = SchemaField(
            description=""Response from Pinecone upsert operation""
        )

    def __init__(self):
        super().__init__(
            id=""477f2168-cd91-475a-8146-9499a5982434"",
            description=""Upload data to a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeInsertBlock.Input,
            output_schema=PineconeInsertBlock.Output,
        )

    def run(
        self,
        input_data: Input,
        *,
        credentials: APIKeyCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            # Create a new client instance
            pc = Pinecone(api_key=credentials.api_key.get_secret_value())
            # Get the index
            idx = pc.Index(input_data.index)
            vectors = []
            for chunk, embedding in zip(input_data.chunks, input_data.embeddings):
                vector_metadata = input_data.metadata.copy()
                vector_metadata[""text""] = chunk
                vectors.append(
                    {
                        ""id"": str(uuid.uuid4()),
                        ""values"": embedding,
                        ""metadata"": vector_metadata,
                    }
                )
            idx.upsert(vectors=vectors, namespace=input_data.namespace)
            yield ""upsert_response"", ""successfully upserted""
        except Exception as e:
            error_msg = f""Error uploading to Pinecone: {str(e)}""
            raise RuntimeError(error_msg) from e","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
196,"import logging

import pytest

from backend.util.test import SpinTestServer

#  NOTE: You can run tests like with the --log-cli-level=INFO to see the logs
# Set up logging
logger = logging.getLogger(__name__)

# Create console handler with formatting
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
formatter = logging.Formatter(""%(asctime)s - %(name)s - %(levelname)s - %(message)s"")
ch.setFormatter(formatter)
logger.addHandler(ch)


@pytest.fixture(scope=""session"")
async def server():
    async with SpinTestServer() as server:
        yield server


@pytest.fixture(scope=""session"", autouse=True)
async def graph_cleanup(server):
    created_graph_ids = []
    original_create_graph = server.agent_server.test_create_graph

    async def create_graph_wrapper(*args, **kwargs):
        created_graph = await original_create_graph(*args, **kwargs)
        # Extract user_id correctly
        user_id = kwargs.get(""user_id"", args[2] if len(args) > 2 else None)
        created_graph_ids.append((created_graph.id, user_id))
        return created_graph

    try:
        server.agent_server.test_create_graph = create_graph_wrapper
        yield  # This runs the test function
    finally:
        server.agent_server.test_create_graph = original_create_graph

        # Delete the created graphs and assert they were deleted
        for graph_id, user_id in created_graph_ids:
            if user_id:
                resp = await server.agent_server.test_delete_graph(graph_id, user_id)
                num_deleted = resp[""version_counts""]
                assert num_deleted > 0, f""Graph {graph_id} was not deleted.""
","import logging
import pytest
from backend.util.test import SpinTestServer

#  NOTE: You can run tests like with the --log-cli-level=INFO to see the logs
# Set up logging
logger = logging.getLogger(__name__)

# Create console handler with formatting
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
formatter = logging.Formatter(""%(asctime)s - %(name)s - %(levelname)s - %(message)s"")
ch.setFormatter(formatter)
logger.addHandler(ch)

@pytest.fixture(scope=""session"")
async def server():
    async with SpinTestServer() as server:
        yield server

@pytest.fixture(scope=""session"", autouse=True)
async def graph_cleanup(server):
    created_graph_ids = []
    original_create_graph = server.agent_server.test_create_graph

    async def create_graph_wrapper(*args, **kwargs):
        created_graph = await original_create_graph(*args, **kwargs)
        # Extract user_id correctly
        user_id = kwargs.get(""user_id"", args[2] if len(args) > 2 else None)
        created_graph_ids.append((created_graph.id, user_id))
        return created_graph

    try:
        server.agent_server.test_create_graph = create_graph_wrapper
        yield  # This runs the test function
    finally:
        server.agent_server.test_create_graph = original_create_graph
        # Delete the created graphs and assert they were deleted
        for graph_id, user_id in created_graph_ids:
            if user_id:
                resp = await server.agent_server.test_delete_graph(graph_id, user_id)
                num_deleted = resp[""version_counts""]
                assert num_deleted > 0, f""Graph {graph_id} was not deleted.""","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
197,"# https://www.tutorialspoint.com/python3/bitwise_operators_example.htm


def binary_or(a: int, b: int) -> str:
    """"""
    Take in 2 integers, convert them to binary, and return a binary number that is the
    result of a binary or operation on the integers provided.

    >>> binary_or(25, 32)
    '0b111001'
    >>> binary_or(37, 50)
    '0b110111'
    >>> binary_or(21, 30)
    '0b11111'
    >>> binary_or(58, 73)
    '0b1111011'
    >>> binary_or(0, 255)
    '0b11111111'
    >>> binary_or(0, 256)
    '0b100000000'
    >>> binary_or(0, -1)
    Traceback (most recent call last):
        ...
    ValueError: the value of both inputs must be positive
    >>> binary_or(0, 1.1)
    Traceback (most recent call last):
        ...
    TypeError: 'float' object cannot be interpreted as an integer
    >>> binary_or(""0"", ""1"")
    Traceback (most recent call last):
        ...
    TypeError: '<' not supported between instances of 'str' and 'int'
    """"""
    if a < 0 or b < 0:
        raise ValueError(""the value of both inputs must be positive"")
    a_binary = str(bin(a))[2:]  # remove the leading ""0b""
    b_binary = str(bin(b))[2:]
    max_len = max(len(a_binary), len(b_binary))
    return ""0b"" + """".join(
        str(int(""1"" in (char_a, char_b)))
        for char_a, char_b in zip(a_binary.zfill(max_len), b_binary.zfill(max_len))
    )


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","# https://www.tutorialspoint.com/python3/bitwise_operators_example.htm

def binary_or(a: int, b: int) -> str:
    """"""
    Take in 2 integers, convert them to binary, and return a binary number that is the
    result of a binary or operation on the integers provided.

    >>> binary_or(25, 32)
    '0b111001'
    >>> binary_or(37, 50)
    '0b110111'
    >>> binary_or(21, 30)
    '0b11111'
    >>> binary_or(58, 73)
    '0b1111011'
    >>> binary_or(0, 255)
    '0b11111111'
    >>> binary_or(0, 256)
    '0b100000000'
    >>> binary_or(0, -1)
    Traceback (most recent call last):
        ...
    ValueError: the value of both inputs must be positive
    >>> binary_or(0, 1.1)
    Traceback (most recent call last):
        ...
    TypeError: 'float' object cannot be interpreted as an integer
    >>> binary_or(""0"", ""1"")
    Traceback (most recent call last):
        ...
    TypeError: '<' not supported between instances of 'str' and 'int'
    """"""
    if a < 0 or b < 0:
        raise ValueError(""the value of both inputs must be positive"")

    a_binary = str(bin(a))[2:]  # remove the leading ""0b""
    b_binary = str(bin(b))[2:]
    max_len = max(len(a_binary), len(b_binary))

    return ""0b"" + """".join(
        str(int(""1"" in (char_a, char_b)))
        for char_a, char_b in zip(a_binary.zfill(max_len), b_binary.zfill(max_len))
    )


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
198,"from backend.util.settings import AppEnvironment, BehaveAs, Settings

settings = Settings()


def configure_logging():
    import logging

    import autogpt_libs.logging.config

    if (
        settings.config.behave_as == BehaveAs.LOCAL
        or settings.config.app_env == AppEnvironment.LOCAL
    ):
        autogpt_libs.logging.config.configure_logging(force_cloud_logging=False)
    else:
        autogpt_libs.logging.config.configure_logging(force_cloud_logging=True)

    # Silence httpx logger
    logging.getLogger(""httpx"").setLevel(logging.WARNING)
","from backend.util.settings import AppEnvironment, BehaveAs, Settings

settings = Settings()

def configure_logging():
    import logging
    import autogpt_libs.logging.config

    if (
        settings.config.behave_as == BehaveAs.LOCAL
        or settings.config.app_env == AppEnvironment.LOCAL
    ):
        autogpt_libs.logging.config.configure_logging(force_cloud_logging=False)
    else:
        autogpt_libs.logging.config.configure_logging(force_cloud_logging=True)
    
    # Silence httpx logger
    logging.getLogger(""httpx"").setLevel(logging.WARNING)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
199,"# Reference: https://www.geeksforgeeks.org/position-of-rightmost-set-bit/


def get_index_of_rightmost_set_bit(number: int) -> int:
    """"""
    Take in a positive integer 'number'.
    Returns the zero-based index of first set bit in that 'number' from right.
    Returns -1, If no set bit found.

    >>> get_index_of_rightmost_set_bit(0)
    -1
    >>> get_index_of_rightmost_set_bit(5)
    0
    >>> get_index_of_rightmost_set_bit(36)
    2
    >>> get_index_of_rightmost_set_bit(8)
    3
    >>> get_index_of_rightmost_set_bit(-18)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    >>> get_index_of_rightmost_set_bit('test')
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    >>> get_index_of_rightmost_set_bit(1.25)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    """"""

    if not isinstance(number, int) or number < 0:
        raise ValueError(""Input must be a non-negative integer"")

    intermediate = number & ~(number - 1)
    index = 0
    while intermediate:
        intermediate >>= 1
        index += 1
    return index - 1


if __name__ == ""__main__"":
    """"""
    Finding the index of rightmost set bit has some very peculiar use-cases,
    especially in finding missing or/and repeating numbers in a list of
    positive integers.
    """"""
    import doctest

    doctest.testmod(verbose=True)
","# Reference: https://www.geeksforgeeks.org/position-of-rightmost-set-bit/

def get_index_of_rightmost_set_bit(number: int) -> int:
    """"""
    Take in a positive integer 'number'.
    Returns the zero-based index of first set bit in that 'number' from right.
    Returns -1, If no set bit found.

    >>> get_index_of_rightmost_set_bit(0)
    -1
    >>> get_index_of_rightmost_set_bit(5)
    0
    >>> get_index_of_rightmost_set_bit(36)
    2
    >>> get_index_of_rightmost_set_bit(8)
    3
    >>> get_index_of_rightmost_set_bit(-18)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    >>> get_index_of_rightmost_set_bit('test')
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    >>> get_index_of_rightmost_set_bit(1.25)
    Traceback (most recent call last):
        ...
    ValueError: Input must be a non-negative integer
    """"""
    if not isinstance(number, int) or number < 0:
        raise ValueError(""Input must be a non-negative integer"")

    intermediate = number & ~(number - 1)
    index = 0

    while intermediate:
        intermediate >>= 1
        index += 1

    return index - 1

if __name__ == ""__main__"":
    """"""
    Finding the index of rightmost set bit has some very peculiar use-cases,
    especially in finding missing or/and repeating numbers in a list of
    positive integers.
    """"""
    import doctest
    doctest.testmod(verbose=True)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
200,"import ipaddress
import re
import socket
from typing import Callable
from urllib.parse import urlparse, urlunparse

import idna
import requests as req

from backend.util.settings import Config

# List of IP networks to block
BLOCKED_IP_NETWORKS = [
    # --8<-- [start:BLOCKED_IP_NETWORKS]
    ipaddress.ip_network(""0.0.0.0/8""),  # ""This"" Network
    ipaddress.ip_network(""10.0.0.0/8""),  # Private-Use
    ipaddress.ip_network(""127.0.0.0/8""),  # Loopback
    ipaddress.ip_network(""169.254.0.0/16""),  # Link Local
    ipaddress.ip_network(""172.16.0.0/12""),  # Private-Use
    ipaddress.ip_network(""192.168.0.0/16""),  # Private-Use
    ipaddress.ip_network(""224.0.0.0/4""),  # Multicast
    ipaddress.ip_network(""240.0.0.0/4""),  # Reserved for Future Use
    # --8<-- [end:BLOCKED_IP_NETWORKS]
]

ALLOWED_SCHEMES = [""http"", ""https""]
HOSTNAME_REGEX = re.compile(r""^[A-Za-z0-9.-]+$"")  # Basic DNS-safe hostname pattern


def _canonicalize_url(url: str) -> str:
    # Strip spaces and trailing slashes
    url = url.strip().strip(""/"")
    # Ensure the URL starts with http:// or https://
    if not url.startswith((""http://"", ""https://"")):
        url = ""http://"" + url

    # Replace backslashes with forward slashes to avoid parsing ambiguities
    url = url.replace(""\\"", ""/"")
    return url


def _is_ip_blocked(ip: str) -> bool:
    """"""
    Checks if the IP address is in a blocked network.
    """"""
    ip_addr = ipaddress.ip_address(ip)
    return any(ip_addr in network for network in BLOCKED_IP_NETWORKS)


def validate_url(url: str, trusted_origins: list[str]) -> str:
    """"""
    Validates the URL to prevent SSRF attacks by ensuring it does not point to a private
    or untrusted IP address, unless whitelisted.
    """"""
    url = _canonicalize_url(url)
    parsed = urlparse(url)

    # Check scheme
    if parsed.scheme not in ALLOWED_SCHEMES:
        raise ValueError(
            f""Scheme '{parsed.scheme}' is not allowed. Only HTTP/HTTPS are supported.""
        )

    # Validate and IDNA encode the hostname
    if not parsed.hostname:
        raise ValueError(""Invalid URL: No hostname found."")

    # IDNA encode to prevent Unicode domain attacks
    try:
        ascii_hostname = idna.encode(parsed.hostname).decode(""ascii"")
    except idna.IDNAError:
        raise ValueError(""Invalid hostname with unsupported characters."")

    # Check hostname characters
    if not HOSTNAME_REGEX.match(ascii_hostname):
        raise ValueError(""Hostname contains invalid characters."")

    # Rebuild the URL with the normalized, IDNA-encoded hostname
    parsed = parsed._replace(netloc=ascii_hostname)
    url = str(urlunparse(parsed))

    # Check if hostname is a trusted origin (exact match)
    if ascii_hostname in trusted_origins:
        return url

    # Resolve all IP addresses for the hostname
    try:
        ip_addresses = {res[4][0] for res in socket.getaddrinfo(ascii_hostname, None)}
    except socket.gaierror:
        raise ValueError(f""Unable to resolve IP address for hostname {ascii_hostname}"")

    if not ip_addresses:
        raise ValueError(f""No IP addresses found for {ascii_hostname}"")

    # Check if any resolved IP address falls into blocked ranges
    for ip in ip_addresses:
        if _is_ip_blocked(ip):
            raise ValueError(
                f""Access to private IP address {ip} for hostname {ascii_hostname} is not allowed.""
            )

    return url


class Requests:
    """"""
    A wrapper around the requests library that validates URLs before making requests.
    """"""

    def __init__(
        self,
        trusted_origins: list[str] | None = None,
        raise_for_status: bool = True,
        extra_url_validator: Callable[[str], str] | None = None,
        extra_headers: dict[str, str] | None = None,
    ):
        self.trusted_origins = []
        for url in trusted_origins or []:
            hostname = urlparse(url).hostname
            if not hostname:
                raise ValueError(f""Invalid URL: Unable to determine hostname of {url}"")
            self.trusted_origins.append(hostname)

        self.raise_for_status = raise_for_status
        self.extra_url_validator = extra_url_validator
        self.extra_headers = extra_headers

    def request(
        self, method, url, headers=None, allow_redirects=False, *args, **kwargs
    ) -> req.Response:
        if self.extra_headers is not None:
            headers = {**(headers or {}), **self.extra_headers}

        url = validate_url(url, self.trusted_origins)
        if self.extra_url_validator is not None:
            url = self.extra_url_validator(url)

        response = req.request(
            method,
            url,
            headers=headers,
            allow_redirects=allow_redirects,
            *args,
            **kwargs,
        )
        if self.raise_for_status:
            response.raise_for_status()

        return response

    def get(self, url, *args, **kwargs) -> req.Response:
        return self.request(""GET"", url, *args, **kwargs)

    def post(self, url, *args, **kwargs) -> req.Response:
        return self.request(""POST"", url, *args, **kwargs)

    def put(self, url, *args, **kwargs) -> req.Response:
        return self.request(""PUT"", url, *args, **kwargs)

    def delete(self, url, *args, **kwargs) -> req.Response:
        return self.request(""DELETE"", url, *args, **kwargs)

    def head(self, url, *args, **kwargs) -> req.Response:
        return self.request(""HEAD"", url, *args, **kwargs)

    def options(self, url, *args, **kwargs) -> req.Response:
        return self.request(""OPTIONS"", url, *args, **kwargs)

    def patch(self, url, *args, **kwargs) -> req.Response:
        return self.request(""PATCH"", url, *args, **kwargs)


requests = Requests(trusted_origins=Config().trust_endpoints_for_requests)
","import ipaddress
import re
import socket
from typing import Callable
from urllib.parse import urlparse, urlunparse

import idna
import requests as req

from backend.util.settings import Config


# List of IP networks to block
BLOCKED_IP_NETWORKS = [
    # --8<-- [start:BLOCKED_IP_NETWORKS]
    ipaddress.ip_network(""0.0.0.0/8""),  # ""This"" Network
    ipaddress.ip_network(""10.0.0.0/8""),  # Private-Use
    ipaddress.ip_network(""127.0.0.0/8""),  # Loopback
    ipaddress.ip_network(""169.254.0.0/16""),  # Link Local
    ipaddress.ip_network(""172.16.0.0/12""),  # Private-Use
    ipaddress.ip_network(""192.168.0.0/16""),  # Private-Use
    ipaddress.ip_network(""224.0.0.0/4""),  # Multicast
    ipaddress.ip_network(""240.0.0.0/4""),  # Reserved for Future Use
    # --8<-- [end:BLOCKED_IP_NETWORKS]
]


ALLOWED_SCHEMES = [""http"", ""https""]
HOSTNAME_REGEX = re.compile(r""^[A-Za-z0-9.-]+$"")  # Basic DNS-safe hostname pattern



def _canonicalize_url(url: str) -> str:
    # Strip spaces and trailing slashes
    url = url.strip().strip(""/"")
    # Ensure the URL starts with http:// or https://
    if not url.startswith((""http://"", ""https://"")):
        url = ""http://"" + url
    # Replace backslashes with forward slashes to avoid parsing ambiguities
    url = url.replace(""\\"", ""/"")
    return url


def _is_ip_blocked(ip: str) -> bool:
    """"""
    Checks if the IP address is in a blocked network.
    """"""
    ip_addr = ipaddress.ip_address(ip)
    return any(ip_addr in network for network in BLOCKED_IP_NETWORKS)


def validate_url(url: str, trusted_origins: list[str]) -> str:
    """"""
    Validates the URL to prevent SSRF attacks by ensuring it does not point to a private
    or untrusted IP address, unless whitelisted.
    """"""
    url = _canonicalize_url(url)
    parsed = urlparse(url)
    # Check scheme
    if parsed.scheme not in ALLOWED_SCHEMES:
        raise ValueError(
            f""Scheme '{parsed.scheme}' is not allowed. Only HTTP/HTTPS are supported.""
        )
    # Validate and IDNA encode the hostname
    if not parsed.hostname:
        raise ValueError(""Invalid URL: No hostname found."")
    # IDNA encode to prevent Unicode domain attacks
    try:
        ascii_hostname = idna.encode(parsed.hostname).decode(""ascii"")
    except idna.IDNAError:
        raise ValueError(""Invalid hostname with unsupported characters."")
    # Check hostname characters
    if not HOSTNAME_REGEX.match(ascii_hostname):
        raise ValueError(""Hostname contains invalid characters."")
    # Rebuild the URL with the normalized, IDNA-encoded hostname
    parsed = parsed._replace(netloc=ascii_hostname)
    url = str(urlunparse(parsed))
    # Check if hostname is a trusted origin (exact match)
    if ascii_hostname in trusted_origins:
        return url
    # Resolve all IP addresses for the hostname
    try:
        ip_addresses = {res[4][0] for res in socket.getaddrinfo(ascii_hostname, None)}
    except socket.gaierror:
        raise ValueError(f""Unable to resolve IP address for hostname {ascii_hostname}"")
    if not ip_addresses:
        raise ValueError(f""No IP addresses found for {ascii_hostname}"")
    # Check if any resolved IP address falls into blocked ranges
    for ip in ip_addresses:
        if _is_ip_blocked(ip):
            raise ValueError(
                f""Access to private IP address {ip} for hostname {ascii_hostname} is not allowed.""
            )
    return url


class Requests:
    """"""
    A wrapper around the requests library that validates URLs before making requests.
    """"""

    def __init__(
        self,
        trusted_origins: list[str] | None = None,
        raise_for_status: bool = True,
        extra_url_validator: Callable[[str], str] | None = None,
        extra_headers: dict[str, str] | None = None,
    ):
        self.trusted_origins = []
        for url in trusted_origins or []:
            hostname = urlparse(url).hostname
            if not hostname:
                raise ValueError(f""Invalid URL: Unable to determine hostname of {url}"")
            self.trusted_origins.append(hostname)
        self.raise_for_status = raise_for_status
        self.extra_url_validator = extra_url_validator
        self.extra_headers = extra_headers

    def request(
        self, method, url, headers=None, allow_redirects=False, *args, **kwargs
    ) -> req.Response:
        if self.extra_headers is not None:
            headers = {**(headers or {}), **self.extra_headers}
        url = validate_url(url, self.trusted_origins)
        if self.extra_url_validator is not None:
            url = self.extra_url_validator(url)
        response = req.request(
            method,
            url,
            headers=headers,
            allow_redirects=allow_redirects,
            *args,
            **kwargs,
        )
        if self.raise_for_status:
            response.raise_for_status()
        return response

    def get(self, url, *args, **kwargs) -> req.Response:
        return self.request(""GET"", url, *args, **kwargs)

    def post(self, url, *args, **kwargs) -> req.Response:
        return self.request(""POST"", url, *args, **kwargs)

    def put(self, url, *args, **kwargs) -> req.Response:
        return self.request(""PUT"", url, *args, **kwargs)

    def delete(self, url, *args, **kwargs) -> req.Response:
        return self.request(""DELETE"", url, *args, **kwargs)

    def head(self, url, *args, **kwargs) -> req.Response:
        return self.request(""HEAD"", url, *args, **kwargs)

    def options(self, url, *args, **kwargs) -> req.Response:
        return self.request(""OPTIONS"", url, *args, **kwargs)

    def patch(self, url, *args, **kwargs) -> req.Response:
        return self.request(""PATCH"", url, *args, **kwargs)


requests = Requests(trusted_origins=Config().trust_endpoints_for_requests)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
201,"from __future__ import annotations

from abc import abstractmethod
from math import pi
from typing import Protocol

import matplotlib.pyplot as plt
import numpy as np


class FilterType(Protocol):
    @abstractmethod
    def process(self, sample: float) -> float:
        """"""
        Calculate y[n]

        >>> issubclass(FilterType, Protocol)
        True
        """"""


def get_bounds(
    fft_results: np.ndarray, samplerate: int
) -> tuple[int | float, int | float]:
    """"""
    Get bounds for printing fft results

    >>> import numpy
    >>> array = numpy.linspace(-20.0, 20.0, 1000)
    >>> get_bounds(array, 1000)
    (-20, 20)
    """"""
    lowest = min([-20, np.min(fft_results[1 : samplerate // 2 - 1])])
    highest = max([20, np.max(fft_results[1 : samplerate // 2 - 1])])
    return lowest, highest


def show_frequency_response(filter_type: FilterType, samplerate: int) -> None:
    """"""
    Show frequency response of a filter

    >>> from audio_filters.iir_filter import IIRFilter
    >>> filt = IIRFilter(4)
    >>> show_frequency_response(filt, 48000)
    """"""

    size = 512
    inputs = [1] + [0] * (size - 1)
    outputs = [filter_type.process(item) for item in inputs]

    filler = [0] * (samplerate - size)  # zero-padding
    outputs += filler
    fft_out = np.abs(np.fft.fft(outputs))
    fft_db = 20 * np.log10(fft_out)

    # Frequencies on log scale from 24 to nyquist frequency
    plt.xlim(24, samplerate / 2 - 1)
    plt.xlabel(""Frequency (Hz)"")
    plt.xscale(""log"")

    # Display within reasonable bounds
    bounds = get_bounds(fft_db, samplerate)
    plt.ylim(max([-80, bounds[0]]), min([80, bounds[1]]))
    plt.ylabel(""Gain (dB)"")

    plt.plot(fft_db)
    plt.show()


def show_phase_response(filter_type: FilterType, samplerate: int) -> None:
    """"""
    Show phase response of a filter

    >>> from audio_filters.iir_filter import IIRFilter
    >>> filt = IIRFilter(4)
    >>> show_phase_response(filt, 48000)
    """"""

    size = 512
    inputs = [1] + [0] * (size - 1)
    outputs = [filter_type.process(item) for item in inputs]

    filler = [0] * (samplerate - size)  # zero-padding
    outputs += filler
    fft_out = np.angle(np.fft.fft(outputs))

    # Frequencies on log scale from 24 to nyquist frequency
    plt.xlim(24, samplerate / 2 - 1)
    plt.xlabel(""Frequency (Hz)"")
    plt.xscale(""log"")

    plt.ylim(-2 * pi, 2 * pi)
    plt.ylabel(""Phase shift (Radians)"")
    plt.plot(np.unwrap(fft_out, -2 * pi))
    plt.show()
","from __future__ import annotations
from abc import abstractmethod
from math import pi
from typing import Protocol
import matplotlib.pyplot as plt
import numpy as np

class FilterType(Protocol):
    @abstractmethod
    def process(self, sample: float) -> float:
        """"""
        Calculate y[n]

        >>> issubclass(FilterType, Protocol)
        True
        """"""


def get_bounds(
    fft_results: np.ndarray, samplerate: int
) -> tuple[int | float, int | float]:
    """"""
    Get bounds for printing fft results

    >>> import numpy
    >>> array = numpy.linspace(-20.0, 20.0, 1000)
    >>> get_bounds(array, 1000)
    (-20, 20)
    """"""
    lowest = min([-20, np.min(fft_results[1 : samplerate // 2 - 1])])
    highest = max([20, np.max(fft_results[1 : samplerate // 2 - 1])])
    return lowest, highest

def show_frequency_response(filter_type: FilterType, samplerate: int) -> None:
    """"""
    Show frequency response of a filter

    >>> from audio_filters.iir_filter import IIRFilter
    >>> filt = IIRFilter(4)
    >>> show_frequency_response(filt, 48000)
    """"""
    size = 512
    inputs = [1] + [0] * (size - 1)
    outputs = [filter_type.process(item) for item in inputs]
    filler = [0] * (samplerate - size)  # zero-padding
    outputs += filler
    fft_out = np.abs(np.fft.fft(outputs))
    fft_db = 20 * np.log10(fft_out)
    # Frequencies on log scale from 24 to nyquist frequency
    plt.xlim(24, samplerate / 2 - 1)
    plt.xlabel(""Frequency (Hz)"")
    plt.xscale(""log"")
    # Display within reasonable bounds
    bounds = get_bounds(fft_db, samplerate)
    plt.ylim(max([-80, bounds[0]]), min([80, bounds[1]]))
    plt.ylabel(""Gain (dB)"")
    plt.plot(fft_db)
    plt.show()

def show_phase_response(filter_type: FilterType, samplerate: int) -> None:
    """"""
    Show phase response of a filter

    >>> from audio_filters.iir_filter import IIRFilter
    >>> filt = IIRFilter(4)
    >>> show_phase_response(filt, 48000)
    """"""
    size = 512
    inputs = [1] + [0] * (size - 1)
    outputs = [filter_type.process(item) for item in inputs]
    filler = [0] * (samplerate - size)  # zero-padding
    outputs += filler
    fft_out = np.angle(np.fft.fft(outputs))
    # Frequencies on log scale from 24 to nyquist frequency
    plt.xlim(24, samplerate / 2 - 1)
    plt.xlabel(""Frequency (Hz)"")
    plt.xscale(""log"")
    plt.ylim(-2 * pi, 2 * pi)
    plt.ylabel(""Phase shift (Radians)"")
    plt.plot(np.unwrap(fft_out, -2 * pi))
    plt.show()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
202,"import fastapi

from .config import Settings
from .middleware import auth_middleware
from .models import DEFAULT_USER_ID, User


def requires_user(payload: dict = fastapi.Depends(auth_middleware)) -> User:
    return verify_user(payload, admin_only=False)


def requires_admin_user(
    payload: dict = fastapi.Depends(auth_middleware),
) -> User:
    return verify_user(payload, admin_only=True)


def verify_user(payload: dict | None, admin_only: bool) -> User:
    if not payload:
        if Settings.ENABLE_AUTH:
            raise fastapi.HTTPException(
                status_code=401, detail=""Authorization header is missing""
            )
        # This handles the case when authentication is disabled
        payload = {""sub"": DEFAULT_USER_ID, ""role"": ""admin""}

    user_id = payload.get(""sub"")

    if not user_id:
        raise fastapi.HTTPException(
            status_code=401, detail=""User ID not found in token""
        )

    if admin_only and payload[""role""] != ""admin"":
        raise fastapi.HTTPException(status_code=403, detail=""Admin access required"")

    return User.from_payload(payload)


def get_user_id(payload: dict = fastapi.Depends(auth_middleware)) -> str:
    user_id = payload.get(""sub"")
    if not user_id:
        raise fastapi.HTTPException(
            status_code=401, detail=""User ID not found in token""
        )
    return user_id
","import fastapi
from .config import Settings
from .middleware import auth_middleware
from .models import DEFAULT_USER_ID, User

def requires_user(payload: dict = fastapi.Depends(auth_middleware)) -> User:
    return verify_user(payload, admin_only=False)

def requires_admin_user(
    payload: dict = fastapi.Depends(auth_middleware),
) -> User:
    return verify_user(payload, admin_only=True)

def verify_user(payload: dict | None, admin_only: bool) -> User:
    if not payload:
        if Settings.ENABLE_AUTH:
            raise fastapi.HTTPException(
                status_code=401, detail=""Authorization header is missing""
            )
        # This handles the case when authentication is disabled
        payload = {""sub"": DEFAULT_USER_ID, ""role"": ""admin""}
    
    user_id = payload.get(""sub"")
    if not user_id:
        raise fastapi.HTTPException(
            status_code=401, detail=""User ID not found in token""
        )
    
    if admin_only and payload[""role""] != ""admin"":
        raise fastapi.HTTPException(status_code=403, detail=""Admin access required"")

    return User.from_payload(payload)

def get_user_id(payload: dict = fastapi.Depends(auth_middleware)) -> str:
    user_id = payload.get(""sub"")
    if not user_id:
        raise fastapi.HTTPException(
            status_code=401, detail=""User ID not found in token""
        )
    return user_id","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
203,"import contextlib
import logging.config
import os

import dotenv
import fastapi
import fastapi.middleware.cors
import fastapi.middleware.gzip
import prisma
import prometheus_fastapi_instrumentator
import sentry_sdk
import sentry_sdk.integrations.asyncio
import sentry_sdk.integrations.fastapi
import sentry_sdk.integrations.starlette

import market.config
import market.routes.admin
import market.routes.agents
import market.routes.analytics
import market.routes.search
import market.routes.submissions

dotenv.load_dotenv()

logging.config.dictConfig(market.config.LogConfig().model_dump())

if os.environ.get(""SENTRY_DSN""):
    sentry_sdk.init(
        dsn=os.environ.get(""SENTRY_DSN""),
        traces_sample_rate=1.0,
        profiles_sample_rate=1.0,
        enable_tracing=True,
        environment=os.environ.get(""RUN_ENV"", default=""CLOUD"").lower(),
        integrations=[
            sentry_sdk.integrations.starlette.StarletteIntegration(
                transaction_style=""url""
            ),
            sentry_sdk.integrations.fastapi.FastApiIntegration(transaction_style=""url""),
            sentry_sdk.integrations.asyncio.AsyncioIntegration(),
        ],
    )

db_client = prisma.Prisma(auto_register=True)


@contextlib.asynccontextmanager
async def lifespan(app: fastapi.FastAPI):
    await db_client.connect()
    yield
    await db_client.disconnect()


docs_url = ""/docs""
app = fastapi.FastAPI(
    title=""Marketplace API"",
    description=""AutoGPT Marketplace API is a service that allows users to share AI agents."",
    summary=""Maketplace API"",
    version=""0.1"",
    lifespan=lifespan,
    root_path=""/api/v1/market"",
    docs_url=docs_url,
)

app.add_middleware(fastapi.middleware.gzip.GZipMiddleware, minimum_size=1000)
app.add_middleware(
    middleware_class=fastapi.middleware.cors.CORSMiddleware,
    allow_origins=os.environ.get(
        ""BACKEND_CORS_ALLOW_ORIGINS"", ""http://localhost:3000,http://127.0.0.1:3000""
    ).split("",""),
    allow_credentials=True,
    allow_methods=[""*""],
    allow_headers=[""*""],
)
app.include_router(market.routes.agents.router, tags=[""agents""])
app.include_router(market.routes.search.router, tags=[""search""])
app.include_router(market.routes.submissions.router, tags=[""submissions""])
app.include_router(market.routes.admin.router, prefix=""/admin"", tags=[""admin""])
app.include_router(
    market.routes.analytics.router, prefix=""/analytics"", tags=[""analytics""]
)


@app.get(""/health"")
def health():
    return fastapi.responses.HTMLResponse(
        content=""<h1>Marketplace API</h1>"", status_code=200
    )


@app.get(""/"")
def default():
    return fastapi.responses.HTMLResponse(
        content=""<h1>Marketplace API</h1>"", status_code=200
    )


prometheus_fastapi_instrumentator.Instrumentator().instrument(app).expose(app)
","import contextlib
import logging.config
import os

import dotenv
import fastapi
import fastapi.middleware.cors
import fastapi.middleware.gzip
import prisma
import prometheus_fastapi_instrumentator
import sentry_sdk
import sentry_sdk.integrations.asyncio
import sentry_sdk.integrations.fastapi
import sentry_sdk.integrations.starlette

import market.config
import market.routes.admin
import market.routes.agents
import market.routes.analytics
import market.routes.search
import market.routes.submissions

dotenv.load_dotenv()

logging.config.dictConfig(market.config.LogConfig().model_dump())

if os.environ.get(""SENTRY_DSN""):
    sentry_sdk.init(
        dsn=os.environ.get(""SENTRY_DSN""),
        traces_sample_rate=1.0,
        profiles_sample_rate=1.0,
        enable_tracing=True,
        environment=os.environ.get(""RUN_ENV"", default=""CLOUD"").lower(),
        integrations=[
            sentry_sdk.integrations.starlette.StarletteIntegration(
                transaction_style=""url""
            ),
            sentry_sdk.integrations.fastapi.FastApiIntegration(transaction_style=""url""),
            sentry_sdk.integrations.asyncio.AsyncioIntegration(),
        ],
    )

db_client = prisma.Prisma(auto_register=True)

@contextlib.asynccontextmanager
async def lifespan(app: fastapi.FastAPI):
    await db_client.connect()
    yield
    await db_client.disconnect()

docs_url = ""/docs""

app = fastapi.FastAPI(
    title=""Marketplace API"",
    description=""AutoGPT Marketplace API is a service that allows users to share AI agents."",
    summary=""Maketplace API"",
    version=""0.1"",
    lifespan=lifespan,
    root_path=""/api/v1/market"",
    docs_url=docs_url,
)

app.add_middleware(fastapi.middleware.gzip.GZipMiddleware, minimum_size=1000)

app.add_middleware(
    middleware_class=fastapi.middleware.cors.CORSMiddleware,
    allow_origins=os.environ.get(
        ""BACKEND_CORS_ALLOW_ORIGINS"", ""http://localhost:3000,http://127.0.0.1:3000""
    ).split("",""),
    allow_credentials=True,
    allow_methods=[""*""],
    allow_headers=[""*""],
)

app.include_router(market.routes.agents.router, tags=[""agents""])
app.include_router(market.routes.search.router, tags=[""search""])
app.include_router(market.routes.submissions.router, tags=[""submissions""])
app.include_router(market.routes.admin.router, prefix=""/admin"", tags=[""admin""])
app.include_router(
    market.routes.analytics.router, prefix=""/analytics"", tags=[""analytics""]
)

@app.get(""/health"")
def health():
    return fastapi.responses.HTMLResponse(
        content=""<h1>Marketplace API</h1>"", status_code=200
    )

@app.get(""/"")
def default():
    return fastapi.responses.HTMLResponse(
        content=""<h1>Marketplace API</h1>"", status_code=200
    )

prometheus_fastapi_instrumentator.Instrumentator().instrument(app).expose(app)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
204,"from backend.blocks.hubspot._auth import (
    HubSpotCredentials,
    HubSpotCredentialsField,
    HubSpotCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class HubSpotCompanyBlock(Block):
    class Input(BlockSchema):
        credentials: HubSpotCredentialsInput = HubSpotCredentialsField()
        operation: str = SchemaField(
            description=""Operation to perform (create, update, get)"", default=""get""
        )
        company_data: dict = SchemaField(
            description=""Company data for create/update operations"", default={}
        )
        domain: str = SchemaField(
            description=""Company domain for get/update operations"", default=""""
        )

    class Output(BlockSchema):
        company: dict = SchemaField(description=""Company information"")
        status: str = SchemaField(description=""Operation status"")

    def __init__(self):
        super().__init__(
            id=""3ae02219-d540-47cd-9c78-3ad6c7d9820a"",
            description=""Manages HubSpot companies - create, update, and retrieve company information"",
            categories={BlockCategory.CRM},
            input_schema=HubSpotCompanyBlock.Input,
            output_schema=HubSpotCompanyBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: HubSpotCredentials, **kwargs
    ) -> BlockOutput:
        base_url = ""https://api.hubapi.com/crm/v3/objects/companies""
        headers = {
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
        }

        if input_data.operation == ""create"":
            response = requests.post(
                base_url, headers=headers, json={""properties"": input_data.company_data}
            )
            result = response.json()
            yield ""company"", result
            yield ""status"", ""created""

        elif input_data.operation == ""get"":
            search_url = f""{base_url}/search""
            search_data = {
                ""filterGroups"": [
                    {
                        ""filters"": [
                            {
                                ""propertyName"": ""domain"",
                                ""operator"": ""EQ"",
                                ""value"": input_data.domain,
                            }
                        ]
                    }
                ]
            }
            response = requests.post(search_url, headers=headers, json=search_data)
            result = response.json()
            yield ""company"", result.get(""results"", [{}])[0]
            yield ""status"", ""retrieved""

        elif input_data.operation == ""update"":
            # First get company ID by domain
            search_response = requests.post(
                f""{base_url}/search"",
                headers=headers,
                json={
                    ""filterGroups"": [
                        {
                            ""filters"": [
                                {
                                    ""propertyName"": ""domain"",
                                    ""operator"": ""EQ"",
                                    ""value"": input_data.domain,
                                }
                            ]
                        }
                    ]
                },
            )
            company_id = search_response.json().get(""results"", [{}])[0].get(""id"")

            if company_id:
                response = requests.patch(
                    f""{base_url}/{company_id}"",
                    headers=headers,
                    json={""properties"": input_data.company_data},
                )
                result = response.json()
                yield ""company"", result
                yield ""status"", ""updated""
            else:
                yield ""company"", {}
                yield ""status"", ""company_not_found""
","from backend.blocks.hubspot._auth import (
    HubSpotCredentials,
    HubSpotCredentialsField,
    HubSpotCredentialsInput,
)
from backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema
from backend.data.model import SchemaField
from backend.util.request import requests


class HubSpotCompanyBlock(Block):
    class Input(BlockSchema):
        credentials: HubSpotCredentialsInput = HubSpotCredentialsField()
        operation: str = SchemaField(
            description=""Operation to perform (create, update, get)"", default=""get""
        )
        company_data: dict = SchemaField(
            description=""Company data for create/update operations"", default={}
        )
        domain: str = SchemaField(
            description=""Company domain for get/update operations"", default=""""
        )

    class Output(BlockSchema):
        company: dict = SchemaField(description=""Company information"")
        status: str = SchemaField(description=""Operation status"")

    def __init__(self):
        super().__init__(
            id=""3ae02219-d540-47cd-9c78-3ad6c7d9820a"",
            description=""Manages HubSpot companies - create, update, and retrieve company information"",
            categories={BlockCategory.CRM},
            input_schema=HubSpotCompanyBlock.Input,
            output_schema=HubSpotCompanyBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: HubSpotCredentials, **kwargs
    ) -> BlockOutput:
        base_url = ""https://api.hubapi.com/crm/v3/objects/companies""
        headers = {
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
        }
        if input_data.operation == ""create"":
            response = requests.post(
                base_url, headers=headers, json={""properties"": input_data.company_data}
            )
            result = response.json()
            yield ""company"", result
            yield ""status"", ""created""

        elif input_data.operation == ""get"":
            search_url = f""{base_url}/search""
            search_data = {
                ""filterGroups"": [
                    {
                        ""filters"": [
                            {
                                ""propertyName"": ""domain"",
                                ""operator"": ""EQ"",
                                ""value"": input_data.domain,
                            }
                        ]
                    }
                ]
            }
            response = requests.post(search_url, headers=headers, json=search_data)
            result = response.json()
            yield ""company"", result.get(""results"", [{}])[0]
            yield ""status"", ""retrieved""

        elif input_data.operation == ""update"":
            # First get company ID by domain
            search_response = requests.post(
                f""{base_url}/search"",
                headers=headers,
                json={
                    ""filterGroups"": [
                        {
                            ""filters"": [
                                {
                                    ""propertyName"": ""domain"",
                                    ""operator"": ""EQ"",
                                    ""value"": input_data.domain,
                                }
                            ]
                        }
                    ]
                },
            )
            company_id = search_response.json().get(""results"", [{}])[0].get(""id"")

            if company_id:
                response = requests.patch(
                    f""{base_url}/{company_id}"",
                    headers=headers,
                    json={""properties"": input_data.company_data},
                )
                result = response.json()
                yield ""company"", result
                yield ""status"", ""updated""
            else:
                yield ""company"", {}
                yield ""status"", ""company_not_found""","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
205,"""""""Analytics API""""""

from typing import Annotated

import fastapi

import backend.data.analytics
from backend.server.utils import get_user_id

router = fastapi.APIRouter()


@router.post(path=""/log_raw_metric"")
async def log_raw_metric(
    user_id: Annotated[str, fastapi.Depends(get_user_id)],
    metric_name: Annotated[str, fastapi.Body(..., embed=True)],
    metric_value: Annotated[float, fastapi.Body(..., embed=True)],
    data_string: Annotated[str, fastapi.Body(..., embed=True)],
):
    result = await backend.data.analytics.log_raw_metric(
        user_id=user_id,
        metric_name=metric_name,
        metric_value=metric_value,
        data_string=data_string,
    )
    return result.id


@router.post(""/log_raw_analytics"")
async def log_raw_analytics(
    user_id: Annotated[str, fastapi.Depends(get_user_id)],
    type: Annotated[str, fastapi.Body(..., embed=True)],
    data: Annotated[
        dict,
        fastapi.Body(..., embed=True, description=""The data to log""),
    ],
    data_index: Annotated[
        str,
        fastapi.Body(
            ...,
            embed=True,
            description=""Indexable field for any count based analytical measures like page order clicking, tutorial step completion, etc."",
        ),
    ],
):
    result = await backend.data.analytics.log_raw_analytics(
        user_id, type, data, data_index
    )
    return result.id
","""""""Analytics API""""""
from typing import Annotated
import fastapi
import backend.data.analytics
from backend.server.utils import get_user_id

router = fastapi.APIRouter()

@router.post(path=""/log_raw_metric"")
async def log_raw_metric(
    user_id: Annotated[str, fastapi.Depends(get_user_id)],
    metric_name: Annotated[str, fastapi.Body(..., embed=True)],
    metric_value: Annotated[float, fastapi.Body(..., embed=True)],
    data_string: Annotated[str, fastapi.Body(..., embed=True)],
):
    result = await backend.data.analytics.log_raw_metric(
        user_id=user_id,
        metric_name=metric_name,
        metric_value=metric_value,
        data_string=data_string,
    )
    return result.id

@router.post(""/log_raw_analytics"")
async def log_raw_analytics(
    user_id: Annotated[str, fastapi.Depends(get_user_id)],
    type: Annotated[str, fastapi.Body(..., embed=True)],
    data: Annotated[
        dict,
        fastapi.Body(..., embed=True, description=""The data to log""),
    ],
    data_index: Annotated[
        str,
        fastapi.Body(
            ...,
            embed=True,
            description=""Indexable field for any count based analytical measures like page order clicking, tutorial step completion, etc."",
        ),
    ],
):
    result = await backend.data.analytics.log_raw_analytics(
        user_id, type, data, data_index
    )
    return result.id","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
206,"import pytest

from backend.data import db
from backend.executor import ExecutionScheduler
from backend.server.model import CreateGraph
from backend.usecases.sample import create_test_graph, create_test_user
from backend.util.service import get_service_client
from backend.util.test import SpinTestServer


@pytest.mark.asyncio(scope=""session"")
async def test_agent_schedule(server: SpinTestServer):
    await db.connect()
    test_user = await create_test_user()
    test_graph = await server.agent_server.test_create_graph(
        create_graph=CreateGraph(graph=create_test_graph()),
        user_id=test_user.id,
    )

    scheduler = get_service_client(ExecutionScheduler)
    schedules = scheduler.get_execution_schedules(test_graph.id, test_user.id)
    assert len(schedules) == 0

    schedule = scheduler.add_execution_schedule(
        graph_id=test_graph.id,
        user_id=test_user.id,
        graph_version=1,
        cron=""0 0 * * *"",
        input_data={""input"": ""data""},
    )
    assert schedule

    schedules = scheduler.get_execution_schedules(test_graph.id, test_user.id)
    assert len(schedules) == 1
    assert schedules[0].cron == ""0 0 * * *""

    scheduler.delete_schedule(schedule.id, user_id=test_user.id)
    schedules = scheduler.get_execution_schedules(test_graph.id, user_id=test_user.id)
    assert len(schedules) == 0
","import pytest
from backend.data import db
from backend.executor import ExecutionScheduler
from backend.server.model import CreateGraph
from backend.usecases.sample import create_test_graph, create_test_user
from backend.util.service import get_service_client
from backend.util.test import SpinTestServer

@pytest.mark.asyncio(scope=""session"")
async def test_agent_schedule(server: SpinTestServer):
    await db.connect()
    test_user = await create_test_user()
    test_graph = await server.agent_server.test_create_graph(
        create_graph=CreateGraph(graph=create_test_graph()),
        user_id=test_user.id,
    )
    scheduler = get_service_client(ExecutionScheduler)
    schedules = scheduler.get_execution_schedules(test_graph.id, test_user.id)
    assert len(schedules) == 0

    schedule = scheduler.add_execution_schedule(
        graph_id=test_graph.id,
        user_id=test_user.id,
        graph_version=1,
        cron=""0 0 * * *"",
        input_data={""input"": ""data""},
    )
    assert schedule

    schedules = scheduler.get_execution_schedules(test_graph.id, test_user.id)
    assert len(schedules) == 1
    assert schedules[0].cron == ""0 0 * * *""

    scheduler.delete_schedule(schedule.id, user_id=test_user.id)
    schedules = scheduler.get_execution_schedules(test_graph.id, user_id=test_user.id)
    assert len(schedules) == 0","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
207,"""""""
Word Ladder is a classic problem in computer science.
The problem is to transform a start word into an end word
by changing one letter at a time.
Each intermediate word must be a valid word from a given list of words.
The goal is to find a transformation sequence
from the start word to the end word.

Wikipedia: https://en.wikipedia.org/wiki/Word_ladder
""""""

import string


def backtrack(
    current_word: str, path: list[str], end_word: str, word_set: set[str]
) -> list[str]:
    """"""
    Helper function to perform backtracking to find the transformation
    from the current_word to the end_word.

    Parameters:
    current_word (str): The current word in the transformation sequence.
    path (list[str]): The list of transformations from begin_word to current_word.
    end_word (str): The target word for transformation.
    word_set (set[str]): The set of valid words for transformation.

    Returns:
    list[str]: The list of transformations from begin_word to end_word.
               Returns an empty list if there is no valid
                transformation from current_word to end_word.

    Example:
    >>> backtrack(""hit"", [""hit""], ""cog"", {""hot"", ""dot"", ""dog"", ""lot"", ""log"", ""cog""})
    ['hit', 'hot', 'dot', 'lot', 'log', 'cog']

    >>> backtrack(""hit"", [""hit""], ""cog"", {""hot"", ""dot"", ""dog"", ""lot"", ""log""})
    []

    >>> backtrack(""lead"", [""lead""], ""gold"", {""load"", ""goad"", ""gold"", ""lead"", ""lord""})
    ['lead', 'lead', 'load', 'goad', 'gold']

    >>> backtrack(""game"", [""game""], ""code"", {""came"", ""cage"", ""code"", ""cade"", ""gave""})
    ['game', 'came', 'cade', 'code']
    """"""

    # Base case: If the current word is the end word, return the path
    if current_word == end_word:
        return path

    # Try all possible single-letter transformations
    for i in range(len(current_word)):
        for c in string.ascii_lowercase:  # Try changing each letter
            transformed_word = current_word[:i] + c + current_word[i + 1 :]
            if transformed_word in word_set:
                word_set.remove(transformed_word)
                # Recur with the new word added to the path
                result = backtrack(
                    transformed_word, [*path, transformed_word], end_word, word_set
                )
                if result:  # valid transformation found
                    return result
                word_set.add(transformed_word)  # backtrack

    return []  # No valid transformation found


def word_ladder(begin_word: str, end_word: str, word_set: set[str]) -> list[str]:
    """"""
    Solve the Word Ladder problem using Backtracking and return
    the list of transformations from begin_word to end_word.

    Parameters:
    begin_word (str): The word from which the transformation starts.
    end_word (str): The target word for transformation.
    word_list (list[str]): The list of valid words for transformation.

    Returns:
    list[str]: The list of transformations from begin_word to end_word.
               Returns an empty list if there is no valid transformation.

    Example:
    >>> word_ladder(""hit"", ""cog"", [""hot"", ""dot"", ""dog"", ""lot"", ""log"", ""cog""])
    ['hit', 'hot', 'dot', 'lot', 'log', 'cog']

    >>> word_ladder(""hit"", ""cog"", [""hot"", ""dot"", ""dog"", ""lot"", ""log""])
    []

    >>> word_ladder(""lead"", ""gold"", [""load"", ""goad"", ""gold"", ""lead"", ""lord""])
    ['lead', 'lead', 'load', 'goad', 'gold']

    >>> word_ladder(""game"", ""code"", [""came"", ""cage"", ""code"", ""cade"", ""gave""])
    ['game', 'came', 'cade', 'code']
    """"""

    if end_word not in word_set:  # no valid transformation possible
        return []

    # Perform backtracking starting from the begin_word
    return backtrack(begin_word, [begin_word], end_word, word_set)
","""""""
Word Ladder is a classic problem in computer science.
The problem is to transform a start word into an end word
by changing one letter at a time.
Each intermediate word must be a valid word from a given list of words.
The goal is to find a transformation sequence
from the start word to the end word.

Wikipedia: https://en.wikipedia.org/wiki/Word_ladder
""""""
import string

def backtrack(
    current_word: str, path: list[str], end_word: str, word_set: set[str]
) -> list[str]:
    """"""
    Helper function to perform backtracking to find the transformation
    from the current_word to the end_word.

    Parameters:
    current_word (str): The current word in the transformation sequence.
    path (list[str]): The list of transformations from begin_word to current_word.
    end_word (str): The target word for transformation.
    word_set (set[str]): The set of valid words for transformation.

    Returns:
    list[str]: The list of transformations from begin_word to end_word.
               Returns an empty list if there is no valid
                transformation from current_word to end_word.

    Example:
    >>> backtrack(""hit"", [""hit""], ""cog"", {""hot"", ""dot"", ""dog"", ""lot"", ""log"", ""cog""})
    ['hit', 'hot', 'dot', 'lot', 'log', 'cog']

    >>> backtrack(""hit"", [""hit""], ""cog"", {""hot"", ""dot"", ""dog"", ""lot"", ""log""})
    []

    >>> backtrack(""lead"", [""lead""], ""gold"", {""load"", ""goad"", ""gold"", ""lead"", ""lord""})
    ['lead', 'lead', 'load', 'goad', 'gold']

    >>> backtrack(""game"", [""game""], ""code"", {""came"", ""cage"", ""code"", ""cade"", ""gave""})
    ['game', 'came', 'cade', 'code']
    """"""

    # Base case: If the current word is the end word, return the path
    if current_word == end_word:
        return path

    # Try all possible single-letter transformations
    for i in range(len(current_word)):
        for c in string.ascii_lowercase:  # Try changing each letter
            transformed_word = current_word[:i] + c + current_word[i + 1 :]
            if transformed_word in word_set:
                word_set.remove(transformed_word)
                # Recur with the new word added to the path
                result = backtrack(
                    transformed_word, [*path, transformed_word], end_word, word_set
                )
                if result:  # valid transformation found
                    return result
                word_set.add(transformed_word)  # backtrack

    return []  # No valid transformation found


def word_ladder(begin_word: str, end_word: str, word_set: set[str]) -> list[str]:
    """"""
    Solve the Word Ladder problem using Backtracking and return
    the list of transformations from begin_word to end_word.

    Parameters:
    begin_word (str): The word from which the transformation starts.
    end_word (str): The target word for transformation.
    word_list (list[str]): The list of valid words for transformation.

    Returns:
    list[str]: The list of transformations from begin_word to end_word.
               Returns an empty list if there is no valid transformation.

    Example:
    >>> word_ladder(""hit"", ""cog"", [""hot"", ""dot"", ""dog"", ""lot"", ""log"", ""cog""])
    ['hit', 'hot', 'dot', 'lot', 'log', 'cog']

    >>> word_ladder(""hit"", ""cog"", [""hot"", ""dot"", ""dog"", ""lot"", ""log""])
    []

    >>> word_ladder(""lead"", ""gold"", [""load"", ""goad"", ""gold"", ""lead"", ""lord""])
    ['lead', 'lead', 'load', 'goad', 'gold']

    >>> word_ladder(""game"", ""code"", [""came"", ""cage"", ""code"", ""cade"", ""gave""])
    ['game', 'came', 'cade', 'code']
    """"""
    if end_word not in word_set:  # no valid transformation possible
        return []
    # Perform backtracking starting from the begin_word
    return backtrack(begin_word, [begin_word], end_word, word_set)","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
208,"import asyncio

import pytest

from backend.util.retry import conn_retry


def test_conn_retry_sync_function():
    retry_count = 0

    @conn_retry(""Test"", ""Test function"", max_retry=2, max_wait=0.1, min_wait=0.1)
    def test_function():
        nonlocal retry_count
        retry_count -= 1
        if retry_count > 0:
            raise ValueError(""Test error"")
        return ""Success""

    retry_count = 2
    res = test_function()
    assert res == ""Success""

    retry_count = 100
    with pytest.raises(ValueError) as e:
        test_function()
        assert str(e.value) == ""Test error""


@pytest.mark.asyncio
async def test_conn_retry_async_function():
    retry_count = 0

    @conn_retry(""Test"", ""Test function"", max_retry=2, max_wait=0.1, min_wait=0.1)
    async def test_function():
        nonlocal retry_count
        await asyncio.sleep(1)
        retry_count -= 1
        if retry_count > 0:
            raise ValueError(""Test error"")
        return ""Success""

    retry_count = 2
    res = await test_function()
    assert res == ""Success""

    retry_count = 100
    with pytest.raises(ValueError) as e:
        await test_function()
        assert str(e.value) == ""Test error""
","import asyncio
import pytest
from backend.util.retry import conn_retry

def test_conn_retry_sync_function():
    retry_count = 0

    @conn_retry(""Test"", ""Test function"", max_retry=2, max_wait=0.1, min_wait=0.1)
    def test_function():
        nonlocal retry_count
        retry_count -= 1
        if retry_count > 0:
            raise ValueError(""Test error"")
        return ""Success""

    retry_count = 2
    res = test_function()
    assert res == ""Success""

    retry_count = 100
    with pytest.raises(ValueError) as e:
        test_function()
        assert str(e.value) == ""Test error""


@pytest.mark.asyncio
async def test_conn_retry_async_function():
    retry_count = 0

    @conn_retry(""Test"", ""Test function"", max_retry=2, max_wait=0.1, min_wait=0.1)
    async def test_function():
        nonlocal retry_count
        await asyncio.sleep(1)
        retry_count -= 1
        if retry_count > 0:
            raise ValueError(""Test error"")
        return ""Success""

    retry_count = 2
    res = await test_function()
    assert res == ""Success""

    retry_count = 100
    with pytest.raises(ValueError) as e:
        await test_function()
        assert str(e.value) == ""Test error""","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
209,"from typing import Any, Dict

import jwt

from .config import settings


def parse_jwt_token(token: str) -> Dict[str, Any]:
    """"""
    Parse and validate a JWT token.

    :param token: The token to parse
    :return: The decoded payload
    :raises ValueError: If the token is invalid or expired
    """"""
    try:
        payload = jwt.decode(
            token,
            settings.JWT_SECRET_KEY,
            algorithms=[settings.JWT_ALGORITHM],
            audience=""authenticated"",
        )
        return payload
    except jwt.ExpiredSignatureError:
        raise ValueError(""Token has expired"")
    except jwt.InvalidTokenError as e:
        raise ValueError(f""Invalid token: {str(e)}"")
","from typing import Any, Dict
import jwt
from .config import settings


def parse_jwt_token(token: str) -> Dict[str, Any]:
    """"""
    Parse and validate a JWT token.

    :param token: The token to parse
    :return: The decoded payload
    :raises ValueError: If the token is invalid or expired
    """"""
    try:
        payload = jwt.decode(
            token,
            settings.JWT_SECRET_KEY,
            algorithms=[settings.JWT_ALGORITHM],
            audience=""authenticated"",
        )
        return payload
    except jwt.ExpiredSignatureError:
        raise ValueError(""Token has expired"")
    except jwt.InvalidTokenError as e:
        raise ValueError(f""Invalid token: {str(e)}"")","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
210,"def binary_coded_decimal(number: int) -> str:
    """"""
    Find binary coded decimal (bcd) of integer base 10.
    Each digit of the number is represented by a 4-bit binary.
    Example:
    >>> binary_coded_decimal(-2)
    '0b0000'
    >>> binary_coded_decimal(-1)
    '0b0000'
    >>> binary_coded_decimal(0)
    '0b0000'
    >>> binary_coded_decimal(3)
    '0b0011'
    >>> binary_coded_decimal(2)
    '0b0010'
    >>> binary_coded_decimal(12)
    '0b00010010'
    >>> binary_coded_decimal(987)
    '0b100110000111'
    """"""
    return ""0b"" + """".join(
        str(bin(int(digit)))[2:].zfill(4) for digit in str(max(0, number))
    )


if __name__ == ""__main__"":
    import doctest

    doctest.testmod()
","def binary_coded_decimal(number: int) -> str:
    """"""
    Find binary coded decimal (bcd) of integer base 10.
    Each digit of the number is represented by a 4-bit binary.
    Example:
    >>> binary_coded_decimal(-2)
    '0b0000'
    >>> binary_coded_decimal(-1)
    '0b0000'
    >>> binary_coded_decimal(0)
    '0b0000'
    >>> binary_coded_decimal(3)
    '0b0011'
    >>> binary_coded_decimal(2)
    '0b0010'
    >>> binary_coded_decimal(12)
    '0b00010010'
    >>> binary_coded_decimal(987)
    '0b100110000111'
    """"""
    return ""0b"" + """".join(
        str(bin(int(digit)))[2:].zfill(4) for digit in str(max(0, number))
    )

if __name__ == ""__main__"":
    import doctest

    doctest.testmod()","{'final_score': 100.0, 'length_similarity': 100.0, 'token_similarity': 100.0, 'structure_similarity': 100.0, 'function_similarity': 100, 'complexity_similarity': 100.0, 'variable_similarity': 100}",
