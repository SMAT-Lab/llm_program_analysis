{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取CSV文件\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('chain_similarity_results.csv')\n",
    "\n",
    "# 按index排序\n",
    "df = df.sort_values('Index')\n",
    "\n",
    "# 保存排序后的结果\n",
    "df.to_csv('chain_similarity_results.csv', index=False)\n",
    "\n",
    "# 对相同Index的行,保留final_score最高的一条\n",
    "df = df.sort_values('Static Similarity', key=lambda x: x.apply(lambda y: eval(y)['final_score'] if isinstance(y, str) else 0), ascending=False)\n",
    "df = df.drop_duplicates(subset=['Index'], keep='first')\n",
    "\n",
    "# 重新按Index排序\n",
    "df = df.sort_values('Index')\n",
    "\n",
    "# 保存结果\n",
    "df.to_csv('chain_similarity_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Index\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 获取所有存在的index\n",
    "existing_indices = set(df['Index'].unique())\n",
    "\n",
    "# 遍历cfg目录下的所有文件\n",
    "cfg_files = [f for f in os.listdir('cfg') if f.startswith('cfg_') and f.endswith('.json')]\n",
    "\n",
    "for cfg_file in cfg_files:\n",
    "    # 从文件名提取index\n",
    "    try:\n",
    "        index = int(cfg_file.replace('cfg_','').replace('.json',''))\n",
    "        # 如果index不在DataFrame中,删除该文件\n",
    "        if index not in existing_indices:\n",
    "            # os.remove(os.path.join('cfg', cfg_file))\n",
    "            print(f\"删除了不存在index的文件: {cfg_file}\")\n",
    "    except ValueError:\n",
    "        print(f\"无法从文件名解析index: {cfg_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清掉数据差的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 提取Static Similarity和LLM Similarity列\n",
    "static_similarities = df['Static Similarity'].apply(lambda x: round(eval(x)['final_score'], 1) if isinstance(x, str) else x)\n",
    "\n",
    "low_static = df[static_similarities < 60]\n",
    "for idx, row in low_static.iterrows():\n",
    "    # 删除cfg/下的json文件\n",
    "    index = row['Index']\n",
    "    cfg_file = f'cfg/{index}.json'\n",
    "    # 从DataFrame中删除对应的行\n",
    "    #df = df[df['Index'] != index]\n",
    "    # 将更新后的DataFrame写回文件\n",
    "    #df.to_csv('chain_similarity_results.csv', index=False)\n",
    "    print(cfg_file)\n",
    "    print(row['Static Similarity'])\n",
    "    if os.path.exists(cfg_file):\n",
    "        #os.remove(cfg_file)\n",
    "        print(f\"删除 {cfg_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 6\n",
      "Processing row 93\n",
      "Processing row 88\n",
      "Processing row 168\n",
      "Processing row 102\n",
      "Processing row 163\n",
      "Processing row 84\n",
      "Processing row 26\n",
      "Processing row 29\n",
      "Processing row 38\n",
      "Processing row 187\n",
      "Processing row 2\n",
      "Processing row 15\n",
      "Processing row 12\n",
      "Processing row 70\n",
      "Processing row 35\n",
      "Processing row 58\n",
      "Processing row 66\n",
      "Processing row 0\n",
      "Processing row 73\n",
      "Processing row 120\n",
      "Processing row 19\n",
      "Processing row 8\n",
      "Processing row 3\n",
      "Processing row 14\n",
      "Processing row 110\n",
      "Processing row 65\n",
      "Processing row 193\n",
      "Processing row 144\n",
      "Processing row 172\n",
      "Processing row 21\n",
      "Processing row 96\n",
      "Processing row 76\n",
      "Processing row 60\n",
      "Processing row 20\n",
      "Processing row 179\n",
      "Processing row 43\n",
      "Processing row 9\n",
      "Processing row 86\n",
      "Processing row 77\n",
      "Processing row 183\n",
      "Processing row 94\n",
      "Processing row 114\n",
      "Processing row 194\n",
      "Processing row 87\n",
      "Processing row 46\n",
      "Processing row 181\n",
      "Processing row 104\n",
      "Processing row 105\n",
      "Processing row 132\n",
      "Processing row 27\n",
      "Processing row 57\n",
      "Processing row 180\n",
      "Processing row 198\n",
      "Processing row 195\n",
      "Processing row 89\n",
      "Processing row 134\n",
      "Processing row 45\n",
      "Processing row 24\n",
      "Processing row 191\n",
      "Processing row 44\n",
      "Processing row 59\n",
      "Processing row 199\n",
      "Processing row 107\n",
      "Processing row 188\n",
      "Processing row 125\n",
      "Processing row 5\n",
      "Processing row 4\n",
      "Processing row 7\n",
      "Processing row 64\n",
      "Processing row 122\n",
      "Processing row 10\n",
      "Processing row 13\n",
      "Processing row 22\n",
      "Processing row 124\n",
      "Processing row 90\n",
      "Processing row 79\n",
      "Processing row 113\n",
      "Processing row 108\n",
      "Processing row 98\n",
      "Processing row 33\n",
      "Processing row 72\n",
      "Processing row 54\n",
      "Processing row 16\n",
      "Processing row 51\n",
      "Processing row 31\n",
      "Processing row 25\n",
      "Processing row 41\n",
      "Processing row 83\n",
      "Processing row 78\n",
      "Processing row 30\n",
      "Processing row 28\n",
      "Processing row 129\n",
      "Processing row 81\n",
      "Processing row 1\n",
      "Processing row 100\n",
      "Processing row 47\n",
      "Processing row 135\n",
      "Processing row 39\n",
      "Processing row 192\n",
      "Processing row 112\n",
      "Processing row 91\n",
      "Processing row 23\n",
      "Processing row 53\n",
      "Processing row 18\n",
      "Processing row 40\n",
      "Processing row 11\n",
      "Processing row 17\n",
      "Processing row 50\n",
      "Processing row 36\n",
      "Processing row 69\n",
      "Processing row 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:04<16:09,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 32\n",
      "Processing row 61\n",
      "Processing row 74\n",
      "Processing row 169\n",
      "Processing row 34\n",
      "Processing row 55\n",
      "Processing row 42\n",
      "Processing row 56\n",
      "Processing row 37\n",
      "Processing row 71\n",
      "Processing row 49\n",
      "Processing row 196Processing row 166\n",
      "\n",
      "Processing row 136\n",
      "Processing row 101\n",
      "Processing row 154\n",
      "Processing row 178\n",
      "Processing row 82\n",
      "Processing row 63\n",
      "Processing row 148\n",
      "Processing row 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:21<39:34, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 75\n",
      "Processing row 176\n",
      "Processing row 92\n",
      "Processing row 52\n",
      "Processing row 103\n",
      "Processing row 145\n",
      "Processing row 80\n",
      "Processing row 68\n",
      "Processing row 137\n",
      "Processing row 141\n",
      "Processing row 67Processing row 165\n",
      "\n",
      "Processing row 151\n",
      "Processing row 149\n",
      "Processing row 189\n",
      "Processing row 171\n",
      "Processing row 118\n",
      "Processing row 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:27<29:58,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 62\n",
      "Processing row 150\n",
      "Processing row 130\n",
      "Processing row 48\n",
      "Processing row 161\n",
      "Processing row 139\n",
      "Processing row 109\n",
      "Processing row 184\n",
      "Processing row 158\n",
      "Processing row 115\n",
      "Processing row 186\n",
      "Processing row 111\n",
      "Processing row 116\n",
      "Processing row 143\n",
      "Processing row 97\n",
      "Processing row 138\n",
      "Processing row 142\n",
      "Processing row 106\n",
      "Processing row 174\n",
      "Processing row 155\n",
      "Processing row 152\n",
      "Processing row 197\n",
      "Processing row 167\n",
      "Processing row 146\n",
      "Processing row 99\n",
      "Processing row 157\n",
      "Processing row 190\n",
      "Processing row 164\n",
      "Processing row 156\n",
      "Processing row 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:36<29:26,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 131Processing row 173\n",
      "Processing row 133\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:37<20:13,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 117\n",
      "Processing row 147\n",
      "Processing row 159\n",
      "Processing row 170\n",
      "Processing row 119\n",
      "Processing row 126\n",
      "Processing row 140\n",
      "Processing row 153\n",
      "Processing row 127\n",
      "Processing row 123\n",
      "Processing row 160\n",
      "Processing row 182\n",
      "Processing row 175\n",
      "Processing row 185\n",
      "Processing row 177\n",
      "Processing row 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:04<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "## 重新计算LLM相似度\n",
    "from multiprocessing import cpu_count\n",
    "from llm import get_llm_answers\n",
    "\n",
    "def compare_code_similarity(original_code: str, code: str):\n",
    "    \"\"\"使用LLM比较代码相似度\"\"\"\n",
    "    # print(\"Starting LLM Code Similarity Comparison\")\n",
    "    \n",
    "    prompt = \"\"\"Please compare the similarity between these two code snippets and provide:\n",
    "1. A similarity score from 0-100\n",
    "2. A detailed analysis explaining the score\n",
    "\n",
    "Original code:\n",
    "\"\"\" + original_code + \"\"\"\n",
    "\n",
    "Generated code:\n",
    "\"\"\" + code + \"\"\"\n",
    "\n",
    "Your response must be in JSON format like this:\n",
    "{\n",
    "    \"score\": 80, \n",
    "    \"analysis\": \"\"\n",
    "}\n",
    "\n",
    "The score is based on the following criteria:\n",
    "- Implementation details\n",
    "- Code structure and organization  \n",
    "- Variable naming and coding style\n",
    "- Error handling and edge cases\n",
    "- Overall functionality and behavior\n",
    "\"\"\"\n",
    "    response = get_llm_answers(prompt, model_name=\"gpt-4o-2024-11-20\", require_json=True)\n",
    "    # print(f\"Similarity Comparison Result:\\n{response}\")\n",
    "    return response\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_row(args):\n",
    "    index, row = args\n",
    "    original_code = row['Original Code']\n",
    "    code = row['Generated Code'] \n",
    "    print(f\"Processing row {index}\")\n",
    "    response = compare_code_similarity(original_code, code)\n",
    "    return index, response\n",
    "\n",
    "# 使用线程池并行处理\n",
    "with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "    # 创建任务列表\n",
    "    tasks = list(df.iterrows())\n",
    "    \n",
    "    # 使用tqdm显示进度条\n",
    "    results = list(tqdm(executor.map(process_row, tasks), total=len(tasks)))\n",
    "    \n",
    "    # 更新DataFrame\n",
    "    for index, response in results:\n",
    "        df.at[index, 'LLM Similarity'] = response\n",
    "\n",
    "df.to_csv('similarity_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4cfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
