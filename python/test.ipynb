{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from multiprocessing import cpu_count\n",
    "import re\n",
    "import json\n",
    "import concurrent.futures\n",
    "import traceback\n",
    "from typing import List, Dict, Any\n",
    "from llm import get_llm_answers\n",
    "import os\n",
    "\n",
    "# 日志设置\n",
    "def setup_logging(log_dir=None, index=None):\n",
    "    \"\"\"为每个样本设置独立的日志记录器\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = os.path.join(log_dir, f\"{index}.log\") if index is not None else os.path.join(log_dir, \"global.log\")\n",
    "    \n",
    "    logger = logging.getLogger(f\"logger_{index}\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(log_file, mode='w')\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def init_conversations(logger) -> List[Dict[str, str]]:\n",
    "    \"\"\"初始化对话,给LLM一个系统提示,告诉它整体流程.\"\"\"\n",
    "    try:\n",
    "        conversations = []\n",
    "        \n",
    "        system_prompt = \"\"\"You are a Code CFG Analysis Assistant. \n",
    "We have a multi-step approach to build a Control Flow Graph (CFG):\n",
    "- Step0: Split large code into top-level declarations. Return a JSON array, each item containing the code lines for one top-level block.\n",
    "- Step1: Multi-level Code Segmentation (for each block).\n",
    "- Step2: Fine-grained Semantic Labeling.\n",
    "- Step3: Outline the control flow.\n",
    "- Step4: Convert control flow to JSON CFG.\n",
    "- Step5: Check CFG correctness, fix if needed (in JSON).\n",
    "- Step6: Merge local CFGs into a global CFG.\n",
    "\n",
    "We will explicitly call each step. Please follow the instructions at each step carefully.\"\"\"\n",
    "        \n",
    "        conversations.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        logger.info(\"Successfully initialized conversations\")\n",
    "        return conversations\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize conversations: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def extract_code_from_markdown_block(markdown_block: str, logger) -> str:\n",
    "    \"\"\"从Markdown代码块中提取代码.\"\"\"\n",
    "    try:\n",
    "        match = re.search(\n",
    "            r'```(?:arkts|javascript|js|ts|json|typescript|cangjie|python|java|cpp|c|go|rust|swift|kotlin|scala|ruby|php|html|css|sql)\\n(.*)\\n```',\n",
    "            markdown_block, re.DOTALL\n",
    "        )\n",
    "        if match is not None:\n",
    "            logger.debug(\"Successfully extracted code from markdown block\")\n",
    "            return match.group(1)\n",
    "        \n",
    "        logger.warning(\"No code block found, returning original text\")\n",
    "        return markdown_block\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting code block: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def step_0_split_top_declarations(whole_code: str, logger) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Step 0: 对大代码做解析,将顶层声明按行号拆分.\n",
    "    \n",
    "    在此处明确要求：如果类/函数中出现嵌套函数/方法，也要拆分为独立块。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting Step 0: Splitting code into top-level declarations\")\n",
    "        \n",
    "        code_lines = whole_code.split('\\n')\n",
    "        code_array = [{\"line\": i, \"content\": line} for i, line in enumerate(code_lines)]\n",
    "        \n",
    "        # 修改提示，强调“嵌套函数/方法”也单独作为块，不与外层块有顺序关系\n",
    "        prompt = \"\"\"\n",
    "You are a senior code analysis assistant. We have a potentially large python code snippet.\n",
    "We want to split it into top-level declarations or blocks, returning a JSON array.\n",
    "\n",
    "### Requirements:\n",
    "1) Each element in the JSON array should be an object with:\n",
    "   - \"decl_name\": an identifier or best guess of the block's name\n",
    "   - \"start_line\" and \"end_line\": line numbers in the original snippet\n",
    "   - No code content in this step.\n",
    "\n",
    "2) Every line from the code must appear in exactly one block (covering the entire file).\n",
    "\n",
    "3) If there's top-level statements not inside a class/function, group them as a block too \n",
    "   (like a 'GlobalBlock').\n",
    "\n",
    "4) **Important**: \n",
    "   - If inside a class or function we detect a nested function/method, \n",
    "     treat that nested function/method as a separate top-level block.\n",
    "   - This means in later steps, there should be no direct 'sequential' edges between the parent block \n",
    "     and the nested block.\n",
    "\n",
    "5) The final response should be strictly valid JSON, e.g.:\n",
    "{\n",
    "  \"blocks\": [\n",
    "    {\n",
    "      \"decl_name\": \"GlobalBlock\",\n",
    "      \"start_line\": 1,\n",
    "      \"end_line\": 30\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "6) Do not output any extra text or code besides the JSON.\n",
    "\n",
    "Python code (with line numbers):\n",
    "\"\"\" + json.dumps(code_array, indent=2)\n",
    "        \n",
    "        logger.info(prompt)\n",
    "\n",
    "        response = get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True)\n",
    "\n",
    "        logger.info(response)\n",
    "        chunks = json.loads(response)[\"blocks\"]\n",
    "        \n",
    "        # 补充上各块的 code 内容\n",
    "        for chunk in chunks:\n",
    "            start_line = chunk[\"start_line\"]\n",
    "            end_line = chunk[\"end_line\"]\n",
    "            chunk[\"code\"] = \"\\n\".join(code_lines[start_line:end_line+1]) + \"\\n\"\n",
    "            \n",
    "        logger.info(f\"Successfully split code into {len(chunks)} top-level declarations\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in step_0_split_top_declarations: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def step_1_basic_block_segmentation(code: str, logger) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Step 1: 对每个代码块做解析,将代码块按行号拆分.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting Step 1: Basic block segmentation\")\n",
    "        \n",
    "        code_lines = code.split('\\n')\n",
    "        code_array = [{\"line\": i, \"content\": line} for i, line in enumerate(code_lines)]\n",
    "\n",
    "        prompt = \"\"\"\n",
    "Please analyze the following code and identify its basic blocks. \n",
    "Return the start and end line numbers for each basic block in JSON format.\n",
    "\n",
    "Input code:\n",
    "\"\"\" + json.dumps(code_array, indent=2) + \"\"\"\n",
    "\n",
    "Expected output format:\n",
    "{\n",
    "    \"basic_blocks\": [\n",
    "        {\n",
    "            \"start_line\": x,\n",
    "            \"end_line\": y\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\n",
    "A basic block is a sequence of consecutive statements where:\n",
    "- Flow of control enters at the beginning\n",
    "- Flow of control leaves at the end\n",
    "- No branching occurs except at the end\n",
    "- No branching targets exist except at the beginning\n",
    "\n",
    "Do not include any extra explanation, just the JSON.\n",
    "\"\"\"\n",
    "        response = get_llm_answers(prompt, model_name=\"deepseek-chat\", require_json=True)\n",
    "        basic_blocks = json.loads(response)[\"basic_blocks\"]\n",
    "        \n",
    "        # 将分好的块加上实际code\n",
    "        for block in basic_blocks:\n",
    "            start_line = block[\"start_line\"]\n",
    "            end_line = block[\"end_line\"]\n",
    "            block[\"code\"] = \"\\n\".join(code_lines[start_line - 1:end_line]) + \"\\n\"\n",
    "            \n",
    "        logger.info(f\"Successfully segmented code into {len(basic_blocks)} basic blocks\")\n",
    "        return basic_blocks\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in step_1_basic_block_segmentation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def step_2_determine_execution_order(basic_blocks: List[Dict[str, Any]], code: str, logger) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Step 2: 根据基本块确定它们之间的控制流关系.\n",
    "    \n",
    "    在此处再度强调：如果块代表的是一个函数/方法与外层互为嵌套，则彼此不应当有顺序边。\n",
    "    同一类下多个方法也不连边，除非显式调用。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting Step 2: Determining execution order\")\n",
    "        \n",
    "        blocks_content = [\n",
    "            {\"block_id\": idx, \"code\": block[\"code\"]} \n",
    "            for idx, block in enumerate(basic_blocks, 1)\n",
    "        ]\n",
    "        \n",
    "        # 修改提示，强调嵌套函数/方法为独立块，不与外层连顺序边\n",
    "        prompt = \"\"\"\n",
    "You are a senior code analysis assistant. Based on the provided basic blocks, determine the control flow between them.\n",
    "\n",
    "Each basic block is identified by a unique \"block_id\" and has some code.\n",
    "\n",
    "### Requirements:\n",
    "1) If a block is recognized as a separate (nested) function/method, do NOT add a control flow edge \n",
    "   from the outer function/class to this block (nor vice versa).\n",
    "2) Similarly, if multiple methods appear within the same class, do NOT have a direct execution flow \n",
    "   unless there's an explicit call or branching in the code.\n",
    "3) For normal sequential blocks within the same function, link them accordingly in the typical order.\n",
    "\n",
    "Return the result in strictly valid JSON:\n",
    "{\n",
    "    \"control_flow\": [\n",
    "        {\n",
    "            \"block_id\": 1,\n",
    "            \"successors\": [2, 3]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\n",
    "Do not include any extra text or explanations.\n",
    "\n",
    "Original code:\n",
    "\"\"\" + code + \"\"\"\n",
    "\n",
    "Basic Blocks:\n",
    "\"\"\" + json.dumps(blocks_content, indent=2) + \"\"\"\n",
    "\"\"\"\n",
    "        response = get_llm_answers(prompt, model_name=\"deepseek-chat\", require_json=True)\n",
    "        control_flow = json.loads(response)[\"control_flow\"]\n",
    "        logger.info(f\"Successfully determined execution order for {len(control_flow)} blocks\")\n",
    "        return control_flow\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in step_2_determine_execution_order: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def step_3_generate_cfg(basic_blocks: List[Dict[str, Any]], control_flow: List[Dict[str, Any]], decl_name: str, logger) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Step 3: 根据基本块和控制流关系生成CFG.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting Step 3: Generating CFG\")\n",
    "        \n",
    "        # 生成节点\n",
    "        nodes = [\n",
    "            {\"id\": f\"{decl_name}_{idx}\", \"code\": block[\"code\"]}\n",
    "            for idx, block in enumerate(basic_blocks, 1)\n",
    "        ]\n",
    "        \n",
    "        # 生成边\n",
    "        edges = []\n",
    "        for flow in control_flow:\n",
    "            from_id = f\"{decl_name}_{flow['block_id']}\"\n",
    "            for to_id in flow[\"successors\"]:\n",
    "                edges.append({\n",
    "                    \"from\": from_id,\n",
    "                    \"to\": f\"{decl_name}_{to_id}\"\n",
    "                })\n",
    "        \n",
    "        cfg = {\"nodes\": nodes, \"edges\": edges}\n",
    "        logger.info(f\"Successfully generated CFG with {len(nodes)} nodes and {len(edges)} edges\")\n",
    "        return cfg\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in step_3_generate_cfg: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def step_4_merge_cfgs(top_level_cfgs: List[Dict[str, Any]], logger) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Step 4: 合并所有顶层声明块的CFG.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting Step 4: Merging CFGs\")\n",
    "        \n",
    "        global_cfg = {\n",
    "            \"nodes\": [],\n",
    "            \"edges\": []\n",
    "        }\n",
    "        \n",
    "        for i, cfg in enumerate(top_level_cfgs):\n",
    "            # 为每个节点添加chunk编号前缀\n",
    "            for node in cfg[\"nodes\"]:\n",
    "                node[\"id\"] = f\"chunk_{i}_{node['id']}\"\n",
    "            \n",
    "            for edge in cfg[\"edges\"]:\n",
    "                edge[\"from\"] = f\"chunk_{i}_{edge['from']}\"\n",
    "                edge[\"to\"] = f\"chunk_{i}_{edge['to']}\"\n",
    "                \n",
    "            global_cfg[\"nodes\"].extend(cfg[\"nodes\"])\n",
    "            global_cfg[\"edges\"].extend(cfg[\"edges\"])\n",
    "            \n",
    "        logger.info(f\"Successfully merged {len(top_level_cfgs)} CFGs\")\n",
    "        return global_cfg\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in step_4_merge_cfgs: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def get_code_from_cfg(cfg: Dict[str, Any], logger) -> str:\n",
    "    \"\"\"\n",
    "    从CFG生成代码.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting code generation from CFG\")\n",
    "        \n",
    "        prompt =  \"\"\"\n",
    "请根据给出的程序控制流图生成python编程语言代码。\n",
    "CFG如下:\n",
    "\"\"\" + json.dumps(cfg, indent=4) + \"\"\"\n",
    "输出请以```python开头，```结尾。\n",
    "\"\"\"\n",
    "\n",
    "        conversations = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        res_step1 = get_llm_answers(conversations, model_name=\"gpt-4o\")\n",
    "        code = extract_code_from_markdown_block(res_step1, logger)\n",
    "        \n",
    "        if not code:\n",
    "            logger.warning(\"Generated empty code\")\n",
    "            return \"\"\n",
    "            \n",
    "        logger.info(\"Successfully generated code from CFG\")\n",
    "        return code\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in get_code_from_cfg: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def compare_code_similarity_static(original_code: str, code: str, logger) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    通过静态指标比较两段代码的相似度.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting static code similarity analysis\")\n",
    "        \n",
    "        def remove_comments(code_str: str) -> str:\n",
    "            if not isinstance(code_str, str):\n",
    "                logger.error(f\"Invalid code type: {type(code_str)}\")\n",
    "                return \"\"\n",
    "            # 移除多行注释\n",
    "            while True:\n",
    "                start = code_str.find(\"/*\")\n",
    "                if start == -1:\n",
    "                    break\n",
    "                end = code_str.find(\"*/\", start)\n",
    "                if end == -1:\n",
    "                    break\n",
    "                code_str = code_str[:start] + code_str[end+2:]\n",
    "            \n",
    "            # 移除单行注释\n",
    "            lines = []\n",
    "            for line in code_str.split('\\n'):\n",
    "                comment_idx = line.find(\"//\")\n",
    "                if comment_idx != -1:\n",
    "                    line = line[:comment_idx]\n",
    "                if line.strip():\n",
    "                    lines.append(line)\n",
    "            return '\\n'.join(lines)\n",
    "        \n",
    "        original_code = remove_comments(original_code)\n",
    "        code = remove_comments(code)\n",
    "        \n",
    "        # 计算各种相似度指标\n",
    "        len_ratio = (\n",
    "            min(len(code), len(original_code)) / max(len(code), len(original_code))\n",
    "            if max(len(code), len(original_code))>0 else 1\n",
    "        )\n",
    "        \n",
    "        def get_tokens(code_str: str) -> set:\n",
    "            if not isinstance(code_str, str):\n",
    "                logger.error(f\"Invalid code type: {type(code_str)}\")\n",
    "                return set()\n",
    "            return set(code_str.replace('{',' { ').replace('}',' } ')\n",
    "                             .replace('(',' ( ').replace(')',' ) ')\n",
    "                             .replace(';',' ; ').split())\n",
    "        \n",
    "        orig_tokens = get_tokens(original_code)\n",
    "        gen_tokens = get_tokens(code)\n",
    "        \n",
    "        intersection = len(orig_tokens.intersection(gen_tokens))\n",
    "        union = len(orig_tokens.union(gen_tokens))\n",
    "        token_similarity = intersection / union if union else 0\n",
    "        \n",
    "        def get_indent_pattern(code_str: str) -> List[int]:\n",
    "            return [\n",
    "                len(line) - len(line.lstrip()) \n",
    "                for line in code_str.split('\\n') \n",
    "                if line.strip()\n",
    "            ]\n",
    "            \n",
    "        orig_pattern = get_indent_pattern(original_code)\n",
    "        gen_pattern = get_indent_pattern(code)\n",
    "        \n",
    "        def levenshtein(s1: List[int], s2: List[int]) -> int:\n",
    "            if len(s1) < len(s2):\n",
    "                return levenshtein(s2, s1)\n",
    "            if len(s2) == 0:\n",
    "                return len(s1)\n",
    "            previous_row = range(len(s2) + 1)\n",
    "            for i, c1 in enumerate(s1):\n",
    "                current_row = [i + 1]\n",
    "                for j, c2 in enumerate(s2):\n",
    "                    insertions = previous_row[j + 1] + 1\n",
    "                    deletions = current_row[j] + 1\n",
    "                    substitutions = previous_row[j] + (c1 != c2)\n",
    "                    current_row.append(min(insertions, deletions, substitutions))\n",
    "                previous_row = current_row\n",
    "            return previous_row[-1]\n",
    "        \n",
    "        structure_diff = levenshtein(orig_pattern, gen_pattern)\n",
    "        max_diff = max(len(orig_pattern), len(gen_pattern))\n",
    "        structure_similarity = 1 - (structure_diff / max_diff if max_diff else 0)\n",
    "        \n",
    "        def count_functions(code_str: str) -> int:\n",
    "            return len(re.findall(r'\\bfn\\s+\\w+\\s*\\(', code_str))\n",
    "        \n",
    "        orig_funcs = count_functions(original_code)\n",
    "        gen_funcs = count_functions(code)\n",
    "        func_ratio = (\n",
    "            min(orig_funcs, gen_funcs) / max(orig_funcs, gen_funcs)\n",
    "            if max(orig_funcs, gen_funcs) > 0 else 1\n",
    "        )\n",
    "        \n",
    "        def calc_cyclomatic_complexity(code_str: str) -> int:\n",
    "            return len(re.findall(r'\\b(if|while|for|match)\\b', code_str)) + 1\n",
    "        \n",
    "        orig_complexity = calc_cyclomatic_complexity(original_code)\n",
    "        gen_complexity = calc_cyclomatic_complexity(code)\n",
    "        complexity_ratio = (\n",
    "            min(orig_complexity, gen_complexity) / max(orig_complexity, gen_complexity)\n",
    "            if max(orig_complexity, gen_complexity) > 0 else 1\n",
    "        )\n",
    "        \n",
    "        def extract_variables(code_str: str) -> set:\n",
    "            return set(re.findall(r'\\blet\\s+(\\w+)\\b', code_str))\n",
    "            \n",
    "        orig_vars = extract_variables(original_code)\n",
    "        gen_vars = extract_variables(code)\n",
    "        var_intersection = len(orig_vars.intersection(gen_vars))\n",
    "        var_union = len(orig_vars.union(gen_vars))\n",
    "        var_similarity = var_intersection / var_union if var_union > 0 else 1\n",
    "        \n",
    "        final_score = (\n",
    "            len_ratio * 0.15 + \n",
    "            token_similarity * 0.3 + \n",
    "            structure_similarity * 0.2 +\n",
    "            func_ratio * 0.15 +\n",
    "            complexity_ratio * 0.1 +\n",
    "            var_similarity * 0.1\n",
    "        ) * 100\n",
    "        \n",
    "        result = {\n",
    "            'final_score': round(final_score, 2),\n",
    "            'length_similarity': round(len_ratio * 100, 2),\n",
    "            'token_similarity': round(token_similarity * 100, 2), \n",
    "            'structure_similarity': round(structure_similarity * 100, 2),\n",
    "            'function_similarity': round(func_ratio * 100, 2),\n",
    "            'complexity_similarity': round(complexity_ratio * 100, 2),\n",
    "            'variable_similarity': round(var_similarity * 100, 2)\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Successfully completed static similarity analysis\")\n",
    "        logger.info(f\"Static similarity result: {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in compare_code_similarity_static: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def handle_output(cfg_res, logger):\n",
    "    \"\"\"处理CFG结果中出现的问题\"\"\"\n",
    "    try:\n",
    "        fix_prompt = (\n",
    "            f\"Please fix the following JSON file: {cfg_res} so that it can be parsed by json.loads. \"\n",
    "            \"Before output, you should check whether the result can be parsed by json.loads(). \"\n",
    "            \"Please output the fixed JSON. If the input is incomplete, please complete it by adding \"\n",
    "            \"empty strings (\\\"\\\") or closing braces (}}). Don't output anything else except the fixed JSON, \"\n",
    "            \"don't output the string like ```json.\"\n",
    "        )\n",
    "        res = get_llm_answers(fix_prompt, model_name=\"gpt-4o-2024-11-20\", require_json=True)\n",
    "        res = extract_code_from_markdown_block(res, logger)\n",
    "        logger.info(f\"Fixed JSON:\\n{res}\")\n",
    "        json_obj = json.loads(res)\n",
    "        return json_obj\n",
    "    except Exception as e:\n",
    "        error_msg = f\"处理CFG结果失败: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        logger.error(error_msg)\n",
    "        print(error_msg)\n",
    "        return {\"nodes\": [], \"edges\": []}\n",
    "\n",
    "def process_chunk(code: str, decl_name: str, index: int, logger) -> Dict[str, Any]:\n",
    "    \"\"\"处理单个代码块.\"\"\"\n",
    "    try:\n",
    "        basic_block = step_1_basic_block_segmentation(code, logger)\n",
    "        control_flow = step_2_determine_execution_order(basic_block, code, logger)\n",
    "        cfg = step_3_generate_cfg(basic_block, control_flow, decl_name, logger)\n",
    "        logger.info(f\"Successfully processed chunk {index}\")\n",
    "        return cfg\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing chunk {index}: {str(e)}\")\n",
    "        return {\"nodes\": [], \"edges\": []}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"deepseek-chat\"\n",
    "    task_type = \"llm_cfg\"\n",
    "    os.makedirs(f\"{task_type}/{model_name}\", exist_ok=True)\n",
    "    \n",
    "    # 初始化全局日志\n",
    "    logger = setup_logging(log_dir=f\"{task_type}/{model_name}/logs\", index=None)\n",
    "    \n",
    "    # 初始化全局对话\n",
    "    conversations = init_conversations(logger)\n",
    "\n",
    "    datas = []\n",
    "    os.makedirs(\"cfg\", exist_ok=True)\n",
    "    for i, file in enumerate(os.listdir(\"data\")):\n",
    "        if file.endswith(\".py\"):\n",
    "            with open(os.path.join(\"data\", file), \"r\", encoding=\"utf-8\") as f:\n",
    "                datas.append({\"text\": f.read()})\n",
    "\n",
    "            json_file = f\"cfg_output/{file.replace('.py', '.json')}\"\n",
    "            if os.path.exists(json_file):\n",
    "                with open(json_file, \"r\") as f:\n",
    "                    cfg_data = json.load(f)\n",
    "                    output_file = f\"cfg/{i}.json\"\n",
    "                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "                    with open(output_file, \"w\") as f:\n",
    "                        json.dump(cfg_data, f, indent=4)\n",
    "\n",
    "    def process_single_code(i, original_code):\n",
    "        \"\"\"处理单个代码样本\"\"\"\n",
    "        cfg_file = f\"{task_type}/{model_name}/cfg/{i}.json\"\n",
    "        os.makedirs(os.path.dirname(cfg_file), exist_ok=True)  # 确保cfg目录存在\n",
    "        \n",
    "        if os.path.exists(cfg_file):\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"Processing sample {i}\")\n",
    "            \n",
    "        logger = setup_logging(f\"{task_type}/{model_name}/logs\", index=i)\n",
    "        logger.info(f\"Processing sample {i}\")\n",
    "        try:\n",
    "            chunks = step_0_split_top_declarations(original_code, logger)\n",
    "            if isinstance(chunks, dict):\n",
    "                # 有时LLM可能返回{\"declarations\":[...]}，做个兼容\n",
    "                chunks = chunks[\"declarations\"]\n",
    "            cfg_res = [None] * len(chunks)\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures_with_index = []\n",
    "                for index, chunk in enumerate(chunks):\n",
    "                    future = executor.submit(\n",
    "                        process_chunk, chunk[\"code\"], chunk[\"decl_name\"], index, logger\n",
    "                    )\n",
    "                    futures_with_index.append((index, future))\n",
    "\n",
    "                for index, future in futures_with_index:\n",
    "                    result = future.result()\n",
    "                    cfg_res[index] = result\n",
    "\n",
    "            all_cfg = step_4_merge_cfgs(cfg_res, logger)\n",
    "            code = get_code_from_cfg(all_cfg, logger)\n",
    "\n",
    "            os.makedirs(f\"{task_type}/{model_name}/cfg_to_code\", exist_ok=True)\n",
    "            with open(f\"{task_type}/{model_name}/cfg_to_code/{i}.cj\", \"w\") as f:\n",
    "                f.write(code)\n",
    "            \n",
    "            with open(cfg_file, \"w\") as f:\n",
    "                json.dump(all_cfg, f, indent=4)\n",
    "\n",
    "            static_similarity = compare_code_similarity_static(original_code, code, logger)\n",
    "            print(\"Completed processing sample\", i)\n",
    "            return code, static_similarity\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing sample {i}: {str(e)}\")\n",
    "            return None, None\n",
    "        \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_file = f\"{task_type}/{model_name}/chain_similarity_results.csv\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        os.makedirs(f\"{task_type}/{model_name}\", exist_ok=True)\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Index', 'Original Code', 'Generated Code', 'Static Similarity', 'LLM Similarity'])\n",
    "\n",
    "    def process_and_save(i, data):\n",
    "        \"\"\"处理单个样本并保存结果\"\"\"\n",
    "        try:\n",
    "            code, static_similarity = process_single_code(i, data[\"text\"])\n",
    "            if code is not None:\n",
    "                with open(csv_file, 'a', newline='', encoding='utf-8') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([i, data[\"text\"], code, static_similarity])\n",
    "                print(f\"Completed processing sample {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results for sample {i}: {str(e)}\")\n",
    "\n",
    "    datas = datas[:1]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_and_save, i, data) for i, data in enumerate(datas)]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Task execution failed: {str(e)}\")\n",
    "    \n",
    "    print(f\"All results saved to {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scalpel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
