{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 Python files in ../dataset/python.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  39%|███▉      | 78/200 [00:36<00:23,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 754 column 27 (char 21951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  40%|███▉      | 79/200 [00:36<00:30,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 737 column 1 (char 30758)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  41%|████      | 82/200 [00:38<00:58,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting value: line 670 column 38 (char 23347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  42%|████▎     | 85/200 [00:39<00:45,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 612 column 27 (char 22599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  44%|████▎     | 87/200 [00:40<00:34,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 404 column 23 (char 18805)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  44%|████▍     | 88/200 [00:41<00:57,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 671 column 39 (char 24094)\n",
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 608 column 16 (char 19523)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  46%|████▋     | 93/200 [00:42<00:28,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 748 column 39 (char 24322)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  47%|████▋     | 94/200 [00:42<00:28,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 741 column 22 (char 30081)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  48%|████▊     | 96/200 [00:42<00:22,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 715 column 1 (char 28015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  50%|█████     | 101/200 [00:43<00:18,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 611 column 35 (char 30174)\n",
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 668 column 1 (char 31231)\n",
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 838 column 24 (char 26406)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 439 column 23 (char 18629)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  53%|█████▎    | 106/200 [00:44<00:11,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting value: line 552 column 18 (char 19166)\n",
      "[Error] llm_build_ast_from_tokens: Expecting value: line 464 column 18 (char 17739)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 189 column 35 (char 15434)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 653 column 27 (char 27173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  55%|█████▌    | 110/200 [00:44<00:10,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 698 column 19 (char 19304)\n",
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 483 column 1 (char 23411)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 558 column 19 (char 22841)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 780 column 39 (char 29669)\n",
      "[Error] llm_build_ast_from_tokens: Expecting value: line 724 column 70 (char 32044)\n",
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 697 column 22 (char 24466)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 771 column 23 (char 27452)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  57%|█████▊    | 115/200 [00:45<00:07, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 3 column 11 (char 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  58%|█████▊    | 117/200 [00:45<00:10,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 767 column 43 (char 25867)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  60%|█████▉    | 119/200 [00:45<00:11,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 603 column 1 (char 27447)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 437 column 27 (char 18771)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  60%|██████    | 121/200 [00:46<00:14,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 734 column 18 (char 28646)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 645 column 45 (char 32639)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  62%|██████▏   | 124/200 [00:47<00:12,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 774 column 44 (char 23555)\n",
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 604 column 1 (char 21276)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  65%|██████▌   | 130/200 [00:47<00:08,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 678 column 43 (char 24551)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 800 column 51 (char 26353)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  66%|██████▋   | 133/200 [00:48<00:09,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 769 column 23 (char 21704)\n",
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 519 column 1 (char 20776)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  68%|██████▊   | 136/200 [00:48<00:08,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 764 column 35 (char 23188)\n",
      "[Error] llm_build_ast_from_tokens: Expecting value: line 633 column 28 (char 22431)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  70%|███████   | 140/200 [00:49<00:10,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 517 column 23 (char 22086)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  72%|███████▏  | 144/200 [00:49<00:08,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 689 column 52 (char 35654)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  73%|███████▎  | 146/200 [00:50<00:09,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 571 column 29 (char 34876)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 687 column 11 (char 25443)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  74%|███████▍  | 148/200 [00:50<00:07,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 717 column 30 (char 26683)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  75%|███████▌  | 150/200 [00:51<00:08,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 475 column 27 (char 18846)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 181 column 23 (char 17037)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  76%|███████▋  | 153/200 [00:52<00:11,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 874 column 39 (char 34735)\n",
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 424 column 18 (char 16567)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  78%|███████▊  | 156/200 [00:53<00:13,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 619 column 27 (char 20574)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  79%|███████▉  | 158/200 [00:54<00:18,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 550 column 23 (char 19455)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  81%|████████  | 162/200 [01:00<00:42,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting value: line 782 column 30 (char 25588)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  82%|████████▏ | 163/200 [01:00<00:32,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting value: line 683 column 30 (char 24758)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  82%|████████▏ | 164/200 [01:01<00:36,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 614 column 1 (char 23733)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  82%|████████▎ | 165/200 [01:02<00:35,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 698 column 27 (char 20550)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  84%|████████▎ | 167/200 [01:03<00:19,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting value: line 533 column 26 (char 18411)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  84%|████████▍ | 168/200 [01:03<00:20,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 709 column 12 (char 22972)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 725 column 23 (char 23890)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  86%|████████▌ | 171/200 [01:04<00:10,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 655 column 31 (char 26727)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  86%|████████▌ | 172/200 [01:04<00:10,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 757 column 15 (char 27927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  86%|████████▋ | 173/200 [01:05<00:09,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 767 column 22 (char 34551)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  88%|████████▊ | 176/200 [01:06<00:07,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 736 column 35 (char 29086)\n",
      "[Error] llm_build_ast_from_tokens: Expecting value: line 800 column 38 (char 32946)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  88%|████████▊ | 177/200 [01:06<00:07,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 737 column 1 (char 37263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  90%|████████▉ | 179/200 [01:07<00:10,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 830 column 15 (char 23984)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  90%|█████████ | 181/200 [01:08<00:09,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting value: line 826 column 1 (char 30599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  91%|█████████ | 182/200 [01:09<00:09,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 682 column 1 (char 25017)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  92%|█████████▏| 183/200 [01:10<00:11,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 617 column 1 (char 19151)\n",
      "[Error] llm_build_ast_from_tokens: Expecting property name enclosed in double quotes: line 753 column 22 (char 26087)\n",
      "[Error] llm_build_ast_from_tokens: Expecting value: line 597 column 20 (char 22268)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  93%|█████████▎| 186/200 [01:10<00:05,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 693 column 35 (char 21861)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  94%|█████████▍| 188/200 [01:12<00:06,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting value: line 612 column 50 (char 27403)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  95%|█████████▌| 190/200 [01:13<00:05,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting value: line 611 column 20 (char 27146)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  96%|█████████▌| 191/200 [01:14<00:04,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 803 column 27 (char 32923)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  96%|█████████▌| 192/200 [01:15<00:04,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 179 column 19 (char 17366)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  96%|█████████▋| 193/200 [01:18<00:09,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting ',' delimiter: line 689 column 1 (char 34258)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  97%|█████████▋| 194/200 [01:18<00:06,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting value: line 685 column 1 (char 28781)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  98%|█████████▊| 195/200 [01:19<00:05,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 783 column 39 (char 28010)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  98%|█████████▊| 196/200 [01:20<00:03,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 574 column 15 (char 20502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  99%|█████████▉| 198/200 [01:20<00:01,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Expecting value: line 836 column 58 (char 35360)\n",
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 701 column 35 (char 26087)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|█████████▉| 199/200 [01:26<00:01,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 631 column 15 (char 20621)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 200/200 [01:29<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] llm_build_ast_from_tokens: Unterminated string starting at: line 752 column 39 (char 29440)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-threaded script for Python code analysis:\n",
    "\n",
    "1) LLM-based AST generation (via CFG + partial block approach):\n",
    "   - parse_cfg_structure() => get line ranges of classes/functions\n",
    "   - build_ast_from_cfg() => recursively exclude child function/class lines from the parent block,\n",
    "     parse only the remaining lines, then insert function/class placeholders back in the correct position.\n",
    "   - Each node has global-level start_token/end_token, thanks to a single global tokenize_code_with_lines().\n",
    "\n",
    "2) Tree-sitter-based static AST:\n",
    "   - generate_tree_sitter_ast() => returns {type, label, children}.\n",
    "\n",
    "3) Compare snippet-level labels (optional).\n",
    "\n",
    "4) Save both ASTs as JSON.\n",
    "\n",
    "5) Multi-file parallel processing with ThreadPoolExecutor.\n",
    "\n",
    "See the \"llm_build_ast_from_tokens\" function's prompt – we keep it intact as requested.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import re\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "###############################################################################\n",
    "#                           LLM interface (stub)                              #\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Replace 'get_llm_answers' with your actual LLM API call or function.\n",
    "Here we just provide a stub or minimal placeholder.\n",
    "\"\"\"\n",
    "try:\n",
    "    from llm import get_llm_answers\n",
    "except ImportError:\n",
    "    # If no llm.py found, define a placeholder\n",
    "    def get_llm_answers(prompt, model_name=\"\", require_json=False, temperature=0):\n",
    "        # Return a minimal JSON for demonstration\n",
    "        # In reality, you'd implement the actual LLM call (OpenAI, local model, etc.)\n",
    "        return \"{}\"\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#                     Tree-sitter initialization (Python)                     #\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#   4) 保持原样: llm_build_ast_from_tokens() (prompt 不变, 勿改)              #\n",
    "###############################################################################\n",
    "def llm_build_ast(code) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    给定 tokens列表 => 调用 LLM => 生成 JSON AST.\n",
    "    - top_level: 是否最外层(只有最外层允许 'module'), 否则用 'block' 等\n",
    "    \"\"\"\n",
    "\n",
    "    # 构造 prompt\n",
    "    allowed_types = [\n",
    "        \"aliased_import\", \"argument_list\", \"as_pattern\", \"as_pattern_target\", \"assert_statement\",\n",
    "        \"assignment\", \"attribute\", \"augmented_assignment\", \"await\", \"binary_operator\", \"block\",\n",
    "        \"boolean_operator\", \"break_statement\", \"call\", \"class_definition\", \"comment\",\n",
    "        \"comparison_operator\", \"concatenated_string\", \"conditional_expression\",\n",
    "        \"continue_statement\", \"decorated_definition\", \"decorator\", \"default_parameter\",\n",
    "        \"delete_statement\", \"dictionary\", \"dictionary_comprehension\", \"dictionary_splat\",\n",
    "        \"dictionary_splat_pattern\", \"dotted_name\", \"elif_clause\", \"ellipsis\", \"else_clause\",\n",
    "        \"escape_interpolation\", \"escape_sequence\", \"except_clause\", \"expression_list\",\n",
    "        \"expression_statement\", \"false\", \"finally_clause\", \"float\", \"for_in_clause\", \"for_statement\",\n",
    "        \"format_specifier\", \"function_definition\", \"future_import_statement\",\n",
    "        \"generator_expression\", \"generic_type\", \"global_statement\", \"identifier\", \"if_clause\",\n",
    "        \"if_statement\", \"import_from_statement\", \"import_prefix\", \"import_statement\", \"integer\",\n",
    "        \"interpolation\", \"keyword_argument\", \"keyword_separator\", \"lambda\", \"lambda_parameters\",\n",
    "        \"line_continuation\", \"list\", \"list_comprehension\", \"list_splat\", \"list_splat_pattern\",\n",
    "        \"module\", \"named_expression\", \"none\", \"nonlocal_statement\", \"not_operator\", \"pair\",\n",
    "        \"parameters\", \"parenthesized_expression\", \"pass_statement\", \"pattern_list\", \"raise_statement\",\n",
    "        \"relative_import\", \"return_statement\", \"set\", \"set_comprehension\", \"slice\", \"string\",\n",
    "        \"string_content\", \"string_end\", \"string_start\", \"subscript\", \"true\", \"try_statement\",\n",
    "        \"tuple\", \"tuple_pattern\", \"type\", \"type_parameter\", \"typed_default_parameter\", \"typed_parameter\",\n",
    "        \"unary_operator\", \"union_type\", \"while_statement\", \"with_clause\", \"with_item\", \"with_statement\",\n",
    "        \"yield\"\n",
    "    ]\n",
    "    allowed_types_str = \", \".join(allowed_types)\n",
    "\n",
    "    top_level_instruction = \"Exactly one 'module' node can appear at the root. Use 'block' if nested.\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        \"Below is the code snippet:\\n\"\n",
    "        f\"{code}\\n\\n\"\n",
    "        \"Create a JSON-based AST with these fields:\\n\"\n",
    "        f\"- 'type': must be in {{{allowed_types_str}}}\\n\"\n",
    "        \"- 'code'\\n\"\n",
    "        \"- 'children' (array)\\n\\n\"\n",
    "        \"Return valid JSON only.\\n\"\n",
    "    )\n",
    "    \n",
    "    prompt += \"\\nAt the root, use 'module'. Do not nest multiple 'module'.\\n\" + top_level_instruction\n",
    "\n",
    "    try:\n",
    "        llm_output = get_llm_answers(\n",
    "            prompt,\n",
    "            model_name=\"gpt-4o\",\n",
    "            require_json=True,\n",
    "            temperature=0\n",
    "        )\n",
    "        ast_dict = json.loads(llm_output)\n",
    "        return ast_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] llm_build_ast_from_tokens: {e}\")\n",
    "        return {\n",
    "            \"type\": \"ErrorNode\",\n",
    "            \"code\": code,\n",
    "            \"children\": []\n",
    "        }\n",
    "    \n",
    "def generate_llm_ast(code: str) -> Dict[str,Any]:\n",
    "    llm_ast = llm_build_ast(code)\n",
    "    return llm_ast\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#     8) 单文件处理: 生成LLM AST, Tree-sitter AST, 存JSON,可选对比             #\n",
    "###############################################################################\n",
    "def process_llm_ast(code: str, file_path: str) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    生成 LLM AST, 并保存到 JSON\n",
    "    \"\"\"\n",
    "    llm_dir = \"llm_ast\"\n",
    "    os.makedirs(llm_dir, exist_ok=True)\n",
    "    llm_path = os.path.join(llm_dir, os.path.basename(file_path) + \".json\")\n",
    "\n",
    "    if os.path.exists(llm_path):\n",
    "        with open(llm_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            llm_ast = json.load(f)\n",
    "        # print(f\"[LLM AST cached] => {llm_path}\")\n",
    "        return llm_ast\n",
    "\n",
    "    llm_ast = generate_llm_ast(code)\n",
    "    with open(llm_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(llm_ast, fout, indent=4, ensure_ascii=False)\n",
    "    return llm_ast\n",
    "\n",
    "def process_single_file(file_path: str):\n",
    "    \"\"\"\n",
    "    1) read code\n",
    "    2) LLM AST\n",
    "    3) Tree-sitter AST\n",
    "    4) optionally compare\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            code = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error reading {file_path}]: {e}\")\n",
    "        return\n",
    "\n",
    "    llm_ast = process_llm_ast(code, file_path)\n",
    "\n",
    "###############################################################################\n",
    "#                            9) main() 并行处理                               #\n",
    "###############################################################################\n",
    "def main():\n",
    "    source_dir = \"../dataset/python\"  # 修改为你的实际源文件目录\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"[Error] Directory {source_dir} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # 收集所有 .py 文件\n",
    "    files = [f for f in os.listdir(source_dir) if f.endswith(\".py\")][:200]\n",
    "    print(f\"Found {len(files)} Python files in {source_dir}.\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = []\n",
    "        pbar = tqdm(total=len(files), desc=\"Processing files\")\n",
    "        for fname in files:\n",
    "            full_path = os.path.join(source_dir, fname)\n",
    "            future = executor.submit(process_single_file, full_path)\n",
    "            future.add_done_callback(lambda _: pbar.update(1))\n",
    "            futures.append(future)\n",
    "        concurrent.futures.wait(futures)\n",
    "        pbar.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short文件（0-50行）中的ErrorNode数量: 7/53\n",
      "Medium文件（51-200行）中的ErrorNode数量: 39/109\n",
      "Long文件（201-1001行）中的ErrorNode数量: 35/37\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "llm_ast_dir = \"llm_ast\"\n",
    "source_dir = \"../dataset/python\"\n",
    "\n",
    "# 定义文件长度分类的字典\n",
    "length_categories = {\n",
    "    \"short\": {\"range\": (0, 50), \"error_count\": 0, \"total_count\": 0},\n",
    "    \"medium\": {\"range\": (51, 200), \"error_count\": 0, \"total_count\": 0},\n",
    "    \"long\": {\"range\": (201, 1001), \"error_count\": 0, \"total_count\": 0}\n",
    "}\n",
    "\n",
    "for file in os.listdir(llm_ast_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(os.path.join(llm_ast_dir, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            llm_ast = json.load(f)\n",
    "\n",
    "        source_file = os.path.join(source_dir, file.replace(\".json\", \"\"))\n",
    "        if not os.path.exists(source_file):\n",
    "            print(f\"Source file not found: {source_file}\")\n",
    "            continue\n",
    "    \n",
    "        # 统计源文件行数\n",
    "        with open(source_file, \"r\", encoding=\"utf-8\") as src_f:\n",
    "            num_lines = sum(1 for _ in src_f)  # 高效逐行计数\n",
    "\n",
    "        # 根据行数分类\n",
    "        for category, data in length_categories.items():\n",
    "            min_len, max_len = data[\"range\"]\n",
    "            if min_len <= num_lines <= max_len:\n",
    "                data[\"total_count\"] += 1\n",
    "                if llm_ast[\"type\"] == \"ErrorNode\":\n",
    "                    data[\"error_count\"] += 1\n",
    "                break\n",
    "\n",
    "# 输出结果\n",
    "for category, data in length_categories.items():\n",
    "    print(f\"{category.capitalize()}文件（{data['range'][0]}-{data['range'][1]}行）中的ErrorNode数量: {data['error_count']}/{data['total_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import re\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "###############################################################################\n",
    "#                           LLM interface (stub)                              #\n",
    "###############################################################################\n",
    "try:\n",
    "    from llm import get_llm_answers\n",
    "except ImportError:\n",
    "    # If no llm.py found, define a placeholder\n",
    "    def get_llm_answers(prompt, model_name=\"\", require_json=False, temperature=0):\n",
    "        # Return a minimal JSON for demonstration\n",
    "        return \"{}\"\n",
    "\n",
    "###############################################################################\n",
    "#                   1) 全局分词，含行号 -> global_tokens                       #\n",
    "###############################################################################\n",
    "def tokenize_code_with_lines(code: str) -> List[Tuple[int, int, str, int]]:\n",
    "    \"\"\"\n",
    "    使用正则分词，直接在原始代码上通过 finditer() 获取匹配位置，\n",
    "    并保留换行符，让 LLM 能识别多行结构。\n",
    "    返回形式: [(start_offset, end_offset, token_text, line_number), ...]\n",
    "    \"\"\"\n",
    "    token_pattern = (\n",
    "        r'[A-Za-z_]\\w*|'  # 标识符\n",
    "        r'[0-9]+|'        # 数字\n",
    "        r'\"[^\"]*\"|'       # 双引号字符串\n",
    "        r\"'[^']*'|\"       # 单引号字符串\n",
    "        r'\\\\[ntr]|'       # 转义符 \\n \\t \\r\n",
    "        r'//.*|'          # 单行注释 (如 C++/Java/JS 风格)\n",
    "        r'/\\*.*?\\*/|'     # 多行注释 (如 C 风格)\n",
    "        r'\\n|\\r|\\t|'      # 换行/回车/制表符\n",
    "        r'\\S'             # 其他符号(如 +, -, {, }, 以及任何其它非空白字符)\n",
    "    )\n",
    "\n",
    "    tokens_with_offset = []\n",
    "    lines = code.splitlines(keepends=True)\n",
    "    current_line = 1\n",
    "    current_pos = 0\n",
    "    \n",
    "    for match in re.finditer(token_pattern, code, re.MULTILINE | re.DOTALL):\n",
    "        tk = match.group(0)\n",
    "        start_offset, end_offset = match.span()\n",
    "        \n",
    "        # 计算当前token所在行号\n",
    "        while current_line <= len(lines) and current_pos + len(lines[current_line-1]) <= start_offset:\n",
    "            current_pos += len(lines[current_line-1])\n",
    "            current_line += 1\n",
    "            \n",
    "        tokens_with_offset.append((start_offset, end_offset, tk, current_line))\n",
    "\n",
    "    return tokens_with_offset\n",
    "\n",
    "###############################################################################\n",
    "#   2) 直接调用 LLM 生成 AST (无需 CFG 结构)                                 #\n",
    "###############################################################################\n",
    "def llm_build_ast_from_tokens(tokens_with_offset: List[Tuple[int, int, str]], top_level=True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    给定 tokens列表 => 调用 LLM => 生成 JSON AST.\n",
    "    - top_level: 是否最外层(只有最外层允许 'module'), 否则用 'block' 等\n",
    "    \"\"\"\n",
    "    indexed_tokens = [(i, t[2]) for i, t in enumerate(tokens_with_offset)]\n",
    "    token_info = \"\\n\".join(f\"{i}: {text}\" for (i, text) in indexed_tokens)\n",
    "\n",
    "    # 构造 prompt\n",
    "    allowed_types = [\n",
    "        \"aliased_import\", \"argument_list\", \"as_pattern\", \"as_pattern_target\", \"assert_statement\",\n",
    "        \"assignment\", \"attribute\", \"augmented_assignment\", \"await\", \"binary_operator\", \"block\",\n",
    "        \"boolean_operator\", \"break_statement\", \"call\", \"class_definition\", \"comment\",\n",
    "        \"comparison_operator\", \"concatenated_string\", \"conditional_expression\",\n",
    "        \"continue_statement\", \"decorated_definition\", \"decorator\", \"default_parameter\",\n",
    "        \"delete_statement\", \"dictionary\", \"dictionary_comprehension\", \"dictionary_splat\",\n",
    "        \"dictionary_splat_pattern\", \"dotted_name\", \"elif_clause\", \"ellipsis\", \"else_clause\",\n",
    "        \"escape_interpolation\", \"escape_sequence\", \"except_clause\", \"expression_list\",\n",
    "        \"expression_statement\", \"false\", \"finally_clause\", \"float\", \"for_in_clause\", \"for_statement\",\n",
    "        \"format_specifier\", \"function_definition\", \"future_import_statement\",\n",
    "        \"generator_expression\", \"generic_type\", \"global_statement\", \"identifier\", \"if_clause\",\n",
    "        \"if_statement\", \"import_from_statement\", \"import_prefix\", \"import_statement\", \"integer\",\n",
    "        \"interpolation\", \"keyword_argument\", \"keyword_separator\", \"lambda\", \"lambda_parameters\",\n",
    "        \"line_continuation\", \"list\", \"list_comprehension\", \"list_splat\", \"list_splat_pattern\",\n",
    "        \"module\", \"named_expression\", \"none\", \"nonlocal_statement\", \"not_operator\", \"pair\",\n",
    "        \"parameters\", \"parenthesized_expression\", \"pass_statement\", \"pattern_list\", \"raise_statement\",\n",
    "        \"relative_import\", \"return_statement\", \"set\", \"set_comprehension\", \"slice\", \"string\",\n",
    "        \"string_content\", \"string_end\", \"string_start\", \"subscript\", \"true\", \"try_statement\",\n",
    "        \"tuple\", \"tuple_pattern\", \"type\", \"type_parameter\", \"typed_default_parameter\", \"typed_parameter\",\n",
    "        \"unary_operator\", \"union_type\", \"while_statement\", \"with_clause\", \"with_item\", \"with_statement\",\n",
    "        \"yield\"\n",
    "    ]\n",
    "    allowed_types_str = \", \".join(allowed_types)\n",
    "\n",
    "    top_level_instruction = \"Exactly one 'module' node can appear at the root. Use 'block' if nested.\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        \"Below is a list of tokens (index -> token_string) for a code snippet:\\n\"\n",
    "        f\"{token_info}\\n\\n\"\n",
    "        \"Create a JSON-based AST with these fields:\\n\"\n",
    "        f\"- 'type': must be in {{{allowed_types_str}}}\\n\"\n",
    "        \"- 'start_token', 'end_token'\\n\"\n",
    "        \"- 'children' (array)\\n\\n\"\n",
    "        \"Leaf nodes => start_token == end_token.\\n\"\n",
    "        \"No overlapping sibling token ranges.\\n\"\n",
    "        \"Return valid JSON only.\\n\"\n",
    "    )\n",
    "    if top_level:\n",
    "        prompt += \"\\nAt the root, use 'module'. Do not nest multiple 'module'.\\n\" + top_level_instruction\n",
    "    else:\n",
    "        prompt += \"\\nInside blocks, do not produce 'module'. Use 'block' or suitable type.\\n\" + top_level_instruction\n",
    "\n",
    "    try:\n",
    "        llm_output = get_llm_answers(\n",
    "            prompt,\n",
    "            model_name=\"gpt-4o\",\n",
    "            require_json=True,\n",
    "            temperature=0\n",
    "        )\n",
    "        ast_dict = json.loads(llm_output)\n",
    "        return ast_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] llm_build_ast_from_tokens: {e}\")\n",
    "        return {\n",
    "            \"type\": \"ErrorNode\",\n",
    "            \"start_token\": -1,\n",
    "            \"end_token\": -1,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "def generate_llm_ast(code: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    直接生成整个文件的 AST\n",
    "    \"\"\"\n",
    "    global_tokens = tokenize_code_with_lines(code)\n",
    "    tokens_for_llm = [(so, eo, tk) for (so, eo, tk, ln) in global_tokens]\n",
    "    return llm_build_ast_from_tokens(tokens_for_llm, top_level=True)\n",
    "\n",
    "###############################################################################\n",
    "#     3) 单文件处理: 生成LLM AST, 存JSON                                       #\n",
    "###############################################################################\n",
    "def process_single_file(file_path: str):\n",
    "    \"\"\"\n",
    "    1) read code\n",
    "    2) LLM AST\n",
    "    3) 保存 JSON\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            code = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error reading {file_path}]: {e}\")\n",
    "        return\n",
    "\n",
    "    llm_ast = generate_llm_ast(code)\n",
    "    llm_dir = \"llm_ast_whole\"\n",
    "    os.makedirs(llm_dir, exist_ok=True)\n",
    "    llm_path = os.path.join(llm_dir, os.path.basename(file_path) + \".json\")\n",
    "    with open(llm_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(llm_ast, fout, indent=4, ensure_ascii=False)\n",
    "    print(f\"[LLM AST saved] => {llm_path}\")\n",
    "\n",
    "###############################################################################\n",
    "#                            4) main() 并行处理                               #\n",
    "###############################################################################\n",
    "def main():\n",
    "    source_dir = \"../dataset/python\"  # 修改为你的实际源文件目录\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"[Error] Directory {source_dir} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # 收集所有 .py 文件\n",
    "    files = [f for f in os.listdir(source_dir) if f.endswith(\".py\")][:200]\n",
    "    print(f\"Found {len(files)} Python files in {source_dir}.\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = []\n",
    "        pbar = tqdm(total=len(files), desc=\"Processing files\")\n",
    "        for fname in files:\n",
    "            full_path = os.path.join(source_dir, fname)\n",
    "            future = executor.submit(process_single_file, full_path)\n",
    "            future.add_done_callback(lambda _: pbar.update(1))\n",
    "            futures.append(future)\n",
    "        concurrent.futures.wait(futures)\n",
    "        pbar.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processed] => llm_ast_whole_processed/48.py.json\n",
      "[Processed] => llm_ast_whole_processed/174.py.json\n",
      "[Processed] => llm_ast_whole_processed/195.py.json\n",
      "[Processed] => llm_ast_whole_processed/84.py.json\n",
      "[Processed] => llm_ast_whole_processed/131.py.json\n",
      "[Processed] => llm_ast_whole_processed/45.py.json\n",
      "[Processed] => llm_ast_whole_processed/165.py.json\n",
      "[Processed] => llm_ast_whole_processed/137.py.json\n",
      "[Processed] => llm_ast_whole_processed/178.py.json\n",
      "[Processed] => llm_ast_whole_processed/189.py.json\n",
      "[Processed] => llm_ast_whole_processed/1.py.json\n",
      "[Processed] => llm_ast_whole_processed/99.py.json\n",
      "[Processed] => llm_ast_whole_processed/21.py.json\n",
      "[Processed] => llm_ast_whole_processed/41.py.json\n",
      "Warning: Invalid child node type <class 'int'> in typed_parameter\n",
      "Warning: Skipping invalid child type <class 'int'> in typed_parameter\n",
      "[Processed] => llm_ast_whole_processed/22.py.json\n",
      "[Processed] => llm_ast_whole_processed/40.py.json\n",
      "[Processed] => llm_ast_whole_processed/11.py.json\n",
      "[Processed] => llm_ast_whole_processed/114.py.json\n",
      "[Processed] => llm_ast_whole_processed/208.py.json\n",
      "[Processed] => llm_ast_whole_processed/9.py.json\n",
      "[Processed] => llm_ast_whole_processed/188.py.json\n",
      "[Processed] => llm_ast_whole_processed/15.py.json\n",
      "[Processed] => llm_ast_whole_processed/13.py.json\n",
      "[Processed] => llm_ast_whole_processed/91.py.json\n",
      "[Processed] => llm_ast_whole_processed/162.py.json\n",
      "[Processed] => llm_ast_whole_processed/50.py.json\n",
      "[Processed] => llm_ast_whole_processed/52.py.json\n",
      "[Processed] => llm_ast_whole_processed/183.py.json\n",
      "[Processed] => llm_ast_whole_processed/90.py.json\n",
      "[Processed] => llm_ast_whole_processed/58.py.json\n",
      "[Processed] => llm_ast_whole_processed/55.py.json\n",
      "[Processed] => llm_ast_whole_processed/12.py.json\n",
      "[Processed] => llm_ast_whole_processed/54.py.json\n",
      "[Processed] => llm_ast_whole_processed/38.py.json\n",
      "[Processed] => llm_ast_whole_processed/100.py.json\n",
      "[Processed] => llm_ast_whole_processed/26.py.json\n",
      "[Processed] => llm_ast_whole_processed/19.py.json\n",
      "[Processed] => llm_ast_whole_processed/33.py.json\n",
      "[Processed] => llm_ast_whole_processed/60.py.json\n",
      "[Processed] => llm_ast_whole_processed/31.py.json\n",
      "[Processed] => llm_ast_whole_processed/190.py.json\n",
      "[Processed] => llm_ast_whole_processed/210.py.json\n",
      "[Processed] => llm_ast_whole_processed/127.py.json\n",
      "[Processed] => llm_ast_whole_processed/69.py.json\n",
      "[Processed] => llm_ast_whole_processed/166.py.json\n",
      "[Processed] => llm_ast_whole_processed/203.py.json\n",
      "[Processed] => llm_ast_whole_processed/161.py.json\n",
      "[Processed] => llm_ast_whole_processed/184.py.json\n",
      "[Processed] => llm_ast_whole_processed/120.py.json\n",
      "[Processed] => llm_ast_whole_processed/123.py.json\n",
      "[Processed] => llm_ast_whole_processed/57.py.json\n",
      "[Processed] => llm_ast_whole_processed/179.py.json\n",
      "[Processed] => llm_ast_whole_processed/177.py.json\n",
      "[Processed] => llm_ast_whole_processed/207.py.json\n",
      "[Processed] => llm_ast_whole_processed/80.py.json\n",
      "[Processed] => llm_ast_whole_processed/119.py.json\n",
      "[Processed] => llm_ast_whole_processed/139.py.json\n",
      "[Processed] => llm_ast_whole_processed/78.py.json\n",
      "[Processed] => llm_ast_whole_processed/150.py.json\n",
      "[Processed] => llm_ast_whole_processed/63.py.json\n",
      "[Processed] => llm_ast_whole_processed/25.py.json\n",
      "[Processed] => llm_ast_whole_processed/153.py.json\n",
      "[Processed] => llm_ast_whole_processed/108.py.json\n",
      "[Processed] => llm_ast_whole_processed/142.py.json\n",
      "[Processed] => llm_ast_whole_processed/32.py.json\n",
      "[Processed] => llm_ast_whole_processed/121.py.json\n",
      "[Processed] => llm_ast_whole_processed/92.py.json\n",
      "[Processed] => llm_ast_whole_processed/144.py.json\n",
      "[Processed] => llm_ast_whole_processed/101.py.json\n",
      "[Processed] => llm_ast_whole_processed/140.py.json\n",
      "[Processed] => llm_ast_whole_processed/65.py.json\n",
      "[Processed] => llm_ast_whole_processed/4.py.json\n",
      "[Processed] => llm_ast_whole_processed/124.py.json\n",
      "[Processed] => llm_ast_whole_processed/88.py.json\n",
      "[Processed] => llm_ast_whole_processed/70.py.json\n",
      "[Processed] => llm_ast_whole_processed/6.py.json\n",
      "[Processed] => llm_ast_whole_processed/18.py.json\n",
      "[Processed] => llm_ast_whole_processed/194.py.json\n",
      "[Processed] => llm_ast_whole_processed/125.py.json\n",
      "[Processed] => llm_ast_whole_processed/192.py.json\n",
      "[Processed] => llm_ast_whole_processed/175.py.json\n",
      "[Processed] => llm_ast_whole_processed/193.py.json\n",
      "[Processed] => llm_ast_whole_processed/167.py.json\n",
      "[Processed] => llm_ast_whole_processed/158.py.json\n",
      "[Processed] => llm_ast_whole_processed/64.py.json\n",
      "[Processed] => llm_ast_whole_processed/172.py.json\n",
      "[Processed] => llm_ast_whole_processed/118.py.json\n",
      "[Processed] => llm_ast_whole_processed/197.py.json\n",
      "[Processed] => llm_ast_whole_processed/10.py.json\n",
      "[Processed] => llm_ast_whole_processed/196.py.json\n",
      "[Processed] => llm_ast_whole_processed/169.py.json\n",
      "[Processed] => llm_ast_whole_processed/107.py.json\n",
      "[Processed] => llm_ast_whole_processed/95.py.json\n",
      "[Processed] => llm_ast_whole_processed/145.py.json\n",
      "[Processed] => llm_ast_whole_processed/102.py.json\n",
      "[Processed] => llm_ast_whole_processed/201.py.json\n",
      "[Processed] => llm_ast_whole_processed/110.py.json\n",
      "[Processed] => llm_ast_whole_processed/171.py.json\n",
      "[Processed] => llm_ast_whole_processed/112.py.json\n",
      "[Processed] => llm_ast_whole_processed/113.py.json\n",
      "[Processed] => llm_ast_whole_processed/202.py.json\n",
      "[Processed] => llm_ast_whole_processed/138.py.json\n",
      "[Processed] => llm_ast_whole_processed/35.py.json\n",
      "[Processed] => llm_ast_whole_processed/164.py.json\n",
      "[Processed] => llm_ast_whole_processed/53.py.json\n",
      "[Processed] => llm_ast_whole_processed/81.py.json\n",
      "[Processed] => llm_ast_whole_processed/96.py.json\n",
      "[Processed] => llm_ast_whole_processed/122.py.json\n",
      "[Processed] => llm_ast_whole_processed/147.py.json\n",
      "[Processed] => llm_ast_whole_processed/205.py.json\n",
      "[Processed] => llm_ast_whole_processed/47.py.json\n",
      "[Processed] => llm_ast_whole_processed/59.py.json\n",
      "[Processed] => llm_ast_whole_processed/87.py.json\n",
      "[Processed] => llm_ast_whole_processed/135.py.json\n",
      "[Processed] => llm_ast_whole_processed/30.py.json\n",
      "[Processed] => llm_ast_whole_processed/83.py.json\n",
      "[Processed] => llm_ast_whole_processed/163.py.json\n",
      "[Processed] => llm_ast_whole_processed/2.py.json\n",
      "[Processed] => llm_ast_whole_processed/159.py.json\n",
      "[Processed] => llm_ast_whole_processed/200.py.json\n",
      "[Processed] => llm_ast_whole_processed/173.py.json\n",
      "[Processed] => llm_ast_whole_processed/16.py.json\n",
      "[Processed] => llm_ast_whole_processed/156.py.json\n",
      "[Processed] => llm_ast_whole_processed/28.py.json\n",
      "[Processed] => llm_ast_whole_processed/103.py.json\n",
      "[Processed] => llm_ast_whole_processed/126.py.json\n",
      "[Processed] => llm_ast_whole_processed/129.py.json\n",
      "[Processed] => llm_ast_whole_processed/37.py.json\n",
      "[Processed] => llm_ast_whole_processed/27.py.json\n",
      "[Processed] => llm_ast_whole_processed/89.py.json\n",
      "[Processed] => llm_ast_whole_processed/36.py.json\n",
      "[Processed] => llm_ast_whole_processed/94.py.json\n",
      "[Processed] => llm_ast_whole_processed/170.py.json\n",
      "[Processed] => llm_ast_whole_processed/209.py.json\n",
      "[Processed] => llm_ast_whole_processed/75.py.json\n",
      "[Processed] => llm_ast_whole_processed/148.py.json\n",
      "[Processed] => llm_ast_whole_processed/44.py.json\n",
      "[Processed] => llm_ast_whole_processed/43.py.json\n",
      "[Processed] => llm_ast_whole_processed/98.py.json\n",
      "[Processed] => llm_ast_whole_processed/29.py.json\n",
      "[Processed] => llm_ast_whole_processed/117.py.json\n",
      "[Processed] => llm_ast_whole_processed/116.py.json\n",
      "[Processed] => llm_ast_whole_processed/104.py.json\n",
      "[Processed] => llm_ast_whole_processed/111.py.json\n",
      "[Processed] => llm_ast_whole_processed/109.py.json\n",
      "[Processed] => llm_ast_whole_processed/155.py.json\n",
      "[Processed] => llm_ast_whole_processed/8.py.json\n",
      "[Processed] => llm_ast_whole_processed/77.py.json\n",
      "[Processed] => llm_ast_whole_processed/3.py.json\n",
      "[Processed] => llm_ast_whole_processed/42.py.json\n",
      "[Processed] => llm_ast_whole_processed/180.py.json\n",
      "[Processed] => llm_ast_whole_processed/133.py.json\n",
      "[Processed] => llm_ast_whole_processed/24.py.json\n",
      "[Processed] => llm_ast_whole_processed/23.py.json\n",
      "[Processed] => llm_ast_whole_processed/198.py.json\n",
      "[Processed] => llm_ast_whole_processed/68.py.json\n",
      "[Processed] => llm_ast_whole_processed/86.py.json\n",
      "[Processed] => llm_ast_whole_processed/136.py.json\n",
      "[Processed] => llm_ast_whole_processed/115.py.json\n",
      "[Processed] => llm_ast_whole_processed/72.py.json\n",
      "[Processed] => llm_ast_whole_processed/17.py.json\n",
      "[Processed] => llm_ast_whole_processed/51.py.json\n",
      "[Processed] => llm_ast_whole_processed/61.py.json\n",
      "[Processed] => llm_ast_whole_processed/14.py.json\n",
      "[Processed] => llm_ast_whole_processed/152.py.json\n",
      "[Processed] => llm_ast_whole_processed/82.py.json\n",
      "[Processed] => llm_ast_whole_processed/130.py.json\n",
      "[Processed] => llm_ast_whole_processed/149.py.json\n",
      "[Processed] => llm_ast_whole_processed/141.py.json\n",
      "[Processed] => llm_ast_whole_processed/5.py.json\n",
      "[Processed] => llm_ast_whole_processed/62.py.json\n",
      "[Processed] => llm_ast_whole_processed/191.py.json\n",
      "[Processed] => llm_ast_whole_processed/74.py.json\n",
      "[Processed] => llm_ast_whole_processed/160.py.json\n",
      "[Processed] => llm_ast_whole_processed/186.py.json\n",
      "[Processed] => llm_ast_whole_processed/34.py.json\n",
      "[Processed] => llm_ast_whole_processed/79.py.json\n",
      "[Processed] => llm_ast_whole_processed/56.py.json\n",
      "[Processed] => llm_ast_whole_processed/176.py.json\n",
      "[Processed] => llm_ast_whole_processed/181.py.json\n",
      "[Processed] => llm_ast_whole_processed/46.py.json\n",
      "[Processed] => llm_ast_whole_processed/185.py.json\n",
      "[Processed] => llm_ast_whole_processed/66.py.json\n",
      "[Processed] => llm_ast_whole_processed/106.py.json\n",
      "[Processed] => llm_ast_whole_processed/151.py.json\n",
      "[Processed] => llm_ast_whole_processed/39.py.json\n",
      "[Processed] => llm_ast_whole_processed/73.py.json\n",
      "[Processed] => llm_ast_whole_processed/93.py.json\n",
      "[Processed] => llm_ast_whole_processed/199.py.json\n",
      "[Processed] => llm_ast_whole_processed/154.py.json\n",
      "[Processed] => llm_ast_whole_processed/128.py.json\n",
      "[Processed] => llm_ast_whole_processed/20.py.json\n",
      "[Processed] => llm_ast_whole_processed/71.py.json\n",
      "[Processed] => llm_ast_whole_processed/182.py.json\n",
      "[Processed] => llm_ast_whole_processed/67.py.json\n",
      "[Processed] => llm_ast_whole_processed/49.py.json\n",
      "[Processed] => llm_ast_whole_processed/157.py.json\n",
      "[Processed] => llm_ast_whole_processed/204.py.json\n",
      "[Processed] => llm_ast_whole_processed/187.py.json\n",
      "[Processed] => llm_ast_whole_processed/132.py.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#                   1) 全局分词，含行号 -> global_tokens                       #\n",
    "###############################################################################\n",
    "def tokenize_code_with_lines(code: str) -> List[Tuple[int, int, str, int]]:\n",
    "    \"\"\"\n",
    "    使用正则分词，直接在原始代码上通过 finditer() 获取匹配位置，\n",
    "    并保留换行符，让 LLM 能识别多行结构。\n",
    "    返回形式: [(start_offset, end_offset, token_text, line_number), ...]\n",
    "    \"\"\"\n",
    "    token_pattern = (\n",
    "        r'[A-Za-z_]\\w*|'  # 标识符\n",
    "        r'[0-9]+|'        # 数字\n",
    "        r'\"[^\"]*\"|'       # 双引号字符串\n",
    "        r\"'[^']*'|\"       # 单引号字符串\n",
    "        r'\\\\[ntr]|'       # 转义符 \\n \\t \\r\n",
    "        r'//.*|'          # 单行注释 (如 C++/Java/JS 风格)\n",
    "        r'/\\*.*?\\*/|'     # 多行注释 (如 C 风格)\n",
    "        r'\\n|\\r|\\t|'      # 换行/回车/制表符\n",
    "        r'\\S'             # 其他符号(如 +, -, {, }, 以及任何其它非空白字符)\n",
    "    )\n",
    "\n",
    "    tokens_with_offset = []\n",
    "    lines = code.splitlines(keepends=True)\n",
    "    current_line = 1\n",
    "    current_pos = 0\n",
    "    \n",
    "    for match in re.finditer(token_pattern, code, re.MULTILINE | re.DOTALL):\n",
    "        tk = match.group(0)\n",
    "        start_offset, end_offset = match.span()\n",
    "        \n",
    "        # 计算当前token所在行号\n",
    "        while current_line <= len(lines) and current_pos + len(lines[current_line-1]) <= start_offset:\n",
    "            current_pos += len(lines[current_line-1])\n",
    "            current_line += 1\n",
    "            \n",
    "        tokens_with_offset.append((start_offset, end_offset, tk, current_line))\n",
    "\n",
    "    return tokens_with_offset\n",
    "\n",
    "###############################################################################\n",
    "# 1) 根据全局 tokens 填充 label\n",
    "###############################################################################\n",
    "def fill_ast_labels(ast_node: dict, code: str, global_tokens: List[Tuple[int,int,str,int]]) -> None:\n",
    "    \"\"\"\n",
    "    把节点的 (start_token, end_token) 当作【token下标】，\n",
    "    去 global_tokens 里拿对应的字符 offset，再到 code 中截取。\n",
    "    存到 ast_node[\"label\"]。\n",
    "    \"\"\"\n",
    "    # 检查当前节点是否为字典类型\n",
    "    if not isinstance(ast_node, dict):\n",
    "        return\n",
    "    \n",
    "    st = ast_node.get(\"start_token\", -1)\n",
    "    et = ast_node.get(\"end_token\", -1)\n",
    "\n",
    "    snippet = \"\"\n",
    "    if (\n",
    "        0 <= st <= et\n",
    "        and st < len(global_tokens)\n",
    "        and et < len(global_tokens)\n",
    "    ):\n",
    "        start_offset = global_tokens[st][0]\n",
    "        end_offset   = global_tokens[et][1]\n",
    "        if 0 <= start_offset < end_offset <= len(code):\n",
    "            snippet = code[start_offset:end_offset]\n",
    "\n",
    "    ast_node[\"label\"] = snippet\n",
    "\n",
    "    for child in ast_node.get(\"children\", []):\n",
    "        # 确保子节点是字典类型再递归处理\n",
    "        if isinstance(child, dict):\n",
    "            fill_ast_labels(child, code, global_tokens)\n",
    "        else:\n",
    "            print(f\"Warning: Invalid child node type {type(child)} in {ast_node.get('type')}\")\n",
    "\n",
    "\n",
    "def safe_flatten_function_placeholders(node: dict) -> dict:\n",
    "    \"\"\"\n",
    "    新建节点，避免循环引用。\n",
    "    处理前检查节点是否为字典类型。\n",
    "    \"\"\"\n",
    "    # 处理非字典节点（如意外类型）\n",
    "    if not isinstance(node, dict):\n",
    "        return {}\n",
    "    \n",
    "    node_type = node.get(\"type\", \"\")\n",
    "    original_children = node.get(\"children\", [])\n",
    "\n",
    "    # 递归处理子节点，仅处理字典类型\n",
    "    flattened_children = []\n",
    "    for ch in original_children:\n",
    "        if isinstance(ch, dict):\n",
    "            processed = safe_flatten_function_placeholders(ch)\n",
    "            flattened_children.append(processed)\n",
    "        else:\n",
    "            print(f\"Warning: Skipping invalid child type {type(ch)} in {node_type}\")\n",
    "    \n",
    "    # 构建新节点\n",
    "    new_node = {key: val for key, val in node.items() if key != \"children\"}\n",
    "    new_node[\"children\"] = flattened_children\n",
    "\n",
    "    # 检查占位符结构\n",
    "    if node_type in (\"function_placeholder\", \"class_placeholder\"):\n",
    "        if len(flattened_children) == 1 and flattened_children[0].get(\"type\") == \"module\":\n",
    "            mod_node = flattened_children[0]\n",
    "            mod_kids = mod_node.get(\"children\", [])\n",
    "            if len(mod_kids) == 1:\n",
    "                real_node = mod_kids[0]\n",
    "                # 复制字段\n",
    "                for field in (\"name\", \"start_line\", \"end_line\", \"start_token\", \"end_token\", \"label\"):\n",
    "                    if field in new_node:\n",
    "                        real_node[field] = new_node[field]\n",
    "                return real_node\n",
    "\n",
    "    return new_node\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) 单文件处理 => 根据同名py文件+json => 生成 global_tokens => 填label => 扁平化\n",
    "###############################################################################\n",
    "def process_ast_json(\n",
    "    input_json_path: str,\n",
    "    output_json_path: str,\n",
    "    py_source_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    预期:\n",
    "      input_json_path = \"llm_ast/chunk_block/1.py.json\"\n",
    "      -> 对应 py_file = \"py_source_dir/1.py\"\n",
    "\n",
    "    假设 JSON 结构如下:\n",
    "    {\n",
    "      \"type\": \"module\",\n",
    "      \"start_token\": 0,\n",
    "      \"end_token\": 307,\n",
    "      ...\n",
    "      \"children\": [...]\n",
    "    }\n",
    "    或者更复杂, 但只要 \"type\"、\"start_token\"/\"end_token\"、\"children\" 就可以\n",
    "\n",
    "    We'll:\n",
    "      1) 找到同名的 .py => 读 code\n",
    "      2) tokenize_code_with_lines(code) => global_tokens\n",
    "      3) fill_ast_labels(ast_root, code, global_tokens)\n",
    "      4) safe_flatten_function_placeholders(ast_root)\n",
    "      5) json.dump()\n",
    "    \"\"\"\n",
    "    base = os.path.basename(input_json_path)  # \"1.py.json\"\n",
    "    # 拆分 => \"1.py\" + \".json\"\n",
    "    # 如果你命名方式不同, 需自行改\n",
    "    # 这里假设 input_json_path 的文件名是 \"<something>.py.json\"\n",
    "    # => python_source = \"<something>.py\"\n",
    "    if base.endswith(\".py.json\"):\n",
    "        py_file_name = base[:-5]  # remove \".json\"\n",
    "    else:\n",
    "        # fallback\n",
    "        py_file_name = base\n",
    "\n",
    "    py_full_path = os.path.join(py_source_dir, py_file_name)\n",
    "\n",
    "    if not os.path.isfile(py_full_path):\n",
    "        print(f\"[Warning] No corresponding .py found for {input_json_path}, skip label fill.\")\n",
    "        code = \"\"\n",
    "        global_tokens = []\n",
    "    else:\n",
    "        # 读取 .py 源码\n",
    "        with open(py_full_path, \"r\", encoding=\"utf-8\") as fpy:\n",
    "            code = fpy.read()\n",
    "        # 分词\n",
    "        global_tokens = tokenize_code_with_lines(code)\n",
    "\n",
    "    # 读取 JSON AST\n",
    "    try:\n",
    "        with open(input_json_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "            ast_data = json.load(fin)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error reading {input_json_path}]: {e}\")\n",
    "        return\n",
    "\n",
    "    # fill label\n",
    "    fill_ast_labels(ast_data, code, global_tokens)\n",
    "\n",
    "    # flatten\n",
    "    flattened_ast = safe_flatten_function_placeholders(ast_data)\n",
    "\n",
    "    # 写出\n",
    "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(flattened_ast, fout, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[Processed] => {output_json_path}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4) main: 遍历 input_json_dir => 对应 .py => output\n",
    "###############################################################################\n",
    "def main():\n",
    "    input_json_dir = \"llm_ast_whole\"              # 你的 AST json 目录\n",
    "    output_json_dir = \"llm_ast_whole_processed\"   # 输出目录\n",
    "    py_source_dir = \"../dataset/python\"              # 对应的 .py 文件目录\n",
    "\n",
    "    if not os.path.isdir(input_json_dir):\n",
    "        print(f\"[Error] input dir {input_json_dir} not found.\")\n",
    "        return\n",
    "    if not os.path.isdir(py_source_dir):\n",
    "        print(f\"[Warning] python source dir {py_source_dir} not found. Label fill will be empty.\")\n",
    "\n",
    "    os.makedirs(output_json_dir, exist_ok=True)\n",
    "\n",
    "    # 遍历\n",
    "    for fname in os.listdir(input_json_dir):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        in_path = os.path.join(input_json_dir, fname)\n",
    "        out_path = os.path.join(output_json_dir, fname)\n",
    "\n",
    "        process_ast_json(in_path, out_path, py_source_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 按行数分类的覆盖率统计 ===\n",
      "\n",
      "Category: 0-50 (0~50 行), file_count=53\n",
      "[NoParent-All] static=5769, llm=3946, matched=2808 => Cov(st)=48.67%, Cov(llm)=71.16%\n",
      "[NoParent-Leaf] static=3077, llm=2141, matched=1492 => Cov(st)=48.49%, Cov(llm)=69.69%\n",
      "[FlexParent-All] static=5769, llm=3946, matched=2357 => Cov(st)=40.86%, Cov(llm)=59.73%\n",
      "[FlexParent-Leaf] static=3077, llm=2141, matched=1344 => Cov(st)=43.68%, Cov(llm)=62.77%\n",
      "\n",
      "Category: 51-200 (51~200 行), file_count=109\n",
      "[NoParent-All] static=39860, llm=9368, matched=5545 => Cov(st)=13.91%, Cov(llm)=59.19%\n",
      "[NoParent-Leaf] static=21757, llm=5185, matched=2338 => Cov(st)=10.75%, Cov(llm)=45.09%\n",
      "[FlexParent-All] static=39860, llm=9368, matched=4003 => Cov(st)=10.04%, Cov(llm)=42.73%\n",
      "[FlexParent-Leaf] static=21757, llm=5185, matched=1904 => Cov(st)=8.75%, Cov(llm)=36.72%\n",
      "\n",
      "Category: 201-9999999 (201~9999999 行), file_count=38\n",
      "[NoParent-All] static=50143, llm=3542, matched=1991 => Cov(st)=3.97%, Cov(llm)=56.21%\n",
      "[NoParent-Leaf] static=27422, llm=2300, matched=620 => Cov(st)=2.26%, Cov(llm)=26.96%\n",
      "[FlexParent-All] static=50143, llm=3542, matched=1090 => Cov(st)=2.17%, Cov(llm)=30.77%\n",
      "[FlexParent-Leaf] static=27422, llm=2300, matched=427 => Cov(st)=1.56%, Cov(llm)=18.57%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "###############################################################################\n",
    "# 覆盖率比较逻辑 (与你已有的保持一致)\n",
    "###############################################################################\n",
    "def collect_node_ranges(ast_node: dict, only_leaves: bool=False) -> set:\n",
    "    result = set()\n",
    "    st = ast_node.get(\"start_token\", -1)\n",
    "    et = ast_node.get(\"end_token\", -1)\n",
    "    children = ast_node.get(\"children\", [])\n",
    "    is_leaf = (len(children) == 0)\n",
    "\n",
    "    if (not only_leaves) or (only_leaves and is_leaf):\n",
    "        if st >= 0 and et >= 0:\n",
    "            result.add((st, et))\n",
    "\n",
    "    for child in children:\n",
    "        result.update(collect_node_ranges(child, only_leaves))\n",
    "    return result\n",
    "\n",
    "def compare_ast_just_tokenRanges(ast_static: dict, ast_llm: dict, only_leaves: bool=False):\n",
    "    \"\"\"\n",
    "    不考虑父节点, 只看 (start_token, end_token) 一致即可。\n",
    "    可以只统计叶子(only_leaves=True)或全部节点。\n",
    "    \"\"\"\n",
    "    static_set = collect_node_ranges(ast_static, only_leaves)\n",
    "    llm_set = collect_node_ranges(ast_llm, only_leaves)\n",
    "    inter = static_set.intersection(llm_set)\n",
    "\n",
    "    total_static = len(static_set)\n",
    "    total_llm = len(llm_set)\n",
    "    matched = len(inter)\n",
    "\n",
    "    cov_static = (matched / total_static) if total_static else 0.0\n",
    "    cov_llm = (matched / total_llm) if total_llm else 0.0\n",
    "\n",
    "    return {\n",
    "        \"total_static\": total_static,\n",
    "        \"total_llm\": total_llm,\n",
    "        \"matched\": matched,\n",
    "        \"coverage_static\": cov_static,\n",
    "        \"coverage_llm\": cov_llm\n",
    "    }\n",
    "\n",
    "def collect_node_ancestors(ast_node: dict, ancestors: list, only_leaves: bool=False) -> dict:\n",
    "    results = {}\n",
    "    st = ast_node.get(\"start_token\", -1)\n",
    "    et = ast_node.get(\"end_token\", -1)\n",
    "    children = ast_node.get(\"children\", [])\n",
    "    node_is_leaf = (len(children)==0)\n",
    "\n",
    "    if (not only_leaves) or (only_leaves and node_is_leaf):\n",
    "        if st>=0 and et>=0:\n",
    "            results[(st, et)] = {\n",
    "                \"ancestors\": set(ancestors),\n",
    "                \"is_leaf\": node_is_leaf\n",
    "            }\n",
    "\n",
    "    new_ancestors = ancestors[:]\n",
    "    if st>=0 and et>=0:\n",
    "        new_ancestors.append((st,et))\n",
    "\n",
    "    for child in children:\n",
    "        submap = collect_node_ancestors(child, new_ancestors, only_leaves)\n",
    "        for k,v in submap.items():\n",
    "            results[k] = v\n",
    "\n",
    "    return results\n",
    "\n",
    "def build_static_parent_map(ast_node: dict, parent: dict, store: dict, only_leaves: bool=False):\n",
    "    st = ast_node.get(\"start_token\",-1)\n",
    "    et = ast_node.get(\"end_token\",-1)\n",
    "    children = ast_node.get(\"children\",[])\n",
    "    node_is_leaf = (len(children)==0)\n",
    "\n",
    "    if parent:\n",
    "        pst = parent.get(\"start_token\",-1)\n",
    "        pet = parent.get(\"end_token\",-1)\n",
    "    else:\n",
    "        pst, pet = -1, -1\n",
    "\n",
    "    if (not only_leaves) or (only_leaves and node_is_leaf):\n",
    "        if st>=0 and et>=0:\n",
    "            store[(st,et)] = (pst,pet)\n",
    "\n",
    "    for ch in children:\n",
    "        build_static_parent_map(ch, ast_node, store, only_leaves)\n",
    "\n",
    "def compare_ast_flexible_parent(ast_static: dict, ast_llm: dict, only_leaves: bool=False):\n",
    "    static_map = {}\n",
    "    build_static_parent_map(ast_static, None, static_map, only_leaves)\n",
    "    llm_map = collect_node_ancestors(ast_llm, [], only_leaves)\n",
    "\n",
    "    matched = 0\n",
    "    total_static = len(static_map)\n",
    "    total_llm = len(llm_map)\n",
    "\n",
    "    for (node_st,node_et), (pst,pet) in static_map.items():\n",
    "        if (node_st,node_et) in llm_map:\n",
    "            ancset = llm_map[(node_st,node_et)][\"ancestors\"]\n",
    "            if (pst,pet)==(-1,-1):\n",
    "                matched += 1\n",
    "            else:\n",
    "                # 对每个祖先节点范围进行宽松匹配\n",
    "                for anc_st,anc_et in ancset:\n",
    "                    # 允许父节点范围有小幅度偏差(比如注释导致的token偏移)\n",
    "                    if abs(anc_st - pst) <= 3 and abs(anc_et - pet) <= 3:\n",
    "                        matched += 1\n",
    "                        break\n",
    "\n",
    "    cov_static = (matched / total_static) if total_static>0 else 0\n",
    "    cov_llm = (matched / total_llm) if total_llm>0 else 0\n",
    "\n",
    "    return {\n",
    "        \"total_static\": total_static,\n",
    "        \"total_llm\": total_llm,\n",
    "        \"matched\": matched,\n",
    "        \"coverage_static\": cov_static,\n",
    "        \"coverage_llm\": cov_llm\n",
    "    }\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) main: 行数分类 + 覆盖率统计\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Python 源文件目录\n",
    "    source_dir = \"../dataset/python\"\n",
    "    # LLM AST 目录\n",
    "    llm_json_dir = \"./llm_ast_whole_processed\"\n",
    "    # 静态 AST 目录\n",
    "    static_json_dir = \"../dataset/python_ast\"\n",
    "\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"[Error] Source dir not found: {source_dir}\")\n",
    "        return\n",
    "\n",
    "    # 定义行数范围\n",
    "    categories = [\n",
    "        (\"0-50\", 0, 50),\n",
    "        (\"51-200\", 51, 200),\n",
    "        (\"201-9999999\", 201, 9999999)\n",
    "    ]\n",
    "\n",
    "    # 为每个分类，存储 4种统计\n",
    "    # np_all, np_leaf, fp_all, fp_leaf\n",
    "    # 每种统计包括 total_static_sum, total_llm_sum, matched_sum, file_count\n",
    "    stats_map = {}\n",
    "    for cat_name, low, high in categories:\n",
    "        stats_map[cat_name] = {\n",
    "            \"file_count\": 0,\n",
    "            # no-parent(all)\n",
    "            \"np_all_static\": 0, \"np_all_llm\":0, \"np_all_match\":0,\n",
    "            # no-parent(leaf)\n",
    "            \"np_leaf_static\": 0, \"np_leaf_llm\":0, \"np_leaf_match\":0,\n",
    "            # flex-parent(all)\n",
    "            \"fp_all_static\": 0, \"fp_all_llm\":0, \"fp_all_match\":0,\n",
    "            # flex-parent(leaf)\n",
    "            \"fp_leaf_static\": 0, \"fp_leaf_llm\":0, \"fp_leaf_match\":0\n",
    "        }\n",
    "\n",
    "    # 遍历 python 源文件 => line_count => cat\n",
    "    for fname in os.listdir(source_dir):\n",
    "        if not fname.endswith(\".py\"):\n",
    "            continue\n",
    "        py_path = os.path.join(source_dir, fname)\n",
    "\n",
    "        # 读取行数\n",
    "        try:\n",
    "            with open(py_path,\"r\",encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "            line_count = len(lines)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error reading {py_path}]: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 判断分类\n",
    "        cat_name = None\n",
    "        for (cname, low, high) in categories:\n",
    "            if low <= line_count <= high:\n",
    "                cat_name = cname\n",
    "                break\n",
    "        if not cat_name:\n",
    "            continue  # 不在任何区间\n",
    "\n",
    "        # 找对应 json => <fname> + \".json\"\n",
    "        # e.g. \"1.py\" => \"1.py.json\"\n",
    "        json_name = fname + \".json\"\n",
    "        llm_file = os.path.join(llm_json_dir, json_name)\n",
    "        static_file = os.path.join(static_json_dir, json_name)\n",
    "\n",
    "        if not os.path.exists(llm_file) or not os.path.exists(static_file):\n",
    "            # print(f\"[Warn] Missing AST for {fname}, skip.\")\n",
    "            continue\n",
    "\n",
    "        # 读取 JSON\n",
    "        try:\n",
    "            with open(llm_file,\"r\",encoding=\"utf-8\") as f1:\n",
    "                ast_llm = json.load(f1)\n",
    "            with open(static_file,\"r\",encoding=\"utf-8\") as f2:\n",
    "                ast_static = json.load(f2)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error reading AST for {fname}]: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 计算4种覆盖率\n",
    "        # no-parent(all)\n",
    "        r_np_all = compare_ast_just_tokenRanges(ast_static, ast_llm, only_leaves=False)\n",
    "        # no-parent(leaf)\n",
    "        r_np_leaf = compare_ast_just_tokenRanges(ast_static, ast_llm, only_leaves=True)\n",
    "        # flex-parent(all)\n",
    "        r_fp_all = compare_ast_flexible_parent(ast_static, ast_llm, only_leaves=False)\n",
    "        # flex-parent(leaf)\n",
    "        r_fp_leaf = compare_ast_flexible_parent(ast_static, ast_llm, only_leaves=True)\n",
    "\n",
    "        # 累加到 stats_map[cat_name]\n",
    "        sdat = stats_map[cat_name]\n",
    "        sdat[\"file_count\"] += 1\n",
    "\n",
    "        # np_all\n",
    "        sdat[\"np_all_static\"] += r_np_all[\"total_static\"]\n",
    "        sdat[\"np_all_llm\"] += r_np_all[\"total_llm\"]\n",
    "        sdat[\"np_all_match\"] += r_np_all[\"matched\"]\n",
    "\n",
    "        # np_leaf\n",
    "        sdat[\"np_leaf_static\"] += r_np_leaf[\"total_static\"]\n",
    "        sdat[\"np_leaf_llm\"] += r_np_leaf[\"total_llm\"]\n",
    "        sdat[\"np_leaf_match\"] += r_np_leaf[\"matched\"]\n",
    "\n",
    "        # fp_all\n",
    "        sdat[\"fp_all_static\"] += r_fp_all[\"total_static\"]\n",
    "        sdat[\"fp_all_llm\"] += r_fp_all[\"total_llm\"]\n",
    "        sdat[\"fp_all_match\"] += r_fp_all[\"matched\"]\n",
    "\n",
    "        # fp_leaf\n",
    "        sdat[\"fp_leaf_static\"] += r_fp_leaf[\"total_static\"]\n",
    "        sdat[\"fp_leaf_llm\"] += r_fp_leaf[\"total_llm\"]\n",
    "        sdat[\"fp_leaf_match\"] += r_fp_leaf[\"matched\"]\n",
    "\n",
    "    # 输出每个分类结果\n",
    "    print(\"\\n=== 按行数分类的覆盖率统计 ===\")\n",
    "    for (cat_name, low, high) in categories:\n",
    "        sdat = stats_map[cat_name]\n",
    "        fcount = sdat[\"file_count\"]\n",
    "        if fcount==0:\n",
    "            print(f\"\\nCategory: {cat_name} ({low}~{high}) => 无文件。\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nCategory: {cat_name} ({low}~{high} 行), file_count={fcount}\")\n",
    "\n",
    "        # np_all\n",
    "        a_s = sdat[\"np_all_static\"]\n",
    "        a_l = sdat[\"np_all_llm\"]\n",
    "        a_m = sdat[\"np_all_match\"]\n",
    "        cov_s = (a_m/a_s)*100 if a_s>0 else 0\n",
    "        cov_l = (a_m/a_l)*100 if a_l>0 else 0\n",
    "        print(\"[NoParent-All] static=%d, llm=%d, matched=%d => Cov(st)=%.2f%%, Cov(llm)=%.2f%%\" % (a_s,a_l,a_m,cov_s,cov_l))\n",
    "\n",
    "        # np_leaf\n",
    "        b_s = sdat[\"np_leaf_static\"]\n",
    "        b_l = sdat[\"np_leaf_llm\"]\n",
    "        b_m = sdat[\"np_leaf_match\"]\n",
    "        cov_s = (b_m/b_s)*100 if b_s>0 else 0\n",
    "        cov_l = (b_m/b_l)*100 if b_l>0 else 0\n",
    "        print(\"[NoParent-Leaf] static=%d, llm=%d, matched=%d => Cov(st)=%.2f%%, Cov(llm)=%.2f%%\" % (b_s,b_l,b_m,cov_s,cov_l))\n",
    "\n",
    "        # fp_all\n",
    "        c_s = sdat[\"fp_all_static\"]\n",
    "        c_l = sdat[\"fp_all_llm\"]\n",
    "        c_m = sdat[\"fp_all_match\"]\n",
    "        cov_s = (c_m/c_s)*100 if c_s>0 else 0\n",
    "        cov_l = (c_m/c_l)*100 if c_l>0 else 0\n",
    "        print(\"[FlexParent-All] static=%d, llm=%d, matched=%d => Cov(st)=%.2f%%, Cov(llm)=%.2f%%\" % (c_s,c_l,c_m,cov_s,cov_l))\n",
    "\n",
    "        # fp_leaf\n",
    "        d_s = sdat[\"fp_leaf_static\"]\n",
    "        d_l = sdat[\"fp_leaf_llm\"]\n",
    "        d_m = sdat[\"fp_leaf_match\"]\n",
    "        cov_s = (d_m/d_s)*100 if d_s>0 else 0\n",
    "        cov_l = (d_m/d_l)*100 if d_l>0 else 0\n",
    "        print(\"[FlexParent-Leaf] static=%d, llm=%d, matched=%d => Cov(st)=%.2f%%, Cov(llm)=%.2f%%\" % (d_s,d_l,d_m,cov_s,cov_l))\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
