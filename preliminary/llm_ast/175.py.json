{
    "type": "ErrorNode",
    "code": "import logging\nimport os\nfrom urllib.parse import parse_qs, urlencode, urlparse, urlunparse\n\nfrom apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED\nfrom apscheduler.job import Job as JobObj\nfrom apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.triggers.cron import CronTrigger\nfrom autogpt_libs.utils.cache import thread_cached\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom sqlalchemy import MetaData, create_engine\n\nfrom backend.data.block import BlockInput\nfrom backend.executor.manager import ExecutionManager\nfrom backend.util.service import AppService, expose, get_service_client\nfrom backend.util.settings import Config\n\n\ndef _extract_schema_from_url(database_url) -> tuple[str, str]:\n    \"\"\"\n    Extracts the schema from the DATABASE_URL and returns the schema and cleaned URL.\n    \"\"\"\n    parsed_url = urlparse(database_url)\n    query_params = parse_qs(parsed_url.query)\n\n    # Extract the 'schema' parameter\n    schema_list = query_params.pop(\"schema\", None)\n    schema = schema_list[0] if schema_list else \"public\"\n\n    # Reconstruct the query string without the 'schema' parameter\n    new_query = urlencode(query_params, doseq=True)\n    new_parsed_url = parsed_url._replace(query=new_query)\n    database_url_clean = str(urlunparse(new_parsed_url))\n\n    return schema, database_url_clean\n\n\nlogger = logging.getLogger(__name__)\nconfig = Config()\n\n\ndef log(msg, **kwargs):\n    logger.info(\"[ExecutionScheduler] \" + msg, **kwargs)\n\n\ndef job_listener(event):\n    \"\"\"Logs job execution outcomes for better monitoring.\"\"\"\n    if event.exception:\n        log(f\"Job {event.job_id} failed.\")\n    else:\n        log(f\"Job {event.job_id} completed successfully.\")\n\n\n@thread_cached\ndef get_execution_client() -> ExecutionManager:\n    return get_service_client(ExecutionManager)\n\n\ndef execute_graph(**kwargs):\n    args = JobArgs(**kwargs)\n    try:\n        log(f\"Executing recurring job for graph #{args.graph_id}\")\n        get_execution_client().add_execution(\n            args.graph_id, args.input_data, args.user_id\n        )\n    except Exception as e:\n        logger.exception(f\"Error executing graph {args.graph_id}: {e}\")\n\n\nclass JobArgs(BaseModel):\n    graph_id: str\n    input_data: BlockInput\n    user_id: str\n    graph_version: int\n    cron: str\n\n\nclass JobInfo(JobArgs):\n    id: str\n    name: str\n    next_run_time: str\n\n    @staticmethod\n    def from_db(job_args: JobArgs, job_obj: JobObj) -> \"JobInfo\":\n        return JobInfo(\n            id=job_obj.id,\n            name=job_obj.name,\n            next_run_time=job_obj.next_run_time.isoformat(),\n            **job_args.model_dump(),\n        )\n\n\nclass ExecutionScheduler(AppService):\n    scheduler: BlockingScheduler\n\n    @classmethod\n    def get_port(cls) -> int:\n        return config.execution_scheduler_port\n\n    @property\n    @thread_cached\n    def execution_client(self) -> ExecutionManager:\n        return get_service_client(ExecutionManager)\n\n    def run_service(self):\n        load_dotenv()\n        db_schema, db_url = _extract_schema_from_url(os.getenv(\"DATABASE_URL\"))\n        self.scheduler = BlockingScheduler(\n            jobstores={\n                \"default\": SQLAlchemyJobStore(\n                    engine=create_engine(db_url),\n                    metadata=MetaData(schema=db_schema),\n                )\n            }\n        )\n        self.scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)\n        self.scheduler.start()\n\n    @expose\n    def add_execution_schedule(\n        self,\n        graph_id: str,\n        graph_version: int,\n        cron: str,\n        input_data: BlockInput,\n        user_id: str,\n    ) -> JobInfo:\n        job_args = JobArgs(\n            graph_id=graph_id,\n            input_data=input_data,\n            user_id=user_id,\n            graph_version=graph_version,\n            cron=cron,\n        )\n        job = self.scheduler.add_job(\n            execute_graph,\n            CronTrigger.from_crontab(cron),\n            kwargs=job_args.model_dump(),\n            replace_existing=True,\n        )\n        log(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")\n        return JobInfo.from_db(job_args, job)\n\n    @expose\n    def delete_schedule(self, schedule_id: str, user_id: str) -> JobInfo:\n        job = self.scheduler.get_job(schedule_id)\n        if not job:\n            log(f\"Job {schedule_id} not found.\")\n            raise ValueError(f\"Job #{schedule_id} not found.\")\n\n        job_args = JobArgs(**job.kwargs)\n        if job_args.user_id != user_id:\n            raise ValueError(\"User ID does not match the job's user ID.\")\n\n        log(f\"Deleting job {schedule_id}\")\n        job.remove()\n\n        return JobInfo.from_db(job_args, job)\n\n    @expose\n    def get_execution_schedules(\n        self, graph_id: str | None = None, user_id: str | None = None\n    ) -> list[JobInfo]:\n        schedules = []\n        for job in self.scheduler.get_jobs():\n            job_args = JobArgs(**job.kwargs)\n            if (\n                job.next_run_time is not None\n                and (graph_id is None or job_args.graph_id == graph_id)\n                and (user_id is None or job_args.user_id == user_id)\n            ):\n                schedules.append(JobInfo.from_db(job_args, job))\n        return schedules\n",
    "children": []
}