{
    "type": "ErrorNode",
    "code": "import random\nfrom collections import defaultdict\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom backend.data.block import Block, BlockCategory, BlockOutput, BlockSchema\nfrom backend.data.model import SchemaField\n\n\nclass SamplingMethod(str, Enum):\n    RANDOM = \"random\"\n    SYSTEMATIC = \"systematic\"\n    TOP = \"top\"\n    BOTTOM = \"bottom\"\n    STRATIFIED = \"stratified\"\n    WEIGHTED = \"weighted\"\n    RESERVOIR = \"reservoir\"\n    CLUSTER = \"cluster\"\n\n\nclass DataSamplingBlock(Block):\n    class Input(BlockSchema):\n        data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(\n            description=\"The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists.\",\n            placeholder=\"{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]\",\n        )\n        sample_size: int = SchemaField(\n            description=\"The number of samples to take from the dataset.\",\n            placeholder=\"10\",\n            default=10,\n        )\n        sampling_method: SamplingMethod = SchemaField(\n            description=\"The method to use for sampling.\",\n            default=SamplingMethod.RANDOM,\n        )\n        accumulate: bool = SchemaField(\n            description=\"Whether to accumulate data before sampling.\",\n            default=False,\n        )\n        random_seed: Optional[int] = SchemaField(\n            description=\"Seed for random number generator (optional).\",\n            default=None,\n        )\n        stratify_key: Optional[str] = SchemaField(\n            description=\"Key to use for stratified sampling (required for stratified sampling).\",\n            default=None,\n        )\n        weight_key: Optional[str] = SchemaField(\n            description=\"Key to use for weighted sampling (required for weighted sampling).\",\n            default=None,\n        )\n        cluster_key: Optional[str] = SchemaField(\n            description=\"Key to use for cluster sampling (required for cluster sampling).\",\n            default=None,\n        )\n\n    class Output(BlockSchema):\n        sampled_data: List[Union[dict, List[Any]]] = SchemaField(\n            description=\"The sampled subset of the input data.\"\n        )\n        sample_indices: List[int] = SchemaField(\n            description=\"The indices of the sampled data in the original dataset.\"\n        )\n\n    def __init__(self):\n        super().__init__(\n            id=\"4a448883-71fa-49cf-91cf-70d793bd7d87\",\n            description=\"This block samples data from a given dataset using various sampling methods.\",\n            categories={BlockCategory.LOGIC},\n            input_schema=DataSamplingBlock.Input,\n            output_schema=DataSamplingBlock.Output,\n            test_input={\n                \"data\": [\n                    {\"id\": i, \"value\": chr(97 + i), \"group\": i % 3} for i in range(10)\n                ],\n                \"sample_size\": 3,\n                \"sampling_method\": SamplingMethod.STRATIFIED,\n                \"accumulate\": False,\n                \"random_seed\": 42,\n                \"stratify_key\": \"group\",\n            },\n            test_output=[\n                (\n                    \"sampled_data\",\n                    [\n                        {\"id\": 0, \"value\": \"a\", \"group\": 0},\n                        {\"id\": 1, \"value\": \"b\", \"group\": 1},\n                        {\"id\": 8, \"value\": \"i\", \"group\": 2},\n                    ],\n                ),\n                (\"sample_indices\", [0, 1, 8]),\n            ],\n        )\n        self.accumulated_data = []\n\n    def run(self, input_data: Input, **kwargs) -> BlockOutput:\n        if input_data.accumulate:\n            if isinstance(input_data.data, dict):\n                self.accumulated_data.append(input_data.data)\n            elif isinstance(input_data.data, list):\n                self.accumulated_data.extend(input_data.data)\n            else:\n                raise ValueError(f\"Unsupported data type: {type(input_data.data)}\")\n\n            # If we don't have enough data yet, return without sampling\n            if len(self.accumulated_data) < input_data.sample_size:\n                return\n\n            data_to_sample = self.accumulated_data\n        else:\n            # If not accumulating, use the input data directly\n            data_to_sample = (\n                input_data.data\n                if isinstance(input_data.data, list)\n                else [input_data.data]\n            )\n\n        if input_data.random_seed is not None:\n            random.seed(input_data.random_seed)\n\n        data_size = len(data_to_sample)\n\n        if input_data.sample_size > data_size:\n            raise ValueError(\n                f\"Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).\"\n            )\n\n        indices = []\n\n        if input_data.sampling_method == SamplingMethod.RANDOM:\n            indices = random.sample(range(data_size), input_data.sample_size)\n        elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:\n            step = data_size // input_data.sample_size\n            start = random.randint(0, step - 1)\n            indices = list(range(start, data_size, step))[: input_data.sample_size]\n        elif input_data.sampling_method == SamplingMethod.TOP:\n            indices = list(range(input_data.sample_size))\n        elif input_data.sampling_method == SamplingMethod.BOTTOM:\n            indices = list(range(data_size - input_data.sample_size, data_size))\n        elif input_data.sampling_method == SamplingMethod.STRATIFIED:\n            if not input_data.stratify_key:\n                raise ValueError(\n                    \"Stratify key must be provided for stratified sampling.\"\n                )\n            strata = defaultdict(list)\n            for i, item in enumerate(data_to_sample):\n                if isinstance(item, dict):\n                    strata_value = item.get(input_data.stratify_key)\n                elif hasattr(item, input_data.stratify_key):\n                    strata_value = getattr(item, input_data.stratify_key)\n                else:\n                    raise ValueError(\n                        f\"Stratify key '{input_data.stratify_key}' not found in item {item}\"\n                    )\n\n                if strata_value is None:\n                    raise ValueError(\n                        f\"Stratify value for key '{input_data.stratify_key}' is None\"\n                    )\n\n                strata[str(strata_value)].append(i)\n\n            # Calculate the number of samples to take from each stratum\n            stratum_sizes = {\n                k: max(1, int(len(v) / data_size * input_data.sample_size))\n                for k, v in strata.items()\n            }\n\n            # Adjust sizes to ensure we get exactly sample_size samples\n            while sum(stratum_sizes.values()) != input_data.sample_size:\n                if sum(stratum_sizes.values()) < input_data.sample_size:\n                    stratum_sizes[\n                        max(stratum_sizes, key=lambda k: stratum_sizes[k])\n                    ] += 1\n                else:\n                    stratum_sizes[\n                        max(stratum_sizes, key=lambda k: stratum_sizes[k])\n                    ] -= 1\n\n            for stratum, size in stratum_sizes.items():\n                indices.extend(random.sample(strata[stratum], size))\n        elif input_data.sampling_method == SamplingMethod.WEIGHTED:\n            if not input_data.weight_key:\n                raise ValueError(\"Weight key must be provided for weighted sampling.\")\n            weights = []\n            for item in data_to_sample:\n                if isinstance(item, dict):\n                    weight = item.get(input_data.weight_key)\n                elif hasattr(item, input_data.weight_key):\n                    weight = getattr(item, input_data.weight_key)\n                else:\n                    raise ValueError(\n                        f\"Weight key '{input_data.weight_key}' not found in item {item}\"\n                    )\n\n                if weight is None:\n                    raise ValueError(\n                        f\"Weight value for key '{input_data.weight_key}' is None\"\n                    )\n                try:\n                    weights.append(float(weight))\n                except ValueError:\n                    raise ValueError(\n                        f\"Weight value '{weight}' cannot be converted to a number\"\n                    )\n\n            if not weights:\n                raise ValueError(\n                    f\"No valid weights found using key '{input_data.weight_key}'\"\n                )\n\n            indices = random.choices(\n                range(data_size), weights=weights, k=input_data.sample_size\n            )\n        elif input_data.sampling_method == SamplingMethod.RESERVOIR:\n            indices = list(range(input_data.sample_size))\n            for i in range(input_data.sample_size, data_size):\n                j = random.randint(0, i)\n                if j < input_data.sample_size:\n                    indices[j] = i\n        elif input_data.sampling_method == SamplingMethod.CLUSTER:\n            if not input_data.cluster_key:\n                raise ValueError(\"Cluster key must be provided for cluster sampling.\")\n            clusters = defaultdict(list)\n            for i, item in enumerate(data_to_sample):\n                if isinstance(item, dict):\n                    cluster_value = item.get(input_data.cluster_key)\n                elif hasattr(item, input_data.cluster_key):\n                    cluster_value = getattr(item, input_data.cluster_key)\n                else:\n                    raise TypeError(\n                        f\"Item {item} does not have the cluster key '{input_data.cluster_key}'\"\n                    )\n\n                clusters[str(cluster_value)].append(i)\n\n            # Randomly select clusters until we have enough samples\n            selected_clusters = []\n            while (\n                sum(len(clusters[c]) for c in selected_clusters)\n                < input_data.sample_size\n            ):\n                available_clusters = [c for c in clusters if c not in selected_clusters]\n                if not available_clusters:\n                    break\n                selected_clusters.append(random.choice(available_clusters))\n\n            for cluster in selected_clusters:\n                indices.extend(clusters[cluster])\n\n            # If we have more samples than needed, randomly remove some\n            if len(indices) > input_data.sample_size:\n                indices = random.sample(indices, input_data.sample_size)\n        else:\n            raise ValueError(f\"Unknown sampling method: {input_data.sampling_method}\")\n\n        sampled_data = [data_to_sample[i] for i in indices]\n\n        # Clear accumulated data after sampling if accumulation is enabled\n        if input_data.accumulate:\n            self.accumulated_data = []\n\n        yield \"sampled_data\", sampled_data\n        yield \"sample_indices\", indices\n",
    "children": []
}