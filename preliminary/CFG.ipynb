{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 全流程处理，文件范围: [0, 200]\n",
      "[GenerateCFG] Processing ../dataset/python/0.py\n",
      "[GenerateCFG] Processing ../dataset/python/3.py\n",
      "[GenerateCFG] Processing ../dataset/python/5.py\n",
      "[GenerateCFG] Processing ../dataset/python/8.py\n",
      "[GenerateCFG] Processing ../dataset/python/11.py\n",
      "[GenerateCFG] Processing ../dataset/python/9.py\n",
      "[GenerateCFG] Processing ../dataset/python/14.py\n",
      "[GenerateCFG] Processing ../dataset/python/15.py\n",
      "[GenerateCFG] Processing ../dataset/python/17.py\n",
      "[GenerateCFG] Processing ../dataset/python/18.py\n",
      "[GenerateCFG] Processing ../dataset/python/20.py\n",
      "[GenerateCFG] Processing ../dataset/python/21.py\n",
      "[GenerateCFG] Processing ../dataset/python/23.py\n",
      "[GenerateCFG] Processing ../dataset/python/25.py\n",
      "[GenerateCFG] Processing ../dataset/python/26.py\n",
      "[GenerateCFG] Processing ../dataset/python/28.py\n",
      "[GenerateCFG] Processing ../dataset/python/29.py\n",
      "[GenerateCFG] Processing ../dataset/python/31.py\n",
      "[GenerateCFG] Processing ../dataset/python/33.py\n",
      "[GenerateCFG] Processing ../dataset/python/30.py\n",
      "[GenerateCFG] Processing ../dataset/python/34.py\n",
      "[GenerateCFG] Processing ../dataset/python/35.py\n",
      "[GenerateCFG] Processing ../dataset/python/36.py\n",
      "[GenerateCFG] Processing ../dataset/python/39.py\n",
      "[GenerateCFG] Processing ../dataset/python/40.py\n",
      "[GenerateCFG] Processing ../dataset/python/41.py\n",
      "[GenerateCFG] Processing ../dataset/python/42.py\n",
      "[GenerateCFG] Processing ../dataset/python/43.py\n",
      "[GenerateCFG] Processing ../dataset/python/45.py\n",
      "[GenerateCFG] Processing ../dataset/python/47.py\n",
      "[GenerateCFG] Processing ../dataset/python/48.py\n",
      "[GenerateCFG] Processing ../dataset/python/49.py\n",
      "[GenerateCFG] Processing ../dataset/python/50.py\n",
      "[GenerateCFG] Processing ../dataset/python/52.py\n",
      "[GenerateCFG] Processing ../dataset/python/51.py\n",
      "[GenerateCFG] Processing ../dataset/python/53.py\n",
      "[GenerateCFG] Processing ../dataset/python/54.py\n",
      "[GenerateCFG] Processing ../dataset/python/55.py\n",
      "[GenerateCFG] Processing ../dataset/python/56.py\n",
      "[GenerateCFG] Processing ../dataset/python/57.py\n",
      "[GenerateCFG] Processing ../dataset/python/58.py\n",
      "[GenerateCFG] Processing ../dataset/python/59.py\n",
      "[GenerateCFG] Processing ../dataset/python/60.py\n",
      "[GenerateCFG] Processing ../dataset/python/61.py\n",
      "[GenerateCFG] Processing ../dataset/python/62.py\n",
      "[GenerateCFG] Processing ../dataset/python/64.py\n",
      "[GenerateCFG] Processing ../dataset/python/63.py\n",
      "[GenerateCFG] Processing ../dataset/python/65.py\n",
      "[GenerateCFG] Processing ../dataset/python/66.py\n",
      "[GenerateCFG] Processing ../dataset/python/68.py\n",
      "[GenerateCFG] Processing ../dataset/python/67.py\n",
      "[GenerateCFG] Processing ../dataset/python/69.py\n",
      "[GenerateCFG] Processing ../dataset/python/70.py\n",
      "[GenerateCFG] Processing ../dataset/python/71.py\n",
      "[GenerateCFG] Processing ../dataset/python/73.py\n",
      "[GenerateCFG] Processing ../dataset/python/72.py\n",
      "[GenerateCFG] Processing ../dataset/python/74.py\n",
      "[GenerateCFG] Processing ../dataset/python/75.py\n",
      "[GenerateCFG] Processing ../dataset/python/76.py\n",
      "[GenerateCFG] Processing ../dataset/python/78.py\n",
      "[GenerateCFG] Processing ../dataset/python/77.py\n",
      "[GenerateCFG] Processing ../dataset/python/79.py\n",
      "[GenerateCFG] Processing ../dataset/python/80.py\n",
      "[GenerateCFG] Processing ../dataset/python/81.py\n",
      "[GenerateCFG] Processing ../dataset/python/82.py\n",
      "[GenerateCFG] Processing ../dataset/python/83.py\n",
      "[GenerateCFG] Processing ../dataset/python/84.py\n",
      "[GenerateCFG] Processing ../dataset/python/86.py\n",
      "[GenerateCFG] Processing ../dataset/python/85.py\n",
      "[GenerateCFG] Processing ../dataset/python/88.py\n",
      "[GenerateCFG] Processing ../dataset/python/87.py\n",
      "[GenerateCFG] Processing ../dataset/python/89.py\n",
      "[GenerateCFG] Processing ../dataset/python/92.py\n",
      "[GenerateCFG] Processing ../dataset/python/91.py\n",
      "[GenerateCFG] Processing ../dataset/python/94.py\n",
      "[GenerateCFG] Processing ../dataset/python/93.py\n",
      "[GenerateCFG] Processing ../dataset/python/90.py\n",
      "[GenerateCFG] Processing ../dataset/python/95.py\n",
      "[GenerateCFG] Processing ../dataset/python/96.py\n",
      "[GenerateCFG] Processing ../dataset/python/98.py\n",
      "[GenerateCFG] Processing ../dataset/python/99.py\n",
      "[GenerateCFG] Processing ../dataset/python/100.py\n",
      "[GenerateCFG] Processing ../dataset/python/101.py\n",
      "[GenerateCFG] Processing ../dataset/python/103.py\n",
      "[GenerateCFG] Processing ../dataset/python/102.py\n",
      "[GenerateCFG] Processing ../dataset/python/104.py\n",
      "[GenerateCFG] Processing ../dataset/python/105.py\n",
      "[GenerateCFG] Processing ../dataset/python/106.py\n",
      "[GenerateCFG] Processing ../dataset/python/107.py\n",
      "[GenerateCFG] Processing ../dataset/python/109.py\n",
      "[GenerateCFG] Processing ../dataset/python/108.py\n",
      "[GenerateCFG] Processing ../dataset/python/110.py\n",
      "[GenerateCFG] Processing ../dataset/python/111.py\n",
      "[GenerateCFG] Processing ../dataset/python/112.py\n",
      "[GenerateCFG] Processing ../dataset/python/113.py\n",
      "[GenerateCFG] Processing ../dataset/python/114.py\n",
      "[GenerateCFG] Processing ../dataset/python/116.py\n",
      "[GenerateCFG] Processing ../dataset/python/115.py\n",
      "[GenerateCFG] Processing ../dataset/python/117.py\n",
      "[GenerateCFG] Processing ../dataset/python/118.py\n",
      "[GenerateCFG] Processing ../dataset/python/119.py\n",
      "[GenerateCFG] Processing ../dataset/python/120.py\n",
      "[GenerateCFG] Processing ../dataset/python/121.py\n",
      "[GenerateCFG] Processing ../dataset/python/122.py\n",
      "[GenerateCFG] Processing ../dataset/python/123.py\n",
      "[GenerateCFG] Processing ../dataset/python/124.py\n",
      "[GenerateCFG] Processing ../dataset/python/125.py\n",
      "[GenerateCFG] Processing ../dataset/python/126.py\n",
      "[GenerateCFG] Processing ../dataset/python/127.py\n",
      "[GenerateCFG] Processing ../dataset/python/128.py\n",
      "[GenerateCFG] Processing ../dataset/python/130.py\n",
      "[GenerateCFG] Processing ../dataset/python/129.py\n",
      "[GenerateCFG] Processing ../dataset/python/131.py\n",
      "[GenerateCFG] Processing ../dataset/python/132.py\n",
      "[GenerateCFG] Processing ../dataset/python/133.py\n",
      "[GenerateCFG] Processing ../dataset/python/134.py\n",
      "[GenerateCFG] Processing ../dataset/python/135.py\n",
      "[GenerateCFG] Processing ../dataset/python/136.py\n",
      "[GenerateCFG] Processing ../dataset/python/137.py\n",
      "[GenerateCFG] Processing ../dataset/python/138.py\n",
      "[GenerateCFG] Processing ../dataset/python/139.py\n",
      "[GenerateCFG] Processing ../dataset/python/140.py\n",
      "[GenerateCFG] Processing ../dataset/python/141.py\n",
      "[GenerateCFG] Processing ../dataset/python/142.py\n",
      "[GenerateCFG] Processing ../dataset/python/143.py\n",
      "[GenerateCFG] Processing ../dataset/python/144.py\n",
      "[GenerateCFG] Processing ../dataset/python/145.py\n",
      "[GenerateCFG] Processing ../dataset/python/146.py\n",
      "[GenerateCFG] Processing ../dataset/python/147.py\n",
      "[GenerateCFG] Processing ../dataset/python/148.py\n",
      "[GenerateCFG] Processing ../dataset/python/149.py\n",
      "[GenerateCFG] Processing ../dataset/python/150.py\n",
      "[GenerateCFG] Processing ../dataset/python/151.py\n",
      "[GenerateCFG] Processing ../dataset/python/152.py\n",
      "[GenerateCFG] Processing ../dataset/python/153.py\n",
      "[GenerateCFG] Processing ../dataset/python/154.py\n",
      "[GenerateCFG] Processing ../dataset/python/155.py\n",
      "[GenerateCFG] Processing ../dataset/python/156.py\n",
      "[GenerateCFG] Processing ../dataset/python/157.py\n",
      "[GenerateCFG] Processing ../dataset/python/158.py\n",
      "[GenerateCFG] Processing ../dataset/python/159.py\n",
      "[GenerateCFG] Processing ../dataset/python/160.py\n",
      "[GenerateCFG] Processing ../dataset/python/161.py\n",
      "[GenerateCFG] Processing ../dataset/python/162.py\n",
      "[GenerateCFG] Processing ../dataset/python/163.py\n",
      "[GenerateCFG] Processing ../dataset/python/164.py\n",
      "[GenerateCFG] Processing ../dataset/python/165.py\n",
      "[GenerateCFG] Processing ../dataset/python/166.py\n",
      "[GenerateCFG] Processing ../dataset/python/167.py\n",
      "[GenerateCFG] Processing ../dataset/python/168.py\n",
      "[GenerateCFG] Processing ../dataset/python/169.py\n",
      "[GenerateCFG] Processing ../dataset/python/170.py\n",
      "[GenerateCFG] Processing ../dataset/python/171.py\n",
      "[GenerateCFG] Processing ../dataset/python/172.py\n",
      "[GenerateCFG] Processing ../dataset/python/173.py\n",
      "[GenerateCFG] Processing ../dataset/python/174.py\n",
      "[GenerateCFG] Processing ../dataset/python/175.py\n",
      "[GenerateCFG] Processing ../dataset/python/176.py\n",
      "[GenerateCFG] Processing ../dataset/python/177.py\n",
      "[GenerateCFG] Processing ../dataset/python/178.py\n",
      "[GenerateCFG] Processing ../dataset/python/179.py\n",
      "[GenerateCFG] Processing ../dataset/python/180.py\n",
      "[GenerateCFG] Processing ../dataset/python/181.py\n",
      "[GenerateCFG] Processing ../dataset/python/182.py\n",
      "[GenerateCFG] Processing ../dataset/python/183.py\n",
      "[GenerateCFG] Processing ../dataset/python/184.py\n",
      "[GenerateCFG] Processing ../dataset/python/185.py\n",
      "[GenerateCFG] Processing ../dataset/python/186.py\n",
      "[GenerateCFG] Processing ../dataset/python/187.py\n",
      "[GenerateCFG] Processing ../dataset/python/188.py\n",
      "[GenerateCFG] Processing ../dataset/python/189.py\n",
      "[GenerateCFG] Processing ../dataset/python/190.py\n",
      "[GenerateCFG] Processing ../dataset/python/191.py\n",
      "[GenerateCFG] Processing ../dataset/python/192.py\n",
      "[GenerateCFG] Processing ../dataset/python/193.py\n",
      "[GenerateCFG] Processing ../dataset/python/194.py\n",
      "[GenerateCFG] Processing ../dataset/python/195.py\n",
      "[GenerateCFG] Processing ../dataset/python/196.py\n",
      "[GenerateCFG] Processing ../dataset/python/197.py\n",
      "[GenerateCFG] Processing ../dataset/python/198.py\n",
      "[GenerateCFG] Processing ../dataset/python/199.py\n",
      "[GenerateCFG] Processing ../dataset/python/200.py\n",
      "[PostProcessCFG] Processing 95.json\n",
      "Error processing 95.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 38.json\n",
      "[PostProcessCFG] Processing 72.json\n",
      "Error processing 72.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 149.json\n",
      "Error processing 149.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 4.json\n",
      "Error processing 4.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 74.json\n",
      "Error processing 74.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 7.json\n",
      "Error processing 7.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 167.json\n",
      "Error processing 167.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 107.json\n",
      "Error processing 107.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 176.json\n",
      "Error processing 176.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 136.json\n",
      "Error processing 136.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 141.json\n",
      "[PostProcessCFG] Processing 133.json\n",
      "Error processing 133.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 123.json\n",
      "[PostProcessCFG] Processing 13.json\n",
      "Error processing 13.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 169.json\n",
      "Error processing 169.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 2.json\n",
      "Error processing 2.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 37.json\n",
      "Error processing 37.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 138.json\n",
      "Error processing 138.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 122.json\n",
      "[PostProcessCFG] Processing 24.json\n",
      "Error processing 24.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 10.json\n",
      "Error processing 10.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 104.json\n",
      "[PostProcessCFG] Processing 130.json\n",
      "Error processing 130.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 101.json\n",
      "Error processing 101.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 184.json\n",
      "Error processing 184.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 120.json\n",
      "Error processing 120.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 140.json\n",
      "Error processing 140.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 131.json\n",
      "[PostProcessCFG] Processing 16.json\n",
      "Error processing 16.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 69.json\n",
      "Error processing 69.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 144.json\n",
      "Error processing 144.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 143.json\n",
      "Error processing 143.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 44.json\n",
      "Error processing 44.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 180.json\n",
      "Error processing 180.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 186.json\n",
      "[PostProcessCFG] Processing 22.json\n",
      "Error processing 22.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 84.json\n",
      "Error processing 84.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 27.json\n",
      "Error processing 27.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 32.json\n",
      "[PostProcessCFG] Processing 124.json\n",
      "Error processing 124.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 1.json\n",
      "[PostProcessCFG] Processing 134.json\n",
      "Error processing 134.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 19.json\n",
      "Error processing 19.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 46.json\n",
      "Error processing 46.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 6.json\n",
      "Error processing 6.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 60.json\n",
      "Error processing 60.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 100.json\n",
      "Error processing 100.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 61.json\n",
      "Error processing 61.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 12.json\n",
      "[PostProcessCFG] Processing 125.json\n",
      "Error processing 125.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 146.json\n",
      "[PostProcessCFG] Processing 172.json\n",
      "Error processing 172.json: 'int' object is not subscriptable\n",
      "[PostProcessCFG] Processing 108.json\n",
      "Error processing 108.json: 'int' object is not subscriptable\n",
      "[CompareCFG] Missing files for index 0: ['merged_llm_cfg/0.json']\n",
      "[CompareCFG] Missing files for index 3: ['merged_llm_cfg/3.json']\n",
      "[CompareCFG] Missing files for index 2: ['merged_llm_cfg/2.json']\n",
      "[CompareCFG] Missing files for index 6: ['merged_llm_cfg/6.json']\n",
      "[CompareCFG] Missing files for index 9: ['merged_llm_cfg/9.json']\n",
      "[CompareCFG] Missing files for index 11: ['merged_llm_cfg/11.json']\n",
      "[CompareCFG] Missing files for index 4: ['merged_llm_cfg/4.json']\n",
      "[CompareCFG] Missing files for index 10: ['merged_llm_cfg/10.json']\n",
      "[CompareCFG] Missing files for index 5: ['merged_llm_cfg/5.json']\n",
      "[CompareCFG] Missing files for index 8: ['merged_llm_cfg/8.json']\n",
      "[CompareCFG] Missing files for index 13: ['merged_llm_cfg/13.json']\n",
      "[CompareCFG] Missing files for index 14: ['merged_llm_cfg/14.json']\n",
      "[CompareCFG] Missing files for index 15: ['merged_llm_cfg/15.json']\n",
      "[CompareCFG] Missing files for index 16: ['merged_llm_cfg/16.json']\n",
      "[CompareCFG] Missing files for index 7: ['merged_llm_cfg/7.json']\n",
      "[CompareCFG] Missing files for index 19: ['merged_llm_cfg/19.json']\n",
      "[CompareCFG] Missing files for index 22: ['merged_llm_cfg/22.json']\n",
      "[CompareCFG] Missing files for index 23: ['merged_llm_cfg/23.json']\n",
      "[CompareCFG] Missing files for index 17: ['merged_llm_cfg/17.json']\n",
      "[CompareCFG] Missing files for index 18: ['merged_llm_cfg/18.json']\n",
      "[CompareCFG] Missing files for index 20: ['merged_llm_cfg/20.json']\n",
      "[CompareCFG] Missing files for index 21: ['merged_llm_cfg/21.json']\n",
      "[CompareCFG] Missing files for index 25: ['merged_llm_cfg/25.json']\n",
      "[CompareCFG] Missing files for index 24: ['merged_llm_cfg/24.json']\n",
      "[CompareCFG] Missing files for index 29: ['merged_llm_cfg/29.json']\n",
      "[CompareCFG] Missing files for index 30: ['merged_llm_cfg/30.json']\n",
      "[CompareCFG] Missing files for index 31: ['merged_llm_cfg/31.json']\n",
      "[CompareCFG] Missing files for index 33: ['merged_llm_cfg/33.json']\n",
      "[CompareCFG] Missing files for index 34: ['merged_llm_cfg/34.json']\n",
      "[CompareCFG] Missing files for index 35: ['merged_llm_cfg/35.json']\n",
      "[CompareCFG] Missing files for index 36: ['merged_llm_cfg/36.json']\n",
      "[CompareCFG] Missing files for index 26: ['merged_llm_cfg/26.json']\n",
      "[CompareCFG] Missing files for index 28: ['merged_llm_cfg/28.json']\n",
      "[CompareCFG] Missing files for index 41: ['merged_llm_cfg/41.json']\n",
      "[CompareCFG] Missing files for index 42: ['merged_llm_cfg/42.json']\n",
      "[CompareCFG] Missing files for index 43: ['merged_llm_cfg/43.json']\n",
      "[CompareCFG] Missing files for index 45: ['merged_llm_cfg/45.json']\n",
      "[CompareCFG] Missing files for index 46: ['merged_llm_cfg/46.json']\n",
      "[CompareCFG] Missing files for index 47: ['merged_llm_cfg/47.json']\n",
      "[CompareCFG] Missing files for index 48: ['merged_llm_cfg/48.json']\n",
      "[CompareCFG] Missing files for index 49: ['merged_llm_cfg/49.json']\n",
      "[CompareCFG] Missing files for index 50: ['merged_llm_cfg/50.json']\n",
      "[CompareCFG] Missing files for index 40: ['merged_llm_cfg/40.json']\n",
      "[CompareCFG] Missing files for index 37: ['merged_llm_cfg/37.json']\n",
      "[CompareCFG] Missing files for index 39: ['merged_llm_cfg/39.json']\n",
      "[CompareCFG] Missing files for index 53: ['merged_llm_cfg/53.json']\n",
      "[CompareCFG] Missing files for index 44: ['merged_llm_cfg/44.json']\n",
      "[CompareCFG] Missing files for index 55: ['merged_llm_cfg/55.json']\n",
      "[CompareCFG] Missing files for index 27: ['merged_llm_cfg/27.json']\n",
      "[CompareCFG] Missing files for index 57: ['merged_llm_cfg/57.json']\n",
      "[CompareCFG] Missing files for index 58: ['merged_llm_cfg/58.json']\n",
      "[CompareCFG] Missing files for index 54: ['merged_llm_cfg/54.json']\n",
      "[CompareCFG] Missing files for index 60: ['merged_llm_cfg/60.json']\n",
      "[CompareCFG] Missing files for index 62: ['merged_llm_cfg/62.json']\n",
      "[CompareCFG] Missing files for index 56: ['merged_llm_cfg/56.json']\n",
      "[CompareCFG] Missing files for index 65: ['merged_llm_cfg/65.json']\n",
      "[CompareCFG] Missing files for index 61: ['merged_llm_cfg/61.json']\n",
      "[CompareCFG] Missing files for index 68: ['merged_llm_cfg/68.json']\n",
      "[CompareCFG] Missing files for index 69: ['merged_llm_cfg/69.json']\n",
      "[CompareCFG] Missing files for index 70: ['merged_llm_cfg/70.json']\n",
      "[CompareCFG] Missing files for index 64: ['merged_llm_cfg/64.json']\n",
      "[CompareCFG] Missing files for index 73: ['merged_llm_cfg/73.json']\n",
      "[CompareCFG] Missing files for index 74: ['merged_llm_cfg/74.json']\n",
      "[CompareCFG] Missing files for index 67: ['merged_llm_cfg/67.json']\n",
      "[CompareCFG] Missing files for index 76: ['merged_llm_cfg/76.json']\n",
      "[CompareCFG] Missing files for index 59: ['merged_llm_cfg/59.json']\n",
      "[CompareCFG] Missing files for index 79: ['merged_llm_cfg/79.json']\n",
      "[CompareCFG] Missing files for index 66: ['merged_llm_cfg/66.json']\n",
      "[CompareCFG] Missing files for index 81: ['merged_llm_cfg/81.json']\n",
      "[CompareCFG] Missing files for index 82: ['merged_llm_cfg/82.json']\n",
      "[CompareCFG] Missing files for index 84: ['merged_llm_cfg/84.json']\n",
      "[CompareCFG] Missing files for index 85: ['merged_llm_cfg/85.json']\n",
      "[CompareCFG] Missing files for index 86: ['merged_llm_cfg/86.json']\n",
      "[CompareCFG] Missing files for index 51: ['merged_llm_cfg/51.json']\n",
      "[CompareCFG] Missing files for index 88: ['merged_llm_cfg/88.json']\n",
      "[CompareCFG] Missing files for index 89: ['merged_llm_cfg/89.json']\n",
      "[CompareCFG] Missing files for index 90: ['merged_llm_cfg/90.json']\n",
      "[CompareCFG] Missing files for index 91: ['merged_llm_cfg/91.json']\n",
      "[CompareCFG] Missing files for index 92: ['merged_llm_cfg/92.json']\n",
      "[CompareCFG] Missing files for index 72: ['merged_llm_cfg/72.json']\n",
      "[CompareCFG] Missing files for index 94: ['merged_llm_cfg/94.json']\n",
      "[CompareCFG] Missing files for index 71: ['merged_llm_cfg/71.json']\n",
      "[CompareCFG] Missing files for index 75: ['merged_llm_cfg/75.json']\n",
      "[CompareCFG] Missing files for index 83: ['merged_llm_cfg/83.json']\n",
      "[CompareCFG] Missing files for index 95: ['merged_llm_cfg/95.json']\n",
      "[CompareCFG] Missing files for index 80: ['merged_llm_cfg/80.json']\n",
      "[CompareCFG] Missing files for index 77: ['merged_llm_cfg/77.json']\n",
      "[CompareCFG] Missing files for index 63: ['merged_llm_cfg/63.json']\n",
      "[CompareCFG] Missing files for index 87: ['merged_llm_cfg/87.json']\n",
      "[CompareCFG] Missing files for index 97: ['../dataset/python/97.py', 'merged_llm_cfg/97.json', '../dataset/python_cfg/97.json']\n",
      "[CompareCFG] Missing files for index 96: ['merged_llm_cfg/96.json']\n",
      "[CompareCFG] Missing files for index 78: ['merged_llm_cfg/78.json']\n",
      "[CompareCFG] Missing files for index 100: ['merged_llm_cfg/100.json']\n",
      "[CompareCFG] Missing files for index 105: ['merged_llm_cfg/105.json']\n",
      "[CompareCFG] Missing files for index 106: ['merged_llm_cfg/106.json']\n",
      "[CompareCFG] Missing files for index 52: ['merged_llm_cfg/52.json']\n",
      "[CompareCFG] Missing files for index 98: ['merged_llm_cfg/98.json']\n",
      "[CompareCFG] Missing files for index 110: ['merged_llm_cfg/110.json']\n",
      "[CompareCFG] Missing files for index 111: ['merged_llm_cfg/111.json']\n",
      "[CompareCFG] Missing files for index 102: ['merged_llm_cfg/102.json']\n",
      "[CompareCFG] Missing files for index 108: ['merged_llm_cfg/108.json']\n",
      "[CompareCFG] Missing files for index 99: ['merged_llm_cfg/99.json']\n",
      "[CompareCFG] Missing files for index 115: ['merged_llm_cfg/115.json']\n",
      "[CompareCFG] Missing files for index 114: ['merged_llm_cfg/114.json']\n",
      "[CompareCFG] Missing files for index 118: ['merged_llm_cfg/118.json']\n",
      "[CompareCFG] Missing files for index 103: ['merged_llm_cfg/103.json']\n",
      "[CompareCFG] Missing files for index 120: ['merged_llm_cfg/120.json']\n",
      "[CompareCFG] Missing files for index 121: ['merged_llm_cfg/121.json']\n",
      "[CompareCFG] Missing files for index 93: ['merged_llm_cfg/93.json']\n",
      "[CompareCFG] Missing files for index 124: ['merged_llm_cfg/124.json']\n",
      "[CompareCFG] Missing files for index 109: ['merged_llm_cfg/109.json']\n",
      "[CompareCFG] Missing files for index 127: ['merged_llm_cfg/127.json']\n",
      "[CompareCFG] Missing files for index 116: ['merged_llm_cfg/116.json']\n",
      "[CompareCFG] Missing files for index 129: ['merged_llm_cfg/129.json']\n",
      "[CompareCFG] Missing files for index 125: ['merged_llm_cfg/125.json']\n",
      "[CompareCFG] Missing files for index 117: ['merged_llm_cfg/117.json']\n",
      "[CompareCFG] Missing files for index 132: ['merged_llm_cfg/132.json']\n",
      "[CompareCFG] Missing files for index 133: ['merged_llm_cfg/133.json']\n",
      "[CompareCFG] Missing files for index 135: ['merged_llm_cfg/135.json']\n",
      "[CompareCFG] Missing files for index 136: ['merged_llm_cfg/136.json']\n",
      "[CompareCFG] Missing files for index 140: ['merged_llm_cfg/140.json']\n",
      "[CompareCFG] Missing files for index 128: ['merged_llm_cfg/128.json']\n",
      "[CompareCFG] Missing files for index 134: ['merged_llm_cfg/134.json']\n",
      "[CompareCFG] Missing files for index 138: ['merged_llm_cfg/138.json']\n",
      "[CompareCFG] Missing files for index 144: ['merged_llm_cfg/144.json']\n",
      "[CompareCFG] Missing files for index 107: ['merged_llm_cfg/107.json']\n",
      "[CompareCFG] Missing files for index 148: ['merged_llm_cfg/148.json']\n",
      "[CompareCFG] Missing files for index 149: ['merged_llm_cfg/149.json']\n",
      "[CompareCFG] Missing files for index 126: ['merged_llm_cfg/126.json']\n",
      "[CompareCFG] Missing files for index 142: ['merged_llm_cfg/142.json']\n",
      "[CompareCFG] Missing files for index 152: ['merged_llm_cfg/152.json']\n",
      "[CompareCFG] Missing files for index 143: ['merged_llm_cfg/143.json']\n",
      "[CompareCFG] Missing files for index 154: ['merged_llm_cfg/154.json']\n",
      "[CompareCFG] Missing files for index 155: ['merged_llm_cfg/155.json']\n",
      "[CompareCFG] Missing files for index 151: ['merged_llm_cfg/151.json']\n",
      "[CompareCFG] Missing files for index 113: ['merged_llm_cfg/113.json']\n",
      "[CompareCFG] Missing files for index 153: ['merged_llm_cfg/153.json']\n",
      "[CompareCFG] Missing files for index 159: ['merged_llm_cfg/159.json']\n",
      "[CompareCFG] Missing files for index 160: ['merged_llm_cfg/160.json']\n",
      "[CompareCFG] Missing files for index 150: ['merged_llm_cfg/150.json']\n",
      "[CompareCFG] Missing files for index 137: ['merged_llm_cfg/137.json']\n",
      "[CompareCFG] Missing files for index 130: ['merged_llm_cfg/130.json']\n",
      "[CompareCFG] Missing files for index 165: ['merged_llm_cfg/165.json']\n",
      "[CompareCFG] Missing files for index 166: ['merged_llm_cfg/166.json']\n",
      "[CompareCFG] Missing files for index 101: ['merged_llm_cfg/101.json']\n",
      "[CompareCFG] Missing files for index 163: ['merged_llm_cfg/163.json']\n",
      "[CompareCFG] Missing files for index 169: ['merged_llm_cfg/169.json']\n",
      "[CompareCFG] Missing files for index 119: ['merged_llm_cfg/119.json']\n",
      "[CompareCFG] Missing files for index 171: ['merged_llm_cfg/171.json']\n",
      "[CompareCFG] Missing files for index 164: ['merged_llm_cfg/164.json']\n",
      "[CompareCFG] Missing files for index 173: ['merged_llm_cfg/173.json']\n",
      "[CompareCFG] Missing files for index 158: ['merged_llm_cfg/158.json']\n",
      "[CompareCFG] Missing files for index 175: ['merged_llm_cfg/175.json']\n",
      "[CompareCFG] Missing files for index 112: ['merged_llm_cfg/112.json']\n",
      "[CompareCFG] Missing files for index 170: ['merged_llm_cfg/170.json']\n",
      "[CompareCFG] Missing files for index 157: ['merged_llm_cfg/157.json']\n",
      "[CompareCFG] Missing files for index 179: ['merged_llm_cfg/179.json']\n",
      "[CompareCFG] Missing files for index 180: ['merged_llm_cfg/180.json']\n",
      "[CompareCFG] Missing files for index 181: ['merged_llm_cfg/181.json']\n",
      "[CompareCFG] Missing files for index 182: ['merged_llm_cfg/182.json']\n",
      "[CompareCFG] Missing files for index 162: ['merged_llm_cfg/162.json']\n",
      "[CompareCFG] Missing files for index 177: ['merged_llm_cfg/177.json']\n",
      "[CompareCFG] Missing files for index 147: ['merged_llm_cfg/147.json']\n",
      "[CompareCFG] Missing files for index 161: ['merged_llm_cfg/161.json']\n",
      "[CompareCFG] Missing files for index 178: ['merged_llm_cfg/178.json']\n",
      "[CompareCFG] Missing files for index 156: ['merged_llm_cfg/156.json']\n",
      "[CompareCFG] Missing files for index 183: ['merged_llm_cfg/183.json']\n",
      "[CompareCFG] Missing files for index 172: ['merged_llm_cfg/172.json']\n",
      "[CompareCFG] Missing files for index 168: ['merged_llm_cfg/168.json']\n",
      "[CompareCFG] Missing files for index 167: ['merged_llm_cfg/167.json']\n",
      "[CompareCFG] Missing files for index 184: ['merged_llm_cfg/184.json']\n",
      "[CompareCFG] Missing files for index 139: ['merged_llm_cfg/139.json']\n",
      "[CompareCFG] Missing files for index 174: ['merged_llm_cfg/174.json']\n",
      "[CompareCFG] Missing files for index 145: ['merged_llm_cfg/145.json']\n",
      "[CompareCFG] Missing files for index 176: ['merged_llm_cfg/176.json']\n",
      "[CompareCFG] Missing files for index 187: ['merged_llm_cfg/187.json']\n",
      "[CompareCFG] Missing files for index 189: ['merged_llm_cfg/189.json']\n",
      "[CompareCFG] Missing files for index 190: ['merged_llm_cfg/190.json']\n",
      "[CompareCFG] Missing files for index 193: ['merged_llm_cfg/193.json']\n",
      "[CompareCFG] Missing files for index 194: ['merged_llm_cfg/194.json']\n",
      "[CompareCFG] Missing files for index 188: ['merged_llm_cfg/188.json']\n",
      "[CompareCFG] Missing files for index 196: ['merged_llm_cfg/196.json']\n",
      "[CompareCFG] Missing files for index 191: ['merged_llm_cfg/191.json']\n",
      "[CompareCFG] Missing files for index 198: ['merged_llm_cfg/198.json']\n",
      "[CompareCFG] Missing files for index 199: ['merged_llm_cfg/199.json']\n",
      "[CompareCFG] Missing files for index 200: ['merged_llm_cfg/200.json']\n",
      "[CompareCFG] Missing files for index 195: ['merged_llm_cfg/195.json']\n",
      "[CompareCFG] Missing files for index 185: ['merged_llm_cfg/185.json']\n",
      "[CompareCFG] Missing files for index 192: ['merged_llm_cfg/192.json']\n",
      "[CompareCFG] Missing files for index 197: ['merged_llm_cfg/197.json']\n",
      "==========================CFG 评估报告==========================\n",
      "分析文件总数: 0\n",
      "\n",
      "[边匹配分析]\n",
      "  静态分析总边数: 0\n",
      "  LLM生成总边数: 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 661\u001b[0m\n\u001b[1;32m    657\u001b[0m     print_report(final_metrics)\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 661\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 657\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# 4) 汇总统计\u001b[39;00m\n\u001b[1;32m    656\u001b[0m final_metrics \u001b[38;5;241m=\u001b[39m calculate_global_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 657\u001b[0m \u001b[43mprint_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_metrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 600\u001b[0m, in \u001b[0;36mprint_report\u001b[0;34m(metrics)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  静态分析总边数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatic_total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  LLM生成总边数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 600\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  精确匹配边数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexact_matches\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (占静态 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexact_matches\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstatic_total\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m )\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  部分匹配边数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartial_matches\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (占静态 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartial_matches\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39mem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatic_total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m )\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  总匹配边数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_matched\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (占静态 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_matched\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39mem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatic_total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m )\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from functools import partial\n",
    "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# =========================================================\n",
    "#  假设在 llm.py 中已有的函数，如下为占位示例\n",
    "#  实际请用你自己的实现替换\n",
    "# =========================================================\n",
    "from llm import get_llm_answers\n",
    "\n",
    "# =========================================================\n",
    "#  第一步：从源文件生成 CFG（带行号），并保存到 llm_whole_cfg/\n",
    "# =========================================================\n",
    "def get_whole_code_cfg_prompt(code_text: str, program_language: str) -> str:\n",
    "    \"\"\"\n",
    "    生成整体CFG的Prompt，直接处理整个代码\n",
    "    \"\"\"\n",
    "    code_lines = code_text.splitlines()\n",
    "    line_array = [{\"lineno\": i+1, \"line\": ln} for i, ln in enumerate(code_lines)]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You will be given a {program_language} code file. Your task is to generate a complete Control Flow Graph (CFG) for the ENTIRE code.\n",
    "\n",
    "Output Requirements:\n",
    "1. Identify ALL basic blocks in the code\n",
    "2. Capture control flow between ALL blocks\n",
    "3. Handle nested structures (if/for/while/try/etc) properly\n",
    "4. Use EXACT line numbers from the original code\n",
    "5. Output format must be:\n",
    "\n",
    "{{\n",
    "    \"blocks\": [\n",
    "        {{\n",
    "            \"id\": 1,\n",
    "            \"start_line\": 1,\n",
    "            \"end_line\": 1,\n",
    "            \"label\": \"code snippet\",\n",
    "            \"successors\": [2, 3]\n",
    "        }},\n",
    "        // ... more blocks\n",
    "    ]\n",
    "}}\n",
    "\n",
    "The code is:\n",
    "{json.dumps(line_array, indent=2)}\n",
    "\n",
    "IMPORTANT:\n",
    "- Use original line numbers\n",
    "- Include ALL code lines in blocks\n",
    "- Maintain execution order\n",
    "- Handle function/method calls as linear flow\n",
    "- For loops, show back edges\n",
    "- For conditionals, show both branches\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "def process_whole_code_cfg(code: str, program_language: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    调用LLM，基于给定源代码生成CFG（JSON格式）\n",
    "    \"\"\"\n",
    "    prompt = get_whole_code_cfg_prompt(code, program_language)\n",
    "    response = get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True)\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        # 如果LLM返回并非严格JSON，做一次简单修复或返回空结构\n",
    "        return {\n",
    "            \"blocks\": []\n",
    "        }\n",
    "\n",
    "def generate_llm_cfgs(start_idx: int, end_idx: int):\n",
    "    \"\"\"\n",
    "    读取 [start_idx, end_idx] 范围内的源文件，调用LLM生成CFG。\n",
    "    结果保存到 llm_whole_cfg/ 目录下。\n",
    "    \"\"\"\n",
    "    source_code_dir = \"../dataset/python\"\n",
    "    target_dir = \"llm_whole_cfg\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # 构造要处理的文件列表\n",
    "    files = []\n",
    "    for i in range(start_idx, end_idx+1):\n",
    "        py_file = os.path.join(source_code_dir, f\"{i}.py\")\n",
    "        out_file = os.path.join(target_dir, f\"{i}.json\")\n",
    "        files.append((py_file, out_file))\n",
    "\n",
    "    def process_single_file(source_file, target_file):\n",
    "        if os.path.exists(target_file):\n",
    "            return  # 已经生成过CFG，跳过\n",
    "\n",
    "        if not os.path.exists(source_file):\n",
    "            return  # 源文件不存在，跳过\n",
    "\n",
    "        print(f\"[GenerateCFG] Processing {source_file}\")\n",
    "        with open(source_file, 'r', encoding='utf-8') as f:\n",
    "            code = f.read()\n",
    "\n",
    "        # 调用LLM生成CFG\n",
    "        cfg_result = process_whole_code_cfg(code, \"python\")\n",
    "\n",
    "        # 包装后保存\n",
    "        result = {\n",
    "            \"source_file\": os.path.basename(source_file),\n",
    "            \"cfg\": cfg_result,\n",
    "            \"code_length\": len(code.splitlines())\n",
    "        }\n",
    "\n",
    "        with open(target_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "            json.dump(result, fout, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # 并行处理\n",
    "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_single_file, src, tgt) for src, tgt in files]\n",
    "        for _ in as_completed(futures):\n",
    "            pass\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "#  第二步：对生成的 CFG 做后处理（去除不可达节点、合并线性块等）\n",
    "#  该过程将结果保存到 merged_llm_cfg/ 目录\n",
    "# =========================================================\n",
    "\n",
    "def process_cfg(cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    对CFG进行后处理：\n",
    "      1. 移除不可达节点（假设 blocks[0] 为 root）\n",
    "      2. 合并线性块\n",
    "      3. 递归处理嵌套的functions/classes\n",
    "    \"\"\"\n",
    "    def filter_connected_blocks(blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        过滤不可达块\n",
    "        \"\"\"\n",
    "        visited_ids = set()\n",
    "        id_to_block = {}\n",
    "\n",
    "        def collect_all_blocks(block_list):\n",
    "            for b in block_list:\n",
    "                id_to_block[b[\"id\"]] = b\n",
    "                if \"successors\" in b:\n",
    "                    collect_all_blocks(b[\"successors\"])\n",
    "\n",
    "        collect_all_blocks(blocks)\n",
    "\n",
    "        def dfs(block):\n",
    "            if block[\"id\"] in visited_ids:\n",
    "                return\n",
    "            visited_ids.add(block[\"id\"])\n",
    "            for succ_block in block.get(\"successors\", []):\n",
    "                dfs(succ_block)\n",
    "\n",
    "        # 假设 blocks[0] 是root\n",
    "        if blocks:\n",
    "            root_block = blocks[0]\n",
    "            dfs(root_block)\n",
    "\n",
    "        def filter_nested(block_list):\n",
    "            filtered = []\n",
    "            for b in block_list:\n",
    "                if b[\"id\"] in visited_ids:\n",
    "                    new_successors = filter_nested(b.get(\"successors\", []))\n",
    "                    filtered.append({\n",
    "                        \"id\": b[\"id\"],\n",
    "                        \"label\": b[\"label\"],\n",
    "                        \"successors\": new_successors\n",
    "                    })\n",
    "            return filtered\n",
    "\n",
    "        return filter_nested(blocks)\n",
    "\n",
    "    def is_loop_header(block: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        简单判断块是否为循环头（以for或while开头）\n",
    "        \"\"\"\n",
    "        code_str = block[\"label\"].strip() if \"label\" in block else \"\"\n",
    "        return code_str.startswith(\"for \") or code_str.startswith(\"while \")\n",
    "\n",
    "    def merge_blocks_in_place(block: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        递归合并线性块\n",
    "        \"\"\"\n",
    "        successors = block.get(\"successors\", [])\n",
    "        if not successors:\n",
    "            return block\n",
    "\n",
    "        # 多个后继，说明是分支点，不合并\n",
    "        if len(successors) > 1:\n",
    "            for i, succ in enumerate(successors):\n",
    "                successors[i] = merge_blocks_in_place(succ)\n",
    "            block[\"successors\"] = successors\n",
    "            return block\n",
    "\n",
    "        # 只有1个后继\n",
    "        single_succ = successors[0]\n",
    "\n",
    "        # 遇到循环头，不合并\n",
    "        if is_loop_header(block) or is_loop_header(single_succ):\n",
    "            block[\"successors\"][0] = merge_blocks_in_place(single_succ)\n",
    "            return block\n",
    "\n",
    "        # 可以合并\n",
    "        block[\"label\"] = block[\"label\"] + \"\\n\" + single_succ[\"label\"]\n",
    "        block[\"successors\"] = single_succ.get(\"successors\", [])\n",
    "\n",
    "        # 递归继续尝试合并\n",
    "        if block[\"successors\"]:\n",
    "            new_succ_list = []\n",
    "            for succ in block[\"successors\"]:\n",
    "                new_succ_list.append(merge_blocks_in_place(succ))\n",
    "            block[\"successors\"] = new_succ_list\n",
    "\n",
    "        return block\n",
    "\n",
    "    # 开始处理\n",
    "    if \"blocks\" in cfg:\n",
    "        # 1. 移除不可达块\n",
    "        cfg[\"blocks\"] = filter_connected_blocks(cfg[\"blocks\"])\n",
    "\n",
    "        # 2. 合并线性块\n",
    "        if cfg[\"blocks\"]:\n",
    "            merged = []\n",
    "            for b in cfg[\"blocks\"]:\n",
    "                merged_block = merge_blocks_in_place(b)\n",
    "                merged.append(merged_block)\n",
    "            cfg[\"blocks\"] = merged\n",
    "\n",
    "    # 递归处理 functions/classes\n",
    "    if \"functions\" in cfg:\n",
    "        for func in cfg[\"functions\"]:\n",
    "            process_cfg(func)\n",
    "\n",
    "    if \"classes\" in cfg:\n",
    "        for cls in cfg[\"classes\"]:\n",
    "            process_cfg(cls)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def postprocess_all_llm_cfgs():\n",
    "    \"\"\"\n",
    "    批量读取 llm_whole_cfg/ 下的文件，进行后处理并保存到 merged_llm_cfg/\n",
    "    \"\"\"\n",
    "    input_dir = \"llm_whole_cfg\"\n",
    "    output_dir = \"merged_llm_cfg\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # 如果已经存在后处理结果，可选择跳过\n",
    "        # if os.path.exists(output_path):\n",
    "        #     continue\n",
    "\n",
    "        print(f\"[PostProcessCFG] Processing {filename}\")\n",
    "        try:\n",
    "            with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                llm_result = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # 注意：llm_result 的结构：{\"source_file\":..., \"cfg\":..., \"code_length\":...}\n",
    "        # 我们只后处理其中的 cfg\n",
    "        cfg_data = llm_result.get(\"cfg\", {})\n",
    "        try:\n",
    "            cfg_data = process_cfg(cfg_data)\n",
    "            # 更新回 llm_result\n",
    "            llm_result[\"cfg\"] = cfg_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(llm_result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 第三步：将后处理过的 CFG 与静态分析结果做比较\n",
    "#         并将对比结果输出到 results/ 目录\n",
    "# =========================================================\n",
    "\n",
    "def convert_cfg_json_to_edges(cfg_json: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    将 CFG 转换为 edges 列表，以便更方便地做文本层面的对比。\n",
    "    返回形如：\n",
    "      [\n",
    "        \"Edge 0: [Source] labelA => [Target] labelB\",\n",
    "        ...\n",
    "      ]\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    edge_counter = 0\n",
    "\n",
    "    def get_block_label(block: Dict[str, Any]) -> str:\n",
    "        label = block.get(\"label\", \"\")\n",
    "        if not isinstance(label, str):\n",
    "            label = str(label)\n",
    "        # 去除多余空白\n",
    "        label = ' '.join(label.replace('\\n', '\\\\n').strip().split())\n",
    "        return label\n",
    "\n",
    "    def process_entity(entity: Dict[str, Any]):\n",
    "        # 处理自身 blocks\n",
    "        for block in entity.get(\"blocks\", []):\n",
    "            process_block(block)\n",
    "        # 递归处理 functions/classes\n",
    "        for key in [\"functions\", \"classes\"]:\n",
    "            for sub_ent in entity.get(key, []):\n",
    "                process_entity(sub_ent)\n",
    "\n",
    "    def process_block(block: Dict[str, Any]):\n",
    "        nonlocal edge_counter\n",
    "        source_label = get_block_label(block)\n",
    "        if not source_label:\n",
    "            return\n",
    "\n",
    "        # 处理后继\n",
    "        for succ in block.get(\"successors\", []):\n",
    "            target_label = get_block_label(succ)\n",
    "            if target_label:\n",
    "                edges.append(f\"Edge {edge_counter}: [Source] {source_label} => [Target] {target_label}\")\n",
    "                edge_counter += 1\n",
    "\n",
    "        # 递归处理内嵌的 blocks 与 successors\n",
    "        for sub_key in [\"blocks\", \"successors\"]:\n",
    "            for sub_block in block.get(sub_key, []):\n",
    "                process_block(sub_block)\n",
    "\n",
    "    process_entity(cfg_json)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def get_compare_prompt(\n",
    "    code: str,\n",
    "    llm_cfg: Dict[str, Any],\n",
    "    static_cfg: Dict[str, Any]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    根据给定的 code, LLM CFG, 静态 CFG 生成对比 prompt，\n",
    "    让 LLM 给出 JSON 格式的对比结果。\n",
    "    \"\"\"\n",
    "    llm_edges = convert_cfg_json_to_edges(llm_cfg)\n",
    "    static_edges = convert_cfg_json_to_edges(static_cfg)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Role: Control Flow Graph Validation Specialist\n",
    "Objective: Accurately compare CFG structures between static analysis (ground truth) and LLM generation\n",
    "\n",
    "### JSON Structure Definition\n",
    "Ground Truth (static_cfg) & LLM Output (llm_cfg) follow:\n",
    "[\n",
    "    \"Edge 0: [Source] node_A -> [Target] node_B\",\n",
    "    \"Edge 1: [Source] node_C -> [Target] node_D\",\n",
    "    ...\n",
    "]\n",
    "\n",
    "### Comparison Criteria\n",
    "1. Structure Matching:\n",
    "   Match edges when:\n",
    "   - Same branching pattern (sequential/conditional/loop)\n",
    "   - Equivalent depth in nested structure\n",
    "   - Matching control flow order\n",
    "\n",
    "2. Mismatch Conditions:\n",
    "   - Different number of successors in equivalent blocks\n",
    "   - Inconsistent branch types (e.g., true/false vs multiple)\n",
    "   - Missing/extra exception handling flows\n",
    "\n",
    "### Analysis Task\n",
    "1. For static_cfg:\n",
    "   - Count total edges (ground truth)\n",
    "   - Map control flow patterns\n",
    "\n",
    "2. For llm_cfg:\n",
    "   - Count total generated edges\n",
    "   - Identify structurally matched edges\n",
    "   - Traverse each edge and check if it corresponds to static analysis\n",
    "\n",
    "3. Output (JSON):\n",
    "{{\n",
    "  \"edge_analysis\": {{\n",
    "    \"static_total\": \"Number of edges from static analysis\",\n",
    "    \"llm_total\": \"Number of edges generated by LLM\",\n",
    "    \"matched_edges\": {{\n",
    "      \"exact_matches\": \"Number of exactly matched edges (type + position)\",\n",
    "      \"partial_matches\": \"Number of type-matched edges with different positions\"\n",
    "    }},\n",
    "    \"accuracy_metrics\": {{\n",
    "      \"precision\": \"exact_matches / llm_total\",\n",
    "      \"recall\": \"exact_matches / static_total\",\n",
    "      \"f1_score\": \"2*(precision*recall)/(precision+recall)\"\n",
    "    }}\n",
    "  }},\n",
    "  \"structure_validation\": {{\n",
    "    \"missing_blocks\": [\"Unmatched static block IDs\"],\n",
    "    \"extra_blocks\": [\"Extra LLM block IDs\"]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "### Input Data\n",
    "Python Code:\n",
    "{code}\n",
    "\n",
    "Static Analysis CFG (Ground Truth):\n",
    "{json.dumps(static_edges, indent=2)}\n",
    "\n",
    "LLM Generated CFG:\n",
    "{json.dumps(llm_edges, indent=2)}\n",
    "\n",
    "Output JSON analysis ONLY.\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "def compare_single_cfg(i: int):\n",
    "    \"\"\"\n",
    "    对单个文件的 CFG 做比较，结果保存到 results/{i}.json\n",
    "    \"\"\"\n",
    "    SOURCE_CODE_DIR = \"../dataset/python\"\n",
    "    LLM_CFG_DIR = \"merged_llm_cfg\"\n",
    "    STATIC_CFG_DIR = \"../dataset/python_cfg\"\n",
    "    RESULTS_DIR = \"cfg_results\"\n",
    "\n",
    "    code_path = os.path.join(SOURCE_CODE_DIR, f\"{i}.py\")\n",
    "    llm_cfg_path = os.path.join(LLM_CFG_DIR, f\"{i}.json\")\n",
    "    static_cfg_path = os.path.join(STATIC_CFG_DIR, f\"{i}.json\")\n",
    "    result_path = os.path.join(RESULTS_DIR, f\"{i}.json\")\n",
    "\n",
    "    # 如果结果已经存在可跳过\n",
    "    if os.path.exists(result_path):\n",
    "        return\n",
    "\n",
    "    missing_files = [\n",
    "        p for p in [code_path, llm_cfg_path, static_cfg_path]\n",
    "        if not os.path.exists(p)\n",
    "    ]\n",
    "    if missing_files:\n",
    "        print(f\"[CompareCFG] Missing files for index {i}: {missing_files}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        code = open(code_path, \"r\", encoding=\"utf-8\").read()\n",
    "        with open(llm_cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            llm_data = json.load(f)\n",
    "        with open(static_cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            static_cfg = json.load(f)\n",
    "\n",
    "        # 注意：llm_data 的核心CFG在 llm_data[\"cfg\"] 中\n",
    "        llm_cfg = llm_data.get(\"cfg\", {})\n",
    "\n",
    "        # 构造 prompt\n",
    "        prompt = get_compare_prompt(code, llm_cfg, static_cfg)\n",
    "        # 调用 LLM 得到对比结果\n",
    "        response = get_llm_answers(prompt, model_name=\"gpt-4o\", require_json=True)\n",
    "        # 解析对比结果为 JSON\n",
    "        result_json = json.loads(response)\n",
    "\n",
    "        # 保存\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        with open(result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[CompareCFG] Error processing file {i}: {str(e)}\")\n",
    "\n",
    "\n",
    "def compare_cfgs(start_idx: int, end_idx: int):\n",
    "    \"\"\"\n",
    "    并行对比 [start_idx, end_idx] 范围内的文件CFG\n",
    "    \"\"\"\n",
    "    os.makedirs(\"cfg_results\", exist_ok=True)\n",
    "    file_indices = range(start_idx, end_idx+1)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        futures = [executor.submit(compare_single_cfg, i) for i in file_indices]\n",
    "        # 这里可以用进度条，也可直接等待\n",
    "        for _ in as_completed(futures):\n",
    "            pass\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 第四步：全局指标统计\n",
    "# =========================================================\n",
    "def calculate_global_metrics(results_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    遍历 results_dir 下所有JSON文件，统计全局的精确率、召回率、F1 等\n",
    "    预期每个 results/xxx.json 都包含:\n",
    "    {\n",
    "      \"edge_analysis\": {\n",
    "        \"static_total\": int,\n",
    "        \"llm_total\": int,\n",
    "        \"matched_edges\": {\n",
    "          \"exact_matches\": int,\n",
    "          \"partial_matches\": int\n",
    "        },\n",
    "        \"accuracy_metrics\": {\n",
    "          \"precision\": float,\n",
    "          \"recall\": float,\n",
    "          \"f1_score\": float\n",
    "        }\n",
    "      },\n",
    "      \"structure_validation\": {\n",
    "        \"missing_blocks\": [...],\n",
    "        \"extra_blocks\": [...]\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"total_files\": 0,\n",
    "        \"gt_edges\": 0,\n",
    "        \"llm_edges\": 0,\n",
    "        \"exact_matches\": 0,\n",
    "        \"partial_matches\": 0,\n",
    "        \"missing_blocks\": set(),\n",
    "        \"extra_blocks\": set(),\n",
    "        \"file_errors\": []\n",
    "    }\n",
    "\n",
    "    for filename in os.listdir(results_dir):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(results_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            edge_analysis = data[\"edge_analysis\"]\n",
    "            metrics[\"gt_edges\"] += edge_analysis[\"static_total\"]\n",
    "            metrics[\"llm_edges\"] += edge_analysis[\"llm_total\"]\n",
    "            metrics[\"exact_matches\"] += edge_analysis[\"matched_edges\"][\"exact_matches\"]\n",
    "            metrics[\"partial_matches\"] += edge_analysis[\"matched_edges\"][\"partial_matches\"]\n",
    "\n",
    "            structure_val = data[\"structure_validation\"]\n",
    "            # missing_blocks/extra_blocks 在不同文件可能是数字或字符串\n",
    "            # 这里统一转为字符串集合\n",
    "            missing_b = structure_val[\"missing_blocks\"]\n",
    "            extra_b = structure_val[\"extra_blocks\"]\n",
    "            metrics[\"missing_blocks\"].update(map(str, missing_b))\n",
    "            metrics[\"extra_blocks\"].update(map(str, extra_b))\n",
    "\n",
    "            metrics[\"total_files\"] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            metrics[\"file_errors\"].append(f\"{filename}: {str(e)}\")\n",
    "\n",
    "    total_matched = metrics[\"exact_matches\"] + metrics[\"partial_matches\"]\n",
    "    precision = (total_matched / metrics[\"llm_edges\"]) if metrics[\"llm_edges\"] > 0 else 0\n",
    "    recall = (total_matched / metrics[\"gt_edges\"]) if metrics[\"gt_edges\"] > 0 else 0\n",
    "    f1 = 0\n",
    "    if (precision + recall) > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return {\n",
    "        \"file_count\": metrics[\"total_files\"],\n",
    "        \"edge_metrics\": {\n",
    "            \"static_total\": metrics[\"gt_edges\"],\n",
    "            \"llm_total\": metrics[\"llm_edges\"],\n",
    "            \"exact_matches\": metrics[\"exact_matches\"],\n",
    "            \"partial_matches\": metrics[\"partial_matches\"],\n",
    "            \"total_matched\": total_matched,\n",
    "            \"precision\": round(precision, 4),\n",
    "            \"recall\": round(recall, 4),\n",
    "            \"f1_score\": round(f1, 4)\n",
    "        },\n",
    "        \"structure_metrics\": {\n",
    "            \"missing_blocks_count\": len(metrics[\"missing_blocks\"]),\n",
    "            \"extra_blocks_count\": len(metrics[\"extra_blocks\"]),\n",
    "            \"missing_block_samples\": list(metrics[\"missing_blocks\"])[:5],\n",
    "            \"extra_block_samples\": list(metrics[\"extra_blocks\"])[:5]\n",
    "        },\n",
    "        \"data_quality\": {\n",
    "            \"error_files\": len(metrics[\"file_errors\"]),\n",
    "            \"error_samples\": metrics[\"file_errors\"][:3]  # 只示例输出前3个错误\n",
    "        }\n",
    "    }\n",
    "\n",
    "def print_report(metrics: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    控制台打印统计报告\n",
    "    \"\"\"\n",
    "    print(\"CFG 评估报告\".center(60, \"=\"))\n",
    "    print(f\"分析文件总数: {metrics['file_count']}\")\n",
    "\n",
    "    em = metrics[\"edge_metrics\"]\n",
    "    print(\"\\n[边匹配分析]\")\n",
    "    print(f\"  静态分析总边数: {em['static_total']}\")\n",
    "    print(f\"  LLM生成总边数: {em['llm_total']}\")\n",
    "    print(f\"  精确匹配边数: {em['exact_matches']} (占静态 {em['exact_matches']/em['static_total']:.2%} )\")\n",
    "    print(f\"  部分匹配边数: {em['partial_matches']} (占静态 {em['partial_matches']/em['static_total']:.2%} )\")\n",
    "    print(f\"  总匹配边数: {em['total_matched']} (占静态 {em['total_matched']/em['static_total']:.2%} )\")\n",
    "    print(f\"  Precision: {em['precision']:.4f}\")\n",
    "    print(f\"  Recall: {em['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {em['f1_score']:.4f}\")\n",
    "\n",
    "    sm = metrics[\"structure_metrics\"]\n",
    "    print(\"\\n[结构验证]\")\n",
    "    print(f\"  缺失块总数: {sm['missing_blocks_count']} (示例: {sm['missing_block_samples']})\")\n",
    "    print(f\"  多余块总数: {sm['extra_blocks_count']} (示例: {sm['extra_block_samples']})\")\n",
    "\n",
    "    dq = metrics[\"data_quality\"]\n",
    "    if dq[\"error_files\"] > 0:\n",
    "        print(\"\\n[数据质量问题]\")\n",
    "        print(f\"  错误文件数: {dq['error_files']}\")\n",
    "        print(\"  示例错误:\")\n",
    "        for err in dq[\"error_samples\"]:\n",
    "            print(f\"    - {err}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 主流程示例：可按需拆分 0-50, 51-200, 201-9999 等\n",
    "# =========================================================\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数示例：\n",
    "      1) 解析命令行参数决定处理范围\n",
    "      2) 生成 LLM CFG\n",
    "      3) 后处理\n",
    "      4) 对比静态分析\n",
    "      5) 汇总统计\n",
    "    用法：python script.py start_idx end_idx\n",
    "      如果不指定，则默认 range(0, 50) 做示例。\n",
    "    \"\"\"\n",
    "    if len(sys.argv) == 3:\n",
    "        start_idx = int(sys.argv[1])\n",
    "        end_idx = int(sys.argv[2])\n",
    "    else:\n",
    "        # 默认只示例处理 0~50\n",
    "        start_idx, end_idx = 0, 200\n",
    "\n",
    "    print(f\"==> 全流程处理，文件范围: [{start_idx}, {end_idx}]\")\n",
    "\n",
    "    # 1) 生成 CFG\n",
    "    generate_llm_cfgs(start_idx, end_idx)\n",
    "\n",
    "    # 2) 后处理\n",
    "    postprocess_all_llm_cfgs()\n",
    "\n",
    "    # 3) 对比静态分析\n",
    "    compare_cfgs(start_idx, end_idx)\n",
    "\n",
    "    # 4) 汇总统计\n",
    "    final_metrics = calculate_global_metrics(\"results\")\n",
    "    print_report(final_metrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_llm_cfgs(3, 3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
